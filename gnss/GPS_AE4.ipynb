{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf   \n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = glob.glob('transfer_gps/*/*/*.txt')\n",
    "train_data = []\n",
    "train_label = []\n",
    "for data in datas:\n",
    "    f = np.loadtxt(data,delimiter=\",\").copy()\n",
    "    o = np.argsort(f,axis=(0))[:,3]\n",
    "    f = f[o]\n",
    "    f.resize((10,4))\n",
    "    train_data.append(f/100000000)\n",
    "    if data.split('/')[1] == \"indoor\":\n",
    "        train_label.append(\"indoor\"+data.split('/')[-2])\n",
    "    else:\n",
    "        train_label.append(\"outdoor\"+data.split('/')[-2])\n",
    "        \n",
    "\n",
    "train_data = np.array(train_data)\n",
    "#train_data[:,:,0] -= train_data[:,:,0].mean()\n",
    "#train_data[:,:,0] /= train_data[:,:,0].var()\n",
    "\n",
    "#train_data[:,:,1] -= train_data[:,:,1].mean()\n",
    "#train_data[:,:,1] /= train_data[:,:,1].var()\n",
    "#train_data[:,:,2] -= train_data[:,:,2].mean()\n",
    "#train_data[:,:,2] /= train_data[:,:,2].var()\n",
    "#train_data[:,:,3] -= train_data[:,:,3].mean()\n",
    "train_data[:,:,3] /= 1000000\n",
    "\n",
    "train_label = pd.get_dummies(train_label).values.argmax(1)\n",
    "train_label = train_label.reshape(len(train_label),1)\n",
    "\n",
    "train_val_split = np.random.rand(len(train_data)) < 0.70\n",
    "train_x = train_data[train_val_split]\n",
    "train_y = train_label[train_val_split]\n",
    "val_x = train_data[~train_val_split]\n",
    "val_y = train_label[~train_val_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indoor01</th>\n",
       "      <th>indoor02</th>\n",
       "      <th>indoor03</th>\n",
       "      <th>indoor04</th>\n",
       "      <th>indoor05</th>\n",
       "      <th>indoor06</th>\n",
       "      <th>indoor07</th>\n",
       "      <th>indoor08</th>\n",
       "      <th>indoor09</th>\n",
       "      <th>outdoor01</th>\n",
       "      <th>outdoor02</th>\n",
       "      <th>outdoor03</th>\n",
       "      <th>outdoor04</th>\n",
       "      <th>outdoor05</th>\n",
       "      <th>outdoor06</th>\n",
       "      <th>outdoor07</th>\n",
       "      <th>outdoor08</th>\n",
       "      <th>outdoor09</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4522</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4523 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      indoor01  indoor02  indoor03  indoor04  indoor05  indoor06  indoor07  \\\n",
       "0            0         0         0         0         1         0         0   \n",
       "1            0         0         0         0         1         0         0   \n",
       "2            0         0         0         0         1         0         0   \n",
       "3            0         0         0         0         1         0         0   \n",
       "4            0         0         0         0         1         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4518         0         0         0         0         0         0         0   \n",
       "4519         0         0         0         0         0         0         0   \n",
       "4520         0         0         0         0         0         0         0   \n",
       "4521         0         0         0         0         0         0         0   \n",
       "4522         0         0         0         0         0         0         0   \n",
       "\n",
       "      indoor08  indoor09  outdoor01  outdoor02  outdoor03  outdoor04  \\\n",
       "0            0         0          0          0          0          0   \n",
       "1            0         0          0          0          0          0   \n",
       "2            0         0          0          0          0          0   \n",
       "3            0         0          0          0          0          0   \n",
       "4            0         0          0          0          0          0   \n",
       "...        ...       ...        ...        ...        ...        ...   \n",
       "4518         0         0          1          0          0          0   \n",
       "4519         0         0          1          0          0          0   \n",
       "4520         0         0          1          0          0          0   \n",
       "4521         0         0          1          0          0          0   \n",
       "4522         0         0          1          0          0          0   \n",
       "\n",
       "      outdoor05  outdoor06  outdoor07  outdoor08  outdoor09  \n",
       "0             0          0          0          0          0  \n",
       "1             0          0          0          0          0  \n",
       "2             0          0          0          0          0  \n",
       "3             0          0          0          0          0  \n",
       "4             0          0          0          0          0  \n",
       "...         ...        ...        ...        ...        ...  \n",
       "4518          0          0          0          0          0  \n",
       "4519          0          0          0          0          0  \n",
       "4520          0          0          0          0          0  \n",
       "4521          0          0          0          0          0  \n",
       "4522          0          0          0          0          0  \n",
       "\n",
       "[4523 rows x 18 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.47137134e-02,  1.99958290e-01,  1.70625568e-01,\n",
       "         2.08821717e-07],\n",
       "       [-1.20591061e-01,  1.41647518e-01,  1.95820414e-01,\n",
       "         2.15508339e-07],\n",
       "       [-1.21782978e-01,  2.25231032e-01, -6.97988411e-02,\n",
       "         2.18700409e-07],\n",
       "       [ 1.02166726e-01,  2.45939673e-01, -1.03706659e-02,\n",
       "         2.37841863e-07],\n",
       "       [-2.31998314e-03,  2.30737047e-01, -1.31359947e-01,\n",
       "         2.40439379e-07],\n",
       "       [ 1.05038078e-01,  1.03898049e-01,  2.20225927e-01,\n",
       "         2.43750731e-07],\n",
       "       [-1.44950110e-01, -5.57104541e-02,  2.15002742e-01,\n",
       "         2.47426486e-07],\n",
       "       [ 1.65842959e-01,  1.91693144e-01,  8.12978238e-02,\n",
       "         2.47500367e-07],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.010454095731284264,\n",
       " 0.012260868914549135,\n",
       " 0.2019408379192828,\n",
       " -0.26673220239630857)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,0].mean(),train_data[:,:,0].var(),train_data[:,:,0].max(),train_data[:,:,0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12378203784847644,\n",
       " 0.010142246237235964,\n",
       " 0.2459402671140822,\n",
       " -0.1686585734740348)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,1].mean(),train_data[:,:,1].var(),train_data[:,:,1].max(),train_data[:,:,1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020749059895218993,\n",
       " 0.011775426685903855,\n",
       " 0.22104323977039841,\n",
       " -0.18239073678468307)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,2].mean(),train_data[:,:,2].var(),train_data[:,:,2].max(),train_data[:,:,2].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001208217386795917, 0.000416290117576147, 0.3461716502643769, 0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,3].mean(),train_data[:,:,3].var(),train_data[:,:,3].max(),train_data[:,:,3].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.033821304849801774, 0.011470643398890461)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.mean(),train_data.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3249, 10, 4), (3249, 1), 1274)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,train_y.shape,len(val_x)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 4523\n",
    "BATCH_SIZE = 5\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x,train_x))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_x,val_x)).batch(len(val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy at 0x7fc72be018d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\",\"/gpu:2\"])\n",
    "mirrored_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 20        \n",
      "=================================================================\n",
      "Total params: 80\n",
      "Trainable params: 80\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer1 (InputLayer)       [(None, 10, 4)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 4)            0           input_layer1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4)            0           input_layer1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 4)            0           input_layer1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 4)            0           input_layer1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 4)            0           input_layer1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 4)            0           input_layer1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 4)            0           input_layer1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 4)            0           input_layer1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 4)            0           input_layer1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 4)            0           input_layer1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   (None, 4)            80          lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 40)           0           model[1][0]                      \n",
      "                                                                 model[2][0]                      \n",
      "                                                                 model[3][0]                      \n",
      "                                                                 model[4][0]                      \n",
      "                                                                 model[5][0]                      \n",
      "                                                                 model[6][0]                      \n",
      "                                                                 model[7][0]                      \n",
      "                                                                 model[8][0]                      \n",
      "                                                                 model[9][0]                      \n",
      "                                                                 model[10][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           1312        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           1056        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           528         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,976\n",
      "Trainable params: 2,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"encoder_decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer2 (InputLayer)    [(None, 10, 4)]           0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 16)                2976      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 40)                1320      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 10, 4)             0         \n",
      "=================================================================\n",
      "Total params: 6,168\n",
      "Trainable params: 6,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    input_o = tf.keras.layers.Input(shape=(4), name='input_layer')\n",
    "    models = layers.Dense(4, activation='sigmoid')(input_o )\n",
    "    models = layers.Dense(4, activation='sigmoid')(models)\n",
    "    models = layers.Dense(4, activation='sigmoid')(models)\n",
    "    models = layers.Dense(4, activation='sigmoid')(models)\n",
    "    model = tf.keras.Model(inputs=input_o, outputs=models)\n",
    "    model.summary()\n",
    "    input = tf.keras.layers.Input(shape=(10,4), name='input_layer1')\n",
    "    input1 = layers.Lambda(lambda x: x[:,0,:], output_shape=(1))(input)\n",
    "    input2 = layers.Lambda(lambda x: x[:,1,:], output_shape=(1))(input)\n",
    "    input3 = layers.Lambda(lambda x: x[:,2,:], output_shape=(1))(input)\n",
    "    input4 = layers.Lambda(lambda x: x[:,3,:], output_shape=(1))(input)\n",
    "    input5 = layers.Lambda(lambda x: x[:,4,:], output_shape=(1))(input)\n",
    "    input6 = layers.Lambda(lambda x: x[:,5,:], output_shape=(1))(input)\n",
    "    input7 = layers.Lambda(lambda x: x[:,6,:], output_shape=(1))(input)\n",
    "    input8 = layers.Lambda(lambda x: x[:,7,:], output_shape=(1))(input)\n",
    "    input9 = layers.Lambda(lambda x: x[:,8,:], output_shape=(1))(input)\n",
    "    input10= layers.Lambda(lambda x: x[:,9,:], output_shape=(1))(input)\n",
    "\n",
    "    model_1 = model(input1)\n",
    "    model_2 = model(input2)\n",
    "    model_3 = model(input3)\n",
    "    model_4 = model(input4)\n",
    "    model_5 = model(input5)\n",
    "    model_6 = model(input6)\n",
    "    model_7 = model(input7)\n",
    "    model_8 = model(input8)\n",
    "    model_9 = model(input9)\n",
    "    model_10= model(input10)\n",
    "    merge_layer = tf.keras.layers.concatenate(inputs=[model_1, model_2,model_3,model_4,model_5,model_6,model_7,model_8,model_9,model_10])\n",
    "    model_encoder = tf.keras.layers.Dense(32, activation='sigmoid')(merge_layer)\n",
    "    model_encoder = tf.keras.layers.Dense(32, activation='sigmoid')(model_encoder)\n",
    "    model_encoder = tf.keras.layers.Dense(16, activation='sigmoid')(model_encoder)\n",
    "    model_down = tf.keras.Model(inputs=[input], outputs=model_encoder,name = \"encoder\")#input1, input2,input3,input4,input5,input6,input7,input8,input9,input10\n",
    "    model_down.summary()\n",
    "    input_encoder = tf.keras.layers.Input(shape=(10,4), name='input_layer2')\n",
    "    input_decoder = model_down(input_encoder)\n",
    "    model_decoder = layers.Dense(16, activation='sigmoid')(input_decoder)\n",
    "    model_decoder = layers.Dense(32, activation='sigmoid')(model_decoder)\n",
    "    model_decoder = layers.Dense(32, activation='sigmoid')(model_decoder)\n",
    "    model_decoder = layers.Dense(40, activation='sigmoid')(model_decoder)\n",
    "    model_decoder = layers.Reshape((10,4))(model_decoder)\n",
    "    model_encoder_decoder = tf.keras.Model(inputs=[input_encoder],outputs=model_decoder,name = 'encoder_decoder')\n",
    "    model_encoder_decoder.summary()\n",
    "    #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=1e-5)\n",
    "    model_encoder_decoder.compile(optimizer = 'sgd', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "latent_dim = 16 \n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, encoding_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim  \n",
    "    \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(32, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(40, activation='sigmoid'),\n",
    "      layers.Reshape((10,4))\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "  \n",
    "autoencoder = Autoencoder(latent_dim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "autoencoder.fit(train_x, train_x,\n",
    "                epochs=1000,\n",
    "                shuffle=True,\n",
    "                validation_data=(val_x, val_x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_dataset = pd.read_csv(\"validationData.csv\",header = 0)\n",
    "\n",
    "test_features = np.asarray(test_dataset.iloc[:,0:520])\n",
    "test_features[test_features == 100] = -110\n",
    "test_features = (test_features - test_features.mean()) / test_features.var()\n",
    "\n",
    "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
    "test_labels = np.asarray(pd.get_dummies(test_labels))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "def decay(epoch):\n",
    "  if epoch < 300:\n",
    "    return 1e-2\n",
    "  elif epoch >= 300 and epoch < 500:\n",
    "    return 1e-4\n",
    "  elif epoch >= 500 and epoch < 700:\n",
    "    return 1e-6\n",
    "  elif epoch >= 700 and epoch < 900:\n",
    "    return 1e-8\n",
    "  else:\n",
    "    return 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model_encoder_decoder.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-15, momentum=1e-17)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_encoder_decoder.compile(optimizer = optimizer, loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "    650/Unknown - 18s 28ms/step - loss: 0.0070INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "\n",
      "Learning rate for epoch 1 is 0.009999999776482582\n",
      "650/650 [==============================] - 20s 30ms/step - loss: 0.0070 - val_loss: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 2 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 3/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 3 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 4/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 4 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 5/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 5 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 6/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 6 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 7/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 7 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 8/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 8 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 9/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 9 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 10/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 10 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 11/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 11 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 12/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 12 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 13/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 13 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 14/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 14 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 15/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 15 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 16/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 16 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 17/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 17 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 18/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 18 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 19/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 19 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 20/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 20 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 21/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 21 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 22/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 22 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 23/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 23 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 24/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 24 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 25/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 25 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 26/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 26 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 27/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 27 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 28/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 28 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 29/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 29 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 30/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 30 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 31/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 31 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 32/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 32 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 33/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 33 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 34/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 34 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 35/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 35 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 36/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 36 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 37/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 37 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 38/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 38 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 39/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 39 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 40/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 40 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 41/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 41 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 42/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 42 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 43/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 43 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 44/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 44 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 45/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 45 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 46/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 46 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 47/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 47 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 48/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 48 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 49/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 49 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 50/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 50 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 51/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 51 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 52/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 52 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 53/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 53 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 54/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 54 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 55/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 55 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 56/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 56 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 57/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 57 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 58/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 58 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 59/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 59 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 60/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 60 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 61/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 61 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 62/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 62 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 63/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 63 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 64/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 64 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 65/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 65 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 66/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 66 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 67/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 67 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 68/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 68 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 69/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 69 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 70/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 70 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 71/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 71 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 72/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 72 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 73/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 73 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 74/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 74 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 75/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 75 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 76/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 76 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 77/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 77 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 78/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 78 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 79/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 79 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 80/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 80 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 81/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 81 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 82/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 82 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 83/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 83 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 84/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 84 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 85/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 85 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 86/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 86 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 87/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 87 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 88/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 88 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 89/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 89 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 90/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 90 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 91/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 91 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 92/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 92 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 93/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 93 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 94/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 94 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 95/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 95 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 96/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 96 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 97/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 97 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 98/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 98 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 99/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 99 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 100/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 100 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 101/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 101 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 102/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 102 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 103/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 103 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 104/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 104 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 105/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 105 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 106/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 106 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 107/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 107 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 108/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 108 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 109/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 109 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 110/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 110 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 111/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 111 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 112/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 112 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 113/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 113 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 114/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 114 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 115/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 115 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 116/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 116 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 117/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 117 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 118/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 118 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 119/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 119 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 120/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 120 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 121/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 121 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 122/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 122 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 123/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 123 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 124/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 124 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 125/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 125 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 126/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 126 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 127/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 127 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 128/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 128 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 129/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 129 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 130/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 130 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 131/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 131 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 132/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 132 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 133/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 133 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 134/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 134 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 135/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 135 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 136/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 136 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 137/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 137 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 138/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 138 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 139/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 139 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 140/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 140 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 141/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 141 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 142/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 142 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 143/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 143 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 144/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 144 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 145/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 145 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 146/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 146 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 147/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 147 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 148/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 148 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 149/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 149 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 150/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 150 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 151/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 151 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 152/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 152 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 153/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 153 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 154/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 154 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 155/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 155 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 156/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 156 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 157/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 157 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 158/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 158 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 159/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 159 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 160/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 160 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 161/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 161 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 162/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 162 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 163/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 163 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 164/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 164 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 165/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 165 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 166/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 166 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 167/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 167 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 168/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 168 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 169/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 169 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 170/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 170 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 171/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 171 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 172/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 172 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 173/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 173 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 174/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 174 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 175/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 175 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 176/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 176 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 177/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 177 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 178/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 178 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 179/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 179 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 180/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 180 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 181/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 181 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 182/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 182 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 183/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 183 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 184/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 184 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 185/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 185 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 186/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 186 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 187/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 187 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 188/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 188 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 189/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 189 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 190/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 190 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 191/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 191 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 192/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 192 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 193/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 193 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 194/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 194 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 195/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 195 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 196/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 196 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 197/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 197 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 198/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 198 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 199/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 199 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 200/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 200 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 201/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 201 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 23ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 202/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 202 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 203/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 203 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 204/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 204 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 205/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 205 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 206/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 206 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 207/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 207 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 208/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 208 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 209/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 209 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 210/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 210 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 211/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 211 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 212/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 212 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 213/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 213 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 214/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 214 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 215/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 215 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 216/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 216 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 217/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 217 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 218/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 218 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 219/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 219 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 220/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 220 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 221/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 221 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 222/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 222 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 223/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 223 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 224/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 224 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 225/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 225 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 226/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 226 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 227/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 227 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 228/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 228 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 229/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 229 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 230/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 235 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 236/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 236 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 237/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 237 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 238/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 238 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 239/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 239 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 240/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 240 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 241/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 241 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 242/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 242 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 243/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 243 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 244/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 244 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 245/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 245 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 246/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 246 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 247/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 247 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 248/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 248 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 249/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 249 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 250/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 250 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 251/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 251 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 252/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 252 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 253/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 253 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 254/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 254 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 255/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 255 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 256/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 256 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 257/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 257 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 258/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 258 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 259/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 259 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 260/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 260 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 261/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 261 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 262/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 262 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 263/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 263 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 264/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 264 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 265/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 265 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 266/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 266 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 267/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 267 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 268/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 268 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 269/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 269 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 270/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 270 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 271/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 271 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 272/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 272 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 273/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 273 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 274/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 274 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 275/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 275 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 276/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 276 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 277/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 277 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 278/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 278 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 279/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 279 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 280/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 280 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 281/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 281 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 282/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 282 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 283/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 283 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 284/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 284 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 285/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 285 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 286/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 286 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 287/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 287 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 288/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 288 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 289/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 289 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 290/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 290 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 291/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 291 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 292/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 292 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 293/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 293 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 294/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 294 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 295/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 295 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 296/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 296 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 297/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 297 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 298/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 298 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 299/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 299 is 0.009999999776482582\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 300/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 300 is 0.009999999776482582\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 301/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 301 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 302/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 302 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 303/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 303 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 304/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 304 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 305/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 305 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 306/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 306 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 307/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 307 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 308/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 308 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 309/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 309 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 310/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 310 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 311/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 311 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 312/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 312 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 313/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 313 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 314/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 314 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 315/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 315 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 316/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 316 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 317/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 317 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 318/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 318 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 319/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 319 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 23ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 320/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 320 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 321/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 321 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 322/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 322 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 323/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 323 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 324/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 324 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 325/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 325 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 326/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 326 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 327/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 327 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 328/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 328 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 329/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 329 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 330/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 330 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 331/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 331 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 332/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 332 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 333/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 333 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 334/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 334 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 335/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 335 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 336/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 336 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 337/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 337 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 338/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 338 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 339/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 339 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 340/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 340 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 341/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 341 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 342/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 342 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 343/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 343 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 344/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 344 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 345/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 345 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 346/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 346 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 347/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 347 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 348/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 348 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 349/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 349 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 350/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 350 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 351/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 351 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 352/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 352 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 353/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 353 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 354/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 354 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 355/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 355 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 356/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 356 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 357/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 357 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 358/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 358 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 359/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 359 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 360/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 360 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 361/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 361 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 362/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 362 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 363/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 363 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 364/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 364 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 365/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 365 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 366/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 366 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 367/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 367 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 368/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069- ETA: 0s - loss: \n",
      "Learning rate for epoch 368 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 369/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 369 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 370/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 370 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 371/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 371 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 372/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 372 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 373/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 373 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 374/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 374 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 375/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 375 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 376/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 376 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 377/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 377 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 378/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 378 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 379/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 379 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 380/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 380 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 381/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 381 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 382/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0068\n",
      "Learning rate for epoch 382 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 383/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 383 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 384/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 384 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 385/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 385 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 386/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 386 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 387/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 387 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 388/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 388 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 389/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 389 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 390/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 390 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 391/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 391 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 392/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 392 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 393/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 393 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 394/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 394 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 395/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 395 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 396/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 396 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 397/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 397 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 398/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 398 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 399/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 399 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 400/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 400 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 401/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 401 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 402/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 402 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 403/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 403 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 404/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 404 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 405/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 405 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 406/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 406 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 407/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 407 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 408/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 408 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 409/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 409 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 410/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 410 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 411/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 411 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 412/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 412 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 413/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 413 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 414/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 414 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 415/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 415 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 416/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 416 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 417/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 417 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 23ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 418/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 418 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 419/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 419 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 420/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 420 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 421/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 421 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 422/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 422 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 423/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 423 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 424/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 424 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 425/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 425 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 426/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 426 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 427/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 427 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 428/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 428 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 429/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 429 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 430/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 430 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 431/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 431 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 432/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 432 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 433/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 433 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 434/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 434 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 435/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 435 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 436/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 436 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 437/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 437 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 438/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 438 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 439/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 439 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 440/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 440 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 441/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 441 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 442/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 442 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 443/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 443 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 444/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 444 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 445/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 445 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 446/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 446 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 447/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 447 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 448/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 448 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 449/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 449 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 450/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 450 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 451/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 451 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 452/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 452 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 453/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 453 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 454/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 454 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 455/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 455 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 456/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 456 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 457/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 457 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 458/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 458 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 459/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 459 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 460/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 460 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 461/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 461 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 462/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 462 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 463/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 463 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 464/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 464 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 465/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 465 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 466/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 466 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 467/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 467 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 468/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 468 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 469/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 469 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 470/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 470 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 471/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 471 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 472/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 472 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 473/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 473 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 474/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 474 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 475/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 475 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 476/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 476 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 477/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 477 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 478/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 478 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 479/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 479 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 480/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 480 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 481/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 481 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 482/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 482 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 483/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 483 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 484/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 484 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 485/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 485 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 486/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 486 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 487/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 487 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 488/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 488 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 489/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 489 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 490/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 490 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 491/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 491 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 492/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 492 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 493/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 493 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 494/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 494 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 495/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 495 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 496/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 496 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 497/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 497 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 498/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 498 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 499/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 499 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 500/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 500 is 9.999999747378752e-05\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 501/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 501 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 502/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 502 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 503/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 503 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 504/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 504 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 505/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 505 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 506/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 506 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 507/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 507 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 508/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 508 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 509/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 509 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 510/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 510 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 511/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 511 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 512/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 512 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 513/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 513 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 514/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 514 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 515/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 515 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 516/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 516 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 517/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 517 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 518/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 518 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 519/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 519 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 520/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 520 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 521/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 521 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 522/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0068\n",
      "Learning rate for epoch 522 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 523/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 523 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 524/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 524 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 525/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 525 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 526/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 526 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 527/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 527 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 528/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 528 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 529/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 529 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 530/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 530 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 531/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 531 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 532/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 532 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 533/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 533 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 534/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 534 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 535/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 535 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 536/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 536 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 537/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 537 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 538/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 538 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 539/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 539 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 540/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 540 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 541/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 541 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 542/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 542 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 543/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 543 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 544/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 544 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 545/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 545 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 546/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 546 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 547/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 547 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 548/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 548 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 549/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 549 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 550/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0068\n",
      "Learning rate for epoch 550 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 551/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 551 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 552/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 552 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 553/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 553 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 554/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 554 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 555/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 555 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 556/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 556 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 557/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 557 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 558/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 558 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 559/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 559 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 560/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 560 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 561/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 561 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 562/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 562 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 563/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 563 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 564/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 564 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 565/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 565 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 566/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 566 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 567/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 567 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 568/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 568 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 569/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 569 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 570/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 570 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 571/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 571 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 572/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 572 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 573/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 573 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 574/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 574 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 575/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 575 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 576/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 576 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 577/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 577 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 578/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 578 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 579/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 579 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 580/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 580 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 581/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 581 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 582/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 582 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 583/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 583 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 584/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 584 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 585/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 585 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 586/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 586 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 587/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 587 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 588/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 588 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 589/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 589 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 590/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 590 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 591/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 591 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 592/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 592 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 593/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 593 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 594/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 594 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 595/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 595 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 596/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 596 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 597/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 597 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 598/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 598 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 599/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 599 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 600/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 600 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 601/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 601 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 602/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 602 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 603/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 603 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 604/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 604 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 605/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 605 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 606/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 606 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 607/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 607 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 608/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 608 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 609/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 609 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 610/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 610 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 611/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 611 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 612/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 612 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 613/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 613 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 614/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 614 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 615/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 615 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 616/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 616 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 617/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 617 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 618/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 618 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 619/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 619 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 620/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 620 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 621/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 621 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 622/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 622 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 623/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 623 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 624/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 624 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 625/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 625 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 626/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 626 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 627/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 627 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 628/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 628 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 629/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 629 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 630/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 630 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 631/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 631 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 632/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 632 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 633/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 633 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 634/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 634 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 635/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 635 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 636/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 636 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 637/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 637 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 638/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 638 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 639/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 639 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 640/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 640 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 641/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 641 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 642/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 642 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 643/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 643 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 644/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 644 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 645/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 645 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 646/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 646 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 647/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 647 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 648/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 648 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 649/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 649 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 650/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 650 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 651/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 651 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 652/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 652 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 653/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 653 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 654/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 654 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 655/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 655 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 656/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 656 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 657/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 657 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 658/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 658 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 659/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 659 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 660/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 660 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 661/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 661 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 15s 23ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 662/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 662 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 663/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 663 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 664/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 664 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 665/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 665 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 666/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 666 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 667/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 667 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 668/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 668 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 669/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 669 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 670/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 670 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 671/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 671 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 672/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 672 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 673/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 673 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 674/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 674 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 675/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 675 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 676/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 676 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 677/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 677 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 678/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 678 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 679/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 679 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 680/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 680 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 681/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 681 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 682/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 682 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 683/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 683 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 684/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 684 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 685/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 685 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 686/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 686 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 687/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 687 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 688/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 688 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 689/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 689 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 690/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 690 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 691/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 691 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 692/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 692 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 693/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 693 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 694/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 694 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 695/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 695 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 696/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 696 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 697/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 697 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 698/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 698 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 699/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 699 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 700/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 700 is 9.999999974752427e-07\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 701/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 701 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 702/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 702 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 703/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 703 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 704/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 704 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 705/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 705 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 706/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 706 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 707/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 707 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 708/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 708 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 709/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 709 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 710/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 710 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 711/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 711 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 712/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 712 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 713/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 713 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 714/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 714 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 715/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 715 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 716/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 716 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 717/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 717 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 718/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 718 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 719/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 719 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 720/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 720 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 721/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 721 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 722/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 722 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 723/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 723 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 724/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 724 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 725/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 725 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 726/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 726 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 727/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 727 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 728/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 728 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 729/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 729 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 730/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 730 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 731/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 731 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 732/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 732 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 733/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0068\n",
      "Learning rate for epoch 733 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 734/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 734 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 735/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 735 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 736/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 736 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 737/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 737 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 738/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 738 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 739/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 739 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 740/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 740 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 741/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 741 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 742/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 742 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 743/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 743 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 744/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 744 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 745/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 745 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 746/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 746 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 747/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 747 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 748/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 748 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 749/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 749 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 750/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 750 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 751/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 751 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 752/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 752 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 753/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 753 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 754/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 754 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 755/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 755 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 756/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 756 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 757/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 757 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 758/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 758 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 759/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 759 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 760/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 760 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 761/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 761 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 762/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 762 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 763/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 763 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 764/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 764 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 765/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 765 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 766/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 766 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 767/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 767 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 768/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 768 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 769/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 769 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 770/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 770 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 771/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 771 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 772/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 772 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 773/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 773 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 774/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 774 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 775/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 775 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 776/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 776 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 777/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 777 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 778/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 778 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 779/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 779 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 780/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 780 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 781/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 781 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 782/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 782 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 783/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 783 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 784/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 784 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 785/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 785 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 786/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 786 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 787/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 787 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 788/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 788 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 789/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 789 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 790/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 790 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 791/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 791 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 792/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 792 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 793/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 793 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 794/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 794 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 795/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 795 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 796/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 796 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 797/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 797 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 798/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 798 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 799/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 799 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 800/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 800 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 801/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 801 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 802/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 802 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 803/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 803 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 804/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 804 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 805/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 805 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 806/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 806 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 807/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 807 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 808/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 808 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 809/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 809 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 810/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 810 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 811/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 811 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 812/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 812 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 813/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 813 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 814/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 814 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 815/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 815 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 816/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 816 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 817/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069- ETA: 0s - loss: 0.00\n",
      "Learning rate for epoch 817 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 818/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 818 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 819/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 819 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 820/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 820 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 821/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 821 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 822/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 822 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 823/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 823 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 824/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 824 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 825/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 825 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 826/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 826 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 827/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 827 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 828/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 828 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 829/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 829 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 830/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 830 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 831/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 831 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 832/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 832 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 833/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 833 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 834/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 834 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 835/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 835 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 836/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 836 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 837/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 837 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 838/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 838 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 839/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 839 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 840/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 840 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 841/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0068\n",
      "Learning rate for epoch 841 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 842/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 842 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 843/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 843 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 844/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 844 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 845/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 845 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 846/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 846 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 847/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 847 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 848/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 848 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 849/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 849 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 850/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 850 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 851/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 851 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 852/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 852 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 853/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 853 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 854/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 854 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 855/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 855 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 856/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 856 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 857/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 857 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 858/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 858 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 859/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 859 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 860/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 860 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 861/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 861 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 862/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 862 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 863/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 863 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 864/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 864 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 865/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 865 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 866/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0068\n",
      "Learning rate for epoch 866 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 867/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 867 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 868/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 868 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 869/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 869 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 870/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 870 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 871/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 871 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 872/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 872 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 873/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 873 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 874/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 874 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 875/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 875 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 876/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 876 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 877/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 877 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 878/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 878 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 879/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 879 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 880/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 880 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 881/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 881 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 882/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 882 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 883/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 883 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 884/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 884 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 885/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 885 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 886/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 886 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 887/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 887 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 888/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 888 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 889/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 889 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 890/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 890 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 891/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 891 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 892/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 892 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 893/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 893 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 894/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 894 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 895/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 895 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 896/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 896 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 897/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 897 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 898/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 898 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 899/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 899 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 900/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 900 is 9.99999993922529e-09\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 901/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 901 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 902/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 902 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 903/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 903 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 904/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 904 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 905/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 905 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 906/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 906 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 907/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 907 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 908/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 908 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 909/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 909 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 910/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 910 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 911/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 911 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 912/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 912 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 913/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 913 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 914/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 914 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 915/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 915 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 916/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 916 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 917/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 917 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 918/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 918 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 919/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 919 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 920/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 920 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 921/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 921 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 922/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 922 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 923/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 923 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 924/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 924 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 925/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 925 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 926/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 926 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 927/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 927 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 928/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 928 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 929/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 929 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 930/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 930 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 931/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 931 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 932/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 932 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 15s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 933/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 933 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 934/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 934 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 935/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 935 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 936/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 936 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 937/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 937 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 938/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 938 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 939/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 939 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 940/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 940 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 941/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 941 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 942/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 942 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 943/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 943 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 944/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 944 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 945/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 945 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 946/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 946 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 947/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 947 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 948/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 948 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 949/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 949 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 950/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 950 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 951/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 951 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 952/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 952 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 953/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 953 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 954/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 954 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 955/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 955 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 956/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 956 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 957/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 957 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 958/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 958 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 959/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 959 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 960/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 960 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 961/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 961 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 962/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 962 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 963/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 963 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 964/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 964 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 965/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 965 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 966/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 966 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 967/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 967 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 968/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 968 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 969/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 969 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 970/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 970 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 971/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 971 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 972/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 972 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 973/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 973 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 974/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 974 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 975/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 975 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 976/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 976 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 977/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 977 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 978/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 978 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 979/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 979 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 980/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 980 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 981/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 981 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 982/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 982 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 983/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 983 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 984/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 984 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 985/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 985 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 986/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 986 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 987/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 987 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 988/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 988 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 989/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 989 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 990/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 990 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 991/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 991 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 992/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 992 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 993/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 993 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 994/1000\n",
      "648/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 994 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 21ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 995/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 995 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 996/1000\n",
      "647/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 996 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 997/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 997 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 998/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 998 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 999/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0068\n",
      "Learning rate for epoch 999 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 1000/1000\n",
      "649/650 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 1000 is 1.000000013351432e-10\n",
      "650/650 [==============================] - 14s 22ms/step - loss: 0.0069 - val_loss: 0.0068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc738a56e10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_encoder_decoder.fit(train_dataset,\n",
    "                epochs=1000,\n",
    "                validation_data=valid_dataset,\n",
    "                callbacks = callbacks\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((None, 10, 4), (None, 10, 4)), types: (tf.float64, tf.float64)>,\n",
       " (1274, 10, 4))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset,val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_latent = model_down(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1274, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold, datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = np.random.rand(len(val_latent)) < 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = manifold.TSNE(n_components=2, init='pca', n_iter=5000, method='exact').fit_transform(val_latent[train_val_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(509, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1274, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = val_y[train_val_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
    "X_norm = (X_tsne - x_min) / (x_max - x_min)\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(X_norm.shape[0]):\n",
    "    plt.text(X_norm[i, 0], X_norm[i, 1], str(y[i,0]), color=plt.cm.Set1(y[i,0]), \n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig(\"result/pca_result_latent16_XYZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.Dense(256, activation='relu'),\n",
    "      layers.Dense(labels.shape[1], activation='softmax'),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer sequential is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_encoded = np.array(autoencoder.encoder(train_x))\n",
    "val_encoded = np.array(autoencoder.encoder(val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN.compile(optimizer='adam', loss=losses.CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DNN.fit(train_encoded, train_y,\n",
    "                epochs=1000,\n",
    "                shuffle=True,\n",
    "                validation_data=(val_encoded, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder = np.array(autoencoder.encoder(test_features))\n",
    "y_ = DNN(test_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array(y_[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gps_wifi",
   "language": "python",
   "name": "gps_wifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
