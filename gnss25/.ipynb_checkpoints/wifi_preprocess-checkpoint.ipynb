{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#File = 1\n",
    "#prFileName = 'dataRssi_at_%d.txt'%File\n",
    "#dirName = '/home/lyt/gnss_wifi/gnss/20201022/indoor/wifi/'\n",
    "#outputname = 'outplot/1out_in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = glob.glob('20201022/indoor/wifi/dataRssi_at_*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[0].split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "origin_data = []\n",
    "train_label = []\n",
    "for data in datas:\n",
    "    load_data = np.loadtxt(data)#f.read()\n",
    "    load_data = load_data[:,:15].astype(int)\n",
    "    load_data[load_data==0] = -100\n",
    "    load_data = load_data/100 + 1\n",
    "    #print(load_data.shape)\n",
    "    for i in range(len(load_data)):\n",
    "        origin_data.append(load_data[i])\n",
    "        mask = np.random.rand((15))\n",
    "        mask = (mask>0.1).astype(int)\n",
    "        noise = np.random.normal(scale = 0.05,size=(15))\n",
    "        load_data[i]*mask\n",
    "        load_data[i]+noise\n",
    "        train_data.append(load_data[i])\n",
    "        train_label.append(data.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0])\n",
    "    #print(load_data.shape)    \n",
    "    \n",
    "    #print(np.array(train_data).shape)\n",
    "origin_data = np.array(origin_data).astype('float32') \n",
    "train_data = np.array(train_data).astype('float32')\n",
    "train_label = np.array(pd.get_dummies(train_label)).astype('float32')\n",
    "#train_label = train_label.reshape(len(train_label),1)\n",
    "train_val_split = np.random.rand(len(train_data)) < 0.70\n",
    "train_x = train_data[train_val_split]\n",
    "train_o = origin_data[train_val_split]\n",
    "train_y = train_label[train_val_split]\n",
    "val_x = train_data[~train_val_split]\n",
    "val_o = origin_data[~train_val_split]\n",
    "val_y = train_label[~train_val_split]\n",
    "BUFFER_SIZE = train_x.shape[0]\n",
    "BATCH_SIZE = 5\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x,train_o,train_y))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_x,val_o)).batch(len(val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1352, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_label).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.array(pd.get_dummies(train_label)).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for a,b,c in train_dataset:\n",
    "    print(a.dtype)\n",
    "    print(b.dtype)\n",
    "    print(c.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        self.dense = dense\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name='bias',shape=[self.dense.input_shape[-1]],initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b = True)\n",
    "        return self.activation(z + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\"])\n",
    "#with mirrored_strategy.scope():\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(15), name='input_layer1')\n",
    "#flatten = tf.keras.layers.Flatten()(input)\n",
    "dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "dense2 = tf.keras.layers.Dense(24, activation='relu')\n",
    "dense3 = tf.keras.layers.Dense(16, activation='relu')\n",
    "model_encoder = dense1(input)\n",
    "model_encoder = dense2(model_encoder)\n",
    "model_encoder = dense3(model_encoder)\n",
    "model_down = tf.keras.Model(inputs=[input], outputs=model_encoder,name = \"encoder\")#input1, input2,input3,input4,input5,input6,input7,input8,input9,input10\n",
    "#model_down.summary()\n",
    "#input_encoder = tf.keras.layers.Input(shape=(15), name='input_layer2')\n",
    "input2 = tf.keras.layers.Input(shape=(16), name='input_layer2')\n",
    "model_decoder = DenseTranspose(dense3, activation = 'relu')(input2)\n",
    "model_decoder = DenseTranspose(dense2, activation = 'relu')(model_decoder)\n",
    "model_decoder = DenseTranspose(dense1, activation = 'relu')(model_decoder)\n",
    "model_up = tf.keras.Model(inputs=[input2], outputs=model_decoder,name = \"decoder\")\n",
    "\n",
    "#model_encoder_decoder.summary()\n",
    "input3 = tf.keras.layers.Input(shape=(16), name='input_layer3')\n",
    "model_ann = tf.keras.layers.Dense(16, activation='relu')(input3)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "#model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "#model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(9, activation='softmax')(model_ann)\n",
    "model_ANN = tf.keras.Model(inputs=[input3], outputs=model_ann,name = \"ann\")\n",
    "\n",
    "input_full = tf.keras.layers.Input(shape=(15), name='input_layer4')\n",
    "encoder_out = model_down(input_full)\n",
    "decoder_out = model_up(encoder_out)\n",
    "ann_out = model_ANN(encoder_out)\n",
    "#model_encoder_decoder_ann = tf.keras.Model(inputs=[input_full],outputs=[decoder_out],name = 'encoder_decoder_ann')\n",
    "model_encoder_decoder_ann = tf.keras.Model(inputs=[input_full],outputs=[decoder_out,ann_out],name = 'encoder_decoder_ann')\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=1e-5)\n",
    "#model_encoder_decoder_ann.compile(optimizer = 'sgd', \n",
    "#                                  loss={'AE_out1': 'mean_squared_error','place_out2': 'categorical_crossentropy'}),\n",
    "#                                  loss_weights={'AE_out1': 0.5,'place_out2': 0.5},\n",
    "#                                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\"])\n",
    "#with mirrored_strategy.scope():\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(15), name='input_layer1')\n",
    "#flatten = tf.keras.layers.Flatten()(input)\n",
    "dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "dense2 = tf.keras.layers.Dense(24, activation='relu')\n",
    "dense3 = tf.keras.layers.Dense(16, activation='relu')\n",
    "model_encoder = dense1(input)\n",
    "model_encoder = dense2(model_encoder)\n",
    "model_encoder = dense3(model_encoder)\n",
    "model_down = tf.keras.Model(inputs=[input], outputs=model_encoder,name = \"encoder\")#input1, input2,input3,input4,input5,input6,input7,input8,input9,input10\n",
    "#model_down.summary()\n",
    "input_encoder = tf.keras.layers.Input(shape=(15), name='input_layer2')\n",
    "input_decoder = model_down(input_encoder)\n",
    "model_decoder = DenseTranspose(dense3, activation = 'relu')(input_decoder)\n",
    "model_decoder = DenseTranspose(dense2, activation = 'relu')(model_decoder)\n",
    "model_decoder = DenseTranspose(dense1, activation = 'relu')(model_decoder)\n",
    "\n",
    "#model_encoder_decoder.summary()\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(input_decoder)\n",
    "model_ann = tf.keras.layers.Dense(9, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(9, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(9, activation='softmax')(model_ann)\n",
    "model_encoder_decoder_ann = tf.keras.Model(inputs=[input_encoder],outputs=[model_decoder,model_ann],name = 'encoder_decoder')\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=1e-5)\n",
    "#model_encoder_decoder_ann.compile(optimizer = 'sgd', \n",
    "#                                  loss={'AE_out1': 'mean_squared_error','place_out2': 'categorical_crossentropy'}),\n",
    "#                                  loss_weights={'AE_out1': 0.5,'place_out2': 0.5},\n",
    "#                                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_decoder_ann\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer4 (InputLayer)       [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 (None, 16)           1704        input_layer4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 15)           1775        encoder[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ann (Model)                     (None, 9)            969         encoder[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,744\n",
      "Trainable params: 2,744\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_encoder_decoder_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_encoder_decoder_ann.trainable_variables)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(15):\n",
    "    print(model_encoder_decoder_ann.trainable_variables[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vars = model_encoder_decoder_ann.trainable_variables[:6]\n",
    "decode_vars = model_encoder_decoder_ann.trainable_variables[6:9]\n",
    "ann_vars = model_encoder_decoder_ann.trainable_variables[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(output,t_o,t_y):\n",
    "    #print(\"in loss\")\n",
    "    output_AE , output_label = output\n",
    "    #output_AE = output\n",
    "    mse = losses.MeanSquaredError()\n",
    "    AE_loss = mse(t_o,output_AE)\n",
    "    ann_loss =losses.categorical_crossentropy(t_y,output_label)\n",
    "    total_loss = AE_loss*100 + ann_loss\n",
    "    \n",
    "    return AE_loss,ann_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_A = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "initial_learning_rate=1e-4, decay_steps=5000, decay_rate=0.9)\n",
    "optimizer_A = tf.optimizers.SGD(learning_rate=learning_rate_A , momentum=1e-5)\n",
    "learning_rate_B = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.8)\n",
    "optimizer_B = tf.optimizers.Adam(learning_rate=1e-3)#learning_rate_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "\n",
    "def train_step(t_x,t_o,t_y):\n",
    "  \n",
    "    with tf.GradientTape() as AE_tape,tf.GradientTape() as ANN_tape:\n",
    "        output = model_encoder_decoder_ann(t_x, training=True)\n",
    "        #AE_loss = model_loss(output,t_o,t_y)\n",
    "        AE_loss,ANN_loss = model_loss(output,t_o,t_y)\n",
    "        #gradients = tape.gradient(total_loss, shared_vars+decode_vars)\n",
    "        #optimizer_A.apply_gradients(zip(gradients, shared_vars+decode_vars))\n",
    "\n",
    "    if np.random.rand() < 0.5:\n",
    "        #print(\"AE\")\n",
    "        gradients_AE = AE_tape.gradient(AE_loss, shared_vars+decode_vars)\n",
    "        #gradients_AE = [tf.clip_by_value(g, -1,1) for g in gradients_AE]\n",
    "\n",
    "        #print(\"AE gradient : \",gradients_AE)\n",
    "        optimizer_A.apply_gradients(zip(gradients_AE, shared_vars+decode_vars))\n",
    "    \n",
    "    else:\n",
    "        #print(\"ANN\")\n",
    "        gradients_ANN = ANN_tape.gradient(ANN_loss, shared_vars+ann_vars)\n",
    "        #print(\"ANN gradient : \",gradients_ANN)\n",
    "        #gradients_ANN = [tf.clip_by_value(g, -1,1) for g in gradients_ANN] \n",
    "        optimizer_B.apply_gradients(zip(gradients_ANN, shared_vars+ann_vars))\n",
    "    \n",
    "    return np.array(AE_loss).mean(),np.array(ANN_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './wifi_checkpoints_1'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_dropout_model_{epoch}\")\n",
    "checkpoint = tf.train.Checkpoint(optimizerA=optimizer_A,\n",
    "                                 optimizerB=optimizer_B,\n",
    "                                 model_encoder_decoder_ann=model_encoder_decoder_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model_encoder_decoder.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(v_x,v_o,v_y):\n",
    "    output = model_encoder_decoder_ann(v_x)\n",
    "    #output_AE = output\n",
    "    output_AE , output_label = output\n",
    "    mse = losses.MeanSquaredError()\n",
    "    AE_loss = mse(v_o,output_AE)\n",
    "    ann_loss = losses.categorical_crossentropy(v_y,output_label)\n",
    "    total_loss = AE_loss*100 + ann_loss\n",
    "    #print(output_label[200])\n",
    "    #print(v_y[200])\n",
    "    #print(ann_loss)\n",
    "    #print(\"AE loss : {},\".format(np.array(AE_loss).mean()))\n",
    "    print(\"AE loss : {}, ANN loss : {}, Total loss : {}\".format(np.array(AE_loss).mean(),np.array(ann_loss).mean(),np.array(total_loss).mean()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "validation(val_x,val_o,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409, 15)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    all_AE = []\n",
    "    all_ANN =[]\n",
    "    for x,o,y in train_dataset:\n",
    "        #AE_loss = train_step(x,o,y)\n",
    "        AE_loss,ANN_loss = train_step(x,o,y)\n",
    "        all_AE.append(AE_loss)\n",
    "        all_ANN.append(ANN_loss)\n",
    "    #print(\"train AE loss : {}\".format(np.array(all_AE).mean()))\n",
    "    print(\"train AE loss : {}, train ANN loss : {}\".format(np.array(all_AE).mean(),np.array(all_ANN).mean()))\n",
    "    validation(val_x,val_o,val_y)\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"learning rate A : \",optimizer_A._decayed_lr(tf.float32))\n",
    "    print(\"learning rate B : \",optimizer_B._decayed_lr(tf.float32))\n",
    "    print(f'Time for epoch {epoch + 1} is {time.time() - start:.4f} sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=475, shape=(), dtype=float32, numpy=2.908749e-05>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0., 0., 1., 0., 0., 0., 0., 0., 0.]#[0.11685812, 0.11185785, 0.11026918, 0.10764945, 0.10728354, 0.11020868,\n",
    " #0.10912149, 0.11363789, 0.11311384]#[0, 1, 0,0,0,0,0,0,0]\n",
    "b = [2.4371104e-08, 4.2344610e-11, 9.9997091e-01, 4.9084689e-13, 6.4827432e-06,\n",
    " 2.1712562e-08, 3.4163491e-11, 3.0665456e-11, 2.2567538e-05]\n",
    "tf.keras.losses.categorical_crossentropy(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train AE loss : 0.025033259764313698, train ANN loss : 2.1985867023468018\n",
      "AE loss : 0.02423946000635624, ANN loss : 2.197774887084961, Total loss : 4.621720790863037\n",
      "learning rate A :  tf.Tensor(9.981053e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1 is 3.8747 sec\n",
      "train AE loss : 0.023399699479341507, train ANN loss : 2.1980767250061035\n",
      "AE loss : 0.02229955978691578, ANN loss : 2.1967196464538574, Total loss : 4.426675319671631\n",
      "learning rate A :  tf.Tensor(9.961931e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 2 is 3.2348 sec\n",
      "train AE loss : 0.020238127559423447, train ANN loss : 2.1969857215881348\n",
      "AE loss : 0.014832168817520142, ANN loss : 2.196174144744873, Total loss : 3.6793909072875977\n",
      "learning rate A :  tf.Tensor(9.943266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 3 is 3.1669 sec\n",
      "train AE loss : 0.018882175907492638, train ANN loss : 2.1983726024627686\n",
      "AE loss : 0.022695466876029968, ANN loss : 2.198091745376587, Total loss : 4.4676384925842285\n",
      "learning rate A :  tf.Tensor(9.9223354e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 4 is 3.1231 sec\n",
      "train AE loss : 0.023541389033198357, train ANN loss : 2.1951088905334473\n",
      "AE loss : 0.02382645756006241, ANN loss : 2.196646213531494, Total loss : 4.579291820526123\n",
      "learning rate A :  tf.Tensor(9.9039535e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 5 is 3.1810 sec\n",
      "train AE loss : 0.023999642580747604, train ANN loss : 2.1949691772460938\n",
      "AE loss : 0.02359943650662899, ANN loss : 2.197826623916626, Total loss : 4.557770252227783\n",
      "learning rate A :  tf.Tensor(9.883314e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 6 is 3.2270 sec\n",
      "train AE loss : 0.021126842126250267, train ANN loss : 2.1935670375823975\n",
      "AE loss : 0.02095913328230381, ANN loss : 2.197497606277466, Total loss : 4.2934112548828125\n",
      "learning rate A :  tf.Tensor(9.863341e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 7 is 3.2242 sec\n",
      "train AE loss : 0.021722525358200073, train ANN loss : 2.1908936500549316\n",
      "AE loss : 0.019669393077492714, ANN loss : 2.1958367824554443, Total loss : 4.162776470184326\n",
      "learning rate A :  tf.Tensor(9.8436154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 8 is 3.2394 sec\n",
      "train AE loss : 0.021244533360004425, train ANN loss : 2.191169261932373\n",
      "AE loss : 0.018913691863417625, ANN loss : 2.1997058391571045, Total loss : 4.091075420379639\n",
      "learning rate A :  tf.Tensor(9.824966e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 9 is 3.2129 sec\n",
      "train AE loss : 0.01865878887474537, train ANN loss : 2.181809425354004\n",
      "AE loss : 0.01773604191839695, ANN loss : 2.2013492584228516, Total loss : 3.9749531745910645\n",
      "learning rate A :  tf.Tensor(9.807177e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 10 is 3.1828 sec\n",
      "train AE loss : 0.01852244883775711, train ANN loss : 2.1692616939544678\n",
      "AE loss : 0.018579786643385887, ANN loss : 2.1692190170288086, Total loss : 4.02719783782959\n",
      "learning rate A :  tf.Tensor(9.7875636e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 11 is 3.1787 sec\n",
      "train AE loss : 0.020910730585455894, train ANN loss : 2.1602678298950195\n",
      "AE loss : 0.02368081361055374, ANN loss : 2.1562845706939697, Total loss : 4.5243659019470215\n",
      "learning rate A :  tf.Tensor(9.770461e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 12 is 3.2721 sec\n",
      "train AE loss : 0.023950129747390747, train ANN loss : 2.1404383182525635\n",
      "AE loss : 0.025160731747746468, ANN loss : 2.126405954360962, Total loss : 4.642479419708252\n",
      "learning rate A :  tf.Tensor(9.751332e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 13 is 3.1749 sec\n",
      "train AE loss : 0.03130390867590904, train ANN loss : 2.137726306915283\n",
      "AE loss : 0.026019377633929253, ANN loss : 2.1160292625427246, Total loss : 4.717967510223389\n",
      "learning rate A :  tf.Tensor(9.734292e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 14 is 3.1808 sec\n",
      "train AE loss : 0.03048531524837017, train ANN loss : 2.135554790496826\n",
      "AE loss : 0.02739291824400425, ANN loss : 2.1228699684143066, Total loss : 4.862161636352539\n",
      "learning rate A :  tf.Tensor(9.718101e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 15 is 3.2336 sec\n",
      "train AE loss : 0.032513827085494995, train ANN loss : 2.104069471359253\n",
      "AE loss : 0.03618805482983589, ANN loss : 2.1014890670776367, Total loss : 5.720294952392578\n",
      "learning rate A :  tf.Tensor(9.70214e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 16 is 3.2450 sec\n",
      "train AE loss : 0.04146011546254158, train ANN loss : 2.0976662635803223\n",
      "AE loss : 0.04258851706981659, ANN loss : 2.0753390789031982, Total loss : 6.334190845489502\n",
      "learning rate A :  tf.Tensor(9.6809024e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 17 is 3.1868 sec\n",
      "train AE loss : 0.05848503112792969, train ANN loss : 2.110455274581909\n",
      "AE loss : 0.060948096215724945, ANN loss : 2.0802955627441406, Total loss : 8.175105094909668\n",
      "learning rate A :  tf.Tensor(9.663374e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 18 is 3.1880 sec\n",
      "train AE loss : 0.05772446468472481, train ANN loss : 2.1080589294433594\n",
      "AE loss : 0.04522677883505821, ANN loss : 2.0783679485321045, Total loss : 6.601045608520508\n",
      "learning rate A :  tf.Tensor(9.643439e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 19 is 3.2521 sec\n",
      "train AE loss : 0.053289372473955154, train ANN loss : 2.0779385566711426\n",
      "AE loss : 0.06931574642658234, ANN loss : 2.0947203636169434, Total loss : 9.026294708251953\n",
      "learning rate A :  tf.Tensor(9.625573e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 20 is 3.2550 sec\n",
      "train AE loss : 0.056419745087623596, train ANN loss : 2.076375722885132\n",
      "AE loss : 0.047488387674093246, ANN loss : 2.0578010082244873, Total loss : 6.806640148162842\n",
      "learning rate A :  tf.Tensor(9.607336e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 21 is 3.3113 sec\n",
      "train AE loss : 0.06064075976610184, train ANN loss : 2.073964834213257\n",
      "AE loss : 0.047461364418268204, ANN loss : 2.064683198928833, Total loss : 6.81082010269165\n",
      "learning rate A :  tf.Tensor(9.586305e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 22 is 3.2986 sec\n",
      "train AE loss : 0.0584375374019146, train ANN loss : 2.063839912414551\n",
      "AE loss : 0.04554193466901779, ANN loss : 2.043063163757324, Total loss : 6.597256183624268\n",
      "learning rate A :  tf.Tensor(9.5687465e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 23 is 3.3427 sec\n",
      "train AE loss : 0.05899687856435776, train ANN loss : 2.0596580505371094\n",
      "AE loss : 0.06119418144226074, ANN loss : 2.035637855529785, Total loss : 8.15505599975586\n",
      "learning rate A :  tf.Tensor(9.550214e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 24 is 3.2845 sec\n",
      "train AE loss : 0.05327192321419716, train ANN loss : 2.0935959815979004\n",
      "AE loss : 0.05993109196424484, ANN loss : 2.0222396850585938, Total loss : 8.015349388122559\n",
      "learning rate A :  tf.Tensor(9.531717e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 25 is 3.2833 sec\n",
      "train AE loss : 0.0545370876789093, train ANN loss : 2.068871259689331\n",
      "AE loss : 0.05170928314328194, ANN loss : 2.020040512084961, Total loss : 7.190969467163086\n",
      "learning rate A :  tf.Tensor(9.511453e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 26 is 3.2969 sec\n",
      "train AE loss : 0.06229047477245331, train ANN loss : 2.0428314208984375\n",
      "AE loss : 0.04641856998205185, ANN loss : 2.0168490409851074, Total loss : 6.658705711364746\n",
      "learning rate A :  tf.Tensor(9.496032e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 27 is 3.3367 sec\n",
      "train AE loss : 0.05634350702166557, train ANN loss : 2.0433552265167236\n",
      "AE loss : 0.060929376631975174, ANN loss : 2.0060362815856934, Total loss : 8.098974227905273\n",
      "learning rate A :  tf.Tensor(9.4760435e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 28 is 3.2862 sec\n",
      "train AE loss : 0.0719706118106842, train ANN loss : 2.0118372440338135\n",
      "AE loss : 0.057495489716529846, ANN loss : 1.9877479076385498, Total loss : 7.737296104431152\n",
      "learning rate A :  tf.Tensor(9.4590854e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 29 is 3.3308 sec\n",
      "train AE loss : 0.06062047928571701, train ANN loss : 2.0385830402374268\n",
      "AE loss : 0.05287439003586769, ANN loss : 1.991639494895935, Total loss : 7.279078483581543\n",
      "learning rate A :  tf.Tensor(9.439971e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 30 is 3.2979 sec\n",
      "train AE loss : 0.05805940553545952, train ANN loss : 2.0174105167388916\n",
      "AE loss : 0.044216010719537735, ANN loss : 1.9910939931869507, Total loss : 6.412695407867432\n",
      "learning rate A :  tf.Tensor(9.4199015e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 31 is 3.2974 sec\n",
      "train AE loss : 0.060803670436143875, train ANN loss : 1.991970181465149\n",
      "AE loss : 0.07141729444265366, ANN loss : 1.967355728149414, Total loss : 9.109085083007812\n",
      "learning rate A :  tf.Tensor(9.399874e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 32 is 3.2602 sec\n",
      "train AE loss : 0.06207127869129181, train ANN loss : 2.0128493309020996\n",
      "AE loss : 0.06123333424329758, ANN loss : 1.9639701843261719, Total loss : 8.08730411529541\n",
      "learning rate A :  tf.Tensor(9.3812734e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 33 is 3.2240 sec\n",
      "train AE loss : 0.05396956950426102, train ANN loss : 1.9937347173690796\n",
      "AE loss : 0.04413604736328125, ANN loss : 1.988060712814331, Total loss : 6.401665210723877\n",
      "learning rate A :  tf.Tensor(9.3613286e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 34 is 3.1768 sec\n",
      "train AE loss : 0.05393931642174721, train ANN loss : 1.9949954748153687\n",
      "AE loss : 0.04229729622602463, ANN loss : 1.9814282655715942, Total loss : 6.211158275604248\n",
      "learning rate A :  tf.Tensor(9.34497e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 35 is 3.3650 sec\n",
      "train AE loss : 0.052979353815317154, train ANN loss : 1.9932565689086914\n",
      "AE loss : 0.05108968913555145, ANN loss : 1.9617805480957031, Total loss : 7.070749282836914\n",
      "learning rate A :  tf.Tensor(9.327264e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 36 is 3.3207 sec\n",
      "train AE loss : 0.0650496706366539, train ANN loss : 1.9885040521621704\n",
      "AE loss : 0.05776936188340187, ANN loss : 1.9501646757125854, Total loss : 7.727100849151611\n",
      "learning rate A :  tf.Tensor(9.308415e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 37 is 3.2903 sec\n",
      "train AE loss : 0.05616747960448265, train ANN loss : 1.9977155923843384\n",
      "AE loss : 0.054882291704416275, ANN loss : 1.9535354375839233, Total loss : 7.4417643547058105\n",
      "learning rate A :  tf.Tensor(9.290582e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 38 is 3.2628 sec\n",
      "train AE loss : 0.05443163216114044, train ANN loss : 1.9792712926864624\n",
      "AE loss : 0.05819575861096382, ANN loss : 1.935192584991455, Total loss : 7.754768371582031\n",
      "learning rate A :  tf.Tensor(9.272394e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 39 is 3.2365 sec\n",
      "train AE loss : 0.06904366612434387, train ANN loss : 1.9775735139846802\n",
      "AE loss : 0.0651976466178894, ANN loss : 1.9154551029205322, Total loss : 8.435219764709473\n",
      "learning rate A :  tf.Tensor(9.25541e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 40 is 3.3145 sec\n",
      "train AE loss : 0.06631027162075043, train ANN loss : 1.9577685594558716\n",
      "AE loss : 0.061769045889377594, ANN loss : 1.95027756690979, Total loss : 8.127182006835938\n",
      "learning rate A :  tf.Tensor(9.235539e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 41 is 3.2461 sec\n",
      "train AE loss : 0.07377099990844727, train ANN loss : 1.944767713546753\n",
      "AE loss : 0.07351461797952652, ANN loss : 1.9341778755187988, Total loss : 9.285639762878418\n",
      "learning rate A :  tf.Tensor(9.215515e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 42 is 3.3242 sec\n",
      "train AE loss : 0.06025918945670128, train ANN loss : 1.9538837671279907\n",
      "AE loss : 0.05771384388208389, ANN loss : 1.9270594120025635, Total loss : 7.69844388961792\n",
      "learning rate A :  tf.Tensor(9.1970855e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 43 is 3.3007 sec\n",
      "train AE loss : 0.06701215356588364, train ANN loss : 1.9531841278076172\n",
      "AE loss : 0.09067953377962112, ANN loss : 1.9316143989562988, Total loss : 10.999567985534668\n",
      "learning rate A :  tf.Tensor(9.1773385e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 44 is 3.2934 sec\n",
      "train AE loss : 0.07726038247346878, train ANN loss : 1.9478827714920044\n",
      "AE loss : 0.09659402072429657, ANN loss : 1.8913177251815796, Total loss : 11.550719261169434\n",
      "learning rate A :  tf.Tensor(9.160916e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 45 is 3.3440 sec\n",
      "train AE loss : 0.08367150276899338, train ANN loss : 1.9361708164215088\n",
      "AE loss : 0.08897310495376587, ANN loss : 1.8784278631210327, Total loss : 10.775736808776855\n",
      "learning rate A :  tf.Tensor(9.142403e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 46 is 3.3211 sec\n",
      "train AE loss : 0.07601968944072723, train ANN loss : 1.94818115234375\n",
      "AE loss : 0.06646455824375153, ANN loss : 1.9032312631607056, Total loss : 8.549687385559082\n",
      "learning rate A :  tf.Tensor(9.124311e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 47 is 3.3524 sec\n",
      "train AE loss : 0.08011501282453537, train ANN loss : 1.9077166318893433\n",
      "AE loss : 0.08394508808851242, ANN loss : 1.8791182041168213, Total loss : 10.273626327514648\n",
      "learning rate A :  tf.Tensor(9.1058726e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 48 is 3.3240 sec\n",
      "train AE loss : 0.09430256485939026, train ANN loss : 1.9065083265304565\n",
      "AE loss : 0.09864963591098785, ANN loss : 1.8623712062835693, Total loss : 11.727334976196289\n",
      "learning rate A :  tf.Tensor(9.089003e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 49 is 3.3189 sec\n",
      "train AE loss : 0.09092686325311661, train ANN loss : 1.9204589128494263\n",
      "AE loss : 0.07074350118637085, ANN loss : 1.8915209770202637, Total loss : 8.96587085723877\n",
      "learning rate A :  tf.Tensor(9.071208e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 50 is 3.3346 sec\n",
      "train AE loss : 0.08365809917449951, train ANN loss : 1.9167929887771606\n",
      "AE loss : 0.09688850492238998, ANN loss : 1.8582797050476074, Total loss : 11.547130584716797\n",
      "learning rate A :  tf.Tensor(9.053067e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 51 is 3.3217 sec\n",
      "train AE loss : 0.09436635673046112, train ANN loss : 1.8956689834594727\n",
      "AE loss : 0.08814109861850739, ANN loss : 1.8575079441070557, Total loss : 10.67161750793457\n",
      "learning rate A :  tf.Tensor(9.036104e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 52 is 3.3229 sec\n",
      "train AE loss : 0.08018644899129868, train ANN loss : 1.9204895496368408\n",
      "AE loss : 0.09162044525146484, ANN loss : 1.8504477739334106, Total loss : 11.012492179870605\n",
      "learning rate A :  tf.Tensor(9.0167036e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 53 is 3.3429 sec\n",
      "train AE loss : 0.10769619047641754, train ANN loss : 1.8946527242660522\n",
      "AE loss : 0.10970980674028397, ANN loss : 1.8563374280929565, Total loss : 12.82731819152832\n",
      "learning rate A :  tf.Tensor(8.9971545e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 54 is 3.3335 sec\n",
      "train AE loss : 0.11300523579120636, train ANN loss : 1.9289231300354004\n",
      "AE loss : 0.10270817577838898, ANN loss : 1.8606067895889282, Total loss : 12.131423950195312\n",
      "learning rate A :  tf.Tensor(8.979351e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 55 is 3.3518 sec\n",
      "train AE loss : 0.1155259981751442, train ANN loss : 1.9034104347229004\n",
      "AE loss : 0.1327444165945053, ANN loss : 1.8538094758987427, Total loss : 15.128251075744629\n",
      "learning rate A :  tf.Tensor(8.959884e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 56 is 3.3328 sec\n",
      "train AE loss : 0.11692465096712112, train ANN loss : 1.8930697441101074\n",
      "AE loss : 0.10091572999954224, ANN loss : 1.8383640050888062, Total loss : 11.929937362670898\n",
      "learning rate A :  tf.Tensor(8.94253e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 57 is 3.3255 sec\n",
      "train AE loss : 0.1292310655117035, train ANN loss : 1.8958549499511719\n",
      "AE loss : 0.15395022928714752, ANN loss : 1.8665752410888672, Total loss : 17.261598587036133\n",
      "learning rate A :  tf.Tensor(8.9248344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 58 is 3.3643 sec\n",
      "train AE loss : 0.10107652097940445, train ANN loss : 1.8968532085418701\n",
      "AE loss : 0.09035451710224152, ANN loss : 1.8680362701416016, Total loss : 10.903489112854004\n",
      "learning rate A :  tf.Tensor(8.907925e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 59 is 3.3068 sec\n",
      "train AE loss : 0.09612661600112915, train ANN loss : 1.8789234161376953\n",
      "AE loss : 0.0878213420510292, ANN loss : 1.88808274269104, Total loss : 10.670215606689453\n",
      "learning rate A :  tf.Tensor(8.8899236e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 60 is 3.3322 sec\n",
      "train AE loss : 0.10679248720407486, train ANN loss : 1.8814313411712646\n",
      "AE loss : 0.11470313370227814, ANN loss : 1.8336199522018433, Total loss : 13.303933143615723\n",
      "learning rate A :  tf.Tensor(8.871023e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 61 is 3.2285 sec\n",
      "train AE loss : 0.11067213118076324, train ANN loss : 1.9066804647445679\n",
      "AE loss : 0.08497845381498337, ANN loss : 1.8515055179595947, Total loss : 10.34935188293457\n",
      "learning rate A :  tf.Tensor(8.851603e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 62 is 3.2970 sec\n",
      "train AE loss : 0.10207247734069824, train ANN loss : 1.8859717845916748\n",
      "AE loss : 0.09159883111715317, ANN loss : 1.8604910373687744, Total loss : 11.020374298095703\n",
      "learning rate A :  tf.Tensor(8.836322e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 63 is 3.3251 sec\n",
      "train AE loss : 0.13154613971710205, train ANN loss : 1.8851149082183838\n",
      "AE loss : 0.14823925495147705, ANN loss : 1.844649076461792, Total loss : 16.668575286865234\n",
      "learning rate A :  tf.Tensor(8.816792e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 64 is 3.3336 sec\n",
      "train AE loss : 0.13502958416938782, train ANN loss : 1.9158470630645752\n",
      "AE loss : 0.13918933272361755, ANN loss : 1.83210289478302, Total loss : 15.751036643981934\n",
      "learning rate A :  tf.Tensor(8.8008295e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 65 is 3.2874 sec\n",
      "train AE loss : 0.11658015847206116, train ANN loss : 1.8774833679199219\n",
      "AE loss : 0.11365045607089996, ANN loss : 1.8304678201675415, Total loss : 13.195512771606445\n",
      "learning rate A :  tf.Tensor(8.7835986e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 66 is 3.2981 sec\n",
      "train AE loss : 0.11826518177986145, train ANN loss : 1.8752186298370361\n",
      "AE loss : 0.11437369883060455, ANN loss : 1.818512201309204, Total loss : 13.255882263183594\n",
      "learning rate A :  tf.Tensor(8.7652945e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 67 is 3.3137 sec\n",
      "train AE loss : 0.12813608348369598, train ANN loss : 1.8414300680160522\n",
      "AE loss : 0.1084364727139473, ANN loss : 1.833867073059082, Total loss : 12.677515029907227\n",
      "learning rate A :  tf.Tensor(8.749424e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 68 is 3.3170 sec\n",
      "train AE loss : 0.13174103200435638, train ANN loss : 1.8565082550048828\n",
      "AE loss : 0.12324585765600204, ANN loss : 1.8165364265441895, Total loss : 14.141121864318848\n",
      "learning rate A :  tf.Tensor(8.732111e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 69 is 3.3324 sec\n",
      "train AE loss : 0.12992678582668304, train ANN loss : 1.8647596836090088\n",
      "AE loss : 0.11800216883420944, ANN loss : 1.8188718557357788, Total loss : 13.619088172912598\n",
      "learning rate A :  tf.Tensor(8.7128115e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 70 is 3.3239 sec\n",
      "train AE loss : 0.14922887086868286, train ANN loss : 1.8891255855560303\n",
      "AE loss : 0.13078884780406952, ANN loss : 1.8185710906982422, Total loss : 14.897456169128418\n",
      "learning rate A :  tf.Tensor(8.696853e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 71 is 3.2809 sec\n",
      "train AE loss : 0.14171861112117767, train ANN loss : 1.8555831909179688\n",
      "AE loss : 0.1468302309513092, ANN loss : 1.7984895706176758, Total loss : 16.48151397705078\n",
      "learning rate A :  tf.Tensor(8.680375e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 72 is 3.3284 sec\n",
      "train AE loss : 0.13848690688610077, train ANN loss : 1.8620328903198242\n",
      "AE loss : 0.13411979377269745, ANN loss : 1.8071424961090088, Total loss : 15.219122886657715\n",
      "learning rate A :  tf.Tensor(8.662104e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 73 is 3.3095 sec\n",
      "train AE loss : 0.1767147183418274, train ANN loss : 1.8338284492492676\n",
      "AE loss : 0.1672981083393097, ANN loss : 1.7939139604568481, Total loss : 18.523723602294922\n",
      "learning rate A :  tf.Tensor(8.646238e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 74 is 3.3368 sec\n",
      "train AE loss : 0.17857275903224945, train ANN loss : 1.8336559534072876\n",
      "AE loss : 0.16176605224609375, ANN loss : 1.8079651594161987, Total loss : 17.98457145690918\n",
      "learning rate A :  tf.Tensor(8.628947e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 75 is 3.2593 sec\n",
      "train AE loss : 0.16700561344623566, train ANN loss : 1.8317861557006836\n",
      "AE loss : 0.15147754549980164, ANN loss : 1.8020257949829102, Total loss : 16.949779510498047\n",
      "learning rate A :  tf.Tensor(8.6125976e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 76 is 3.2176 sec\n",
      "train AE loss : 0.14384624361991882, train ANN loss : 1.8877230882644653\n",
      "AE loss : 0.12930187582969666, ANN loss : 1.8061821460723877, Total loss : 14.736370086669922\n",
      "learning rate A :  tf.Tensor(8.59646e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 77 is 3.2317 sec\n",
      "train AE loss : 0.1478227972984314, train ANN loss : 1.8415117263793945\n",
      "AE loss : 0.17230187356472015, ANN loss : 1.7771234512329102, Total loss : 19.007308959960938\n",
      "learning rate A :  tf.Tensor(8.578908e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 78 is 3.2942 sec\n",
      "train AE loss : 0.1844896525144577, train ANN loss : 1.8294341564178467\n",
      "AE loss : 0.13811607658863068, ANN loss : 1.7997548580169678, Total loss : 15.61136245727539\n",
      "learning rate A :  tf.Tensor(8.562472e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 79 is 3.2917 sec\n",
      "train AE loss : 0.15903154015541077, train ANN loss : 1.815018892288208\n",
      "AE loss : 0.16666194796562195, ANN loss : 1.7713474035263062, Total loss : 18.437541961669922\n",
      "learning rate A :  tf.Tensor(8.5451684e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 80 is 3.3421 sec\n",
      "train AE loss : 0.19983729720115662, train ANN loss : 1.8391872644424438\n",
      "AE loss : 0.23653991520404816, ANN loss : 1.8357956409454346, Total loss : 25.489788055419922\n",
      "learning rate A :  tf.Tensor(8.527001e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 81 is 3.3334 sec\n",
      "train AE loss : 0.20102982223033905, train ANN loss : 1.8563333749771118\n",
      "AE loss : 0.17052659392356873, ANN loss : 1.7691045999526978, Total loss : 18.82176399230957\n",
      "learning rate A :  tf.Tensor(8.5099484e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 82 is 3.3372 sec\n",
      "train AE loss : 0.18167610466480255, train ANN loss : 1.792446255683899\n",
      "AE loss : 0.13105405867099762, ANN loss : 1.7761051654815674, Total loss : 14.881511688232422\n",
      "learning rate A :  tf.Tensor(8.493646e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 83 is 3.1977 sec\n",
      "train AE loss : 0.16406898200511932, train ANN loss : 1.8086564540863037\n",
      "AE loss : 0.1396477222442627, ANN loss : 1.7883148193359375, Total loss : 15.753087043762207\n",
      "learning rate A :  tf.Tensor(8.47541e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 84 is 3.1793 sec\n",
      "train AE loss : 0.18833523988723755, train ANN loss : 1.8295369148254395\n",
      "AE loss : 0.1037614643573761, ANN loss : 1.8099192380905151, Total loss : 12.186064720153809\n",
      "learning rate A :  tf.Tensor(8.459708e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 85 is 3.2287 sec\n",
      "train AE loss : 0.14869454503059387, train ANN loss : 1.8458393812179565\n",
      "AE loss : 0.18043763935565948, ANN loss : 1.7840689420700073, Total loss : 19.82783317565918\n",
      "learning rate A :  tf.Tensor(8.443679e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 86 is 3.3193 sec\n",
      "train AE loss : 0.2048332542181015, train ANN loss : 1.797047734260559\n",
      "AE loss : 0.22648192942142487, ANN loss : 1.8299672603607178, Total loss : 24.478158950805664\n",
      "learning rate A :  tf.Tensor(8.427504e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 87 is 3.3299 sec\n",
      "train AE loss : 0.20261412858963013, train ANN loss : 1.8326677083969116\n",
      "AE loss : 0.17769023776054382, ANN loss : 1.7498202323913574, Total loss : 19.518842697143555\n",
      "learning rate A :  tf.Tensor(8.4136635e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 88 is 3.3568 sec\n",
      "train AE loss : 0.2200009524822235, train ANN loss : 1.7986528873443604\n",
      "AE loss : 0.2176847904920578, ANN loss : 1.7784122228622437, Total loss : 23.546892166137695\n",
      "learning rate A :  tf.Tensor(8.39843e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 89 is 3.3603 sec\n",
      "train AE loss : 0.20861577987670898, train ANN loss : 1.7733924388885498\n",
      "AE loss : 0.16030600666999817, ANN loss : 1.7513391971588135, Total loss : 17.781940460205078\n",
      "learning rate A :  tf.Tensor(8.383048e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 90 is 3.3047 sec\n",
      "train AE loss : 0.19594864547252655, train ANN loss : 1.7964386940002441\n",
      "AE loss : 0.1741524040699005, ANN loss : 1.7477463483810425, Total loss : 19.162988662719727\n",
      "learning rate A :  tf.Tensor(8.3676925e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 91 is 3.3235 sec\n",
      "train AE loss : 0.21857325732707977, train ANN loss : 1.7869166135787964\n",
      "AE loss : 0.1973104625940323, ANN loss : 1.7350926399230957, Total loss : 21.466136932373047\n",
      "learning rate A :  tf.Tensor(8.352014e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 92 is 3.3244 sec\n",
      "train AE loss : 0.20859381556510925, train ANN loss : 1.7917617559432983\n",
      "AE loss : 0.16709205508232117, ANN loss : 1.7277270555496216, Total loss : 18.436933517456055\n",
      "learning rate A :  tf.Tensor(8.3347855e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 93 is 3.3238 sec\n",
      "train AE loss : 0.19936148822307587, train ANN loss : 1.7881381511688232\n",
      "AE loss : 0.2046467810869217, ANN loss : 1.7419184446334839, Total loss : 22.20659637451172\n",
      "learning rate A :  tf.Tensor(8.317941e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 94 is 3.3537 sec\n",
      "train AE loss : 0.22686301171779633, train ANN loss : 1.7697392702102661\n",
      "AE loss : 0.22819437086582184, ANN loss : 1.7176140546798706, Total loss : 24.537052154541016\n",
      "learning rate A :  tf.Tensor(8.300432e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 95 is 3.3209 sec\n",
      "train AE loss : 0.22178605198860168, train ANN loss : 1.788133144378662\n",
      "AE loss : 0.18196038901805878, ANN loss : 1.7210010290145874, Total loss : 19.91703987121582\n",
      "learning rate A :  tf.Tensor(8.283833e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 96 is 3.3848 sec\n",
      "train AE loss : 0.20960506796836853, train ANN loss : 1.759358525276184\n",
      "AE loss : 0.19577154517173767, ANN loss : 1.7333402633666992, Total loss : 21.31049346923828\n",
      "learning rate A :  tf.Tensor(8.268312e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 97 is 3.2901 sec\n",
      "train AE loss : 0.20911537110805511, train ANN loss : 1.761443018913269\n",
      "AE loss : 0.15632036328315735, ANN loss : 1.7372229099273682, Total loss : 17.369258880615234\n",
      "learning rate A :  tf.Tensor(8.2509076e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 98 is 3.2601 sec\n",
      "train AE loss : 0.2034047394990921, train ANN loss : 1.7589120864868164\n",
      "AE loss : 0.18162581324577332, ANN loss : 1.6965769529342651, Total loss : 19.859155654907227\n",
      "learning rate A :  tf.Tensor(8.234927e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 99 is 3.2912 sec\n",
      "train AE loss : 0.20037345588207245, train ANN loss : 1.778876781463623\n",
      "AE loss : 0.1760530322790146, ANN loss : 1.713127851486206, Total loss : 19.318431854248047\n",
      "learning rate A :  tf.Tensor(8.218978e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 100 is 3.2924 sec\n",
      "train AE loss : 0.17482224106788635, train ANN loss : 1.7738792896270752\n",
      "AE loss : 0.11303162574768066, ANN loss : 1.7678171396255493, Total loss : 13.0709810256958\n",
      "learning rate A :  tf.Tensor(8.201677e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 101 is 3.2932 sec\n",
      "train AE loss : 0.1847756952047348, train ANN loss : 1.7612013816833496\n",
      "AE loss : 0.15776988863945007, ANN loss : 1.7316951751708984, Total loss : 17.508684158325195\n",
      "learning rate A :  tf.Tensor(8.185965e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 102 is 3.2985 sec\n",
      "train AE loss : 0.22211609780788422, train ANN loss : 1.7348971366882324\n",
      "AE loss : 0.20560979843139648, ANN loss : 1.7030434608459473, Total loss : 22.264022827148438\n",
      "learning rate A :  tf.Tensor(8.1692495e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 103 is 3.3359 sec\n",
      "train AE loss : 0.23538443446159363, train ANN loss : 1.757002592086792\n",
      "AE loss : 0.1882535070180893, ANN loss : 1.7275818586349487, Total loss : 20.55293083190918\n",
      "learning rate A :  tf.Tensor(8.152225e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 104 is 3.3252 sec\n",
      "train AE loss : 0.21726438403129578, train ANN loss : 1.758651614189148\n",
      "AE loss : 0.22523313760757446, ANN loss : 1.7220431566238403, Total loss : 24.245357513427734\n",
      "learning rate A :  tf.Tensor(8.1333514e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 105 is 3.2828 sec\n",
      "train AE loss : 0.23190461099147797, train ANN loss : 1.7843363285064697\n",
      "AE loss : 0.17404447495937347, ANN loss : 1.7350157499313354, Total loss : 19.139463424682617\n",
      "learning rate A :  tf.Tensor(8.1153754e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 106 is 3.5086 sec\n",
      "train AE loss : 0.2199728786945343, train ANN loss : 1.78386390209198\n",
      "AE loss : 0.18816985189914703, ANN loss : 1.7027792930603027, Total loss : 20.519765853881836\n",
      "learning rate A :  tf.Tensor(8.0988044e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 107 is 3.3322 sec\n",
      "train AE loss : 0.22844171524047852, train ANN loss : 1.7256401777267456\n",
      "AE loss : 0.16253139078617096, ANN loss : 1.731980800628662, Total loss : 17.98512077331543\n",
      "learning rate A :  tf.Tensor(8.083801e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 108 is 3.3213 sec\n",
      "train AE loss : 0.22800712287425995, train ANN loss : 1.7177295684814453\n",
      "AE loss : 0.19391535222530365, ANN loss : 1.693002462387085, Total loss : 21.084537506103516\n",
      "learning rate A :  tf.Tensor(8.069504e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 109 is 3.3284 sec\n",
      "train AE loss : 0.23411422967910767, train ANN loss : 1.7188584804534912\n",
      "AE loss : 0.2060018628835678, ANN loss : 1.6618385314941406, Total loss : 22.26202392578125\n",
      "learning rate A :  tf.Tensor(8.054894e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 110 is 3.2705 sec\n",
      "train AE loss : 0.2213699072599411, train ANN loss : 1.750406265258789\n",
      "AE loss : 0.17504741251468658, ANN loss : 1.6849192380905151, Total loss : 19.189661026000977\n",
      "learning rate A :  tf.Tensor(8.03743e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 111 is 3.3540 sec\n",
      "train AE loss : 0.21082907915115356, train ANN loss : 1.7354613542556763\n",
      "AE loss : 0.18622714281082153, ANN loss : 1.678523302078247, Total loss : 20.301237106323242\n",
      "learning rate A :  tf.Tensor(8.022371e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 112 is 3.2759 sec\n",
      "train AE loss : 0.19869530200958252, train ANN loss : 1.755475401878357\n",
      "AE loss : 0.1972038298845291, ANN loss : 1.7056249380111694, Total loss : 21.426008224487305\n",
      "learning rate A :  tf.Tensor(8.006834e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 113 is 3.3091 sec\n",
      "train AE loss : 0.24769903719425201, train ANN loss : 1.7335742712020874\n",
      "AE loss : 0.20075511932373047, ANN loss : 1.7054139375686646, Total loss : 21.780925750732422\n",
      "learning rate A :  tf.Tensor(7.9906524e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 114 is 3.3323 sec\n",
      "train AE loss : 0.22530463337898254, train ANN loss : 1.744512915611267\n",
      "AE loss : 0.18069812655448914, ANN loss : 1.6896682977676392, Total loss : 19.759479522705078\n",
      "learning rate A :  tf.Tensor(7.974505e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 115 is 3.3007 sec\n",
      "train AE loss : 0.21531717479228973, train ANN loss : 1.7340292930603027\n",
      "AE loss : 0.19481582939624786, ANN loss : 1.6996686458587646, Total loss : 21.181249618530273\n",
      "learning rate A :  tf.Tensor(7.9593956e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 116 is 3.3218 sec\n",
      "train AE loss : 0.19558486342430115, train ANN loss : 1.7313529253005981\n",
      "AE loss : 0.19671310484409332, ANN loss : 1.6738433837890625, Total loss : 21.34515380859375\n",
      "learning rate A :  tf.Tensor(7.94398e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 117 is 3.3145 sec\n",
      "train AE loss : 0.21246172487735748, train ANN loss : 1.7278692722320557\n",
      "AE loss : 0.17128287255764008, ANN loss : 1.6565191745758057, Total loss : 18.784805297851562\n",
      "learning rate A :  tf.Tensor(7.9309335e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 118 is 3.3486 sec\n",
      "train AE loss : 0.21777206659317017, train ANN loss : 1.705541968345642\n",
      "AE loss : 0.2146703451871872, ANN loss : 1.6473822593688965, Total loss : 23.114416122436523\n",
      "learning rate A :  tf.Tensor(7.9157406e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 119 is 3.3521 sec\n",
      "train AE loss : 0.21281923353672028, train ANN loss : 1.721908688545227\n",
      "AE loss : 0.1527993530035019, ANN loss : 1.8030450344085693, Total loss : 17.082979202270508\n",
      "learning rate A :  tf.Tensor(7.901908e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 120 is 3.3197 sec\n",
      "train AE loss : 0.168894425034523, train ANN loss : 1.7573208808898926\n",
      "AE loss : 0.19995136559009552, ANN loss : 1.74538254737854, Total loss : 21.74051856994629\n",
      "learning rate A :  tf.Tensor(7.8862715e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 121 is 3.2860 sec\n",
      "train AE loss : 0.22183102369308472, train ANN loss : 1.700188159942627\n",
      "AE loss : 0.23129937052726746, ANN loss : 1.6852874755859375, Total loss : 24.815221786499023\n",
      "learning rate A :  tf.Tensor(7.869505e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 122 is 3.2049 sec\n",
      "train AE loss : 0.2562924027442932, train ANN loss : 1.737898588180542\n",
      "AE loss : 0.24841274321079254, ANN loss : 1.6373348236083984, Total loss : 26.478609085083008\n",
      "learning rate A :  tf.Tensor(7.8534365e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 123 is 3.2676 sec\n",
      "train AE loss : 0.27583298087120056, train ANN loss : 1.7083230018615723\n",
      "AE loss : 0.2229621559381485, ANN loss : 1.6438349485397339, Total loss : 23.94005012512207\n",
      "learning rate A :  tf.Tensor(7.837069e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 124 is 3.2920 sec\n",
      "train AE loss : 0.30054140090942383, train ANN loss : 1.6923459768295288\n",
      "AE loss : 0.24411475658416748, ANN loss : 1.6402250528335571, Total loss : 26.051700592041016\n",
      "learning rate A :  tf.Tensor(7.820738e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 125 is 3.2888 sec\n",
      "train AE loss : 0.2627476751804352, train ANN loss : 1.6827409267425537\n",
      "AE loss : 0.2359236776828766, ANN loss : 1.6536520719528198, Total loss : 25.246017456054688\n",
      "learning rate A :  tf.Tensor(7.8041114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 126 is 3.2736 sec\n",
      "train AE loss : 0.26231977343559265, train ANN loss : 1.6838735342025757\n",
      "AE loss : 0.2208789885044098, ANN loss : 1.6455761194229126, Total loss : 23.733474731445312\n",
      "learning rate A :  tf.Tensor(7.787027e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 127 is 3.2158 sec\n",
      "train AE loss : 0.22707539796829224, train ANN loss : 1.7350250482559204\n",
      "AE loss : 0.17895765602588654, ANN loss : 1.6538113355636597, Total loss : 19.549575805664062\n",
      "learning rate A :  tf.Tensor(7.7727644e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 128 is 3.2347 sec\n",
      "train AE loss : 0.2684762477874756, train ANN loss : 1.6983506679534912\n",
      "AE loss : 0.23184643685817719, ANN loss : 1.6431132555007935, Total loss : 24.827754974365234\n",
      "learning rate A :  tf.Tensor(7.758201e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 129 is 3.3096 sec\n",
      "train AE loss : 0.16961944103240967, train ANN loss : 1.804918885231018\n",
      "AE loss : 0.09878205507993698, ANN loss : 1.719849705696106, Total loss : 11.598053932189941\n",
      "learning rate A :  tf.Tensor(7.743175e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 130 is 3.2844 sec\n",
      "train AE loss : 0.15120060741901398, train ANN loss : 1.7152855396270752\n",
      "AE loss : 0.15854795277118683, ANN loss : 1.6584409475326538, Total loss : 17.51323699951172\n",
      "learning rate A :  tf.Tensor(7.726713e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 131 is 3.2789 sec\n",
      "train AE loss : 0.25158247351646423, train ANN loss : 1.6834675073623657\n",
      "AE loss : 0.2517337501049042, ANN loss : 1.6398475170135498, Total loss : 26.813220977783203\n",
      "learning rate A :  tf.Tensor(7.712398e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 132 is 3.2080 sec\n",
      "train AE loss : 0.28358861804008484, train ANN loss : 1.6758251190185547\n",
      "AE loss : 0.3060309588909149, ANN loss : 1.619516134262085, Total loss : 32.22261428833008\n",
      "learning rate A :  tf.Tensor(7.6971366e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 133 is 3.2362 sec\n",
      "train AE loss : 0.2863229513168335, train ANN loss : 1.67453134059906\n",
      "AE loss : 0.21996217966079712, ANN loss : 1.6657357215881348, Total loss : 23.661951065063477\n",
      "learning rate A :  tf.Tensor(7.680934e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 134 is 3.2375 sec\n",
      "train AE loss : 0.27842941880226135, train ANN loss : 1.675835132598877\n",
      "AE loss : 0.25769418478012085, ANN loss : 1.6326894760131836, Total loss : 27.402109146118164\n",
      "learning rate A :  tf.Tensor(7.66735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 135 is 3.7239 sec\n",
      "train AE loss : 0.30466488003730774, train ANN loss : 1.6557443141937256\n",
      "AE loss : 0.2675189971923828, ANN loss : 1.7395422458648682, Total loss : 28.49144172668457\n",
      "learning rate A :  tf.Tensor(7.6531454e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 136 is 3.2829 sec\n",
      "train AE loss : 0.28351953625679016, train ANN loss : 1.6854435205459595\n",
      "AE loss : 0.25303876399993896, ANN loss : 1.6534819602966309, Total loss : 26.957361221313477\n",
      "learning rate A :  tf.Tensor(7.637357e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 137 is 3.2746 sec\n",
      "train AE loss : 0.32063028216362, train ANN loss : 1.6653058528900146\n",
      "AE loss : 0.2843756377696991, ANN loss : 1.6279690265655518, Total loss : 30.065532684326172\n",
      "learning rate A :  tf.Tensor(7.6217635e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 138 is 3.1738 sec\n",
      "train AE loss : 0.2812950909137726, train ANN loss : 1.7020235061645508\n",
      "AE loss : 0.26973146200180054, ANN loss : 1.6362119913101196, Total loss : 28.609355926513672\n",
      "learning rate A :  tf.Tensor(7.607643e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 139 is 3.3045 sec\n",
      "train AE loss : 0.2874350845813751, train ANN loss : 1.659595012664795\n",
      "AE loss : 0.2006331980228424, ANN loss : 1.648983120918274, Total loss : 21.712303161621094\n",
      "learning rate A :  tf.Tensor(7.590508e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 140 is 3.2673 sec\n",
      "train AE loss : 0.28694742918014526, train ANN loss : 1.6365689039230347\n",
      "AE loss : 0.26237067580223083, ANN loss : 1.6297144889831543, Total loss : 27.86678123474121\n",
      "learning rate A :  tf.Tensor(7.5770855e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 141 is 3.2317 sec\n",
      "train AE loss : 0.28642478585243225, train ANN loss : 1.6871676445007324\n",
      "AE loss : 0.26565080881118774, ANN loss : 1.6168525218963623, Total loss : 28.18193244934082\n",
      "learning rate A :  tf.Tensor(7.560976e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 142 is 3.2616 sec\n",
      "train AE loss : 0.3100036382675171, train ANN loss : 1.6438004970550537\n",
      "AE loss : 0.24730592966079712, ANN loss : 1.6161144971847534, Total loss : 26.34670639038086\n",
      "learning rate A :  tf.Tensor(7.544742e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 143 is 3.2753 sec\n",
      "train AE loss : 0.2776135802268982, train ANN loss : 1.6485323905944824\n",
      "AE loss : 0.27882495522499084, ANN loss : 1.7427972555160522, Total loss : 29.62529182434082\n",
      "learning rate A :  tf.Tensor(7.531241e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 144 is 3.3077 sec\n",
      "train AE loss : 0.27386772632598877, train ANN loss : 1.6788712739944458\n",
      "AE loss : 0.22118385136127472, ANN loss : 1.6600673198699951, Total loss : 23.778451919555664\n",
      "learning rate A :  tf.Tensor(7.515863e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 145 is 3.2683 sec\n",
      "train AE loss : 0.24436722695827484, train ANN loss : 1.6722439527511597\n",
      "AE loss : 0.245853453874588, ANN loss : 1.6213003396987915, Total loss : 26.206645965576172\n",
      "learning rate A :  tf.Tensor(7.500833e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 146 is 3.2956 sec\n",
      "train AE loss : 0.2785505950450897, train ANN loss : 1.6789432764053345\n",
      "AE loss : 0.26459112763404846, ANN loss : 1.6943174600601196, Total loss : 28.153427124023438\n",
      "learning rate A :  tf.Tensor(7.485043e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 147 is 3.2054 sec\n",
      "train AE loss : 0.23499827086925507, train ANN loss : 1.6648123264312744\n",
      "AE loss : 0.25490328669548035, ANN loss : 1.6684329509735107, Total loss : 27.158761978149414\n",
      "learning rate A :  tf.Tensor(7.469129e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 148 is 3.2876 sec\n",
      "train AE loss : 0.29612085223197937, train ANN loss : 1.6460729837417603\n",
      "AE loss : 0.21176689863204956, ANN loss : 1.6523864269256592, Total loss : 22.82907485961914\n",
      "learning rate A :  tf.Tensor(7.454663e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 149 is 3.3004 sec\n",
      "train AE loss : 0.2179059237241745, train ANN loss : 1.6895315647125244\n",
      "AE loss : 0.1795421540737152, ANN loss : 1.7047456502914429, Total loss : 19.658960342407227\n",
      "learning rate A :  tf.Tensor(7.441167e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 150 is 3.3020 sec\n",
      "train AE loss : 0.2639404237270355, train ANN loss : 1.6639517545700073\n",
      "AE loss : 0.27393975853919983, ANN loss : 1.617862582206726, Total loss : 29.011838912963867\n",
      "learning rate A :  tf.Tensor(7.424408e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 151 is 3.3103 sec\n",
      "train AE loss : 0.33326223492622375, train ANN loss : 1.6573392152786255\n",
      "AE loss : 0.2383784055709839, ANN loss : 1.6701449155807495, Total loss : 25.507986068725586\n",
      "learning rate A :  tf.Tensor(7.409092e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 152 is 3.3102 sec\n",
      "train AE loss : 0.16528773307800293, train ANN loss : 1.6987075805664062\n",
      "AE loss : 0.17359371483325958, ANN loss : 1.6991560459136963, Total loss : 19.05852699279785\n",
      "learning rate A :  tf.Tensor(7.395054e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 153 is 3.3218 sec\n",
      "train AE loss : 0.2618250846862793, train ANN loss : 1.7518486976623535\n",
      "AE loss : 0.2064451277256012, ANN loss : 1.6859647035598755, Total loss : 22.330476760864258\n",
      "learning rate A :  tf.Tensor(7.381042e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 154 is 3.3208 sec\n",
      "train AE loss : 0.24753797054290771, train ANN loss : 1.645841360092163\n",
      "AE loss : 0.2090730220079422, ANN loss : 1.6218093633651733, Total loss : 22.529109954833984\n",
      "learning rate A :  tf.Tensor(7.363644e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 155 is 3.2498 sec\n",
      "train AE loss : 0.23974154889583588, train ANN loss : 1.6694408655166626\n",
      "AE loss : 0.24454696476459503, ANN loss : 1.6834492683410645, Total loss : 26.138147354125977\n",
      "learning rate A :  tf.Tensor(7.3482974e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 156 is 3.2903 sec\n",
      "train AE loss : 0.264432430267334, train ANN loss : 1.6586723327636719\n",
      "AE loss : 0.1683218777179718, ANN loss : 1.660857081413269, Total loss : 18.493043899536133\n",
      "learning rate A :  tf.Tensor(7.3351475e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 157 is 3.2663 sec\n",
      "train AE loss : 0.20932762324810028, train ANN loss : 1.6876202821731567\n",
      "AE loss : 0.19435237348079681, ANN loss : 1.6276456117630005, Total loss : 21.062885284423828\n",
      "learning rate A :  tf.Tensor(7.319399e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 158 is 3.2205 sec\n",
      "train AE loss : 0.24051545560359955, train ANN loss : 1.6308225393295288\n",
      "AE loss : 0.2032044231891632, ANN loss : 1.6068764925003052, Total loss : 21.927318572998047\n",
      "learning rate A :  tf.Tensor(7.305222e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 159 is 3.2633 sec\n",
      "train AE loss : 0.23983480036258698, train ANN loss : 1.6406335830688477\n",
      "AE loss : 0.21733640134334564, ANN loss : 1.6574684381484985, Total loss : 23.391109466552734\n",
      "learning rate A :  tf.Tensor(7.290153e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 160 is 3.2678 sec\n",
      "train AE loss : 0.24063469469547272, train ANN loss : 1.7108725309371948\n",
      "AE loss : 0.19107531011104584, ANN loss : 1.6846681833267212, Total loss : 20.792200088500977\n",
      "learning rate A :  tf.Tensor(7.275727e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 161 is 3.2494 sec\n",
      "train AE loss : 0.24046987295150757, train ANN loss : 1.7103602886199951\n",
      "AE loss : 0.19680972397327423, ANN loss : 1.733891248703003, Total loss : 21.414865493774414\n",
      "learning rate A :  tf.Tensor(7.261023e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 162 is 3.2721 sec\n",
      "train AE loss : 0.23716850578784943, train ANN loss : 1.6527196168899536\n",
      "AE loss : 0.20728422701358795, ANN loss : 1.6405439376831055, Total loss : 22.368967056274414\n",
      "learning rate A :  tf.Tensor(7.246502e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 163 is 3.2164 sec\n",
      "train AE loss : 0.2512652575969696, train ANN loss : 1.6382285356521606\n",
      "AE loss : 0.22581444680690765, ANN loss : 1.599195957183838, Total loss : 24.18064308166504\n",
      "learning rate A :  tf.Tensor(7.231249e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 164 is 3.2385 sec\n",
      "train AE loss : 0.2652904689311981, train ANN loss : 1.6249198913574219\n",
      "AE loss : 0.23003710806369781, ANN loss : 1.6034390926361084, Total loss : 24.607149124145508\n",
      "learning rate A :  tf.Tensor(7.2170915e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 165 is 3.2283 sec\n",
      "train AE loss : 0.31182584166526794, train ANN loss : 1.6297804117202759\n",
      "AE loss : 0.22900155186653137, ANN loss : 1.5671888589859009, Total loss : 24.467344284057617\n",
      "learning rate A :  tf.Tensor(7.200382e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 166 is 3.2406 sec\n",
      "train AE loss : 0.2770112454891205, train ANN loss : 1.6428190469741821\n",
      "AE loss : 0.23481668531894684, ANN loss : 1.5874981880187988, Total loss : 25.06916618347168\n",
      "learning rate A :  tf.Tensor(7.185226e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 167 is 3.2512 sec\n",
      "train AE loss : 0.2652603089809418, train ANN loss : 1.6410828828811646\n",
      "AE loss : 0.2701641321182251, ANN loss : 1.589465856552124, Total loss : 28.605878829956055\n",
      "learning rate A :  tf.Tensor(7.1726696e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 168 is 3.2401 sec\n",
      "train AE loss : 0.2771000564098358, train ANN loss : 1.6093170642852783\n",
      "AE loss : 0.19924551248550415, ANN loss : 1.625897765159607, Total loss : 21.550447463989258\n",
      "learning rate A :  tf.Tensor(7.1578725e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 169 is 3.3141 sec\n",
      "train AE loss : 0.2787506580352783, train ANN loss : 1.6430189609527588\n",
      "AE loss : 0.21696633100509644, ANN loss : 1.6010173559188843, Total loss : 23.297649383544922\n",
      "learning rate A :  tf.Tensor(7.143709e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 170 is 3.2787 sec\n",
      "train AE loss : 0.26961642503738403, train ANN loss : 1.6542730331420898\n",
      "AE loss : 0.22582176327705383, ANN loss : 1.5954762697219849, Total loss : 24.17765235900879\n",
      "learning rate A :  tf.Tensor(7.130775e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 171 is 3.2863 sec\n",
      "train AE loss : 0.2774345874786377, train ANN loss : 1.6645029783248901\n",
      "AE loss : 0.24310384690761566, ANN loss : 1.5992103815078735, Total loss : 25.909597396850586\n",
      "learning rate A :  tf.Tensor(7.115764e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 172 is 3.2916 sec\n",
      "train AE loss : 0.26256832480430603, train ANN loss : 1.5995934009552002\n",
      "AE loss : 0.30684101581573486, ANN loss : 1.6883258819580078, Total loss : 32.3724250793457\n",
      "learning rate A :  tf.Tensor(7.101534e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 173 is 3.3018 sec\n",
      "train AE loss : 0.30287593603134155, train ANN loss : 1.6639207601547241\n",
      "AE loss : 0.25560519099235535, ANN loss : 1.6555813550949097, Total loss : 27.21609878540039\n",
      "learning rate A :  tf.Tensor(7.0879294e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 174 is 3.3123 sec\n",
      "train AE loss : 0.33479222655296326, train ANN loss : 1.6511644124984741\n",
      "AE loss : 0.2328779250383377, ANN loss : 1.5712199211120605, Total loss : 24.859012603759766\n",
      "learning rate A :  tf.Tensor(7.075394e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 175 is 3.3195 sec\n",
      "train AE loss : 0.2778538167476654, train ANN loss : 1.605780005455017\n",
      "AE loss : 0.21643467247486115, ANN loss : 1.630764126777649, Total loss : 23.27423095703125\n",
      "learning rate A :  tf.Tensor(7.0615424e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 176 is 3.3125 sec\n",
      "train AE loss : 0.32002270221710205, train ANN loss : 1.6462279558181763\n",
      "AE loss : 0.2948894202709198, ANN loss : 1.6292415857315063, Total loss : 31.118183135986328\n",
      "learning rate A :  tf.Tensor(7.046529e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 177 is 3.2857 sec\n",
      "train AE loss : 0.31985363364219666, train ANN loss : 1.5950959920883179\n",
      "AE loss : 0.25910457968711853, ANN loss : 1.569370150566101, Total loss : 27.479829788208008\n",
      "learning rate A :  tf.Tensor(7.032437e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 178 is 3.1668 sec\n",
      "train AE loss : 0.28115490078926086, train ANN loss : 1.6603593826293945\n",
      "AE loss : 0.15615306794643402, ANN loss : 1.6116257905960083, Total loss : 17.226932525634766\n",
      "learning rate A :  tf.Tensor(7.0188165e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 179 is 3.1585 sec\n",
      "train AE loss : 0.2065420001745224, train ANN loss : 1.745155930519104\n",
      "AE loss : 0.15640811622142792, ANN loss : 1.6398162841796875, Total loss : 17.280628204345703\n",
      "learning rate A :  tf.Tensor(7.005666e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 180 is 3.1831 sec\n",
      "train AE loss : 0.1801687628030777, train ANN loss : 1.7050998210906982\n",
      "AE loss : 0.16081005334854126, ANN loss : 1.6575119495391846, Total loss : 17.73851776123047\n",
      "learning rate A :  tf.Tensor(6.9913614e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 181 is 3.2337 sec\n",
      "train AE loss : 0.28868380188941956, train ANN loss : 1.6531950235366821\n",
      "AE loss : 0.25288552045822144, ANN loss : 1.6328486204147339, Total loss : 26.921401977539062\n",
      "learning rate A :  tf.Tensor(6.976938e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 182 is 3.2808 sec\n",
      "train AE loss : 0.3266851305961609, train ANN loss : 1.6735097169876099\n",
      "AE loss : 0.2739975154399872, ANN loss : 1.6243915557861328, Total loss : 29.024145126342773\n",
      "learning rate A :  tf.Tensor(6.963132e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 183 is 3.3008 sec\n",
      "train AE loss : 0.3407323956489563, train ANN loss : 1.6172372102737427\n",
      "AE loss : 0.22192531824111938, ANN loss : 1.5950020551681519, Total loss : 23.787532806396484\n",
      "learning rate A :  tf.Tensor(6.9495e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 184 is 3.2613 sec\n",
      "train AE loss : 0.27849122881889343, train ANN loss : 1.6137919425964355\n",
      "AE loss : 0.16561850905418396, ANN loss : 1.6350232362747192, Total loss : 18.196874618530273\n",
      "learning rate A :  tf.Tensor(6.934433e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 185 is 3.2568 sec\n",
      "train AE loss : 0.2792840898036957, train ANN loss : 1.6279387474060059\n",
      "AE loss : 0.2156190574169159, ANN loss : 1.6145113706588745, Total loss : 23.176416397094727\n",
      "learning rate A :  tf.Tensor(6.919545e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 186 is 3.2364 sec\n",
      "train AE loss : 0.2519589364528656, train ANN loss : 1.6200565099716187\n",
      "AE loss : 0.17290028929710388, ANN loss : 1.6715508699417114, Total loss : 18.961580276489258\n",
      "learning rate A :  tf.Tensor(6.906143e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 187 is 3.2865 sec\n",
      "train AE loss : 0.24200062453746796, train ANN loss : 1.6213418245315552\n",
      "AE loss : 0.20474576950073242, ANN loss : 1.5916446447372437, Total loss : 22.066219329833984\n",
      "learning rate A :  tf.Tensor(6.8917514e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 188 is 3.2740 sec\n",
      "train AE loss : 0.31384775042533875, train ANN loss : 1.598618984222412\n",
      "AE loss : 0.2068653553724289, ANN loss : 1.5755178928375244, Total loss : 22.262052536010742\n",
      "learning rate A :  tf.Tensor(6.877389e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 189 is 3.2861 sec\n",
      "train AE loss : 0.29809632897377014, train ANN loss : 1.6215797662734985\n",
      "AE loss : 0.24903705716133118, ANN loss : 1.5615638494491577, Total loss : 26.465269088745117\n",
      "learning rate A :  tf.Tensor(6.863345e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 190 is 3.2606 sec\n",
      "train AE loss : 0.3198426365852356, train ANN loss : 1.593517780303955\n",
      "AE loss : 0.197610005736351, ANN loss : 1.55584716796875, Total loss : 21.316844940185547\n",
      "learning rate A :  tf.Tensor(6.849909e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 191 is 3.3123 sec\n",
      "train AE loss : 0.26889926195144653, train ANN loss : 1.6044936180114746\n",
      "AE loss : 0.22740277647972107, ANN loss : 1.5742554664611816, Total loss : 24.31453514099121\n",
      "learning rate A :  tf.Tensor(6.834626e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 192 is 3.3032 sec\n",
      "train AE loss : 0.33670493960380554, train ANN loss : 1.5926154851913452\n",
      "AE loss : 0.20686276257038116, ANN loss : 1.6425572633743286, Total loss : 22.328832626342773\n",
      "learning rate A :  tf.Tensor(6.820813e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 193 is 3.2712 sec\n",
      "train AE loss : 0.30393186211586, train ANN loss : 1.6081149578094482\n",
      "AE loss : 0.21283164620399475, ANN loss : 1.5506064891815186, Total loss : 22.833772659301758\n",
      "learning rate A :  tf.Tensor(6.8070294e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 194 is 3.3087 sec\n",
      "train AE loss : 0.3106919229030609, train ANN loss : 1.5949065685272217\n",
      "AE loss : 0.31034672260284424, ANN loss : 1.602880597114563, Total loss : 32.637550354003906\n",
      "learning rate A :  tf.Tensor(6.7925575e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 195 is 3.3055 sec\n",
      "train AE loss : 0.29653751850128174, train ANN loss : 1.642701268196106\n",
      "AE loss : 0.16992825269699097, ANN loss : 1.5833135843276978, Total loss : 18.57613754272461\n",
      "learning rate A :  tf.Tensor(6.7776884e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 196 is 3.2826 sec\n",
      "train AE loss : 0.29956451058387756, train ANN loss : 1.5959455966949463\n",
      "AE loss : 0.1896890550851822, ANN loss : 1.5963026285171509, Total loss : 20.565208435058594\n",
      "learning rate A :  tf.Tensor(6.7639914e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 197 is 3.2862 sec\n",
      "train AE loss : 0.24723823368549347, train ANN loss : 1.6139312982559204\n",
      "AE loss : 0.1887274980545044, ANN loss : 1.5434876680374146, Total loss : 20.416234970092773\n",
      "learning rate A :  tf.Tensor(6.749753e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 198 is 3.2767 sec\n",
      "train AE loss : 0.3042251467704773, train ANN loss : 1.5731372833251953\n",
      "AE loss : 0.19323872029781342, ANN loss : 1.5499184131622314, Total loss : 20.873788833618164\n",
      "learning rate A :  tf.Tensor(6.7365385e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 199 is 3.3648 sec\n",
      "train AE loss : 0.28643858432769775, train ANN loss : 1.5838556289672852\n",
      "AE loss : 0.22141185402870178, ANN loss : 1.5353353023529053, Total loss : 23.67652130126953\n",
      "learning rate A :  tf.Tensor(6.723067e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 200 is 3.3639 sec\n"
     ]
    }
   ],
   "source": [
    "train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_latent = model_down(val_x)\n",
    "from sklearn import manifold, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "train_val_split = np.random.rand(len(val_latent)) < 0.4\n",
    "X_tsne = manifold.TSNE(n_components=2, init='pca', n_iter=5000, method='exact').fit_transform(val_latent[train_val_split])\n",
    "y = val_y[train_val_split].argmax(axis=1).reshape([-1,1])\n",
    "x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
    "X_norm = (X_tsne - x_min) / (x_max - x_min)\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(X_norm.shape[0]):\n",
    "    plt.text(X_norm[i, 0], X_norm[i, 1], str(y[i,0]), color=plt.cm.Set1(y[i,0]), \n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig(\"result_wifi/pca_result_latent16_exp11_3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = val_y[train_val_split].argmax(axis=1).reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8178186, shape=(32,), dtype=float32, numpy=\n",
       "array([ 0.10331075, -0.06326175, -0.29088065,  0.21179575,  0.12315437,\n",
       "        0.10544614, -0.14924976, -0.34411174, -0.13394117,  0.0375573 ,\n",
       "       -0.01708016,  0.01575394, -0.3141009 ,  0.49903595,  0.11130818,\n",
       "        0.01808045,  0.01696049,  0.10931037,  0.20861815,  0.11364696,\n",
       "       -0.32622164,  0.30268705, -0.04367344,  0.23157775, -0.03328072,\n",
       "        0.09133655, -0.11977081, -0.26723856, -0.05216331,  0.4191464 ,\n",
       "        0.15531218,  0.12522706], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_encoder_decoder_ann.weights[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8178191, shape=(32,), dtype=float32, numpy=\n",
       "array([ 0.10331075, -0.06326175, -0.29088065,  0.21179575,  0.12315437,\n",
       "        0.10544614, -0.14924976, -0.34411174, -0.13394117,  0.0375573 ,\n",
       "       -0.01708016,  0.01575394, -0.3141009 ,  0.49903595,  0.11130818,\n",
       "        0.01808045,  0.01696049,  0.10931037,  0.20861815,  0.11364696,\n",
       "       -0.32622164,  0.30268705, -0.04367344,  0.23157775, -0.03328072,\n",
       "        0.09133655, -0.11977081, -0.26723856, -0.05216331,  0.4191464 ,\n",
       "        0.15531218,  0.12522706], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_down.weights[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gps_wifi",
   "language": "python",
   "name": "gps_wifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
