{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf   \n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 6378137\n",
    "e = 8.1819190842622e-2\n",
    "asq = np.power(a,2)\n",
    "esq = np.power(e,2)\n",
    "def ecef2lla(ecef):\n",
    "    x = ecef[0]\n",
    "    y = ecef[1]\n",
    "    z = ecef[2]\n",
    "    r = ecef[3]\n",
    "    b = np.sqrt( asq * (1-esq) )\n",
    "    bsq = np.power(b,2)\n",
    "    ep = np.sqrt( (asq - bsq)/bsq)\n",
    "    p = np.sqrt(np.power(x,2) + np.power(y,2) )\n",
    "    th = np.arctan2(a*z, b*p)\n",
    "    lon = np.arctan2(y,x)\n",
    "    lat = np.arctan2( (z + np.power(ep,2)*b*np.power(np.sin(th),3) ), (p - esq*a*np.power(np.cos(th),3)) )\n",
    "    N = a/( np.sqrt(1-esq*np.power(np.sin(lat),2)) )\n",
    "    alt = p / np.cos(lat) - N\n",
    "    lon = lon % (2*np.pi)\n",
    "    ret = [lat, lon, alt , r]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = glob.glob('transfer_gps/*/*/*.txt')\n",
    "train_data = []\n",
    "train_label = [] \n",
    "for data in datas:\n",
    "    f = np.loadtxt(data,delimiter=\",\").copy()\n",
    "    o = np.argsort(f,axis=(0))[:,3]\n",
    "    f = np.array([ecef2lla(d) for d in f])\n",
    "    f = f/np.array([(2*np.pi),(2*np.pi),1e+8,1e+14])\n",
    "    f = f[o]\n",
    "    f.resize((10,4))\n",
    "    train_data.append(f) #/100000000\n",
    "    if data.split('/')[1] == \"indoor\":\n",
    "        train_label.append(\"indoor\"+data.split('/')[-2])\n",
    "    else:\n",
    "        train_label.append(\"outdoor\"+data.split('/')[-2])\n",
    "        \n",
    "\n",
    "train_data = np.array(train_data)\n",
    "#train_data[:,:,0] -= train_data[:,:,0].mean()\n",
    "#train_data[:,:,0] /= train_data[:,:,0].var()\n",
    "\n",
    "#train_data[:,:,1] -= train_data[:,:,1].mean()\n",
    "#train_data[:,:,1] /= train_data[:,:,1].var()\n",
    "#train_data[:,:,2] -= train_data[:,:,2].mean()\n",
    "#train_data[:,:,2] /= train_data[:,:,2].var()\n",
    "#train_data[:,:,3] -= train_data[:,:,3].mean()\n",
    "#train_data[:,:,3] /= 1000000\n",
    "train_label = pd.get_dummies(train_label).values.argmax(1)\n",
    "train_label = train_label.reshape(len(train_label),1)\n",
    "\n",
    "\n",
    "train_val_split = np.random.rand(len(train_data)) < 0.70\n",
    "train_x = train_data[train_val_split]\n",
    "train_y = train_label[train_val_split]\n",
    "val_x = train_data[~train_val_split]\n",
    "val_y = train_label[~train_val_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.get_dummies(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.11960483e-01,  2.69571399e-01,  2.00329096e-01,\n",
       "         2.08821717e-07],\n",
       "       [ 1.29206666e-01,  3.62248006e-01,  2.06427048e-01,\n",
       "         2.15508339e-07],\n",
       "       [-4.24217888e-02,  3.28889443e-01,  2.01623680e-01,\n",
       "         2.18700409e-07],\n",
       "       [-6.20447863e-03,  1.87337025e-01,  2.02737084e-01,\n",
       "         2.37841863e-07],\n",
       "       [-8.24765149e-02,  2.51600195e-01,  2.01790088e-01,\n",
       "         2.40439379e-07],\n",
       "       [ 1.56073299e-01,  1.24131603e-01,  2.01558979e-01,\n",
       "         2.43750731e-07],\n",
       "       [ 1.50569053e-01,  5.58399661e-01,  2.01576751e-01,\n",
       "         2.47426486e-07],\n",
       "       [ 4.94707264e-02,  1.36487059e-01,  2.02433024e-01,\n",
       "         2.47500367e-07],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.014776827514534156,\n",
       " 0.005121139506658792,\n",
       " 0.15694372069982268,\n",
       " -0.11999484730962863)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,0].mean(),train_data[:,:,0].var(),train_data[:,:,0].max(),train_data[:,:,0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19140741436903502, 0.025928020129686057, 0.9196117884719567, 0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,1].mean(),train_data[:,:,1].var(),train_data[:,:,1].max(),train_data[:,:,1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14311596114850597, 0.0084405349235764, 0.20702939989770788, 0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,2].mean(),train_data[:,:,2].var(),train_data[:,:,2].max(),train_data[:,:,2].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001208217386795917, 0.000416290117576147, 0.3461716502643769, 0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,3].mean(),train_data[:,:,3].var(),train_data[:,:,3].max(),train_data[:,:,3].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08762710510471775, 0.016632684378228145)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.mean(),train_data.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train_x.shape\n",
    "a\n",
    "b = (10,1)\n",
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3124, 10, 4), (3124, 1), 1399)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,train_y.shape,len(val_x)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 4523\n",
    "BATCH_SIZE = 5\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x,train_x))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_x,val_x)).batch(len(val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\"])\n",
    "with mirrored_strategy.scope():\n",
    "    input_o = tf.keras.layers.Input(shape=(4), name='input_layer')\n",
    "    models = layers.Dense(4, activation='sigmoid')(input_o )\n",
    "    models = layers.Dense(4, activation='sigmoid')(models)\n",
    "    models = layers.Dense(4, activation='sigmoid')(models)\n",
    "    models = layers.Dense(4, activation='sigmoid')(models)\n",
    "    model = tf.keras.Model(inputs=input_o, outputs=models)\n",
    "    model2 = tf.keras.Model(inputs=input_o, outputs=models)\n",
    "    #model.summary()\n",
    "    input = tf.keras.layers.Input(shape=(10,4), name='input_layer1')\n",
    "    \n",
    "    input1 = layers.Lambda(lambda x: x[:,0,:], output_shape=(1))(input)\n",
    "    input2 = layers.Lambda(lambda x: x[:,1,:], output_shape=(1))(input)\n",
    "    input3 = layers.Lambda(lambda x: x[:,2,:], output_shape=(1))(input)\n",
    "    input4 = layers.Lambda(lambda x: x[:,3,:], output_shape=(1))(input)\n",
    "    input5 = layers.Lambda(lambda x: x[:,4,:], output_shape=(1))(input)\n",
    "    input6 = layers.Lambda(lambda x: x[:,5,:], output_shape=(1))(input)\n",
    "    input7 = layers.Lambda(lambda x: x[:,6,:], output_shape=(1))(input)\n",
    "    input8 = layers.Lambda(lambda x: x[:,7,:], output_shape=(1))(input)\n",
    "    input9 = layers.Lambda(lambda x: x[:,8,:], output_shape=(1))(input)\n",
    "    input10= layers.Lambda(lambda x: x[:,9,:], output_shape=(1))(input)\n",
    "\n",
    "    model_1 = model(input1)\n",
    "    model_2 = model(input2)\n",
    "    model_3 = model(input3)\n",
    "    model_4 = model(input4)\n",
    "    model_5 = model(input5)\n",
    "    model_6 = model(input6)\n",
    "    model_7 = model(input7)\n",
    "    model_8 = model(input8)\n",
    "    model_9 = model(input9)\n",
    "    model_10= model(input10)\n",
    "    merge_layer = tf.keras.layers.concatenate(inputs=[model_1, model_2,model_3,model_4,model_5,model_6,model_7,model_8,model_9,model_10])\n",
    "    \n",
    "    #flatten = tf.keras.layers.Flatten()(input)\n",
    "    model_encoder = tf.keras.layers.Dense(32, activation='sigmoid')(merge_layer)\n",
    "    model_encoder = tf.keras.layers.Dense(24, activation='sigmoid')(model_encoder)\n",
    "    model_encoder = tf.keras.layers.Dense(16, activation='sigmoid')(model_encoder)\n",
    "    model_down = tf.keras.Model(inputs=[input], outputs=model_encoder,name = \"encoder\")#input1, input2,input3,input4,input5,input6,input7,input8,input9,input10\n",
    "    #model_down.summary()\n",
    "    input_encoder = tf.keras.layers.Input(shape=(10,4), name='input_layer2')\n",
    "    input_decoder = model_down(input_encoder)\n",
    "    model_decoder = layers.Dense(16, activation='sigmoid')(input_decoder)\n",
    "    model_decoder = layers.Dense(24, activation='sigmoid')(model_decoder)\n",
    "    model_decoder = layers.Dense(32, activation='sigmoid')(model_decoder)\n",
    "    model_decoder = layers.Dense(40, activation='sigmoid')(model_decoder)\n",
    "    model_decoder = layers.Reshape((10,4))(model_decoder)\n",
    "    out1 = layers.Lambda(lambda x: x[:,0,:], output_shape=(1))(model_decoder)\n",
    "    out2 = layers.Lambda(lambda x: x[:,1,:], output_shape=(1))(model_decoder)\n",
    "    out3 = layers.Lambda(lambda x: x[:,2,:], output_shape=(1))(model_decoder)\n",
    "    out4 = layers.Lambda(lambda x: x[:,3,:], output_shape=(1))(model_decoder)\n",
    "    out5 = layers.Lambda(lambda x: x[:,4,:], output_shape=(1))(model_decoder)\n",
    "    out6 = layers.Lambda(lambda x: x[:,5,:], output_shape=(1))(model_decoder)\n",
    "    out7 = layers.Lambda(lambda x: x[:,6,:], output_shape=(1))(model_decoder)\n",
    "    out8 = layers.Lambda(lambda x: x[:,7,:], output_shape=(1))(model_decoder)\n",
    "    out9 = layers.Lambda(lambda x: x[:,8,:], output_shape=(1))(model_decoder)\n",
    "    out10= layers.Lambda(lambda x: x[:,9,:], output_shape=(1))(model_decoder)\n",
    "    model_1 = model2(out1)\n",
    "    model_2 = model2(out2)\n",
    "    model_3 = model2(out3)\n",
    "    model_4 = model2(out4)\n",
    "    model_5 = model2(out5)\n",
    "    model_6 = model2(out6)\n",
    "    model_7 = model2(out7)\n",
    "    model_8 = model2(out8)\n",
    "    model_9 = model2(out9)\n",
    "    model_10= model2(out10)\n",
    "    merge_layer_out = tf.keras.layers.concatenate(inputs=[model_1, model_2,model_3,model_4,model_5,model_6,model_7,model_8,model_9,model_10])\n",
    "    model_decoder2 = layers.Reshape((10,4))(merge_layer_out)\n",
    "    model_encoder_decoder = tf.keras.Model(inputs=[input_encoder],outputs=model_decoder,name = 'encoder_decoder')\n",
    "    #model_encoder_decoder.summary()\n",
    "    #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=1e-5)\n",
    "    model_encoder_decoder.compile(optimizer = 'sgd', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(model_encoder_decoder.get_weights()))\n",
    "#for i in range(22):\n",
    "#    print(model_encoder_decoder.get_weights()[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "latent_dim = 16 \n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, encoding_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim  \n",
    "    \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(32, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(40, activation='sigmoid'),\n",
    "      layers.Reshape((10,4))\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "  \n",
    "autoencoder = Autoencoder(latent_dim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "autoencoder.fit(train_x, train_x,\n",
    "                epochs=1000,\n",
    "                shuffle=True,\n",
    "                validation_data=(val_x, val_x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_dataset = pd.read_csv(\"validationData.csv\",header = 0)\n",
    "\n",
    "test_features = np.asarray(test_dataset.iloc[:,0:520])\n",
    "test_features[test_features == 100] = -110\n",
    "test_features = (test_features - test_features.mean()) / test_features.var()\n",
    "\n",
    "test_labels = np.asarray(test_dataset[\"BUILDINGID\"].map(str) + test_dataset[\"FLOOR\"].map(str))\n",
    "test_labels = np.asarray(pd.get_dummies(test_labels))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints_lla_model1_2'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_16_model_{epoch}\")\n",
    "def decay(epoch):\n",
    "  if epoch < 300:\n",
    "    return 1e-2\n",
    "  elif epoch >= 300 and epoch < 500:\n",
    "    return 1e-4\n",
    "  elif epoch >= 500 and epoch < 700:\n",
    "    return 1e-6\n",
    "  elif epoch >= 700 and epoch < 900:\n",
    "    return 1e-8\n",
    "  else:\n",
    "    return 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model_encoder_decoder.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-15, momentum=1e-17)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_encoder_decoder.compile(optimizer = optimizer, loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "    625/Unknown - 20s 31ms/step - loss: 0.1714INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "\n",
      "Learning rate for epoch 1 is 0.009999999776482582\n",
      "625/625 [==============================] - 22s 35ms/step - loss: 0.1714 - val_loss: 0.0000e+00\n",
      "Epoch 2/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.1230\n",
      "Learning rate for epoch 2 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.1230 - val_loss: 0.1035\n",
      "Epoch 3/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0886\n",
      "Learning rate for epoch 3 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0885 - val_loss: 0.0751\n",
      "Epoch 4/500\n",
      "621/625 [============================>.] - ETA: 0s - loss: 0.0651\n",
      "Learning rate for epoch 4 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0650 - val_loss: 0.0559\n",
      "Epoch 5/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0492\n",
      "Learning rate for epoch 5 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0492 - val_loss: 0.0430\n",
      "Epoch 6/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0384\n",
      "Learning rate for epoch 6 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0384 - val_loss: 0.0342\n",
      "Epoch 7/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0310\n",
      "Learning rate for epoch 7 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0310 - val_loss: 0.0280\n",
      "Epoch 8/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0259\n",
      "Learning rate for epoch 8 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0259 - val_loss: 0.0237\n",
      "Epoch 9/500\n",
      "621/625 [============================>.] - ETA: 0s - loss: 0.0221\n",
      "Learning rate for epoch 9 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0221 - val_loss: 0.0205\n",
      "Epoch 10/500\n",
      "621/625 [============================>.] - ETA: 0s - loss: 0.0194\n",
      "Learning rate for epoch 10 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0194 - val_loss: 0.0181\n",
      "Epoch 11/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0173\n",
      "Learning rate for epoch 11 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.0173 - val_loss: 0.0163\n",
      "Epoch 12/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0157\n",
      "Learning rate for epoch 12 is 0.009999999776482582\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.0157 - val_loss: 0.0149\n",
      "Epoch 13/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0144\n",
      "Learning rate for epoch 13 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0144 - val_loss: 0.0137\n",
      "Epoch 14/500\n",
      "621/625 [============================>.] - ETA: 0s - loss: 0.0134\n",
      "Learning rate for epoch 14 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.0134 - val_loss: 0.0128\n",
      "Epoch 15/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0125\n",
      "Learning rate for epoch 15 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.0125 - val_loss: 0.0121\n",
      "Epoch 16/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0118\n",
      "Learning rate for epoch 16 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0118 - val_loss: 0.0114\n",
      "Epoch 17/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0112\n",
      "Learning rate for epoch 17 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 18/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0107\n",
      "Learning rate for epoch 18 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 19/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0103\n",
      "Learning rate for epoch 19 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 20/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0100\n",
      "Learning rate for epoch 20 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.0100 - val_loss: 0.0097\n",
      "Epoch 21/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Learning rate for epoch 21 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 22/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0093\n",
      "Learning rate for epoch 22 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 23/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0091\n",
      "Learning rate for epoch 23 is 0.009999999776482582\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 24/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0089\n",
      "Learning rate for epoch 24 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 25/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0087\n",
      "Learning rate for epoch 25 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 26/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0085\n",
      "Learning rate for epoch 26 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 27/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0083\n",
      "Learning rate for epoch 27 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 28/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0082\n",
      "Learning rate for epoch 28 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 29/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0081\n",
      "Learning rate for epoch 29 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 30/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0080\n",
      "Learning rate for epoch 30 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0080 - val_loss: 0.0078\n",
      "Epoch 31/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0079\n",
      "Learning rate for epoch 31 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 32/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0078\n",
      "Learning rate for epoch 32 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0078 - val_loss: 0.0076\n",
      "Epoch 33/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0076\n",
      "Learning rate for epoch 33 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 34/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0076\n",
      "Learning rate for epoch 34 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 35/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0075\n",
      "Learning rate for epoch 35 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 36/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0074\n",
      "Learning rate for epoch 36 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 37/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0074\n",
      "Learning rate for epoch 37 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 38/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0073\n",
      "Learning rate for epoch 38 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 39/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0073\n",
      "Learning rate for epoch 39 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 40/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Learning rate for epoch 40 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 41/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0072\n",
      "Learning rate for epoch 41 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 42/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0071\n",
      "Learning rate for epoch 42 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 43/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 43 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 44/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 44 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 45/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0070\n",
      "Learning rate for epoch 45 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 46/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 46 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 47/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 47 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 48/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Learning rate for epoch 48 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 49/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0068\n",
      "Learning rate for epoch 49 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 50/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0068\n",
      "Learning rate for epoch 50 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 51/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0067\n",
      "Learning rate for epoch 51 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 52/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0067\n",
      "Learning rate for epoch 52 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 53/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0067\n",
      "Learning rate for epoch 53 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 54/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0067\n",
      "Learning rate for epoch 54 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 55/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0067\n",
      "Learning rate for epoch 55 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 56/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0066\n",
      "Learning rate for epoch 56 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 57/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0066\n",
      "Learning rate for epoch 57 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 58/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0066\n",
      "Learning rate for epoch 58 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 59/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0066\n",
      "Learning rate for epoch 59 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 60/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0066\n",
      "Learning rate for epoch 60 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 61/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0065\n",
      "Learning rate for epoch 61 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 62/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0065\n",
      "Learning rate for epoch 62 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 63/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0065\n",
      "Learning rate for epoch 63 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 64/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0065\n",
      "Learning rate for epoch 64 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 65/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0064\n",
      "Learning rate for epoch 65 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 66/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0064\n",
      "Learning rate for epoch 66 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 67/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0064\n",
      "Learning rate for epoch 67 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 68/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0064\n",
      "Learning rate for epoch 68 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 69/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0064\n",
      "Learning rate for epoch 69 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 70/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 70 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 71/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0064\n",
      "Learning rate for epoch 71 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 72/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 72 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 73/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 73 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 74/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 74 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 75/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 75 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 76/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 76 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 77/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 77 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 78/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 78 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 79/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 79 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 80/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 80 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 81/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 81 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0063 - val_loss: 0.0062\n",
      "Epoch 82/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 82 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 83/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 83 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 84/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 84 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 85/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 85 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 86/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0063\n",
      "Learning rate for epoch 86 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0063 - val_loss: 0.0061\n",
      "Epoch 87/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 87 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 88/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 88 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 89/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 89 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 90/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 90 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 91/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 91 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 92/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 92 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 93/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 93 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 94/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 94 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 95/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 95 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 96/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 96 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 97/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 97 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 98/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0062\n",
      "Learning rate for epoch 98 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0062 - val_loss: 0.0060\n",
      "Epoch 99/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 99 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 100/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 100 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 101/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 101 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 102/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 102 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 103/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 103 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 104/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 104 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 105/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 105 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 106/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 106 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 107/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 107 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 108/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 108 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 109/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 109 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 110/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 110 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 111/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Learning rate for epoch 111 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 112/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 112 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 113/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 113 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 114/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 114 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 115/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 115 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 116/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 116 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 117/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 117 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 118/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 118 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 119/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 119 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 120/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 120 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 121/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 121 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 122/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 122 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 123/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 123 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 124/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 124 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 125/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 125 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 126/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 126 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 127/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 127 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 128/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 128 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 129/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 129 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 130/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 130 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 131/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 131 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 132/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 132 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 133/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 133 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 134/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 134 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 135/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 135 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 136/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 136 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 137/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 137 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 138/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 138 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 139/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 139 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 140/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 140 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 141/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 141 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 142/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 142 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 143/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 143 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 144/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 144 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 145/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 145 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 146/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 146 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 147/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 147 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 148/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 148 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 149/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 149 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 150/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 150 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 151/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Learning rate for epoch 151 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 152/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 152 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 153/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 153 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 154/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 154 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 155/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 155 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 156/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 156 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 157/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 157 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 158/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 158 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 159/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 159 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 160/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 160 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 161/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 161 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 162/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 162 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 163/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 163 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 164/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 164 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 165/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 165 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 166/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 166 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 167/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 167 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 168/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 168 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 169/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 169 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 170/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 170 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 171/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 171 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 172/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 172 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 173/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 173 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 174/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 174 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 175/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 175 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 176/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 176 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 177/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 177 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 178/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 178 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 179/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 179 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 180/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 180 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 181/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 181 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 182/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 182 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 183/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 183 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 184/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 184 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 185/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 185 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 186/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 186 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 187/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 187 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 188/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 188 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 189/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 189 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 190/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 190 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 191/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 191 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 192/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 192 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 193/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 193 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 194/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 194 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 195/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 195 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 196/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 196 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 197/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 197 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 198/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 198 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 199/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 199 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 200/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 200 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 201/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 201 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 202/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 202 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 203/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 203 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 204/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 204 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 205/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 205 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 206/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 206 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 207/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 207 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 208/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 208 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 209/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 209 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 210/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 210 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 211/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 211 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 212/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 212 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 213/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 213 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 214/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 214 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 215/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 215 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 216/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 216 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 217/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 217 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 218/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 218 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 219/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 219 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 220/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 220 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 221/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 221 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 222/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 222 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 223/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 223 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 224/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 224 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 225/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 225 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 226/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 226 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 227/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 227 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 228/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 228 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 229/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 229 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 230/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 230 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 231/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 231 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 232/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 232 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0058\n",
      "Epoch 233/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 233 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 234/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 234 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 235/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 235 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 236/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 236 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 237/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 237 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 238/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 238 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 239/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 239 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 240/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 240 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 241/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 241 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 242/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 242 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 243/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 243 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 244/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 244 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 245/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 245 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 246/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 246 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 247/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 247 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 248/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 248 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 249/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 249 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 250/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 250 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 251/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 251 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 252/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 252 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 253/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 253 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 254/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 254 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 255/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 255 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 256/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Learning rate for epoch 256 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0059 - val_loss: 0.0057\n",
      "Epoch 257/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 257 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 258/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 258 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 259/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 259 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 260/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 260 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 261/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 261 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 262/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 262 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 263/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 263 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 264/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 264 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 265/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 265 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 266/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 266 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 267/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 267 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 268/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 268 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 269/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 269 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 270/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 270 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 271/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 271 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 272/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 272 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 273/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 273 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 274/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 274 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 275/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 275 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 276/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 276 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 277/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 277 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 278/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 278 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 279/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 279 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 280/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 280 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 281/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 281 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 282/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 282 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 283/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 283 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 284/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 284 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 285/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 285 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 286/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 286 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 287/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 287 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 288/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 288 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 289/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 289 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 290/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 290 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 291/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 291 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 292/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 292 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 293/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 293 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 294/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 294 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 295/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 295 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 296/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 296 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 297/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 297 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 298/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 298 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 299/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 299 is 0.009999999776482582\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 300/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 300 is 0.009999999776482582\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 301/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 301 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 302/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 302 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 303/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 303 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 304/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 304 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 305/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 305 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 306/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 306 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 307/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 307 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 308/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0057\n",
      "Learning rate for epoch 308 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 309/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 309 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 310/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 310 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 311/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 311 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 312/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 312 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 313/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 313 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 314/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 314 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 315/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0057\n",
      "Learning rate for epoch 315 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 316/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 316 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 317/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 317 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 318/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 318 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 319/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 319 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 320/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 320 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 321/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 321 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 322/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 322 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 323/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 323 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 324/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 324 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 325/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 325 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 326/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 326 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 327/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 327 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 328/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 328 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 329/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 329 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 330/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 330 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 331/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 331 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 332/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0057\n",
      "Learning rate for epoch 332 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 333/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 333 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 334/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 334 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 335/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 335 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 336/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 336 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 337/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 337 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 338/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 338 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 339/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 339 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 340/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 340 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 341/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 341 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 342/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 342 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 343/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 343 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 344/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 344 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 345/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 345 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 346/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 346 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 347/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 347 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 348/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 348 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 349/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 349 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 350/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 350 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 351/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 351 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 352/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 352 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 353/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 353 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 354/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 354 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 355/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 355 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 356/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 356 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 357/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 357 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 358/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 358 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 359/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 359 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 360/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 360 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 361/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 361 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 362/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 362 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 363/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 363 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 364/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 364 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 365/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 365 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 366/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 366 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 367/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 367 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 368/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0057\n",
      "Learning rate for epoch 368 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 369/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 369 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 370/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 370 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 371/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0057\n",
      "Learning rate for epoch 371 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 372/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 372 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 373/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 373 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 374/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 374 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 375/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 375 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 376/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 376 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 377/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 377 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 378/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 378 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 379/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 379 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 380/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 380 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 381/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 381 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 382/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 382 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 383/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 383 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 384/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 384 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 385/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 385 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 386/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 386 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 387/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 387 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 388/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 388 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 389/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 389 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 390/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 390 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 391/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 391 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 392/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 392 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 393/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 393 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 394/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 394 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 395/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 395 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 396/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 396 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 397/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 397 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 398/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 398 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 399/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 399 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 400/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 400 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 401/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 401 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 402/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 402 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 403/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 403 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 404/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 404 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 405/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 405 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 406/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0057\n",
      "Learning rate for epoch 406 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 407/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 407 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 408/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 408 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 409/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 409 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 410/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 410 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 411/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 411 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 412/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 412 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 413/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 413 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 414/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 414 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 415/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 415 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 416/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 416 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 417/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 417 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 418/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 418 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 419/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 419 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 420/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 420 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 421/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 421 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 422/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 422 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 423/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 423 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 424/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 424 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 425/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 425 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 426/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 426 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 427/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 427 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 428/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 428 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 429/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 429 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 430/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 430 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 431/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 431 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 432/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 432 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 433/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 433 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 434/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 434 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 435/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 435 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 436/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 436 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 437/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 437 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 438/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 438 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 439/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 439 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 440/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 440 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 441/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 441 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 442/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 442 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 443/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 443 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 444/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0057\n",
      "Learning rate for epoch 444 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 445/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 445 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 446/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 446 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 447/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 447 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 448/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 448 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 449/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 449 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 450/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 450 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 451/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 451 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 452/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 452 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 453/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 453 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 454/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 454 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 455/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 455 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 456/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 456 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 457/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 457 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 458/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 458 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 459/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 459 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 460/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 460 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 461/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 461 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 462/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 462 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 463/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 463 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 464/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 464 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 465/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 465 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 466/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 466 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 467/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 467 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 468/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 468 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 469/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 469 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 470/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 470 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 471/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 471 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 472/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 472 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 473/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 473 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 474/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 474 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 475/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 475 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 476/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 476 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 477/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 477 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 478/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 478 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 479/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 479 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 480/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 480 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 481/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 481 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 482/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 482 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 483/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 483 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 484/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 484 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 485/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 485 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 486/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 486 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 487/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 487 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 12s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 488/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 488 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 489/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 489 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 490/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 490 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 491/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 491 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 492/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 492 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 493/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 493 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 494/500\n",
      "623/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 494 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 495/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 495 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 496/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 496 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 497/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 497 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 498/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 498 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 499/500\n",
      "622/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 499 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 500/500\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Learning rate for epoch 500 is 9.999999747378752e-05\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0058 - val_loss: 0.0057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc748231b70>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_encoder_decoder.fit(train_dataset,\n",
    "                epochs=500,\n",
    "                validation_data=valid_dataset,\n",
    "                callbacks = callbacks\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((None, 10, 4), (None, 10, 4)), types: (tf.float64, tf.float64)>,\n",
       " (1399, 10, 4))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset,val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_latent = model_down(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1399, 16])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold, datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = np.random.rand(len(val_latent)) < 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = manifold.TSNE(n_components=2, init='pca', n_iter=5000, method='exact').fit_transform(val_latent[train_val_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(573, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1399, 16])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
    "X_norm = (X_tsne - x_min) / (x_max - x_min)\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(X_norm.shape[0]):\n",
    "    plt.text(X_norm[i, 0], X_norm[i, 1], str(y[i,0]), color=plt.cm.Set1(y[i,0]), \n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig(\"result_lla/pca_result_latent16_model1_2_XYZ\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.Dense(256, activation='relu'),\n",
    "      layers.Dense(labels.shape[1], activation='softmax'),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer sequential is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_encoded = np.array(autoencoder.encoder(train_x))\n",
    "val_encoded = np.array(autoencoder.encoder(val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN.compile(optimizer='adam', loss=losses.CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DNN.fit(train_encoded, train_y,\n",
    "                epochs=1000,\n",
    "                shuffle=True,\n",
    "                validation_data=(val_encoded, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder = np.array(autoencoder.encoder(test_features))\n",
    "y_ = DNN(test_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array(y_[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gps_wifi",
   "language": "python",
   "name": "gps_wifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
