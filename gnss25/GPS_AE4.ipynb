{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import os\n",
    "import tensorflow as tf   \n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 6378137\n",
    "e = 8.1819190842622e-2\n",
    "asq = np.power(a,2)\n",
    "esq = np.power(e,2)\n",
    "def ecef2lla(ecef):\n",
    "    x = ecef[0]\n",
    "    y = ecef[1]\n",
    "    z = ecef[2]\n",
    "    r = ecef[3]\n",
    "    b = np.sqrt( asq * (1-esq) )\n",
    "    bsq = np.power(b,2)\n",
    "    ep = np.sqrt( (asq - bsq)/bsq)\n",
    "    p = np.sqrt(np.power(x,2) + np.power(y,2) )\n",
    "    th = np.arctan2(a*z, b*p)\n",
    "    lon = np.arctan2(y,x)\n",
    "    lat = np.arctan2( (z + np.power(ep,2)*b*np.power(np.sin(th),3) ), (p - esq*a*np.power(np.cos(th),3)) )\n",
    "    N = a/( np.sqrt(1-esq*np.power(np.sin(lat),2)) )\n",
    "    alt = p / np.cos(lat) - N\n",
    "    lon = lon % (2*np.pi)\n",
    "    ret = [lat, lon, alt , r]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = glob.glob('transfer/*/*/*.txt')\n",
    "train_data = []\n",
    "train_label = [] \n",
    "for data in datas:\n",
    "    if data.find('timestamp') == -1:\n",
    "        f = np.loadtxt(data,delimiter=\" \").copy()\n",
    "        if f.size == 4:\n",
    "            f = f.reshape((1,-1))\n",
    "        o = np.argsort(f,axis=(0))[:,3]\n",
    "        f = np.array([ecef2lla(d) for d in f])\n",
    "        f = f/np.array([(2*np.pi),(2*np.pi),1e+8,1e+14])\n",
    "        f = f[o]\n",
    "        f.resize((10,4))\n",
    "        train_data.append(f) #/100000000\n",
    "        if data.split('/')[1] == \"indoor\":\n",
    "            train_label.append(\"indoor\"+data.split('/')[-2])\n",
    "        else:\n",
    "            train_label.append(\"outdoor\"+data.split('/')[-2])\n",
    "        \n",
    "\n",
    "train_data = np.array(train_data).astype('float32')\n",
    "#train_data[:,:,0] -= train_data[:,:,0].mean()\n",
    "#train_data[:,:,0] /= train_data[:,:,0].var()\n",
    "\n",
    "#train_data[:,:,1] -= train_data[:,:,1].mean()\n",
    "#train_data[:,:,1] /= train_data[:,:,1].var()\n",
    "#train_data[:,:,2] -= train_data[:,:,2].mean()\n",
    "#train_data[:,:,2] /= train_data[:,:,2].var()\n",
    "#train_data[:,:,3] -= train_data[:,:,3].mean()\n",
    "#train_data[:,:,3] /= 1000000\n",
    "#train_label = pd.get_dummies(train_label).values.argmax(1)\n",
    "#train_label = train_label.reshape(len(train_label),1)\n",
    "train_label = np.array(pd.get_dummies(train_label)).astype('float32')\n",
    "\n",
    "train_val_split = np.random.rand(len(train_data)) < 0.70\n",
    "train_x = train_data[train_val_split]\n",
    "train_y = train_label[train_val_split]\n",
    "val_x = train_data[~train_val_split]\n",
    "val_y = train_label[~train_val_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.get_dummies(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.024973173, 0.0050530783, 0.15684304, -0.14916982)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,0].mean(),train_data[:,:,0].var(),train_data[:,:,0].max(),train_data[:,:,0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17398076, 0.024483193, 0.9873238, 0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,1].mean(),train_data[:,:,1].var(),train_data[:,:,1].max(),train_data[:,:,1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12767042, 0.009381889, 0.20791996, 0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,2].mean(),train_data[:,:,2].var(),train_data[:,:,2].max(),train_data[:,:,2].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0006215025, 0.00044229845, 8.975531e-07, -0.71392393)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,:,3].mean(),train_data[:,:,3].var(),train_data[:,:,3].max(),train_data[:,:,3].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08150072, 0.01499602)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.mean(),train_data.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14861, 10, 4), (14861, 50), 6443)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,train_y.shape,len(val_x)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = train_x.shape[0]\n",
    "BATCH_SIZE = train_x.shape[0]\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_x,val_y)).batch(len(val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        self.dense = dense\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name='bias',shape=[self.dense.input_shape[-1]],initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b = True)\n",
    "        return self.activation(z + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\"])\n",
    "#with mirrored_strategy.scope():\n",
    "input_o = tf.keras.layers.Input(shape=(4), name='input_layer')\n",
    "dense1 = layers.Dense(4, activation='relu')\n",
    "dense2 = layers.Dense(4, activation='relu')\n",
    "dense3 = layers.Dense(4, activation='relu')\n",
    "dense4 = layers.Dense(4, activation='relu')\n",
    "models = dense1(input_o )\n",
    "models = dense2(models)\n",
    "models = dense3(models)\n",
    "models = dense4(models)\n",
    "model = tf.keras.Model(inputs=input_o, outputs=models)\n",
    "\n",
    "input_oo = tf.keras.layers.Input(shape=(4), name='input_layer0')\n",
    "models2 = DenseTranspose(dense4, activation = 'relu')(input_oo)\n",
    "models2 = DenseTranspose(dense3, activation = 'relu')(models2)\n",
    "models2 = DenseTranspose(dense2, activation = 'relu')(models2)\n",
    "models2 = DenseTranspose(dense1, activation = 'relu')(models2)\n",
    "model2 = tf.keras.Model(inputs=input_oo, outputs=models2)\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(10,4), name='input_layer1')\n",
    "input1 = layers.Lambda(lambda x: x[:,0,:], output_shape=(1))(input)\n",
    "input2 = layers.Lambda(lambda x: x[:,1,:], output_shape=(1))(input)\n",
    "input3 = layers.Lambda(lambda x: x[:,2,:], output_shape=(1))(input)\n",
    "input4 = layers.Lambda(lambda x: x[:,3,:], output_shape=(1))(input)\n",
    "input5 = layers.Lambda(lambda x: x[:,4,:], output_shape=(1))(input)\n",
    "input6 = layers.Lambda(lambda x: x[:,5,:], output_shape=(1))(input)\n",
    "input7 = layers.Lambda(lambda x: x[:,6,:], output_shape=(1))(input)\n",
    "input8 = layers.Lambda(lambda x: x[:,7,:], output_shape=(1))(input)\n",
    "input9 = layers.Lambda(lambda x: x[:,8,:], output_shape=(1))(input)\n",
    "input10= layers.Lambda(lambda x: x[:,9,:], output_shape=(1))(input)\n",
    "model_1 = model(input1)\n",
    "model_2 = model(input2)\n",
    "model_3 = model(input3)\n",
    "model_4 = model(input4)\n",
    "model_5 = model(input5)\n",
    "model_6 = model(input6)\n",
    "model_7 = model(input7)\n",
    "model_8 = model(input8)\n",
    "model_9 = model(input9)\n",
    "model_10= model(input10)\n",
    "merge_layer = tf.keras.layers.concatenate(inputs=[model_1, model_2,model_3,model_4,model_5,model_6,model_7,model_8,model_9,model_10])\n",
    "dense5 = layers.Dense(32, activation='relu')\n",
    "dense6 = layers.Dense(24, activation='relu')\n",
    "dense7 = layers.Dense(16, activation='relu')\n",
    "model_encoder = dense5(merge_layer)\n",
    "model_encoder = dense6(model_encoder)\n",
    "model_encoder = dense7(model_encoder)\n",
    "model_down = tf.keras.Model(inputs=[input], outputs=model_encoder,name = \"encoder\")#input1, input2,input3,input4,input5,input6,input7,input8,input9,input10\n",
    "\n",
    "\n",
    "input_decoder = tf.keras.layers.Input(shape=(16), name='input_layer2')#model_down(input_encoder)\n",
    "model_decoder = DenseTranspose(dense7, activation = 'relu')(input_decoder)\n",
    "model_decoder = DenseTranspose(dense6, activation = 'relu')(model_decoder)\n",
    "model_decoder = DenseTranspose(dense5, activation = 'relu')(model_decoder)\n",
    "model_decoder = layers.Reshape((10,4))(model_decoder)\n",
    "\"\"\"\n",
    "out1 = layers.Lambda(lambda x: x[:,0,:], output_shape=(1))(model_decoder)\n",
    "out2 = layers.Lambda(lambda x: x[:,1,:], output_shape=(1))(model_decoder)\n",
    "out3 = layers.Lambda(lambda x: x[:,2,:], output_shape=(1))(model_decoder)\n",
    "out4 = layers.Lambda(lambda x: x[:,3,:], output_shape=(1))(model_decoder)\n",
    "out5 = layers.Lambda(lambda x: x[:,4,:], output_shape=(1))(model_decoder)\n",
    "out6 = layers.Lambda(lambda x: x[:,5,:], output_shape=(1))(model_decoder)\n",
    "out7 = layers.Lambda(lambda x: x[:,6,:], output_shape=(1))(model_decoder)\n",
    "out8 = layers.Lambda(lambda x: x[:,7,:], output_shape=(1))(model_decoder)\n",
    "out9 = layers.Lambda(lambda x: x[:,8,:], output_shape=(1))(model_decoder)\n",
    "out10= layers.Lambda(lambda x: x[:,9,:], output_shape=(1))(model_decoder)\n",
    "model_1 = model2(out1)\n",
    "model_2 = model2(out2)\n",
    "model_3 = model2(out3)\n",
    "model_4 = model2(out4)\n",
    "model_5 = model2(out5)\n",
    "model_6 = model2(out6)\n",
    "model_7 = model2(out7)\n",
    "model_8 = model2(out8)\n",
    "model_9 = model2(out9)\n",
    "model_10= model2(out10)\n",
    "merge_layer_out = tf.keras.layers.concatenate(inputs=[model_1, model_2,model_3,model_4,model_5,model_6,model_7,model_8,model_9,model_10])\n",
    "merge_layer_out = layers.Reshape((10,4))(merge_layer_out)\n",
    "\"\"\"\n",
    "model_up = tf.keras.Model(inputs=[input_decoder], outputs=[model_decoder],name = \"decoder\")\n",
    "#model_decoder2 = layers.Reshape((10,4))(merge_layer_out)\n",
    "\n",
    "input_ann = tf.keras.layers.Input(shape=(16), name='input_layer3')\n",
    "model_ann = tf.keras.layers.Dense(24, activation='relu')(input_ann)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(32, activation='relu')(model_ann)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(32, activation='relu')(model_ann)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(32, activation='relu')(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(50, activation='softmax')(model_ann)\n",
    "model_ANN = tf.keras.Model(inputs=[input_ann], outputs=model_ann,name = \"ann\")\n",
    "\n",
    "\n",
    "input_full = tf.keras.layers.Input(shape=(10,4), name='input_layer4')\n",
    "encoder_out = model_down(input_full)\n",
    "decoder_out = model_up(encoder_out)\n",
    "ann_out = model_ANN(encoder_out)\n",
    "model_encoder_decoder = tf.keras.Model(inputs=[input_full],outputs=[decoder_out,ann_out],name = 'encoder_decoder_ann')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model_encoder_decoder.summary()\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=1e-5)\n",
    "#model_encoder_decoder.compile(optimizer = 'sgd', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vars = model_encoder_decoder.trainable_variables[:14]\n",
    "decode_vars = model_encoder_decoder.trainable_variables[14:17]\n",
    "ann_vars = model_encoder_decoder.trainable_variables[17:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_encoder_decoder.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(4,)\n",
      "(4, 4)\n",
      "(4,)\n",
      "(4, 4)\n",
      "(4,)\n",
      "(4, 4)\n",
      "(4,)\n",
      "(40, 32)\n",
      "(32,)\n",
      "(32, 24)\n",
      "(24,)\n",
      "(24, 16)\n",
      "(16,)\n",
      "(24,)\n",
      "(32,)\n",
      "(40,)\n",
      "(16, 24)\n",
      "(24,)\n",
      "(24, 32)\n",
      "(32,)\n",
      "(32, 32)\n",
      "(32,)\n",
      "(32, 32)\n",
      "(32,)\n",
      "(32, 50)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(27):\n",
    "    print(model_encoder_decoder.trainable_variables[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(output,t_x,t_y):\n",
    "    #print(\"in loss\")\n",
    "    output_AE , output_label = output\n",
    "    #output_AE = output\n",
    "    mse = losses.MeanSquaredError()\n",
    "    AE_loss = mse(t_x,output_AE)\n",
    "    ann_loss =losses.categorical_crossentropy(t_y,output_label)\n",
    "    total_loss = AE_loss*100 + ann_loss\n",
    "    \n",
    "    return AE_loss,ann_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_A = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "initial_learning_rate=1e-2, decay_steps=100, decay_rate=0.9)\n",
    "optimizer_A = tf.optimizers.SGD(learning_rate=learning_rate_A , momentum=1e-5)\n",
    "learning_rate_B = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "initial_learning_rate=1e-2, decay_steps=100, decay_rate=0.9)\n",
    "optimizer_B = tf.optimizers.Adam(learning_rate=learning_rate_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(t_x,t_y):\n",
    "  \n",
    "    with tf.GradientTape() as AE_tape,tf.GradientTape() as ANN_tape:\n",
    "        output = model_encoder_decoder(t_x, training=True)\n",
    "        #AE_loss = model_loss(output,t_o,t_y)\n",
    "        AE_loss,ANN_loss = model_loss(output,t_x,t_y)\n",
    "        #gradients = tape.gradient(total_loss, shared_vars+decode_vars)\n",
    "        #optimizer_A.apply_gradients(zip(gradients, shared_vars+decode_vars))\n",
    "\n",
    "    if np.random.rand() < 0.5:\n",
    "        #print(\"AE\")\n",
    "        gradients_AE = AE_tape.gradient(AE_loss, shared_vars+decode_vars)\n",
    "        #gradients_AE = [tf.clip_by_value(g, -1,1) for g in gradients_AE]\n",
    "\n",
    "        #print(\"AE gradient : \",gradients_AE)\n",
    "        optimizer_A.apply_gradients(zip(gradients_AE, shared_vars+decode_vars))\n",
    "    \n",
    "    else:\n",
    "        #print(\"ANN\")\n",
    "        gradients_ANN = ANN_tape.gradient(ANN_loss, shared_vars+ann_vars)\n",
    "        #print(\"ANN gradient : \",gradients_ANN)\n",
    "        #gradients_ANN = [tf.clip_by_value(g, -1,1) for g in gradients_ANN] \n",
    "        optimizer_B.apply_gradients(zip(gradients_ANN, shared_vars+ann_vars))\n",
    "    \n",
    "    return np.array(AE_loss).mean(),np.array(ANN_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './gps_checkpoints/checkpoints_1222'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_1222_model_{epoch}\")\n",
    "checkpoint = tf.train.Checkpoint(optimizerA=optimizer_A,\n",
    "                                 optimizerB=optimizer_B,\n",
    "                                 model_encoder_decoder=model_encoder_decoder,\n",
    "                                 model_down=model_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(v_x,v_y):\n",
    "    output = model_encoder_decoder(v_x)\n",
    "    #output_AE = output\n",
    "    output_AE , output_label = output\n",
    "    mse = losses.MeanSquaredError()\n",
    "    AE_loss = mse(v_x,output_AE)\n",
    "    ann_loss = losses.categorical_crossentropy(v_y,output_label)\n",
    "    total_loss = AE_loss*100 + ann_loss\n",
    "    #print(output_label[200])\n",
    "    #print(v_y[200])\n",
    "    #print(ann_loss)\n",
    "    #print(\"AE loss : {},\".format(np.array(AE_loss).mean()))\n",
    "    print(\"AE loss : {}, ANN loss : {}, Total loss : {}\".format(np.array(AE_loss).mean(),np.array(ann_loss).mean(),np.array(total_loss).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    all_AE = []\n",
    "    all_ANN =[]\n",
    "    for x,y in train_dataset:\n",
    "        #AE_loss = train_step(x,y)\n",
    "        AE_loss,ANN_loss = train_step(x,y)\n",
    "        all_AE.append(AE_loss)\n",
    "        all_ANN.append(ANN_loss)\n",
    "    #print(\"train AE loss : {}\".format(np.array(all_AE).mean()))\n",
    "    print(\"train AE loss : {}, train ANN loss : {}\".format(np.array(all_AE).mean(),np.array(all_ANN).mean()))\n",
    "    validation(val_x,val_y)\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"learning rate A : \",optimizer_A._decayed_lr(tf.float32))\n",
    "    print(\"learning rate B : \",optimizer_B._decayed_lr(tf.float32))\n",
    "    print(f'Time for epoch {epoch + 1} is {time.time() - start:.4f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train AE loss : 0.021512070670723915, train ANN loss : 3.9119906425476074\n",
      "AE loss : 0.021644750609993935, ANN loss : 3.9120073318481445, Total loss : 6.076481819152832\n",
      "learning rate A :  tf.Tensor(0.009989469, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.01, shape=(), dtype=float32)\n",
      "Time for epoch 1 is 0.4966 sec\n",
      "train AE loss : 0.02149840071797371, train ANN loss : 3.912012815475464\n",
      "AE loss : 0.020776230841875076, ANN loss : 3.908710479736328, Total loss : 5.98633337020874\n",
      "learning rate A :  tf.Tensor(0.009989469, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009989469, shape=(), dtype=float32)\n",
      "Time for epoch 2 is 0.2872 sec\n",
      "train AE loss : 0.020631253719329834, train ANN loss : 3.908583879470825\n",
      "AE loss : 0.020755525678396225, ANN loss : 3.9087088108062744, Total loss : 5.9842610359191895\n",
      "learning rate A :  tf.Tensor(0.00997895, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009989469, shape=(), dtype=float32)\n",
      "Time for epoch 3 is 0.1828 sec\n",
      "train AE loss : 0.020610595121979713, train ANN loss : 3.908639669418335\n",
      "AE loss : 0.020734915509819984, ANN loss : 3.9087066650390625, Total loss : 5.982198238372803\n",
      "learning rate A :  tf.Tensor(0.009968442, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009989469, shape=(), dtype=float32)\n",
      "Time for epoch 4 is 0.1840 sec\n",
      "train AE loss : 0.020590022206306458, train ANN loss : 3.90869402885437\n",
      "AE loss : 0.02071438729763031, ANN loss : 3.9087047576904297, Total loss : 5.980143070220947\n",
      "learning rate A :  tf.Tensor(0.009957945, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009989469, shape=(), dtype=float32)\n",
      "Time for epoch 5 is 0.1792 sec\n",
      "train AE loss : 0.020569540560245514, train ANN loss : 3.908797025680542\n",
      "AE loss : 0.019545476883649826, ANN loss : 3.9046366214752197, Total loss : 5.8591837882995605\n",
      "learning rate A :  tf.Tensor(0.009957945, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00997895, shape=(), dtype=float32)\n",
      "Time for epoch 6 is 0.1809 sec\n",
      "train AE loss : 0.019403481855988503, train ANN loss : 3.904233932495117\n",
      "AE loss : 0.019521288573741913, ANN loss : 3.904633045196533, Total loss : 5.856761932373047\n",
      "learning rate A :  tf.Tensor(0.009947457, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00997895, shape=(), dtype=float32)\n",
      "Time for epoch 7 is 0.1797 sec\n",
      "train AE loss : 0.01937936805188656, train ANN loss : 3.9048638343811035\n",
      "AE loss : 0.0184725821018219, ANN loss : 3.90036678314209, Total loss : 5.747624397277832\n",
      "learning rate A :  tf.Tensor(0.009947457, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009968442, shape=(), dtype=float32)\n",
      "Time for epoch 8 is 0.1933 sec\n",
      "train AE loss : 0.018337253481149673, train ANN loss : 3.90101957321167\n",
      "AE loss : 0.0184524767100811, ANN loss : 3.9003639221191406, Total loss : 5.745611667633057\n",
      "learning rate A :  tf.Tensor(0.009936983, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009968442, shape=(), dtype=float32)\n",
      "Time for epoch 9 is 0.1856 sec\n",
      "train AE loss : 0.018317213281989098, train ANN loss : 3.9015040397644043\n",
      "AE loss : 0.019141776487231255, ANN loss : 3.8977153301239014, Total loss : 5.811893939971924\n",
      "learning rate A :  tf.Tensor(0.009936983, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009957945, shape=(), dtype=float32)\n",
      "Time for epoch 10 is 0.1830 sec\n",
      "train AE loss : 0.019005469977855682, train ANN loss : 3.8981549739837646\n",
      "AE loss : 0.019123436883091927, ANN loss : 3.897712469100952, Total loss : 5.810056686401367\n",
      "learning rate A :  tf.Tensor(0.009926518, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009957945, shape=(), dtype=float32)\n",
      "Time for epoch 11 is 0.1910 sec\n",
      "train AE loss : 0.018987182527780533, train ANN loss : 3.8981754779815674\n",
      "AE loss : 0.01910516433417797, ANN loss : 3.897709608078003, Total loss : 5.808225631713867\n",
      "learning rate A :  tf.Tensor(0.009916065, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009957945, shape=(), dtype=float32)\n",
      "Time for epoch 12 is 0.1787 sec\n",
      "train AE loss : 0.018968960270285606, train ANN loss : 3.898477554321289\n",
      "AE loss : 0.019086958840489388, ANN loss : 3.897706985473633, Total loss : 5.806402683258057\n",
      "learning rate A :  tf.Tensor(0.009905623, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009957945, shape=(), dtype=float32)\n",
      "Time for epoch 13 is 0.1776 sec\n",
      "train AE loss : 0.0189508069306612, train ANN loss : 3.8982839584350586\n",
      "AE loss : 0.019764531403779984, ANN loss : 3.895829916000366, Total loss : 5.8722825050354\n",
      "learning rate A :  tf.Tensor(0.009905623, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009947457, shape=(), dtype=float32)\n",
      "Time for epoch 14 is 0.1811 sec\n",
      "train AE loss : 0.019625801593065262, train ANN loss : 3.8963236808776855\n",
      "AE loss : 0.019744975492358208, ANN loss : 3.8958280086517334, Total loss : 5.870326042175293\n",
      "learning rate A :  tf.Tensor(0.009895192, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009947457, shape=(), dtype=float32)\n",
      "Time for epoch 15 is 0.1783 sec\n",
      "train AE loss : 0.019606290385127068, train ANN loss : 3.896557092666626\n",
      "AE loss : 0.020250387489795685, ANN loss : 3.8942408561706543, Total loss : 5.919280052185059\n",
      "learning rate A :  tf.Tensor(0.009895192, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009936983, shape=(), dtype=float32)\n",
      "Time for epoch 16 is 0.1971 sec\n",
      "train AE loss : 0.020108578726649284, train ANN loss : 3.8937859535217285\n",
      "AE loss : 0.02061866596341133, ANN loss : 3.893293619155884, Total loss : 5.955159664154053\n",
      "learning rate A :  tf.Tensor(0.009895192, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009926518, shape=(), dtype=float32)\n",
      "Time for epoch 17 is 0.1743 sec\n",
      "train AE loss : 0.020475249737501144, train ANN loss : 3.8924167156219482\n",
      "AE loss : 0.020601244643330574, ANN loss : 3.8932924270629883, Total loss : 5.9534173011779785\n",
      "learning rate A :  tf.Tensor(0.009884772, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009926518, shape=(), dtype=float32)\n",
      "Time for epoch 18 is 0.1698 sec\n",
      "train AE loss : 0.020457852631807327, train ANN loss : 3.8926515579223633\n",
      "AE loss : 0.02090793289244175, ANN loss : 3.892911911010742, Total loss : 5.983705043792725\n",
      "learning rate A :  tf.Tensor(0.009884772, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009916065, shape=(), dtype=float32)\n",
      "Time for epoch 19 is 0.1806 sec\n",
      "train AE loss : 0.020763441920280457, train ANN loss : 3.8916680812835693\n",
      "AE loss : 0.020773321390151978, ANN loss : 3.89250111579895, Total loss : 5.969832897186279\n",
      "learning rate A :  tf.Tensor(0.009884772, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009905623, shape=(), dtype=float32)\n",
      "Time for epoch 20 is 0.1754 sec\n",
      "train AE loss : 0.02063138224184513, train ANN loss : 3.8921968936920166\n",
      "AE loss : 0.020752940326929092, ANN loss : 3.8924977779388428, Total loss : 5.96779203414917\n",
      "learning rate A :  tf.Tensor(0.0098743625, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009905623, shape=(), dtype=float32)\n",
      "Time for epoch 21 is 0.1696 sec\n",
      "train AE loss : 0.020611073821783066, train ANN loss : 3.891580581665039\n",
      "AE loss : 0.02073248289525509, ANN loss : 3.892493963241577, Total loss : 5.965742111206055\n",
      "learning rate A :  tf.Tensor(0.009863964, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009905623, shape=(), dtype=float32)\n",
      "Time for epoch 22 is 0.1704 sec\n",
      "train AE loss : 0.020590687170624733, train ANN loss : 3.8913698196411133\n",
      "AE loss : 0.02071191929280758, ANN loss : 3.8924899101257324, Total loss : 5.963681697845459\n",
      "learning rate A :  tf.Tensor(0.009853577, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009905623, shape=(), dtype=float32)\n",
      "Time for epoch 23 is 0.1710 sec\n",
      "train AE loss : 0.020570190623402596, train ANN loss : 3.89152193069458\n",
      "AE loss : 0.020829373970627785, ANN loss : 3.8922297954559326, Total loss : 5.9751667976379395\n",
      "learning rate A :  tf.Tensor(0.009853577, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009895192, shape=(), dtype=float32)\n",
      "Time for epoch 24 is 0.1748 sec\n",
      "train AE loss : 0.020687896758317947, train ANN loss : 3.8906991481781006\n",
      "AE loss : 0.020502181723713875, ANN loss : 3.8913350105285645, Total loss : 5.94155216217041\n",
      "learning rate A :  tf.Tensor(0.009853577, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009884772, shape=(), dtype=float32)\n",
      "Time for epoch 25 is 0.1716 sec\n",
      "train AE loss : 0.020364398136734962, train ANN loss : 3.890359401702881\n",
      "AE loss : 0.02019091323018074, ANN loss : 3.8905746936798096, Total loss : 5.909665584564209\n",
      "learning rate A :  tf.Tensor(0.009853577, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0098743625, shape=(), dtype=float32)\n",
      "Time for epoch 26 is 0.1877 sec\n",
      "train AE loss : 0.02005654387176037, train ANN loss : 3.8889946937561035\n",
      "AE loss : 0.02016107179224491, ANN loss : 3.889728307723999, Total loss : 5.905835151672363\n",
      "learning rate A :  tf.Tensor(0.009853577, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009863964, shape=(), dtype=float32)\n",
      "Time for epoch 27 is 0.1745 sec\n",
      "train AE loss : 0.020030930638313293, train ANN loss : 3.8885419368743896\n",
      "AE loss : 0.02013796754181385, ANN loss : 3.8897273540496826, Total loss : 5.903524398803711\n",
      "learning rate A :  tf.Tensor(0.0098432, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009863964, shape=(), dtype=float32)\n",
      "Time for epoch 28 is 0.1723 sec\n",
      "train AE loss : 0.020007900893688202, train ANN loss : 3.8882594108581543\n",
      "AE loss : 0.02061338908970356, ANN loss : 3.8894107341766357, Total loss : 5.950749397277832\n",
      "learning rate A :  tf.Tensor(0.0098432, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009853577, shape=(), dtype=float32)\n",
      "Time for epoch 29 is 0.1845 sec\n",
      "train AE loss : 0.020481660962104797, train ANN loss : 3.888401985168457\n",
      "AE loss : 0.01933922991156578, ANN loss : 3.8868327140808105, Total loss : 5.820755481719971\n",
      "learning rate A :  tf.Tensor(0.0098432, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0098432, shape=(), dtype=float32)\n",
      "Time for epoch 30 is 0.1827 sec\n",
      "train AE loss : 0.019218476489186287, train ANN loss : 3.88765549659729\n",
      "AE loss : 0.01932239904999733, ANN loss : 3.886855125427246, Total loss : 5.819095134735107\n",
      "learning rate A :  tf.Tensor(0.009832836, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0098432, shape=(), dtype=float32)\n",
      "Time for epoch 31 is 0.1821 sec\n",
      "train AE loss : 0.019201643764972687, train ANN loss : 3.886523485183716\n",
      "AE loss : 0.01930580660700798, ANN loss : 3.8868772983551025, Total loss : 5.817458152770996\n",
      "learning rate A :  tf.Tensor(0.009822481, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0098432, shape=(), dtype=float32)\n",
      "Time for epoch 32 is 0.1728 sec\n",
      "train AE loss : 0.019185049459338188, train ANN loss : 3.8878347873687744\n",
      "AE loss : 0.019869765266776085, ANN loss : 3.885056734085083, Total loss : 5.872032642364502\n",
      "learning rate A :  tf.Tensor(0.009822481, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009832836, shape=(), dtype=float32)\n",
      "Time for epoch 33 is 0.1924 sec\n",
      "train AE loss : 0.019750982522964478, train ANN loss : 3.882965326309204\n",
      "AE loss : 0.01997992768883705, ANN loss : 3.8829100131988525, Total loss : 5.8809027671813965\n",
      "learning rate A :  tf.Tensor(0.009822481, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009822481, shape=(), dtype=float32)\n",
      "Time for epoch 34 is 0.1732 sec\n",
      "train AE loss : 0.019863132387399673, train ANN loss : 3.8812174797058105\n",
      "AE loss : 0.019149351865053177, ANN loss : 3.8756017684936523, Total loss : 5.790536880493164\n",
      "learning rate A :  tf.Tensor(0.009822481, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009812137, shape=(), dtype=float32)\n",
      "Time for epoch 35 is 0.1768 sec\n",
      "train AE loss : 0.0190355833619833, train ANN loss : 3.8782589435577393\n",
      "AE loss : 0.019140398129820824, ANN loss : 3.8756210803985596, Total loss : 5.789661407470703\n",
      "learning rate A :  tf.Tensor(0.009812137, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009812137, shape=(), dtype=float32)\n",
      "Time for epoch 36 is 0.1717 sec\n",
      "train AE loss : 0.01902659609913826, train ANN loss : 3.8789315223693848\n",
      "AE loss : 0.019131524488329887, ANN loss : 3.8756396770477295, Total loss : 5.788792133331299\n",
      "learning rate A :  tf.Tensor(0.009801805, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009812137, shape=(), dtype=float32)\n",
      "Time for epoch 37 is 0.1689 sec\n",
      "train AE loss : 0.019017687067389488, train ANN loss : 3.879012107849121\n",
      "AE loss : 0.01912272907793522, ANN loss : 3.8756580352783203, Total loss : 5.787930965423584\n",
      "learning rate A :  tf.Tensor(0.009791484, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009812137, shape=(), dtype=float32)\n",
      "Time for epoch 38 is 0.1701 sec\n",
      "train AE loss : 0.019008856266736984, train ANN loss : 3.879384994506836\n",
      "AE loss : 0.019956214353442192, ANN loss : 3.8747787475585938, Total loss : 5.8703999519348145\n",
      "learning rate A :  tf.Tensor(0.009791484, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009801805, shape=(), dtype=float32)\n",
      "Time for epoch 39 is 0.1863 sec\n",
      "train AE loss : 0.019842224195599556, train ANN loss : 3.8724734783172607\n",
      "AE loss : 0.01966169662773609, ANN loss : 3.8624680042266846, Total loss : 5.828638076782227\n",
      "learning rate A :  tf.Tensor(0.009791484, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009791484, shape=(), dtype=float32)\n",
      "Time for epoch 40 is 0.1741 sec\n",
      "train AE loss : 0.019554568454623222, train ANN loss : 3.8624801635742188\n",
      "AE loss : 0.019640477374196053, ANN loss : 3.862339496612549, Total loss : 5.826387405395508\n",
      "learning rate A :  tf.Tensor(0.0097811725, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009791484, shape=(), dtype=float32)\n",
      "Time for epoch 41 is 0.1693 sec\n",
      "train AE loss : 0.019533473998308182, train ANN loss : 3.862736940383911\n",
      "AE loss : 0.01961984671652317, ANN loss : 3.8622140884399414, Total loss : 5.8241987228393555\n",
      "learning rate A :  tf.Tensor(0.009770872, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009791484, shape=(), dtype=float32)\n",
      "Time for epoch 42 is 0.1804 sec\n",
      "train AE loss : 0.019512951374053955, train ANN loss : 3.861588954925537\n",
      "AE loss : 0.01964016817510128, ANN loss : 3.8472900390625, Total loss : 5.811306953430176\n",
      "learning rate A :  tf.Tensor(0.009770872, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0097811725, shape=(), dtype=float32)\n",
      "Time for epoch 43 is 0.1808 sec\n",
      "train AE loss : 0.0195306483656168, train ANN loss : 3.864851713180542\n",
      "AE loss : 0.02058468386530876, ANN loss : 3.8684282302856445, Total loss : 5.926896572113037\n",
      "learning rate A :  tf.Tensor(0.009770872, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009770872, shape=(), dtype=float32)\n",
      "Time for epoch 44 is 0.1850 sec\n",
      "train AE loss : 0.020465346053242683, train ANN loss : 3.8641204833984375\n",
      "AE loss : 0.020446401089429855, ANN loss : 3.8538482189178467, Total loss : 5.898488521575928\n",
      "learning rate A :  tf.Tensor(0.009770872, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009760584, shape=(), dtype=float32)\n",
      "Time for epoch 45 is 0.1744 sec\n",
      "train AE loss : 0.02033124305307865, train ANN loss : 3.8509042263031006\n",
      "AE loss : 0.020423652604222298, ANN loss : 3.8532395362854004, Total loss : 5.895605087280273\n",
      "learning rate A :  tf.Tensor(0.009760584, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009760584, shape=(), dtype=float32)\n",
      "Time for epoch 46 is 0.1704 sec\n",
      "train AE loss : 0.020308641716837883, train ANN loss : 3.8506126403808594\n",
      "AE loss : 0.020401084795594215, ANN loss : 3.8526346683502197, Total loss : 5.892743110656738\n",
      "learning rate A :  tf.Tensor(0.009750305, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009760584, shape=(), dtype=float32)\n",
      "Time for epoch 47 is 0.1816 sec\n",
      "train AE loss : 0.020286189392209053, train ANN loss : 3.8497488498687744\n",
      "AE loss : 0.020272791385650635, ANN loss : 3.8213419914245605, Total loss : 5.848621368408203\n",
      "learning rate A :  tf.Tensor(0.009750305, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009750305, shape=(), dtype=float32)\n",
      "Time for epoch 48 is 0.1733 sec\n",
      "train AE loss : 0.02015843614935875, train ANN loss : 3.8597168922424316\n",
      "AE loss : 0.02024545706808567, ANN loss : 3.8207924365997314, Total loss : 5.845338344573975\n",
      "learning rate A :  tf.Tensor(0.009740037, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009750305, shape=(), dtype=float32)\n",
      "Time for epoch 49 is 0.1685 sec\n",
      "train AE loss : 0.020131288096308708, train ANN loss : 3.855905055999756\n",
      "AE loss : 0.02018366940319538, ANN loss : 3.8200950622558594, Total loss : 5.838461875915527\n",
      "learning rate A :  tf.Tensor(0.009740037, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009740037, shape=(), dtype=float32)\n",
      "Time for epoch 50 is 0.1739 sec\n",
      "train AE loss : 0.020069928839802742, train ANN loss : 3.823347806930542\n",
      "AE loss : 0.020170191302895546, ANN loss : 3.819894552230835, Total loss : 5.836913585662842\n",
      "learning rate A :  tf.Tensor(0.00972978, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009740037, shape=(), dtype=float32)\n",
      "Time for epoch 51 is 0.1765 sec\n",
      "train AE loss : 0.020056454464793205, train ANN loss : 3.8253257274627686\n",
      "AE loss : 0.020156845450401306, ANN loss : 3.819702625274658, Total loss : 5.835387229919434\n",
      "learning rate A :  tf.Tensor(0.009719534, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009740037, shape=(), dtype=float32)\n",
      "Time for epoch 52 is 0.1764 sec\n",
      "train AE loss : 0.020043114200234413, train ANN loss : 3.824272632598877\n",
      "AE loss : 0.020143380388617516, ANN loss : 3.8195152282714844, Total loss : 5.833853244781494\n",
      "learning rate A :  tf.Tensor(0.009709299, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009740037, shape=(), dtype=float32)\n",
      "Time for epoch 53 is 0.1694 sec\n",
      "train AE loss : 0.020029645413160324, train ANN loss : 3.822786808013916\n",
      "AE loss : 0.020365647971630096, ANN loss : 3.836559772491455, Total loss : 5.873124122619629\n",
      "learning rate A :  tf.Tensor(0.009709299, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00972978, shape=(), dtype=float32)\n",
      "Time for epoch 54 is 0.1708 sec\n",
      "train AE loss : 0.02024690993130207, train ANN loss : 3.8334195613861084\n",
      "AE loss : 0.020242929458618164, ANN loss : 3.80678391456604, Total loss : 5.831077575683594\n",
      "learning rate A :  tf.Tensor(0.009709299, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009719534, shape=(), dtype=float32)\n",
      "Time for epoch 55 is 0.1745 sec\n",
      "train AE loss : 0.02012183703482151, train ANN loss : 3.812673807144165\n",
      "AE loss : 0.020697325468063354, ANN loss : 3.7897427082061768, Total loss : 5.859475612640381\n",
      "learning rate A :  tf.Tensor(0.009709299, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009709299, shape=(), dtype=float32)\n",
      "Time for epoch 56 is 0.1732 sec\n",
      "train AE loss : 0.020567063242197037, train ANN loss : 3.829061508178711\n",
      "AE loss : 0.020400121808052063, ANN loss : 3.792261838912964, Total loss : 5.832273960113525\n",
      "learning rate A :  tf.Tensor(0.009709299, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009699075, shape=(), dtype=float32)\n",
      "Time for epoch 57 is 0.1828 sec\n",
      "train AE loss : 0.020278476178646088, train ANN loss : 3.800123929977417\n",
      "AE loss : 0.020513083785772324, ANN loss : 3.806123733520508, Total loss : 5.8574323654174805\n",
      "learning rate A :  tf.Tensor(0.009709299, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009688861, shape=(), dtype=float32)\n",
      "Time for epoch 58 is 0.1730 sec\n",
      "train AE loss : 0.020395919680595398, train ANN loss : 3.808103322982788\n",
      "AE loss : 0.020500672981142998, ANN loss : 3.8058481216430664, Total loss : 5.855915546417236\n",
      "learning rate A :  tf.Tensor(0.009699075, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009688861, shape=(), dtype=float32)\n",
      "Time for epoch 59 is 0.1716 sec\n",
      "train AE loss : 0.02038346417248249, train ANN loss : 3.809453010559082\n",
      "AE loss : 0.020543396472930908, ANN loss : 3.7712130546569824, Total loss : 5.825552940368652\n",
      "learning rate A :  tf.Tensor(0.009699075, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009678658, shape=(), dtype=float32)\n",
      "Time for epoch 60 is 0.1842 sec\n",
      "train AE loss : 0.02042517252266407, train ANN loss : 3.785623788833618\n",
      "AE loss : 0.020849650725722313, ANN loss : 3.7550675868988037, Total loss : 5.84003210067749\n",
      "learning rate A :  tf.Tensor(0.009699075, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009668467, shape=(), dtype=float32)\n",
      "Time for epoch 61 is 0.1718 sec\n",
      "train AE loss : 0.020722439512610435, train ANN loss : 3.7998530864715576\n",
      "AE loss : 0.02082543820142746, ANN loss : 3.7546188831329346, Total loss : 5.837162971496582\n",
      "learning rate A :  tf.Tensor(0.009688861, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009668467, shape=(), dtype=float32)\n",
      "Time for epoch 62 is 0.1687 sec\n",
      "train AE loss : 0.02069855108857155, train ANN loss : 3.799445152282715\n",
      "AE loss : 0.020802199840545654, ANN loss : 3.7542335987091064, Total loss : 5.834453105926514\n",
      "learning rate A :  tf.Tensor(0.009678658, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009668467, shape=(), dtype=float32)\n",
      "Time for epoch 63 is 0.1806 sec\n",
      "train AE loss : 0.02067560702562332, train ANN loss : 3.7976503372192383\n",
      "AE loss : 0.020604025572538376, ANN loss : 3.778672218322754, Total loss : 5.839075088500977\n",
      "learning rate A :  tf.Tensor(0.009678658, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0096582845, shape=(), dtype=float32)\n",
      "Time for epoch 64 is 0.1744 sec\n",
      "train AE loss : 0.020491404458880424, train ANN loss : 3.785227060317993\n",
      "AE loss : 0.0205913707613945, ANN loss : 3.778339147567749, Total loss : 5.8374762535095215\n",
      "learning rate A :  tf.Tensor(0.009668467, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0096582845, shape=(), dtype=float32)\n",
      "Time for epoch 65 is 0.1689 sec\n",
      "train AE loss : 0.020478704944252968, train ANN loss : 3.7826144695281982\n",
      "AE loss : 0.020578719675540924, ANN loss : 3.778012275695801, Total loss : 5.835884094238281\n",
      "learning rate A :  tf.Tensor(0.0096582845, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0096582845, shape=(), dtype=float32)\n",
      "Time for epoch 66 is 0.1694 sec\n",
      "train AE loss : 0.020466018468141556, train ANN loss : 3.7798869609832764\n",
      "AE loss : 0.020599542185664177, ANN loss : 3.7752764225006104, Total loss : 5.835230350494385\n",
      "learning rate A :  tf.Tensor(0.0096582845, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009648114, shape=(), dtype=float32)\n",
      "Time for epoch 67 is 0.1732 sec\n",
      "train AE loss : 0.020486796274781227, train ANN loss : 3.7806506156921387\n",
      "AE loss : 0.020671306177973747, ANN loss : 3.726285934448242, Total loss : 5.7934160232543945\n",
      "learning rate A :  tf.Tensor(0.0096582845, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009637954, shape=(), dtype=float32)\n",
      "Time for epoch 68 is 0.1764 sec\n",
      "train AE loss : 0.020540593191981316, train ANN loss : 3.7568204402923584\n",
      "AE loss : 0.020656796172261238, ANN loss : 3.7263636589050293, Total loss : 5.792043685913086\n",
      "learning rate A :  tf.Tensor(0.009648114, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009637954, shape=(), dtype=float32)\n",
      "Time for epoch 69 is 0.1707 sec\n",
      "train AE loss : 0.0205263439565897, train ANN loss : 3.7584822177886963\n",
      "AE loss : 0.02062208019196987, ANN loss : 3.7168989181518555, Total loss : 5.779106616973877\n",
      "learning rate A :  tf.Tensor(0.009648114, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009627804, shape=(), dtype=float32)\n",
      "Time for epoch 70 is 0.1740 sec\n",
      "train AE loss : 0.02049059420824051, train ANN loss : 3.7444729804992676\n",
      "AE loss : 0.02055577002465725, ANN loss : 3.7470247745513916, Total loss : 5.802601337432861\n",
      "learning rate A :  tf.Tensor(0.009648114, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009617667, shape=(), dtype=float32)\n",
      "Time for epoch 71 is 0.1731 sec\n",
      "train AE loss : 0.02043645828962326, train ANN loss : 3.7478199005126953\n",
      "AE loss : 0.020516008138656616, ANN loss : 3.7060956954956055, Total loss : 5.757697105407715\n",
      "learning rate A :  tf.Tensor(0.009648114, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009607539, shape=(), dtype=float32)\n",
      "Time for epoch 72 is 0.1732 sec\n",
      "train AE loss : 0.020387938246130943, train ANN loss : 3.7244491577148438\n",
      "AE loss : 0.020502842962741852, ANN loss : 3.7060678005218506, Total loss : 5.756352424621582\n",
      "learning rate A :  tf.Tensor(0.009637954, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009607539, shape=(), dtype=float32)\n",
      "Time for epoch 73 is 0.1678 sec\n",
      "train AE loss : 0.02037481591105461, train ANN loss : 3.7231812477111816\n",
      "AE loss : 0.02065584808588028, ANN loss : 3.691230058670044, Total loss : 5.756814479827881\n",
      "learning rate A :  tf.Tensor(0.009637954, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009597422, shape=(), dtype=float32)\n",
      "Time for epoch 74 is 0.1725 sec\n",
      "train AE loss : 0.020521029829978943, train ANN loss : 3.7323009967803955\n",
      "AE loss : 0.02063990943133831, ANN loss : 3.6910791397094727, Total loss : 5.755069732666016\n",
      "learning rate A :  tf.Tensor(0.009627804, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009597422, shape=(), dtype=float32)\n",
      "Time for epoch 75 is 0.1816 sec\n",
      "train AE loss : 0.020505430176854134, train ANN loss : 3.733363389968872\n",
      "AE loss : 0.020624151453375816, ANN loss : 3.690948009490967, Total loss : 5.753363132476807\n",
      "learning rate A :  tf.Tensor(0.009617667, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009597422, shape=(), dtype=float32)\n",
      "Time for epoch 76 is 0.1699 sec\n",
      "train AE loss : 0.020490001887083054, train ANN loss : 3.732429265975952\n",
      "AE loss : 0.020608557388186455, ANN loss : 3.690833806991577, Total loss : 5.7516889572143555\n",
      "learning rate A :  tf.Tensor(0.009607539, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009597422, shape=(), dtype=float32)\n",
      "Time for epoch 77 is 0.1675 sec\n",
      "train AE loss : 0.020474720746278763, train ANN loss : 3.7305634021759033\n",
      "AE loss : 0.020666131749749184, ANN loss : 3.7460978031158447, Total loss : 5.812710762023926\n",
      "learning rate A :  tf.Tensor(0.009607539, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009587315, shape=(), dtype=float32)\n",
      "Time for epoch 78 is 0.1719 sec\n",
      "train AE loss : 0.020549681037664413, train ANN loss : 3.7404239177703857\n",
      "AE loss : 0.020651800557971, ANN loss : 3.7457163333892822, Total loss : 5.810896396636963\n",
      "learning rate A :  tf.Tensor(0.009597422, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009587315, shape=(), dtype=float32)\n",
      "Time for epoch 79 is 0.1840 sec\n",
      "train AE loss : 0.020535310730338097, train ANN loss : 3.7448089122772217\n",
      "AE loss : 0.020637456327676773, ANN loss : 3.7453243732452393, Total loss : 5.809069633483887\n",
      "learning rate A :  tf.Tensor(0.009587315, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009587315, shape=(), dtype=float32)\n",
      "Time for epoch 80 is 0.1693 sec\n",
      "train AE loss : 0.020520925521850586, train ANN loss : 3.7420268058776855\n",
      "AE loss : 0.02062312699854374, ANN loss : 3.744926691055298, Total loss : 5.807239532470703\n",
      "learning rate A :  tf.Tensor(0.009577219, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009587315, shape=(), dtype=float32)\n",
      "Time for epoch 81 is 0.1690 sec\n",
      "train AE loss : 0.02050655335187912, train ANN loss : 3.740185499191284\n",
      "AE loss : 0.02060881070792675, ANN loss : 3.744521141052246, Total loss : 5.8054022789001465\n",
      "learning rate A :  tf.Tensor(0.009567133, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009587315, shape=(), dtype=float32)\n",
      "Time for epoch 82 is 0.1690 sec\n",
      "train AE loss : 0.020492179319262505, train ANN loss : 3.7384274005889893\n",
      "AE loss : 0.02059449814260006, ANN loss : 3.7441062927246094, Total loss : 5.803555965423584\n",
      "learning rate A :  tf.Tensor(0.009557059, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009587315, shape=(), dtype=float32)\n",
      "Time for epoch 83 is 0.1753 sec\n",
      "train AE loss : 0.02047780528664589, train ANN loss : 3.7405154705047607\n",
      "AE loss : 0.02066810242831707, ANN loss : 3.725137233734131, Total loss : 5.791947364807129\n",
      "learning rate A :  tf.Tensor(0.009557059, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009577219, shape=(), dtype=float32)\n",
      "Time for epoch 84 is 0.1832 sec\n",
      "train AE loss : 0.020553501322865486, train ANN loss : 3.7238566875457764\n",
      "AE loss : 0.020940402522683144, ANN loss : 3.6867024898529053, Total loss : 5.780742645263672\n",
      "learning rate A :  tf.Tensor(0.009557059, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009567133, shape=(), dtype=float32)\n",
      "Time for epoch 85 is 0.1712 sec\n",
      "train AE loss : 0.020810816437005997, train ANN loss : 3.7474818229675293\n",
      "AE loss : 0.020754249766469002, ANN loss : 3.739229679107666, Total loss : 5.81465482711792\n",
      "learning rate A :  tf.Tensor(0.009557059, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009557059, shape=(), dtype=float32)\n",
      "Time for epoch 86 is 0.1732 sec\n",
      "train AE loss : 0.02064030058681965, train ANN loss : 3.7321949005126953\n",
      "AE loss : 0.02076495811343193, ANN loss : 3.7741165161132812, Total loss : 5.850612640380859\n",
      "learning rate A :  tf.Tensor(0.009557059, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009546995, shape=(), dtype=float32)\n",
      "Time for epoch 87 is 0.1851 sec\n",
      "train AE loss : 0.020648691803216934, train ANN loss : 3.7602949142456055\n",
      "AE loss : 0.02088421769440174, ANN loss : 3.731452465057373, Total loss : 5.8198747634887695\n",
      "learning rate A :  tf.Tensor(0.009557059, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009536942, shape=(), dtype=float32)\n",
      "Time for epoch 88 is 0.1739 sec\n",
      "train AE loss : 0.020768865942955017, train ANN loss : 3.7192494869232178\n",
      "AE loss : 0.021668097004294395, ANN loss : 3.651350975036621, Total loss : 5.8181610107421875\n",
      "learning rate A :  tf.Tensor(0.009557059, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009526899, shape=(), dtype=float32)\n",
      "Time for epoch 89 is 0.1712 sec\n",
      "train AE loss : 0.021532995626330376, train ANN loss : 3.68595290184021\n",
      "AE loss : 0.022146157920360565, ANN loss : 3.650167226791382, Total loss : 5.864782810211182\n",
      "learning rate A :  tf.Tensor(0.009557059, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009516866, shape=(), dtype=float32)\n",
      "Time for epoch 90 is 0.1738 sec\n",
      "train AE loss : 0.02200009860098362, train ANN loss : 3.699385166168213\n",
      "AE loss : 0.0213570948690176, ANN loss : 3.6894571781158447, Total loss : 5.825167179107666\n",
      "learning rate A :  tf.Tensor(0.009557059, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009506844, shape=(), dtype=float32)\n",
      "Time for epoch 91 is 0.1833 sec\n",
      "train AE loss : 0.02124256081879139, train ANN loss : 3.680330276489258\n",
      "AE loss : 0.02133920229971409, ANN loss : 3.6902239322662354, Total loss : 5.82414436340332\n",
      "learning rate A :  tf.Tensor(0.009546995, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009506844, shape=(), dtype=float32)\n",
      "Time for epoch 92 is 0.1783 sec\n",
      "train AE loss : 0.021224776282906532, train ANN loss : 3.6843862533569336\n",
      "AE loss : 0.021321486681699753, ANN loss : 3.6909642219543457, Total loss : 5.823113441467285\n",
      "learning rate A :  tf.Tensor(0.009536942, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009506844, shape=(), dtype=float32)\n",
      "Time for epoch 93 is 0.1689 sec\n",
      "train AE loss : 0.021207159385085106, train ANN loss : 3.6852593421936035\n",
      "AE loss : 0.02130386419594288, ANN loss : 3.691697835922241, Total loss : 5.822083950042725\n",
      "learning rate A :  tf.Tensor(0.009526899, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009506844, shape=(), dtype=float32)\n",
      "Time for epoch 94 is 0.1700 sec\n",
      "train AE loss : 0.021189618855714798, train ANN loss : 3.68632173538208\n",
      "AE loss : 0.021173352375626564, ANN loss : 3.7236826419830322, Total loss : 5.841018199920654\n",
      "learning rate A :  tf.Tensor(0.009526899, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009496833, shape=(), dtype=float32)\n",
      "Time for epoch 95 is 0.1735 sec\n",
      "train AE loss : 0.021060002967715263, train ANN loss : 3.7066636085510254\n",
      "AE loss : 0.021468283608555794, ANN loss : 3.689638614654541, Total loss : 5.8364667892456055\n",
      "learning rate A :  tf.Tensor(0.009526899, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009486833, shape=(), dtype=float32)\n",
      "Time for epoch 96 is 0.1738 sec\n",
      "train AE loss : 0.021354084834456444, train ANN loss : 3.6758930683135986\n",
      "AE loss : 0.021448036655783653, ANN loss : 3.690579891204834, Total loss : 5.835383415222168\n",
      "learning rate A :  tf.Tensor(0.009516866, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009486833, shape=(), dtype=float32)\n",
      "Time for epoch 97 is 0.1876 sec\n",
      "train AE loss : 0.021333957090973854, train ANN loss : 3.678518772125244\n",
      "AE loss : 0.021427923813462257, ANN loss : 3.691527843475342, Total loss : 5.834320068359375\n",
      "learning rate A :  tf.Tensor(0.009506844, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009486833, shape=(), dtype=float32)\n",
      "Time for epoch 98 is 0.1686 sec\n",
      "train AE loss : 0.021313946694135666, train ANN loss : 3.6811437606811523\n",
      "AE loss : 0.021407919004559517, ANN loss : 3.692476749420166, Total loss : 5.833268642425537\n",
      "learning rate A :  tf.Tensor(0.009496833, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009486833, shape=(), dtype=float32)\n",
      "Time for epoch 99 is 0.1836 sec\n",
      "train AE loss : 0.02129403129220009, train ANN loss : 3.6835787296295166\n",
      "AE loss : 0.021388094872236252, ANN loss : 3.693408727645874, Total loss : 5.832218170166016\n",
      "learning rate A :  tf.Tensor(0.009486833, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009486833, shape=(), dtype=float32)\n",
      "Time for epoch 100 is 0.1724 sec\n",
      "train AE loss : 0.02127428725361824, train ANN loss : 3.68312931060791\n",
      "AE loss : 0.021368296816945076, ANN loss : 3.6943345069885254, Total loss : 5.831164360046387\n",
      "learning rate A :  tf.Tensor(0.009476842, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009486833, shape=(), dtype=float32)\n",
      "Time for epoch 101 is 0.1699 sec\n",
      "train AE loss : 0.021254565566778183, train ANN loss : 3.684194326400757\n",
      "AE loss : 0.02134856954216957, ANN loss : 3.6952576637268066, Total loss : 5.830114841461182\n",
      "learning rate A :  tf.Tensor(0.009466862, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009486833, shape=(), dtype=float32)\n",
      "Time for epoch 102 is 0.1684 sec\n",
      "train AE loss : 0.02123493142426014, train ANN loss : 3.682664632797241\n",
      "AE loss : 0.022447975352406502, ANN loss : 3.6311774253845215, Total loss : 5.875975131988525\n",
      "learning rate A :  tf.Tensor(0.009466862, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009476842, shape=(), dtype=float32)\n",
      "Time for epoch 103 is 0.1740 sec\n",
      "train AE loss : 0.022311311215162277, train ANN loss : 3.6587140560150146\n",
      "AE loss : 0.02323785051703453, ANN loss : 3.6310367584228516, Total loss : 5.954821586608887\n",
      "learning rate A :  tf.Tensor(0.009466862, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009466862, shape=(), dtype=float32)\n",
      "Time for epoch 104 is 0.1739 sec\n",
      "train AE loss : 0.02306712046265602, train ANN loss : 3.6988205909729004\n",
      "AE loss : 0.021567700430750847, ANN loss : 3.6680357456207275, Total loss : 5.824805736541748\n",
      "learning rate A :  tf.Tensor(0.009466862, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009456893, shape=(), dtype=float32)\n",
      "Time for epoch 105 is 0.1713 sec\n",
      "train AE loss : 0.021447855979204178, train ANN loss : 3.654839038848877\n",
      "AE loss : 0.021540436893701553, ANN loss : 3.6693503856658936, Total loss : 5.823394775390625\n",
      "learning rate A :  tf.Tensor(0.009456893, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009456893, shape=(), dtype=float32)\n",
      "Time for epoch 106 is 0.1698 sec\n",
      "train AE loss : 0.021420951932668686, train ANN loss : 3.655768871307373\n",
      "AE loss : 0.021086644381284714, ANN loss : 3.7174010276794434, Total loss : 5.826065540313721\n",
      "learning rate A :  tf.Tensor(0.009456893, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009446935, shape=(), dtype=float32)\n",
      "Time for epoch 107 is 0.1725 sec\n",
      "train AE loss : 0.020968493074178696, train ANN loss : 3.69203519821167\n",
      "AE loss : 0.021062694489955902, ANN loss : 3.7189342975616455, Total loss : 5.825203895568848\n",
      "learning rate A :  tf.Tensor(0.009446935, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009446935, shape=(), dtype=float32)\n",
      "Time for epoch 108 is 0.1745 sec\n",
      "train AE loss : 0.020944669842720032, train ANN loss : 3.6922874450683594\n",
      "AE loss : 0.02103702910244465, ANN loss : 3.7206006050109863, Total loss : 5.824303150177002\n",
      "learning rate A :  tf.Tensor(0.009436986, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009446935, shape=(), dtype=float32)\n",
      "Time for epoch 109 is 0.1694 sec\n",
      "train AE loss : 0.020919065922498703, train ANN loss : 3.6928889751434326\n",
      "AE loss : 0.021166345104575157, ANN loss : 3.6864380836486816, Total loss : 5.803072452545166\n",
      "learning rate A :  tf.Tensor(0.009436986, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009436986, shape=(), dtype=float32)\n",
      "Time for epoch 110 is 0.1726 sec\n",
      "train AE loss : 0.02104426734149456, train ANN loss : 3.6613593101501465\n",
      "AE loss : 0.021137479692697525, ANN loss : 3.6883203983306885, Total loss : 5.80206823348999\n",
      "learning rate A :  tf.Tensor(0.009427049, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009436986, shape=(), dtype=float32)\n",
      "Time for epoch 111 is 0.1702 sec\n",
      "train AE loss : 0.021015768870711327, train ANN loss : 3.6633880138397217\n",
      "AE loss : 0.02203402668237686, ANN loss : 3.6244239807128906, Total loss : 5.827826499938965\n",
      "learning rate A :  tf.Tensor(0.009427049, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009427049, shape=(), dtype=float32)\n",
      "Time for epoch 112 is 0.1731 sec\n",
      "train AE loss : 0.021886369213461876, train ANN loss : 3.6445388793945312\n",
      "AE loss : 0.021975770592689514, ANN loss : 3.6248180866241455, Total loss : 5.822394847869873\n",
      "learning rate A :  tf.Tensor(0.009417123, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009427049, shape=(), dtype=float32)\n",
      "Time for epoch 113 is 0.1697 sec\n",
      "train AE loss : 0.0218295119702816, train ANN loss : 3.6431374549865723\n",
      "AE loss : 0.02291559986770153, ANN loss : 3.6046981811523438, Total loss : 5.8962578773498535\n",
      "learning rate A :  tf.Tensor(0.009417123, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009417123, shape=(), dtype=float32)\n",
      "Time for epoch 114 is 0.1754 sec\n",
      "train AE loss : 0.022752635180950165, train ANN loss : 3.646595001220703\n",
      "AE loss : 0.022833572700619698, ANN loss : 3.603994369506836, Total loss : 5.8873515129089355\n",
      "learning rate A :  tf.Tensor(0.0094072055, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009417123, shape=(), dtype=float32)\n",
      "Time for epoch 115 is 0.1747 sec\n",
      "train AE loss : 0.022673653438687325, train ANN loss : 3.6430130004882812\n",
      "AE loss : 0.022337431088089943, ANN loss : 3.6172871589660645, Total loss : 5.851030349731445\n",
      "learning rate A :  tf.Tensor(0.0094072055, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0094072055, shape=(), dtype=float32)\n",
      "Time for epoch 116 is 0.1749 sec\n",
      "train AE loss : 0.02220696210861206, train ANN loss : 3.6219935417175293\n",
      "AE loss : 0.022288022562861443, ANN loss : 3.6186656951904297, Total loss : 5.84746789932251\n",
      "learning rate A :  tf.Tensor(0.0093973, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0094072055, shape=(), dtype=float32)\n",
      "Time for epoch 117 is 0.1689 sec\n",
      "train AE loss : 0.02215884067118168, train ANN loss : 3.6237077713012695\n",
      "AE loss : 0.021935690194368362, ANN loss : 3.6407947540283203, Total loss : 5.8343634605407715\n",
      "learning rate A :  tf.Tensor(0.0093973, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0093973, shape=(), dtype=float32)\n",
      "Time for epoch 118 is 0.1720 sec\n",
      "train AE loss : 0.021817488595843315, train ANN loss : 3.6336820125579834\n",
      "AE loss : 0.021976765245199203, ANN loss : 3.637619733810425, Total loss : 5.835296154022217\n",
      "learning rate A :  tf.Tensor(0.0093973, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009387404, shape=(), dtype=float32)\n",
      "Time for epoch 119 is 0.1755 sec\n",
      "train AE loss : 0.021858034655451775, train ANN loss : 3.6320528984069824\n",
      "AE loss : 0.02194000408053398, ANN loss : 3.6392154693603516, Total loss : 5.833216667175293\n",
      "learning rate A :  tf.Tensor(0.009387404, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009387404, shape=(), dtype=float32)\n",
      "Time for epoch 120 is 0.1722 sec\n",
      "train AE loss : 0.02182186022400856, train ANN loss : 3.631272554397583\n",
      "AE loss : 0.02190312370657921, ANN loss : 3.6408607959747314, Total loss : 5.831173419952393\n",
      "learning rate A :  tf.Tensor(0.009377518, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009387404, shape=(), dtype=float32)\n",
      "Time for epoch 121 is 0.1675 sec\n",
      "train AE loss : 0.021785574033856392, train ANN loss : 3.6318888664245605\n",
      "AE loss : 0.022300943732261658, ANN loss : 3.6161599159240723, Total loss : 5.846253871917725\n",
      "learning rate A :  tf.Tensor(0.009377518, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009377518, shape=(), dtype=float32)\n",
      "Time for epoch 122 is 0.1728 sec\n",
      "train AE loss : 0.02216961793601513, train ANN loss : 3.6223177909851074\n",
      "AE loss : 0.022748824208974838, ANN loss : 3.599574565887451, Total loss : 5.874456882476807\n",
      "learning rate A :  tf.Tensor(0.009377518, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009367643, shape=(), dtype=float32)\n",
      "Time for epoch 123 is 0.1833 sec\n",
      "train AE loss : 0.02259468287229538, train ANN loss : 3.6260125637054443\n",
      "AE loss : 0.022462956607341766, ANN loss : 3.6078360080718994, Total loss : 5.854131698608398\n",
      "learning rate A :  tf.Tensor(0.009377518, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009357778, shape=(), dtype=float32)\n",
      "Time for epoch 124 is 0.1728 sec\n",
      "train AE loss : 0.02232172340154648, train ANN loss : 3.6012871265411377\n",
      "AE loss : 0.022421764209866524, ANN loss : 3.600064992904663, Total loss : 5.8422417640686035\n",
      "learning rate A :  tf.Tensor(0.009377518, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009347924, shape=(), dtype=float32)\n",
      "Time for epoch 125 is 0.1941 sec\n",
      "train AE loss : 0.022274401038885117, train ANN loss : 3.589170217514038\n",
      "AE loss : 0.022704070433974266, ANN loss : 3.5887279510498047, Total loss : 5.859135150909424\n",
      "learning rate A :  tf.Tensor(0.009377518, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00933808, shape=(), dtype=float32)\n",
      "Time for epoch 126 is 0.1834 sec\n",
      "train AE loss : 0.02253568544983864, train ANN loss : 3.5937294960021973\n",
      "AE loss : 0.02248268760740757, ANN loss : 3.5966668128967285, Total loss : 5.844935894012451\n",
      "learning rate A :  tf.Tensor(0.009377518, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009328247, shape=(), dtype=float32)\n",
      "Time for epoch 127 is 0.1844 sec\n",
      "train AE loss : 0.02232113480567932, train ANN loss : 3.583380937576294\n",
      "AE loss : 0.022474002093076706, ANN loss : 3.6083545684814453, Total loss : 5.85575532913208\n",
      "learning rate A :  tf.Tensor(0.009377518, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009318423, shape=(), dtype=float32)\n",
      "Time for epoch 128 is 0.1825 sec\n",
      "train AE loss : 0.0223186444491148, train ANN loss : 3.5795204639434814\n",
      "AE loss : 0.0224093496799469, ANN loss : 3.6115825176239014, Total loss : 5.8525166511535645\n",
      "learning rate A :  tf.Tensor(0.009367643, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009318423, shape=(), dtype=float32)\n",
      "Time for epoch 129 is 0.1699 sec\n",
      "train AE loss : 0.022255392745137215, train ANN loss : 3.585559606552124\n",
      "AE loss : 0.023246606811881065, ANN loss : 3.5727086067199707, Total loss : 5.897369384765625\n",
      "learning rate A :  tf.Tensor(0.009367643, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009308611, shape=(), dtype=float32)\n",
      "Time for epoch 130 is 0.1835 sec\n",
      "train AE loss : 0.0230766199529171, train ANN loss : 3.5656397342681885\n",
      "AE loss : 0.02314499393105507, ANN loss : 3.576099395751953, Total loss : 5.890599727630615\n",
      "learning rate A :  tf.Tensor(0.009357778, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009308611, shape=(), dtype=float32)\n",
      "Time for epoch 131 is 0.1707 sec\n",
      "train AE loss : 0.022978393360972404, train ANN loss : 3.566666841506958\n",
      "AE loss : 0.023049242794513702, ANN loss : 3.5795388221740723, Total loss : 5.884463310241699\n",
      "learning rate A :  tf.Tensor(0.009347924, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009308611, shape=(), dtype=float32)\n",
      "Time for epoch 132 is 0.1704 sec\n",
      "train AE loss : 0.02288546785712242, train ANN loss : 3.566249132156372\n",
      "AE loss : 0.022957604378461838, ANN loss : 3.5831174850463867, Total loss : 5.878877639770508\n",
      "learning rate A :  tf.Tensor(0.00933808, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009308611, shape=(), dtype=float32)\n",
      "Time for epoch 133 is 0.1771 sec\n",
      "train AE loss : 0.022796278819441795, train ANN loss : 3.570949077606201\n",
      "AE loss : 0.024374589323997498, ANN loss : 3.5408267974853516, Total loss : 5.978286266326904\n",
      "learning rate A :  tf.Tensor(0.00933808, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009298808, shape=(), dtype=float32)\n",
      "Time for epoch 134 is 0.1775 sec\n",
      "train AE loss : 0.02417495846748352, train ANN loss : 3.57767653465271\n",
      "AE loss : 0.023376064375042915, ANN loss : 3.578918695449829, Total loss : 5.916525363922119\n",
      "learning rate A :  tf.Tensor(0.00933808, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009289017, shape=(), dtype=float32)\n",
      "Time for epoch 135 is 0.1734 sec\n",
      "train AE loss : 0.023217124864459038, train ANN loss : 3.5589401721954346\n",
      "AE loss : 0.02326507866382599, ANN loss : 3.584914207458496, Total loss : 5.911421775817871\n",
      "learning rate A :  tf.Tensor(0.009328247, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009289017, shape=(), dtype=float32)\n",
      "Time for epoch 136 is 0.1688 sec\n",
      "train AE loss : 0.023109449073672295, train ANN loss : 3.5627522468566895\n",
      "AE loss : 0.023157190531492233, ANN loss : 3.5911502838134766, Total loss : 5.906869411468506\n",
      "learning rate A :  tf.Tensor(0.009318423, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009289017, shape=(), dtype=float32)\n",
      "Time for epoch 137 is 0.1710 sec\n",
      "train AE loss : 0.02300468645989895, train ANN loss : 3.5654702186584473\n",
      "AE loss : 0.023572364822030067, ANN loss : 3.5594513416290283, Total loss : 5.916687488555908\n",
      "learning rate A :  tf.Tensor(0.009318423, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009279234, shape=(), dtype=float32)\n",
      "Time for epoch 138 is 0.1726 sec\n",
      "train AE loss : 0.023405198007822037, train ANN loss : 3.5510354042053223\n",
      "AE loss : 0.023442020639777184, ANN loss : 3.5645864009857178, Total loss : 5.908788681030273\n",
      "learning rate A :  tf.Tensor(0.009308611, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009279234, shape=(), dtype=float32)\n",
      "Time for epoch 139 is 0.1722 sec\n",
      "train AE loss : 0.023279324173927307, train ANN loss : 3.5511045455932617\n",
      "AE loss : 0.023317286744713783, ANN loss : 3.569890022277832, Total loss : 5.901618480682373\n",
      "learning rate A :  tf.Tensor(0.009298808, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009279234, shape=(), dtype=float32)\n",
      "Time for epoch 140 is 0.1706 sec\n",
      "train AE loss : 0.02315863035619259, train ANN loss : 3.553912878036499\n",
      "AE loss : 0.02319950982928276, ANN loss : 3.5753095149993896, Total loss : 5.895260810852051\n",
      "learning rate A :  tf.Tensor(0.009289017, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009279234, shape=(), dtype=float32)\n",
      "Time for epoch 141 is 0.1692 sec\n",
      "train AE loss : 0.02304474078118801, train ANN loss : 3.5578160285949707\n",
      "AE loss : 0.024520713835954666, ANN loss : 3.535327911376953, Total loss : 5.987399101257324\n",
      "learning rate A :  tf.Tensor(0.009289017, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009269463, shape=(), dtype=float32)\n",
      "Time for epoch 142 is 0.1739 sec\n",
      "train AE loss : 0.02431451342999935, train ANN loss : 3.5703699588775635\n",
      "AE loss : 0.02431776002049446, ANN loss : 3.533512592315674, Total loss : 5.9652886390686035\n",
      "learning rate A :  tf.Tensor(0.009279234, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009269463, shape=(), dtype=float32)\n",
      "Time for epoch 143 is 0.1915 sec\n",
      "train AE loss : 0.02411951683461666, train ANN loss : 3.557644844055176\n",
      "AE loss : 0.024127855896949768, ANN loss : 3.533318281173706, Total loss : 5.946103572845459\n",
      "learning rate A :  tf.Tensor(0.009269463, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009269463, shape=(), dtype=float32)\n",
      "Time for epoch 144 is 0.1824 sec\n",
      "train AE loss : 0.023936675861477852, train ANN loss : 3.554948091506958\n",
      "AE loss : 0.023946549743413925, ANN loss : 3.5344412326812744, Total loss : 5.92909574508667\n",
      "learning rate A :  tf.Tensor(0.009259702, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009269463, shape=(), dtype=float32)\n",
      "Time for epoch 145 is 0.1707 sec\n",
      "train AE loss : 0.023761948570609093, train ANN loss : 3.5472121238708496\n",
      "AE loss : 0.023851776495575905, ANN loss : 3.533367156982422, Total loss : 5.918545246124268\n",
      "learning rate A :  tf.Tensor(0.009259702, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009259702, shape=(), dtype=float32)\n",
      "Time for epoch 146 is 0.1829 sec\n",
      "train AE loss : 0.02366510033607483, train ANN loss : 3.5416126251220703\n",
      "AE loss : 0.022855514660477638, ANN loss : 3.580580472946167, Total loss : 5.86613130569458\n",
      "learning rate A :  tf.Tensor(0.009259702, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009249951, shape=(), dtype=float32)\n",
      "Time for epoch 147 is 0.1841 sec\n",
      "train AE loss : 0.022700238972902298, train ANN loss : 3.5438828468322754\n",
      "AE loss : 0.023472348228096962, ANN loss : 3.5394675731658936, Total loss : 5.886702537536621\n",
      "learning rate A :  tf.Tensor(0.009259702, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00924021, shape=(), dtype=float32)\n",
      "Time for epoch 148 is 0.1730 sec\n",
      "train AE loss : 0.023293407633900642, train ANN loss : 3.52233624458313\n",
      "AE loss : 0.023327788338065147, ANN loss : 3.5458412170410156, Total loss : 5.878620147705078\n",
      "learning rate A :  tf.Tensor(0.009249951, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00924021, shape=(), dtype=float32)\n",
      "Time for epoch 149 is 0.1696 sec\n",
      "train AE loss : 0.023154109716415405, train ANN loss : 3.52536940574646\n",
      "AE loss : 0.02319144643843174, ANN loss : 3.552128314971924, Total loss : 5.871273040771484\n",
      "learning rate A :  tf.Tensor(0.00924021, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00924021, shape=(), dtype=float32)\n",
      "Time for epoch 150 is 0.1678 sec\n",
      "train AE loss : 0.023022670298814774, train ANN loss : 3.526721477508545\n",
      "AE loss : 0.023066049441695213, ANN loss : 3.5580785274505615, Total loss : 5.864683628082275\n",
      "learning rate A :  tf.Tensor(0.00923048, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00924021, shape=(), dtype=float32)\n",
      "Time for epoch 151 is 0.1704 sec\n",
      "train AE loss : 0.02290186658501625, train ANN loss : 3.5305440425872803\n",
      "AE loss : 0.02294813096523285, ANN loss : 3.5639381408691406, Total loss : 5.8587517738342285\n",
      "learning rate A :  tf.Tensor(0.009220759, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00924021, shape=(), dtype=float32)\n",
      "Time for epoch 152 is 0.1705 sec\n",
      "train AE loss : 0.022788383066654205, train ANN loss : 3.5318241119384766\n",
      "AE loss : 0.025026479735970497, ANN loss : 3.5268304347991943, Total loss : 6.029478549957275\n",
      "learning rate A :  tf.Tensor(0.009220759, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00923048, shape=(), dtype=float32)\n",
      "Time for epoch 153 is 0.1942 sec\n",
      "train AE loss : 0.024786192923784256, train ANN loss : 3.585141658782959\n",
      "AE loss : 0.024760954082012177, ANN loss : 3.517864942550659, Total loss : 5.993960380554199\n",
      "learning rate A :  tf.Tensor(0.00921105, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00923048, shape=(), dtype=float32)\n",
      "Time for epoch 154 is 0.1974 sec\n",
      "train AE loss : 0.02453063800930977, train ANN loss : 3.5650548934936523\n",
      "AE loss : 0.02451556921005249, ANN loss : 3.512697219848633, Total loss : 5.964253902435303\n",
      "learning rate A :  tf.Tensor(0.00920135, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00923048, shape=(), dtype=float32)\n",
      "Time for epoch 155 is 0.1709 sec\n",
      "train AE loss : 0.02429514192044735, train ANN loss : 3.552698850631714\n",
      "AE loss : 0.024275798350572586, ANN loss : 3.51015305519104, Total loss : 5.937732696533203\n",
      "learning rate A :  tf.Tensor(0.00919166, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00923048, shape=(), dtype=float32)\n",
      "Time for epoch 156 is 0.1817 sec\n",
      "train AE loss : 0.02406432293355465, train ANN loss : 3.535574197769165\n",
      "AE loss : 0.023933015763759613, ANN loss : 3.5104012489318848, Total loss : 5.903703689575195\n",
      "learning rate A :  tf.Tensor(0.009181982, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00923048, shape=(), dtype=float32)\n",
      "Time for epoch 157 is 0.1711 sec\n",
      "train AE loss : 0.02373320609331131, train ANN loss : 3.527384042739868\n",
      "AE loss : 0.02392445132136345, ANN loss : 3.5025978088378906, Total loss : 5.89504337310791\n",
      "learning rate A :  tf.Tensor(0.009181982, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009220759, shape=(), dtype=float32)\n",
      "Time for epoch 158 is 0.1726 sec\n",
      "train AE loss : 0.02372654899954796, train ANN loss : 3.5122134685516357\n",
      "AE loss : 0.02296501211822033, ANN loss : 3.5635430812835693, Total loss : 5.860044479370117\n",
      "learning rate A :  tf.Tensor(0.009181982, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00921105, shape=(), dtype=float32)\n",
      "Time for epoch 159 is 0.1736 sec\n",
      "train AE loss : 0.02280285954475403, train ANN loss : 3.5289368629455566\n",
      "AE loss : 0.024567637592554092, ANN loss : 3.4883947372436523, Total loss : 5.9451584815979\n",
      "learning rate A :  tf.Tensor(0.009181982, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00920135, shape=(), dtype=float32)\n",
      "Time for epoch 160 is 0.1721 sec\n",
      "train AE loss : 0.02434602566063404, train ANN loss : 3.5007779598236084\n",
      "AE loss : 0.02416287735104561, ANN loss : 3.4976885318756104, Total loss : 5.913975715637207\n",
      "learning rate A :  tf.Tensor(0.009181982, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00919166, shape=(), dtype=float32)\n",
      "Time for epoch 161 is 0.1734 sec\n",
      "train AE loss : 0.023953480646014214, train ANN loss : 3.489257335662842\n",
      "AE loss : 0.02360101044178009, ANN loss : 3.525627374649048, Total loss : 5.88572883605957\n",
      "learning rate A :  tf.Tensor(0.009181982, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009181982, shape=(), dtype=float32)\n",
      "Time for epoch 162 is 0.1735 sec\n",
      "train AE loss : 0.023409167304635048, train ANN loss : 3.4935050010681152\n",
      "AE loss : 0.02330680377781391, ANN loss : 3.538613796234131, Total loss : 5.869294166564941\n",
      "learning rate A :  tf.Tensor(0.009172312, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009181982, shape=(), dtype=float32)\n",
      "Time for epoch 163 is 0.1688 sec\n",
      "train AE loss : 0.02312377654016018, train ANN loss : 3.501267433166504\n",
      "AE loss : 0.02491741068661213, ANN loss : 3.4862618446350098, Total loss : 5.978003025054932\n",
      "learning rate A :  tf.Tensor(0.009172312, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009172312, shape=(), dtype=float32)\n",
      "Time for epoch 164 is 0.1729 sec\n",
      "train AE loss : 0.024681847542524338, train ANN loss : 3.49521803855896\n",
      "AE loss : 0.02324633300304413, ANN loss : 3.556380033493042, Total loss : 5.881012916564941\n",
      "learning rate A :  tf.Tensor(0.009172312, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009162653, shape=(), dtype=float32)\n",
      "Time for epoch 165 is 0.1722 sec\n",
      "train AE loss : 0.023070141673088074, train ANN loss : 3.511876344680786\n",
      "AE loss : 0.02299170009791851, ANN loss : 3.5752317905426025, Total loss : 5.874402046203613\n",
      "learning rate A :  tf.Tensor(0.009162653, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009162653, shape=(), dtype=float32)\n",
      "Time for epoch 166 is 0.1834 sec\n",
      "train AE loss : 0.0228225439786911, train ANN loss : 3.5246217250823975\n",
      "AE loss : 0.0227668359875679, ANN loss : 3.594482421875, Total loss : 5.871165752410889\n",
      "learning rate A :  tf.Tensor(0.009153005, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009162653, shape=(), dtype=float32)\n",
      "Time for epoch 167 is 0.1707 sec\n",
      "train AE loss : 0.0226034764200449, train ANN loss : 3.5392653942108154\n",
      "AE loss : 0.02256387285888195, ANN loss : 3.6144027709960938, Total loss : 5.870790004730225\n",
      "learning rate A :  tf.Tensor(0.0091433665, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009162653, shape=(), dtype=float32)\n",
      "Time for epoch 168 is 0.1699 sec\n",
      "train AE loss : 0.022405538707971573, train ANN loss : 3.5568833351135254\n",
      "AE loss : 0.022381577640771866, ANN loss : 3.634542226791382, Total loss : 5.872699737548828\n",
      "learning rate A :  tf.Tensor(0.0091337375, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009162653, shape=(), dtype=float32)\n",
      "Time for epoch 169 is 0.1695 sec\n",
      "train AE loss : 0.022227786481380463, train ANN loss : 3.5737476348876953\n",
      "AE loss : 0.02614624798297882, ANN loss : 3.490434408187866, Total loss : 6.105059623718262\n",
      "learning rate A :  tf.Tensor(0.0091337375, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009153005, shape=(), dtype=float32)\n",
      "Time for epoch 170 is 0.1745 sec\n",
      "train AE loss : 0.02587677724659443, train ANN loss : 3.5528838634490967\n",
      "AE loss : 0.02343541942536831, ANN loss : 3.532257080078125, Total loss : 5.875799179077148\n",
      "learning rate A :  tf.Tensor(0.0091337375, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0091433665, shape=(), dtype=float32)\n",
      "Time for epoch 171 is 0.1740 sec\n",
      "train AE loss : 0.023253949359059334, train ANN loss : 3.4900143146514893\n",
      "AE loss : 0.02314026840031147, ANN loss : 3.548123359680176, Total loss : 5.862149715423584\n",
      "learning rate A :  tf.Tensor(0.00912412, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0091433665, shape=(), dtype=float32)\n",
      "Time for epoch 172 is 0.1774 sec\n",
      "train AE loss : 0.022966600954532623, train ANN loss : 3.5032262802124023\n",
      "AE loss : 0.02287551388144493, ANN loss : 3.5642755031585693, Total loss : 5.8518266677856445\n",
      "learning rate A :  tf.Tensor(0.009114511, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0091433665, shape=(), dtype=float32)\n",
      "Time for epoch 173 is 0.2012 sec\n",
      "train AE loss : 0.022708317264914513, train ANN loss : 3.512895345687866\n",
      "AE loss : 0.0235263891518116, ANN loss : 3.524435520172119, Total loss : 5.877074241638184\n",
      "learning rate A :  tf.Tensor(0.009114511, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0091337375, shape=(), dtype=float32)\n",
      "Time for epoch 174 is 0.1806 sec\n",
      "train AE loss : 0.02334468439221382, train ANN loss : 3.4868831634521484\n",
      "AE loss : 0.02320430986583233, ANN loss : 3.5395021438598633, Total loss : 5.859932899475098\n",
      "learning rate A :  tf.Tensor(0.009104913, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0091337375, shape=(), dtype=float32)\n",
      "Time for epoch 175 is 0.1875 sec\n",
      "train AE loss : 0.023030348122119904, train ANN loss : 3.49391770362854\n",
      "AE loss : 0.022920921444892883, ANN loss : 3.5551700592041016, Total loss : 5.847262382507324\n",
      "learning rate A :  tf.Tensor(0.009095325, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0091337375, shape=(), dtype=float32)\n",
      "Time for epoch 176 is 0.1805 sec\n",
      "train AE loss : 0.022753490135073662, train ANN loss : 3.5078439712524414\n",
      "AE loss : 0.02669368125498295, ANN loss : 3.4902710914611816, Total loss : 6.15963888168335\n",
      "learning rate A :  tf.Tensor(0.009095325, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00912412, shape=(), dtype=float32)\n",
      "Time for epoch 177 is 0.1833 sec\n",
      "train AE loss : 0.02642766572535038, train ANN loss : 3.561038017272949\n",
      "AE loss : 0.02490433119237423, ANN loss : 3.4835426807403564, Total loss : 5.973976135253906\n",
      "learning rate A :  tf.Tensor(0.009095325, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009114511, shape=(), dtype=float32)\n",
      "Time for epoch 178 is 0.1832 sec\n",
      "train AE loss : 0.024705635383725166, train ANN loss : 3.461812734603882\n",
      "AE loss : 0.024418286979198456, ANN loss : 3.503293991088867, Total loss : 5.945122718811035\n",
      "learning rate A :  tf.Tensor(0.009085747, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009114511, shape=(), dtype=float32)\n",
      "Time for epoch 179 is 0.1792 sec\n",
      "train AE loss : 0.024232711642980576, train ANN loss : 3.4732232093811035\n",
      "AE loss : 0.025087179616093636, ANN loss : 3.5126118659973145, Total loss : 6.021329879760742\n",
      "learning rate A :  tf.Tensor(0.009085747, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009104913, shape=(), dtype=float32)\n",
      "Time for epoch 180 is 0.1840 sec\n",
      "train AE loss : 0.024893928319215775, train ANN loss : 3.4803738594055176\n",
      "AE loss : 0.02458956092596054, ANN loss : 3.5382604598999023, Total loss : 5.997215747833252\n",
      "learning rate A :  tf.Tensor(0.00907618, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009104913, shape=(), dtype=float32)\n",
      "Time for epoch 181 is 0.1820 sec\n",
      "train AE loss : 0.02440999075770378, train ANN loss : 3.497833490371704\n",
      "AE loss : 0.029758306220173836, ANN loss : 3.48873233795166, Total loss : 6.464562892913818\n",
      "learning rate A :  tf.Tensor(0.00907618, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009095325, shape=(), dtype=float32)\n",
      "Time for epoch 182 is 0.1827 sec\n",
      "train AE loss : 0.029410699382424355, train ANN loss : 3.5790717601776123\n",
      "AE loss : 0.024890316650271416, ANN loss : 3.533876657485962, Total loss : 6.0229082107543945\n",
      "learning rate A :  tf.Tensor(0.00907618, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009085747, shape=(), dtype=float32)\n",
      "Time for epoch 183 is 0.1824 sec\n",
      "train AE loss : 0.024696165695786476, train ANN loss : 3.478057384490967\n",
      "AE loss : 0.02439766563475132, ANN loss : 3.5578043460845947, Total loss : 5.9975714683532715\n",
      "learning rate A :  tf.Tensor(0.009066622, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009085747, shape=(), dtype=float32)\n",
      "Time for epoch 184 is 0.1812 sec\n",
      "train AE loss : 0.024215828627347946, train ANN loss : 3.494131565093994\n",
      "AE loss : 0.024184556677937508, ANN loss : 3.570016384124756, Total loss : 5.9884724617004395\n",
      "learning rate A :  tf.Tensor(0.009066622, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00907618, shape=(), dtype=float32)\n",
      "Time for epoch 185 is 0.1833 sec\n",
      "train AE loss : 0.023999324068427086, train ANN loss : 3.498975992202759\n",
      "AE loss : 0.02377142570912838, ANN loss : 3.5884313583374023, Total loss : 5.965573787689209\n",
      "learning rate A :  tf.Tensor(0.009057074, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00907618, shape=(), dtype=float32)\n",
      "Time for epoch 186 is 0.1896 sec\n",
      "train AE loss : 0.023596253246068954, train ANN loss : 3.5126495361328125\n",
      "AE loss : 0.026735490188002586, ANN loss : 3.543757438659668, Total loss : 6.217305660247803\n",
      "learning rate A :  tf.Tensor(0.009057074, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009066622, shape=(), dtype=float32)\n",
      "Time for epoch 187 is 0.1860 sec\n",
      "train AE loss : 0.026486190035939217, train ANN loss : 3.524235725402832\n",
      "AE loss : 0.025932561606168747, ANN loss : 3.5477335453033447, Total loss : 6.140989780426025\n",
      "learning rate A :  tf.Tensor(0.009047537, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009066622, shape=(), dtype=float32)\n",
      "Time for epoch 188 is 0.1808 sec\n",
      "train AE loss : 0.025703944265842438, train ANN loss : 3.5233571529388428\n",
      "AE loss : 0.029009344056248665, ANN loss : 3.4852869510650635, Total loss : 6.386220932006836\n",
      "learning rate A :  tf.Tensor(0.009047537, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009057074, shape=(), dtype=float32)\n",
      "Time for epoch 189 is 0.1856 sec\n",
      "train AE loss : 0.028714483603835106, train ANN loss : 3.4843409061431885\n",
      "AE loss : 0.027788670733571053, ANN loss : 3.489424705505371, Total loss : 6.268291473388672\n",
      "learning rate A :  tf.Tensor(0.00903801, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009057074, shape=(), dtype=float32)\n",
      "Time for epoch 190 is 0.1939 sec\n",
      "train AE loss : 0.02752629667520523, train ANN loss : 3.470496654510498\n",
      "AE loss : 0.02681228332221508, ANN loss : 3.497865676879883, Total loss : 6.1790947914123535\n",
      "learning rate A :  tf.Tensor(0.0090284925, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009057074, shape=(), dtype=float32)\n",
      "Time for epoch 191 is 0.1794 sec\n",
      "train AE loss : 0.02657347172498703, train ANN loss : 3.4642767906188965\n",
      "AE loss : 0.02995261736214161, ANN loss : 3.4431397914886475, Total loss : 6.438401699066162\n",
      "learning rate A :  tf.Tensor(0.0090284925, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009047537, shape=(), dtype=float32)\n",
      "Time for epoch 192 is 0.1845 sec\n",
      "train AE loss : 0.02965281717479229, train ANN loss : 3.4416236877441406\n",
      "AE loss : 0.03171549737453461, ANN loss : 3.4418816566467285, Total loss : 6.613431453704834\n",
      "learning rate A :  tf.Tensor(0.0090284925, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00903801, shape=(), dtype=float32)\n",
      "Time for epoch 193 is 0.1841 sec\n",
      "train AE loss : 0.031378209590911865, train ANN loss : 3.451594591140747\n",
      "AE loss : 0.031525902450084686, ANN loss : 3.496955156326294, Total loss : 6.649545192718506\n",
      "learning rate A :  tf.Tensor(0.0090284925, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0090284925, shape=(), dtype=float32)\n",
      "Time for epoch 194 is 0.1922 sec\n",
      "train AE loss : 0.031199393793940544, train ANN loss : 3.478847026824951\n",
      "AE loss : 0.029788589105010033, ANN loss : 3.5552470684051514, Total loss : 6.534106254577637\n",
      "learning rate A :  tf.Tensor(0.009018985, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0090284925, shape=(), dtype=float32)\n",
      "Time for epoch 195 is 0.1784 sec\n",
      "train AE loss : 0.02951514720916748, train ANN loss : 3.516231060028076\n",
      "AE loss : 0.037414394319057465, ANN loss : 3.4468045234680176, Total loss : 7.188243865966797\n",
      "learning rate A :  tf.Tensor(0.009018985, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009018985, shape=(), dtype=float32)\n",
      "Time for epoch 196 is 0.1831 sec\n",
      "train AE loss : 0.03687999024987221, train ANN loss : 3.5345304012298584\n",
      "AE loss : 0.030665790662169456, ANN loss : 3.4871373176574707, Total loss : 6.55371618270874\n",
      "learning rate A :  tf.Tensor(0.009018985, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009009487, shape=(), dtype=float32)\n",
      "Time for epoch 197 is 0.1943 sec\n",
      "train AE loss : 0.03034432791173458, train ANN loss : 3.430058240890503\n",
      "AE loss : 0.027608608826994896, ANN loss : 3.5703234672546387, Total loss : 6.331184387207031\n",
      "learning rate A :  tf.Tensor(0.009018985, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009, shape=(), dtype=float32)\n",
      "Time for epoch 198 is 0.1808 sec\n",
      "train AE loss : 0.027356846258044243, train ANN loss : 3.4735708236694336\n",
      "AE loss : 0.02665247768163681, ANN loss : 3.6057627201080322, Total loss : 6.271010398864746\n",
      "learning rate A :  tf.Tensor(0.009009487, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.009, shape=(), dtype=float32)\n",
      "Time for epoch 199 is 0.1789 sec\n",
      "train AE loss : 0.02642194554209709, train ANN loss : 3.5090222358703613\n",
      "AE loss : 0.0295685026794672, ANN loss : 3.516413450241089, Total loss : 6.473263740539551\n",
      "learning rate A :  tf.Tensor(0.009009487, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008990522, shape=(), dtype=float32)\n",
      "Time for epoch 200 is 0.1843 sec\n",
      "train AE loss : 0.029262559488415718, train ANN loss : 3.45742130279541\n",
      "AE loss : 0.03367811068892479, ANN loss : 3.5203661918640137, Total loss : 6.888176918029785\n",
      "learning rate A :  tf.Tensor(0.009009487, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008981055, shape=(), dtype=float32)\n",
      "Time for epoch 201 is 0.2535 sec\n",
      "train AE loss : 0.033264920115470886, train ANN loss : 3.509861707687378\n",
      "AE loss : 0.032806627452373505, ANN loss : 3.4743964672088623, Total loss : 6.755059242248535\n",
      "learning rate A :  tf.Tensor(0.009009487, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008971597, shape=(), dtype=float32)\n",
      "Time for epoch 202 is 0.2089 sec\n",
      "train AE loss : 0.03242893144488335, train ANN loss : 3.4328744411468506\n",
      "AE loss : 0.03140327334403992, ANN loss : 3.5163588523864746, Total loss : 6.656686305999756\n",
      "learning rate A :  tf.Tensor(0.009009487, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00896215, shape=(), dtype=float32)\n",
      "Time for epoch 203 is 0.1805 sec\n",
      "train AE loss : 0.031075814738869667, train ANN loss : 3.440351963043213\n",
      "AE loss : 0.029537972062826157, ANN loss : 3.5716068744659424, Total loss : 6.525404453277588\n",
      "learning rate A :  tf.Tensor(0.009, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00896215, shape=(), dtype=float32)\n",
      "Time for epoch 204 is 0.1771 sec\n",
      "train AE loss : 0.029256289824843407, train ANN loss : 3.4822006225585938\n",
      "AE loss : 0.028100278228521347, ANN loss : 3.633427381515503, Total loss : 6.443454742431641\n",
      "learning rate A :  tf.Tensor(0.008990522, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00896215, shape=(), dtype=float32)\n",
      "Time for epoch 205 is 0.1925 sec\n",
      "train AE loss : 0.027849635109305382, train ANN loss : 3.5285449028015137\n",
      "AE loss : 0.026931647211313248, ANN loss : 3.694720983505249, Total loss : 6.387886047363281\n",
      "learning rate A :  tf.Tensor(0.008981055, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00896215, shape=(), dtype=float32)\n",
      "Time for epoch 206 is 0.1797 sec\n",
      "train AE loss : 0.026705347001552582, train ANN loss : 3.587958335876465\n",
      "AE loss : 0.03677169233560562, ANN loss : 3.419440269470215, Total loss : 7.096609115600586\n",
      "learning rate A :  tf.Tensor(0.008981055, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008952713, shape=(), dtype=float32)\n",
      "Time for epoch 207 is 0.1822 sec\n",
      "train AE loss : 0.036332372575998306, train ANN loss : 3.4168589115142822\n",
      "AE loss : 0.03349444270133972, ANN loss : 3.4534618854522705, Total loss : 6.802905559539795\n",
      "learning rate A :  tf.Tensor(0.008971597, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008952713, shape=(), dtype=float32)\n",
      "Time for epoch 208 is 0.1823 sec\n",
      "train AE loss : 0.03314461186528206, train ANN loss : 3.4133453369140625\n",
      "AE loss : 0.0514150895178318, ANN loss : 3.4490604400634766, Total loss : 8.590569496154785\n",
      "learning rate A :  tf.Tensor(0.008971597, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008943285, shape=(), dtype=float32)\n",
      "Time for epoch 209 is 0.1963 sec\n",
      "train AE loss : 0.05055946856737137, train ANN loss : 3.681631565093994\n",
      "AE loss : 0.042704224586486816, ANN loss : 3.396237850189209, Total loss : 7.666660308837891\n",
      "learning rate A :  tf.Tensor(0.00896215, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008943285, shape=(), dtype=float32)\n",
      "Time for epoch 210 is 0.1830 sec\n",
      "train AE loss : 0.04213118553161621, train ANN loss : 3.4919509887695312\n",
      "AE loss : 0.037566252052783966, ANN loss : 3.400259256362915, Total loss : 7.15688419342041\n",
      "learning rate A :  tf.Tensor(0.008952713, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008943285, shape=(), dtype=float32)\n",
      "Time for epoch 211 is 0.1781 sec\n",
      "train AE loss : 0.03713882341980934, train ANN loss : 3.4334163665771484\n",
      "AE loss : 0.03422724828124046, ANN loss : 3.4201526641845703, Total loss : 6.842877388000488\n",
      "learning rate A :  tf.Tensor(0.008943285, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008943285, shape=(), dtype=float32)\n",
      "Time for epoch 212 is 0.1784 sec\n",
      "train AE loss : 0.03388180211186409, train ANN loss : 3.4141807556152344\n",
      "AE loss : 0.04806175455451012, ANN loss : 3.452190637588501, Total loss : 8.258365631103516\n",
      "learning rate A :  tf.Tensor(0.008943285, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008933866, shape=(), dtype=float32)\n",
      "Time for epoch 213 is 0.1947 sec\n",
      "train AE loss : 0.04736034944653511, train ANN loss : 3.6526501178741455\n",
      "AE loss : 0.03259618952870369, ANN loss : 3.465862989425659, Total loss : 6.7254815101623535\n",
      "learning rate A :  tf.Tensor(0.008943285, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008924458, shape=(), dtype=float32)\n",
      "Time for epoch 214 is 0.1837 sec\n",
      "train AE loss : 0.032292839139699936, train ANN loss : 3.4098715782165527\n",
      "AE loss : 0.03044144995510578, ANN loss : 3.5077736377716064, Total loss : 6.551919460296631\n",
      "learning rate A :  tf.Tensor(0.008933866, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008924458, shape=(), dtype=float32)\n",
      "Time for epoch 215 is 0.1784 sec\n",
      "train AE loss : 0.03018372878432274, train ANN loss : 3.4317736625671387\n",
      "AE loss : 0.026827167719602585, ANN loss : 3.6462857723236084, Total loss : 6.329002857208252\n",
      "learning rate A :  tf.Tensor(0.008933866, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00891506, shape=(), dtype=float32)\n",
      "Time for epoch 216 is 0.1896 sec\n",
      "train AE loss : 0.026630079373717308, train ANN loss : 3.5419507026672363\n",
      "AE loss : 0.031219951808452606, ANN loss : 3.498979091644287, Total loss : 6.620974540710449\n",
      "learning rate A :  tf.Tensor(0.008933866, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0089056725, shape=(), dtype=float32)\n",
      "Time for epoch 217 is 0.1839 sec\n",
      "train AE loss : 0.030935056507587433, train ANN loss : 3.4392008781433105\n",
      "AE loss : 0.03747757896780968, ANN loss : 3.54921555519104, Total loss : 7.29697322845459\n",
      "learning rate A :  tf.Tensor(0.008933866, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008896295, shape=(), dtype=float32)\n",
      "Time for epoch 218 is 0.1853 sec\n",
      "train AE loss : 0.03704586625099182, train ANN loss : 3.545897960662842\n",
      "AE loss : 0.034002047032117844, ANN loss : 3.5364339351654053, Total loss : 6.936637878417969\n",
      "learning rate A :  tf.Tensor(0.008924458, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008896295, shape=(), dtype=float32)\n",
      "Time for epoch 219 is 0.1780 sec\n",
      "train AE loss : 0.033655066043138504, train ANN loss : 3.50700044631958\n",
      "AE loss : 0.03150365874171257, ANN loss : 3.533787965774536, Total loss : 6.6841535568237305\n",
      "learning rate A :  tf.Tensor(0.00891506, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008896295, shape=(), dtype=float32)\n",
      "Time for epoch 220 is 0.1780 sec\n",
      "train AE loss : 0.031216289848089218, train ANN loss : 3.494040012359619\n",
      "AE loss : 0.029639694839715958, ANN loss : 3.5371005535125732, Total loss : 6.50106954574585\n",
      "learning rate A :  tf.Tensor(0.0089056725, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008896295, shape=(), dtype=float32)\n",
      "Time for epoch 221 is 0.1800 sec\n",
      "train AE loss : 0.029392829164862633, train ANN loss : 3.4796504974365234\n",
      "AE loss : 0.034821730107069016, ANN loss : 3.483828067779541, Total loss : 6.966001033782959\n",
      "learning rate A :  tf.Tensor(0.0089056725, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008886926, shape=(), dtype=float32)\n",
      "Time for epoch 222 is 0.1925 sec\n",
      "train AE loss : 0.03446732088923454, train ANN loss : 3.449507713317871\n",
      "AE loss : 0.037329453974962234, ANN loss : 3.448619842529297, Total loss : 7.181565761566162\n",
      "learning rate A :  tf.Tensor(0.0089056725, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008877568, shape=(), dtype=float32)\n",
      "Time for epoch 223 is 0.1832 sec\n",
      "train AE loss : 0.03692968189716339, train ANN loss : 3.4005684852600098\n",
      "AE loss : 0.03410680592060089, ANN loss : 3.4724767208099365, Total loss : 6.883157253265381\n",
      "learning rate A :  tf.Tensor(0.008896295, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008877568, shape=(), dtype=float32)\n",
      "Time for epoch 224 is 0.1789 sec\n",
      "train AE loss : 0.03378820791840553, train ANN loss : 3.403848648071289\n",
      "AE loss : 0.031697649508714676, ANN loss : 3.500659942626953, Total loss : 6.670424938201904\n",
      "learning rate A :  tf.Tensor(0.008886926, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008877568, shape=(), dtype=float32)\n",
      "Time for epoch 225 is 0.1790 sec\n",
      "train AE loss : 0.0314301922917366, train ANN loss : 3.4124817848205566\n",
      "AE loss : 0.0372406430542469, ANN loss : 3.459517240524292, Total loss : 7.183581829071045\n",
      "learning rate A :  tf.Tensor(0.008886926, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008868219, shape=(), dtype=float32)\n",
      "Time for epoch 226 is 0.3025 sec\n",
      "train AE loss : 0.03687269613146782, train ANN loss : 3.3993589878082275\n",
      "AE loss : 0.03411565721035004, ANN loss : 3.5055735111236572, Total loss : 6.917139530181885\n",
      "learning rate A :  tf.Tensor(0.008877568, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008868219, shape=(), dtype=float32)\n",
      "Time for epoch 227 is 0.1764 sec\n",
      "train AE loss : 0.033818528056144714, train ANN loss : 3.420989513397217\n",
      "AE loss : 0.031796734780073166, ANN loss : 3.558073043823242, Total loss : 6.7377471923828125\n",
      "learning rate A :  tf.Tensor(0.008868219, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008868219, shape=(), dtype=float32)\n",
      "Time for epoch 228 is 0.1888 sec\n",
      "train AE loss : 0.03154534474015236, train ANN loss : 3.459268808364868\n",
      "AE loss : 0.04792251065373421, ANN loss : 3.3975930213928223, Total loss : 8.189844131469727\n",
      "learning rate A :  tf.Tensor(0.008868219, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008858881, shape=(), dtype=float32)\n",
      "Time for epoch 229 is 0.1852 sec\n",
      "train AE loss : 0.0473160594701767, train ANN loss : 3.4693338871002197\n",
      "AE loss : 0.05171094462275505, ANN loss : 3.3823657035827637, Total loss : 8.553460121154785\n",
      "learning rate A :  tf.Tensor(0.008868219, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008849552, shape=(), dtype=float32)\n",
      "Time for epoch 230 is 0.1828 sec\n",
      "train AE loss : 0.05101696774363518, train ANN loss : 3.4644086360931396\n",
      "AE loss : 0.039543066173791885, ANN loss : 3.4916558265686035, Total loss : 7.445962429046631\n",
      "learning rate A :  tf.Tensor(0.008868219, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008840233, shape=(), dtype=float32)\n",
      "Time for epoch 231 is 0.1792 sec\n",
      "train AE loss : 0.0391424298286438, train ANN loss : 3.409956693649292\n",
      "AE loss : 0.03584994375705719, ANN loss : 3.5649001598358154, Total loss : 7.1498942375183105\n",
      "learning rate A :  tf.Tensor(0.008858881, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008840233, shape=(), dtype=float32)\n",
      "Time for epoch 232 is 0.1803 sec\n",
      "train AE loss : 0.03552822768688202, train ANN loss : 3.466947555541992\n",
      "AE loss : 0.033114343881607056, ANN loss : 3.641545534133911, Total loss : 6.952979564666748\n",
      "learning rate A :  tf.Tensor(0.008849552, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008840233, shape=(), dtype=float32)\n",
      "Time for epoch 233 is 0.1792 sec\n",
      "train AE loss : 0.03284549340605736, train ANN loss : 3.525425672531128\n",
      "AE loss : 0.0370267778635025, ANN loss : 3.4980967044830322, Total loss : 7.2007737159729\n",
      "learning rate A :  tf.Tensor(0.008849552, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008830924, shape=(), dtype=float32)\n",
      "Time for epoch 234 is 0.1918 sec\n",
      "train AE loss : 0.036673903465270996, train ANN loss : 3.4085140228271484\n",
      "AE loss : 0.04798901453614235, ANN loss : 3.4387621879577637, Total loss : 8.237663269042969\n",
      "learning rate A :  tf.Tensor(0.008849552, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "Time for epoch 235 is 0.1814 sec\n",
      "train AE loss : 0.047392457723617554, train ANN loss : 3.4576003551483154\n",
      "AE loss : 0.05011937767267227, ANN loss : 3.456381320953369, Total loss : 8.4683198928833\n",
      "learning rate A :  tf.Tensor(0.008849552, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008812334, shape=(), dtype=float32)\n",
      "Time for epoch 236 is 0.1802 sec\n",
      "train AE loss : 0.049463070929050446, train ANN loss : 3.4747345447540283\n",
      "AE loss : 0.04211313650012016, ANN loss : 3.480907678604126, Total loss : 7.692221164703369\n",
      "learning rate A :  tf.Tensor(0.008849552, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008803055, shape=(), dtype=float32)\n",
      "Time for epoch 237 is 0.1831 sec\n",
      "train AE loss : 0.041654035449028015, train ANN loss : 3.3929593563079834\n",
      "AE loss : 0.03770004212856293, ANN loss : 3.521026849746704, Total loss : 7.2910308837890625\n",
      "learning rate A :  tf.Tensor(0.008840233, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008803055, shape=(), dtype=float32)\n",
      "Time for epoch 238 is 0.1834 sec\n",
      "train AE loss : 0.037339091300964355, train ANN loss : 3.4130678176879883\n",
      "AE loss : 0.03645144775509834, ANN loss : 3.5963099002838135, Total loss : 7.241454601287842\n",
      "learning rate A :  tf.Tensor(0.008840233, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008793785, shape=(), dtype=float32)\n",
      "Time for epoch 239 is 0.1913 sec\n",
      "train AE loss : 0.03612111508846283, train ANN loss : 3.4552040100097656\n",
      "AE loss : 0.03355947881937027, ANN loss : 3.663687229156494, Total loss : 7.01963472366333\n",
      "learning rate A :  tf.Tensor(0.008830924, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008793785, shape=(), dtype=float32)\n",
      "Time for epoch 240 is 0.1773 sec\n",
      "train AE loss : 0.03328286111354828, train ANN loss : 3.507416009902954\n",
      "AE loss : 0.04477161541581154, ANN loss : 3.4738893508911133, Total loss : 7.951050281524658\n",
      "learning rate A :  tf.Tensor(0.008830924, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008784524, shape=(), dtype=float32)\n",
      "Time for epoch 241 is 0.1813 sec\n",
      "train AE loss : 0.044269856065511703, train ANN loss : 3.3846652507781982\n",
      "AE loss : 0.03952489420771599, ANN loss : 3.5185484886169434, Total loss : 7.471037864685059\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008784524, shape=(), dtype=float32)\n",
      "Time for epoch 242 is 0.1809 sec\n",
      "train AE loss : 0.03915645182132721, train ANN loss : 3.3991281986236572\n",
      "AE loss : 0.060130152851343155, ANN loss : 3.437105894088745, Total loss : 9.450121879577637\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008775274, shape=(), dtype=float32)\n",
      "Time for epoch 243 is 0.1812 sec\n",
      "train AE loss : 0.05923762172460556, train ANN loss : 3.467334747314453\n",
      "AE loss : 0.06176621466875076, ANN loss : 3.4246373176574707, Total loss : 9.601258277893066\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008766033, shape=(), dtype=float32)\n",
      "Time for epoch 244 is 0.1926 sec\n",
      "train AE loss : 0.06083902344107628, train ANN loss : 3.4416470527648926\n",
      "AE loss : 0.04799600690603256, ANN loss : 3.4817159175872803, Total loss : 8.281315803527832\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0087568015, shape=(), dtype=float32)\n",
      "Time for epoch 245 is 0.1822 sec\n",
      "train AE loss : 0.04745056852698326, train ANN loss : 3.3724253177642822\n",
      "AE loss : 0.04183840751647949, ANN loss : 3.625969648361206, Total loss : 7.809810638427734\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00874758, shape=(), dtype=float32)\n",
      "Time for epoch 246 is 0.1946 sec\n",
      "train AE loss : 0.04143185913562775, train ANN loss : 3.4689815044403076\n",
      "AE loss : 0.049161359667778015, ANN loss : 3.4948229789733887, Total loss : 8.410958290100098\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008738369, shape=(), dtype=float32)\n",
      "Time for epoch 247 is 0.1741 sec\n",
      "train AE loss : 0.04861264303326607, train ANN loss : 3.372892379760742\n",
      "AE loss : 0.06552356481552124, ANN loss : 3.4035096168518066, Total loss : 9.955866813659668\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008729166, shape=(), dtype=float32)\n",
      "Time for epoch 248 is 0.1704 sec\n",
      "train AE loss : 0.06458916515111923, train ANN loss : 3.392390727996826\n",
      "AE loss : 0.07373184710741043, ANN loss : 3.3996336460113525, Total loss : 10.772819519042969\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008719974, shape=(), dtype=float32)\n",
      "Time for epoch 249 is 0.1722 sec\n",
      "train AE loss : 0.07259751856327057, train ANN loss : 3.432255506515503\n",
      "AE loss : 0.06448870152235031, ANN loss : 3.4154229164123535, Total loss : 9.864294052124023\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008710793, shape=(), dtype=float32)\n",
      "Time for epoch 250 is 0.1733 sec\n",
      "train AE loss : 0.06361395120620728, train ANN loss : 3.3737871646881104\n",
      "AE loss : 0.053389474749565125, ANN loss : 3.4905548095703125, Total loss : 8.82950210571289\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00870162, shape=(), dtype=float32)\n",
      "Time for epoch 251 is 0.1754 sec\n",
      "train AE loss : 0.05277854576706886, train ANN loss : 3.3698556423187256\n",
      "AE loss : 0.05069534108042717, ANN loss : 3.5488336086273193, Total loss : 8.618368148803711\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008692456, shape=(), dtype=float32)\n",
      "Time for epoch 252 is 0.1711 sec\n",
      "train AE loss : 0.05014815926551819, train ANN loss : 3.3995816707611084\n",
      "AE loss : 0.05942786484956741, ANN loss : 3.465430736541748, Total loss : 9.408218383789062\n",
      "learning rate A :  tf.Tensor(0.008821624, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008683302, shape=(), dtype=float32)\n",
      "Time for epoch 253 is 0.1743 sec\n",
      "train AE loss : 0.058707788586616516, train ANN loss : 3.3548080921173096\n",
      "AE loss : 0.049176175147295, ANN loss : 3.5442116260528564, Total loss : 8.46182918548584\n",
      "learning rate A :  tf.Tensor(0.008812334, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008683302, shape=(), dtype=float32)\n",
      "Time for epoch 254 is 0.1808 sec\n",
      "train AE loss : 0.048676084727048874, train ANN loss : 3.400071382522583\n",
      "AE loss : 0.04239563271403313, ANN loss : 3.630549907684326, Total loss : 7.870113372802734\n",
      "learning rate A :  tf.Tensor(0.008803055, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008683302, shape=(), dtype=float32)\n",
      "Time for epoch 255 is 0.1801 sec\n",
      "train AE loss : 0.04200874641537666, train ANN loss : 3.4700570106506348\n",
      "AE loss : 0.03759955242276192, ANN loss : 3.7352311611175537, Total loss : 7.4951863288879395\n",
      "learning rate A :  tf.Tensor(0.008793785, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008683302, shape=(), dtype=float32)\n",
      "Time for epoch 256 is 0.1698 sec\n",
      "train AE loss : 0.03729815408587456, train ANN loss : 3.5639967918395996\n",
      "AE loss : 0.0646858736872673, ANN loss : 3.4266645908355713, Total loss : 9.895252227783203\n",
      "learning rate A :  tf.Tensor(0.008793785, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008674159, shape=(), dtype=float32)\n",
      "Time for epoch 257 is 0.1715 sec\n",
      "train AE loss : 0.0639076977968216, train ANN loss : 3.3940165042877197\n",
      "AE loss : 0.052524384111166, ANN loss : 3.4543251991271973, Total loss : 8.706764221191406\n",
      "learning rate A :  tf.Tensor(0.008784524, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008674159, shape=(), dtype=float32)\n",
      "Time for epoch 258 is 0.1711 sec\n",
      "train AE loss : 0.05201068893074989, train ANN loss : 3.3828985691070557\n",
      "AE loss : 0.04482284188270569, ANN loss : 3.4908204078674316, Total loss : 7.973104476928711\n",
      "learning rate A :  tf.Tensor(0.008775274, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008674159, shape=(), dtype=float32)\n",
      "Time for epoch 259 is 0.1686 sec\n",
      "train AE loss : 0.04444286599755287, train ANN loss : 3.390824556350708\n",
      "AE loss : 0.0820816308259964, ANN loss : 3.500077962875366, Total loss : 11.708240509033203\n",
      "learning rate A :  tf.Tensor(0.008775274, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008665024, shape=(), dtype=float32)\n",
      "Time for epoch 260 is 0.1736 sec\n",
      "train AE loss : 0.08090637624263763, train ANN loss : 3.642554759979248\n",
      "AE loss : 0.08839225769042969, ANN loss : 3.497799873352051, Total loss : 12.337024688720703\n",
      "learning rate A :  tf.Tensor(0.008775274, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0086559, shape=(), dtype=float32)\n",
      "Time for epoch 261 is 0.1738 sec\n",
      "train AE loss : 0.08708053827285767, train ANN loss : 3.6433677673339844\n",
      "AE loss : 0.0636853352189064, ANN loss : 3.4806127548217773, Total loss : 9.849145889282227\n",
      "learning rate A :  tf.Tensor(0.008766033, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0086559, shape=(), dtype=float32)\n",
      "Time for epoch 262 is 0.1719 sec\n",
      "train AE loss : 0.0630062147974968, train ANN loss : 3.4992570877075195\n",
      "AE loss : 0.05067138373851776, ANN loss : 3.493203639984131, Total loss : 8.560341835021973\n",
      "learning rate A :  tf.Tensor(0.0087568015, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0086559, shape=(), dtype=float32)\n",
      "Time for epoch 263 is 0.1682 sec\n",
      "train AE loss : 0.050301067531108856, train ANN loss : 3.456782341003418\n",
      "AE loss : 0.04282375052571297, ANN loss : 3.5141825675964355, Total loss : 7.796557426452637\n",
      "learning rate A :  tf.Tensor(0.00874758, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0086559, shape=(), dtype=float32)\n",
      "Time for epoch 264 is 0.1699 sec\n",
      "train AE loss : 0.04260418936610222, train ANN loss : 3.458385467529297\n",
      "AE loss : 0.04539323225617409, ANN loss : 3.4832892417907715, Total loss : 8.022611618041992\n",
      "learning rate A :  tf.Tensor(0.00874758, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008646785, shape=(), dtype=float32)\n",
      "Time for epoch 265 is 0.1734 sec\n",
      "train AE loss : 0.04512057825922966, train ANN loss : 3.3879222869873047\n",
      "AE loss : 0.05056696757674217, ANN loss : 3.4905917644500732, Total loss : 8.54728889465332\n",
      "learning rate A :  tf.Tensor(0.00874758, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008637679, shape=(), dtype=float32)\n",
      "Time for epoch 266 is 0.1845 sec\n",
      "train AE loss : 0.050199102610349655, train ANN loss : 3.371507167816162\n",
      "AE loss : 0.06284962594509125, ANN loss : 3.4712212085723877, Total loss : 9.756183624267578\n",
      "learning rate A :  tf.Tensor(0.00874758, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0086285835, shape=(), dtype=float32)\n",
      "Time for epoch 267 is 0.1821 sec\n",
      "train AE loss : 0.062273815274238586, train ANN loss : 3.3798904418945312\n",
      "AE loss : 0.048951320350170135, ANN loss : 3.5744831562042236, Total loss : 8.46961498260498\n",
      "learning rate A :  tf.Tensor(0.008738369, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0086285835, shape=(), dtype=float32)\n",
      "Time for epoch 268 is 0.1695 sec\n",
      "train AE loss : 0.04864078387618065, train ANN loss : 3.4212393760681152\n",
      "AE loss : 0.041039060801267624, ANN loss : 3.673539876937866, Total loss : 7.7774457931518555\n",
      "learning rate A :  tf.Tensor(0.008729166, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0086285835, shape=(), dtype=float32)\n",
      "Time for epoch 269 is 0.1691 sec\n",
      "train AE loss : 0.04086257144808769, train ANN loss : 3.505600690841675\n",
      "AE loss : 0.07560756802558899, ANN loss : 3.406466484069824, Total loss : 10.967223167419434\n",
      "learning rate A :  tf.Tensor(0.008729166, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008619497, shape=(), dtype=float32)\n",
      "Time for epoch 270 is 0.1734 sec\n",
      "train AE loss : 0.07481326907873154, train ANN loss : 3.4133338928222656\n",
      "AE loss : 0.1005556657910347, ANN loss : 3.381232500076294, Total loss : 13.436799049377441\n",
      "learning rate A :  tf.Tensor(0.008729166, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00861042, shape=(), dtype=float32)\n",
      "Time for epoch 271 is 0.1820 sec\n",
      "train AE loss : 0.09914662688970566, train ANN loss : 3.5021016597747803\n",
      "AE loss : 0.06312760710716248, ANN loss : 3.43117356300354, Total loss : 9.743934631347656\n",
      "learning rate A :  tf.Tensor(0.008719974, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00861042, shape=(), dtype=float32)\n",
      "Time for epoch 272 is 0.1696 sec\n",
      "train AE loss : 0.06258577108383179, train ANN loss : 3.385826826095581\n",
      "AE loss : 0.04751945286989212, ANN loss : 3.5052013397216797, Total loss : 8.257146835327148\n",
      "learning rate A :  tf.Tensor(0.008710793, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00861042, shape=(), dtype=float32)\n",
      "Time for epoch 273 is 0.1699 sec\n",
      "train AE loss : 0.04728573188185692, train ANN loss : 3.398026943206787\n",
      "AE loss : 0.03996904939413071, ANN loss : 3.563190221786499, Total loss : 7.560094833374023\n",
      "learning rate A :  tf.Tensor(0.00870162, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00861042, shape=(), dtype=float32)\n",
      "Time for epoch 274 is 0.1778 sec\n",
      "train AE loss : 0.03983140364289284, train ANN loss : 3.431729316711426\n",
      "AE loss : 0.05841813236474991, ANN loss : 3.4485106468200684, Total loss : 9.290323257446289\n",
      "learning rate A :  tf.Tensor(0.00870162, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008601353, shape=(), dtype=float32)\n",
      "Time for epoch 275 is 0.1745 sec\n",
      "train AE loss : 0.05797494202852249, train ANN loss : 3.427403211593628\n",
      "AE loss : 0.06916195899248123, ANN loss : 3.408695697784424, Total loss : 10.324891090393066\n",
      "learning rate A :  tf.Tensor(0.00870162, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0085922945, shape=(), dtype=float32)\n",
      "Time for epoch 276 is 0.1725 sec\n",
      "train AE loss : 0.06854227930307388, train ANN loss : 3.403921604156494\n",
      "AE loss : 0.049594517797231674, ANN loss : 3.4739058017730713, Total loss : 8.433357238769531\n",
      "learning rate A :  tf.Tensor(0.008692456, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0085922945, shape=(), dtype=float32)\n",
      "Time for epoch 277 is 0.1702 sec\n",
      "train AE loss : 0.04929322749376297, train ANN loss : 3.403834819793701\n",
      "AE loss : 0.04204697906970978, ANN loss : 3.5143978595733643, Total loss : 7.7190961837768555\n",
      "learning rate A :  tf.Tensor(0.008683302, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0085922945, shape=(), dtype=float32)\n",
      "Time for epoch 278 is 0.1830 sec\n",
      "train AE loss : 0.04187199845910072, train ANN loss : 3.4095098972320557\n",
      "AE loss : 0.037653010338544846, ANN loss : 3.552938938140869, Total loss : 7.318240165710449\n",
      "learning rate A :  tf.Tensor(0.008674159, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0085922945, shape=(), dtype=float32)\n",
      "Time for epoch 279 is 0.1673 sec\n",
      "train AE loss : 0.03753136098384857, train ANN loss : 3.432906150817871\n",
      "AE loss : 0.03475197032094002, ANN loss : 3.591691732406616, Total loss : 7.066888332366943\n",
      "learning rate A :  tf.Tensor(0.008665024, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0085922945, shape=(), dtype=float32)\n",
      "Time for epoch 280 is 0.1804 sec\n",
      "train AE loss : 0.034654371440410614, train ANN loss : 3.4570834636688232\n",
      "AE loss : 0.04848778620362282, ANN loss : 3.4656336307525635, Total loss : 8.314412117004395\n",
      "learning rate A :  tf.Tensor(0.008665024, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008583247, shape=(), dtype=float32)\n",
      "Time for epoch 281 is 0.1732 sec\n",
      "train AE loss : 0.04825321584939957, train ANN loss : 3.403019428253174\n",
      "AE loss : 0.04144268110394478, ANN loss : 3.506016731262207, Total loss : 7.650284290313721\n",
      "learning rate A :  tf.Tensor(0.0086559, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008583247, shape=(), dtype=float32)\n",
      "Time for epoch 282 is 0.1705 sec\n",
      "train AE loss : 0.04127063602209091, train ANN loss : 3.432109832763672\n",
      "AE loss : 0.03706234693527222, ANN loss : 3.5314137935638428, Total loss : 7.237648963928223\n",
      "learning rate A :  tf.Tensor(0.008646785, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008583247, shape=(), dtype=float32)\n",
      "Time for epoch 283 is 0.1704 sec\n",
      "train AE loss : 0.03693464398384094, train ANN loss : 3.4416425228118896\n",
      "AE loss : 0.03411572799086571, ANN loss : 3.5547471046447754, Total loss : 6.966320037841797\n",
      "learning rate A :  tf.Tensor(0.008637679, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008583247, shape=(), dtype=float32)\n",
      "Time for epoch 284 is 0.1688 sec\n",
      "train AE loss : 0.03400542587041855, train ANN loss : 3.4515271186828613\n",
      "AE loss : 0.03198659047484398, ANN loss : 3.577106475830078, Total loss : 6.775765419006348\n",
      "learning rate A :  tf.Tensor(0.0086285835, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008583247, shape=(), dtype=float32)\n",
      "Time for epoch 285 is 0.1697 sec\n",
      "train AE loss : 0.03188180923461914, train ANN loss : 3.473430871963501\n",
      "AE loss : 0.05168983340263367, ANN loss : 3.4525375366210938, Total loss : 8.62152099609375\n",
      "learning rate A :  tf.Tensor(0.0086285835, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008574208, shape=(), dtype=float32)\n",
      "Time for epoch 286 is 0.1712 sec\n",
      "train AE loss : 0.05140415206551552, train ANN loss : 3.462212562561035\n",
      "AE loss : 0.06579422205686569, ANN loss : 3.3764052391052246, Total loss : 9.955826759338379\n",
      "learning rate A :  tf.Tensor(0.0086285835, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008565179, shape=(), dtype=float32)\n",
      "Time for epoch 287 is 0.1808 sec\n",
      "train AE loss : 0.065385602414608, train ANN loss : 3.4177141189575195\n",
      "AE loss : 0.0490482896566391, ANN loss : 3.4280478954315186, Total loss : 8.332877159118652\n",
      "learning rate A :  tf.Tensor(0.008619497, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008565179, shape=(), dtype=float32)\n",
      "Time for epoch 288 is 0.1796 sec\n",
      "train AE loss : 0.048838697373867035, train ANN loss : 3.4010438919067383\n",
      "AE loss : 0.0525786392390728, ANN loss : 3.4721410274505615, Total loss : 8.730005264282227\n",
      "learning rate A :  tf.Tensor(0.008619497, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008556159, shape=(), dtype=float32)\n",
      "Time for epoch 289 is 0.1729 sec\n",
      "train AE loss : 0.05239440128207207, train ANN loss : 3.3887546062469482\n",
      "AE loss : 0.06874997168779373, ANN loss : 3.4179203510284424, Total loss : 10.292917251586914\n",
      "learning rate A :  tf.Tensor(0.008619497, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008547149, shape=(), dtype=float32)\n",
      "Time for epoch 290 is 0.1703 sec\n",
      "train AE loss : 0.06842169165611267, train ANN loss : 3.3823604583740234\n",
      "AE loss : 0.04794514179229736, ANN loss : 3.5538671016693115, Total loss : 8.348381042480469\n",
      "learning rate A :  tf.Tensor(0.00861042, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008547149, shape=(), dtype=float32)\n",
      "Time for epoch 291 is 0.1701 sec\n",
      "train AE loss : 0.04781953990459442, train ANN loss : 3.4356184005737305\n",
      "AE loss : 0.038914281874895096, ANN loss : 3.650148391723633, Total loss : 7.541575908660889\n",
      "learning rate A :  tf.Tensor(0.008601353, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008547149, shape=(), dtype=float32)\n",
      "Time for epoch 292 is 0.1730 sec\n",
      "train AE loss : 0.03883267194032669, train ANN loss : 3.5095221996307373\n",
      "AE loss : 0.08082815259695053, ANN loss : 3.345047950744629, Total loss : 11.427862167358398\n",
      "learning rate A :  tf.Tensor(0.008601353, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008538149, shape=(), dtype=float32)\n",
      "Time for epoch 293 is 0.1731 sec\n",
      "train AE loss : 0.08033273369073868, train ANN loss : 3.44608473777771\n",
      "AE loss : 0.05255566164851189, ANN loss : 3.405125379562378, Total loss : 8.660691261291504\n",
      "learning rate A :  tf.Tensor(0.0085922945, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008538149, shape=(), dtype=float32)\n",
      "Time for epoch 294 is 0.1697 sec\n",
      "train AE loss : 0.05243062973022461, train ANN loss : 3.366396427154541\n",
      "AE loss : 0.04417799785733223, ANN loss : 3.4685709476470947, Total loss : 7.8863701820373535\n",
      "learning rate A :  tf.Tensor(0.008583247, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008538149, shape=(), dtype=float32)\n",
      "Time for epoch 295 is 0.1703 sec\n",
      "train AE loss : 0.04406500235199928, train ANN loss : 3.375863790512085\n",
      "AE loss : 0.10642218589782715, ANN loss : 3.5220017433166504, Total loss : 14.164219856262207\n",
      "learning rate A :  tf.Tensor(0.008583247, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008529157, shape=(), dtype=float32)\n",
      "Time for epoch 296 is 0.1874 sec\n",
      "train AE loss : 0.10536408424377441, train ANN loss : 3.977851629257202\n",
      "AE loss : 0.06638609617948532, ANN loss : 3.5100114345550537, Total loss : 10.148621559143066\n",
      "learning rate A :  tf.Tensor(0.008574208, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008529157, shape=(), dtype=float32)\n",
      "Time for epoch 297 is 0.1784 sec\n",
      "train AE loss : 0.06595641374588013, train ANN loss : 3.7189528942108154\n",
      "AE loss : 0.0511230044066906, ANN loss : 3.4670302867889404, Total loss : 8.579330444335938\n",
      "learning rate A :  tf.Tensor(0.008565179, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008529157, shape=(), dtype=float32)\n",
      "Time for epoch 298 is 0.1716 sec\n",
      "train AE loss : 0.05087964981794357, train ANN loss : 3.52405047416687\n",
      "AE loss : 0.05651591345667839, ANN loss : 3.4057533740997314, Total loss : 9.057344436645508\n",
      "learning rate A :  tf.Tensor(0.008565179, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008520177, shape=(), dtype=float32)\n",
      "Time for epoch 299 is 0.1714 sec\n",
      "train AE loss : 0.056330472230911255, train ANN loss : 3.3978328704833984\n",
      "AE loss : 0.060990411788225174, ANN loss : 3.4196088314056396, Total loss : 9.51865005493164\n",
      "learning rate A :  tf.Tensor(0.008565179, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008511204, shape=(), dtype=float32)\n",
      "Time for epoch 300 is 0.1731 sec\n",
      "train AE loss : 0.060735102742910385, train ANN loss : 3.36225962638855\n",
      "AE loss : 0.06349629163742065, ANN loss : 3.4722933769226074, Total loss : 9.821922302246094\n",
      "learning rate A :  tf.Tensor(0.008565179, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008502242, shape=(), dtype=float32)\n",
      "Time for epoch 301 is 0.1738 sec\n",
      "train AE loss : 0.06305935233831406, train ANN loss : 3.3623297214508057\n",
      "AE loss : 0.045228857547044754, ANN loss : 3.6111562252044678, Total loss : 8.134041786193848\n",
      "learning rate A :  tf.Tensor(0.008556159, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008502242, shape=(), dtype=float32)\n",
      "Time for epoch 302 is 0.1686 sec\n",
      "train AE loss : 0.044964563101530075, train ANN loss : 3.459867238998413\n",
      "AE loss : 0.0789370909333229, ANN loss : 3.4198358058929443, Total loss : 11.313545227050781\n",
      "learning rate A :  tf.Tensor(0.008556159, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008493288, shape=(), dtype=float32)\n",
      "Time for epoch 303 is 0.1729 sec\n",
      "train AE loss : 0.07827763259410858, train ANN loss : 3.3476622104644775\n",
      "AE loss : 0.12260940670967102, ANN loss : 3.405761957168579, Total loss : 15.666702270507812\n",
      "learning rate A :  tf.Tensor(0.008556159, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008484344, shape=(), dtype=float32)\n",
      "Time for epoch 304 is 0.1851 sec\n",
      "train AE loss : 0.12114457041025162, train ANN loss : 3.461228132247925\n",
      "AE loss : 0.0878196433186531, ANN loss : 3.487971544265747, Total loss : 12.269935607910156\n",
      "learning rate A :  tf.Tensor(0.008556159, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00847541, shape=(), dtype=float32)\n",
      "Time for epoch 305 is 0.1747 sec\n",
      "train AE loss : 0.08695507049560547, train ANN loss : 3.3671212196350098\n",
      "AE loss : 0.05520414933562279, ANN loss : 3.7014362812042236, Total loss : 9.221850395202637\n",
      "learning rate A :  tf.Tensor(0.008547149, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00847541, shape=(), dtype=float32)\n",
      "Time for epoch 306 is 0.1693 sec\n",
      "train AE loss : 0.05470190569758415, train ANN loss : 3.5049569606781006\n",
      "AE loss : 0.09028179198503494, ANN loss : 3.468930721282959, Total loss : 12.497109413146973\n",
      "learning rate A :  tf.Tensor(0.008547149, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008466485, shape=(), dtype=float32)\n",
      "Time for epoch 307 is 0.1736 sec\n",
      "train AE loss : 0.08929084241390228, train ANN loss : 3.3631505966186523\n",
      "AE loss : 0.055936358869075775, ANN loss : 3.6013197898864746, Total loss : 9.194955825805664\n",
      "learning rate A :  tf.Tensor(0.008538149, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008466485, shape=(), dtype=float32)\n",
      "Time for epoch 308 is 0.1713 sec\n",
      "train AE loss : 0.05541747435927391, train ANN loss : 3.424739122390747\n",
      "AE loss : 0.042804423719644547, ANN loss : 3.7559027671813965, Total loss : 8.036345481872559\n",
      "learning rate A :  tf.Tensor(0.008529157, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008466485, shape=(), dtype=float32)\n",
      "Time for epoch 309 is 0.1718 sec\n",
      "train AE loss : 0.042413901537656784, train ANN loss : 3.5637688636779785\n",
      "AE loss : 0.036101777106523514, ANN loss : 3.887159824371338, Total loss : 7.497337341308594\n",
      "learning rate A :  tf.Tensor(0.008520177, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008466485, shape=(), dtype=float32)\n",
      "Time for epoch 310 is 0.1681 sec\n",
      "train AE loss : 0.03574080392718315, train ANN loss : 3.6958649158477783\n",
      "AE loss : 0.03221477195620537, ANN loss : 3.993924379348755, Total loss : 7.215401649475098\n",
      "learning rate A :  tf.Tensor(0.008511204, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008466485, shape=(), dtype=float32)\n",
      "Time for epoch 311 is 0.1881 sec\n",
      "train AE loss : 0.03186913952231407, train ANN loss : 3.8122591972351074\n",
      "AE loss : 0.11184541881084442, ANN loss : 3.602095365524292, Total loss : 14.786637306213379\n",
      "learning rate A :  tf.Tensor(0.008511204, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008457569, shape=(), dtype=float32)\n",
      "Time for epoch 312 is 0.1756 sec\n",
      "train AE loss : 0.11039360612630844, train ANN loss : 3.683194637298584\n",
      "AE loss : 0.18832102417945862, ANN loss : 3.7296102046966553, Total loss : 22.56171417236328\n",
      "learning rate A :  tf.Tensor(0.008511204, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008448662, shape=(), dtype=float32)\n",
      "Time for epoch 313 is 0.1734 sec\n",
      "train AE loss : 0.1851026862859726, train ANN loss : 4.091083526611328\n",
      "AE loss : 0.10847749561071396, ANN loss : 3.6665475368499756, Total loss : 14.514297485351562\n",
      "learning rate A :  tf.Tensor(0.008511204, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008439766, shape=(), dtype=float32)\n",
      "Time for epoch 314 is 0.1732 sec\n",
      "train AE loss : 0.10720938444137573, train ANN loss : 3.660775899887085\n",
      "AE loss : 0.05415267124772072, ANN loss : 3.711731195449829, Total loss : 9.126997947692871\n",
      "learning rate A :  tf.Tensor(0.008502242, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008439766, shape=(), dtype=float32)\n",
      "Time for epoch 315 is 0.1694 sec\n",
      "train AE loss : 0.05402451381087303, train ANN loss : 3.646890878677368\n",
      "AE loss : 0.045961473137140274, ANN loss : 3.7253942489624023, Total loss : 8.321541786193848\n",
      "learning rate A :  tf.Tensor(0.008493288, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008439766, shape=(), dtype=float32)\n",
      "Time for epoch 316 is 0.1794 sec\n",
      "train AE loss : 0.045896418392658234, train ANN loss : 3.659174919128418\n",
      "AE loss : 0.04079398885369301, ANN loss : 3.7390074729919434, Total loss : 7.818406581878662\n",
      "learning rate A :  tf.Tensor(0.008484344, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008439766, shape=(), dtype=float32)\n",
      "Time for epoch 317 is 0.1711 sec\n",
      "train AE loss : 0.04073730483651161, train ANN loss : 3.674424171447754\n",
      "AE loss : 0.03718774765729904, ANN loss : 3.752985715866089, Total loss : 7.4717607498168945\n",
      "learning rate A :  tf.Tensor(0.00847541, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008439766, shape=(), dtype=float32)\n",
      "Time for epoch 318 is 0.1790 sec\n",
      "train AE loss : 0.03712977096438408, train ANN loss : 3.6944193840026855\n",
      "AE loss : 0.03174351528286934, ANN loss : 3.7799487113952637, Total loss : 6.9542999267578125\n",
      "learning rate A :  tf.Tensor(0.00847541, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008430879, shape=(), dtype=float32)\n",
      "Time for epoch 319 is 0.1732 sec\n",
      "train AE loss : 0.03166806697845459, train ANN loss : 3.678023099899292\n",
      "AE loss : 0.03024740144610405, ANN loss : 3.7913596630096436, Total loss : 6.816100120544434\n",
      "learning rate A :  tf.Tensor(0.008466485, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008430879, shape=(), dtype=float32)\n",
      "Time for epoch 320 is 0.1838 sec\n",
      "train AE loss : 0.030157385393977165, train ANN loss : 3.6908607482910156\n",
      "AE loss : 0.028348589316010475, ANN loss : 3.860536575317383, Total loss : 6.6953959465026855\n",
      "learning rate A :  tf.Tensor(0.008466485, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008422, shape=(), dtype=float32)\n",
      "Time for epoch 321 is 0.1738 sec\n",
      "train AE loss : 0.028215257450938225, train ANN loss : 3.723335027694702\n",
      "AE loss : 0.030441028997302055, ANN loss : 3.8677914142608643, Total loss : 6.91189432144165\n",
      "learning rate A :  tf.Tensor(0.008466485, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008413131, shape=(), dtype=float32)\n",
      "Time for epoch 322 is 0.1707 sec\n",
      "train AE loss : 0.030276509001851082, train ANN loss : 3.7233920097351074\n",
      "AE loss : 0.028870809823274612, ANN loss : 3.8863065242767334, Total loss : 6.773387908935547\n",
      "learning rate A :  tf.Tensor(0.008457569, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008413131, shape=(), dtype=float32)\n",
      "Time for epoch 323 is 0.1676 sec\n",
      "train AE loss : 0.02869461104273796, train ANN loss : 3.743973731994629\n",
      "AE loss : 0.02758798561990261, ANN loss : 3.9008941650390625, Total loss : 6.659692287445068\n",
      "learning rate A :  tf.Tensor(0.008448662, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008413131, shape=(), dtype=float32)\n",
      "Time for epoch 324 is 0.1705 sec\n",
      "train AE loss : 0.027407418936491013, train ANN loss : 3.761824131011963\n",
      "AE loss : 0.03545369580388069, ANN loss : 3.778597593307495, Total loss : 7.323966979980469\n",
      "learning rate A :  tf.Tensor(0.008448662, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008404272, shape=(), dtype=float32)\n",
      "Time for epoch 325 is 0.1925 sec\n",
      "train AE loss : 0.03517850115895271, train ANN loss : 3.649878740310669\n",
      "AE loss : 0.03211744874715805, ANN loss : 3.800539970397949, Total loss : 7.012284278869629\n",
      "learning rate A :  tf.Tensor(0.008439766, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008404272, shape=(), dtype=float32)\n",
      "Time for epoch 326 is 0.1801 sec\n",
      "train AE loss : 0.03184831887483597, train ANN loss : 3.681377410888672\n",
      "AE loss : 0.05371778830885887, ANN loss : 3.7091567516326904, Total loss : 9.08093547821045\n",
      "learning rate A :  tf.Tensor(0.008439766, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008395422, shape=(), dtype=float32)\n",
      "Time for epoch 327 is 0.1928 sec\n",
      "train AE loss : 0.05291540175676346, train ANN loss : 3.618581771850586\n",
      "AE loss : 0.10436587780714035, ANN loss : 3.6605260372161865, Total loss : 14.097113609313965\n",
      "learning rate A :  tf.Tensor(0.008439766, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008386581, shape=(), dtype=float32)\n",
      "Time for epoch 328 is 0.1737 sec\n",
      "train AE loss : 0.10224736481904984, train ANN loss : 3.6237757205963135\n",
      "AE loss : 0.11452124267816544, ANN loss : 3.5653374195098877, Total loss : 15.017461776733398\n",
      "learning rate A :  tf.Tensor(0.008439766, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0083777495, shape=(), dtype=float32)\n",
      "Time for epoch 329 is 0.1750 sec\n",
      "train AE loss : 0.11249123513698578, train ANN loss : 3.4972469806671143\n",
      "AE loss : 0.08904130011796951, ANN loss : 3.5959551334381104, Total loss : 12.50008487701416\n",
      "learning rate A :  tf.Tensor(0.008439766, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008368927, shape=(), dtype=float32)\n",
      "Time for epoch 330 is 0.1899 sec\n",
      "train AE loss : 0.08770179748535156, train ANN loss : 3.4541211128234863\n",
      "AE loss : 0.0642336755990982, ANN loss : 3.695878744125366, Total loss : 10.119246482849121\n",
      "learning rate A :  tf.Tensor(0.008430879, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008368927, shape=(), dtype=float32)\n",
      "Time for epoch 331 is 0.1679 sec\n",
      "train AE loss : 0.06345196068286896, train ANN loss : 3.538376569747925\n",
      "AE loss : 0.05161435902118683, ANN loss : 3.8117263317108154, Total loss : 8.973162651062012\n",
      "learning rate A :  tf.Tensor(0.008422, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008368927, shape=(), dtype=float32)\n",
      "Time for epoch 332 is 0.1788 sec\n",
      "train AE loss : 0.050983529537916183, train ANN loss : 3.648293972015381\n",
      "AE loss : 0.04508453235030174, ANN loss : 3.91105055809021, Total loss : 8.419504165649414\n",
      "learning rate A :  tf.Tensor(0.008413131, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008368927, shape=(), dtype=float32)\n",
      "Time for epoch 333 is 0.1807 sec\n",
      "train AE loss : 0.04448825120925903, train ANN loss : 3.750098466873169\n",
      "AE loss : 0.07006650418043137, ANN loss : 3.6578567028045654, Total loss : 10.664507865905762\n",
      "learning rate A :  tf.Tensor(0.008413131, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008360115, shape=(), dtype=float32)\n",
      "Time for epoch 334 is 0.1708 sec\n",
      "train AE loss : 0.06896752864122391, train ANN loss : 3.513395071029663\n",
      "AE loss : 0.055040400475263596, ANN loss : 3.733170986175537, Total loss : 9.237211227416992\n",
      "learning rate A :  tf.Tensor(0.008404272, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008360115, shape=(), dtype=float32)\n",
      "Time for epoch 335 is 0.1690 sec\n",
      "train AE loss : 0.054164547473192215, train ANN loss : 3.5866971015930176\n",
      "AE loss : 0.04706621915102005, ANN loss : 3.8268518447875977, Total loss : 8.53347396850586\n",
      "learning rate A :  tf.Tensor(0.008395422, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008360115, shape=(), dtype=float32)\n",
      "Time for epoch 336 is 0.1701 sec\n",
      "train AE loss : 0.04621422663331032, train ANN loss : 3.6815743446350098\n",
      "AE loss : 0.11551685631275177, ANN loss : 3.5166077613830566, Total loss : 15.068293571472168\n",
      "learning rate A :  tf.Tensor(0.008395422, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00835131, shape=(), dtype=float32)\n",
      "Time for epoch 337 is 0.1822 sec\n",
      "train AE loss : 0.11326182633638382, train ANN loss : 3.472609043121338\n",
      "AE loss : 0.2490292340517044, ANN loss : 3.53669810295105, Total loss : 28.43962287902832\n",
      "learning rate A :  tf.Tensor(0.008395422, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008342517, shape=(), dtype=float32)\n",
      "Time for epoch 338 is 0.1718 sec\n",
      "train AE loss : 0.2435697615146637, train ANN loss : 3.841508150100708\n",
      "AE loss : 0.10164342075586319, ANN loss : 3.556041717529297, Total loss : 13.720383644104004\n",
      "learning rate A :  tf.Tensor(0.008386581, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008342517, shape=(), dtype=float32)\n",
      "Time for epoch 339 is 0.1773 sec\n",
      "train AE loss : 0.09916834533214569, train ANN loss : 3.5338869094848633\n",
      "AE loss : 0.19403694570064545, ANN loss : 3.504756450653076, Total loss : 22.908451080322266\n",
      "learning rate A :  tf.Tensor(0.008386581, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008333731, shape=(), dtype=float32)\n",
      "Time for epoch 340 is 0.1727 sec\n",
      "train AE loss : 0.189132422208786, train ANN loss : 3.6354122161865234\n",
      "AE loss : 0.19955085217952728, ANN loss : 3.5082781314849854, Total loss : 23.46336555480957\n",
      "learning rate A :  tf.Tensor(0.008386581, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008324956, shape=(), dtype=float32)\n",
      "Time for epoch 341 is 0.1928 sec\n",
      "train AE loss : 0.1945858895778656, train ANN loss : 3.580550193786621\n",
      "AE loss : 0.0907464474439621, ANN loss : 3.560108184814453, Total loss : 12.634754180908203\n",
      "learning rate A :  tf.Tensor(0.0083777495, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008324956, shape=(), dtype=float32)\n",
      "Time for epoch 342 is 0.1697 sec\n",
      "train AE loss : 0.08832947909832001, train ANN loss : 3.4985806941986084\n",
      "AE loss : 0.09654311835765839, ANN loss : 3.5499460697174072, Total loss : 13.204258918762207\n",
      "learning rate A :  tf.Tensor(0.0083777495, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008316189, shape=(), dtype=float32)\n",
      "Time for epoch 343 is 0.1720 sec\n",
      "train AE loss : 0.09408079087734222, train ANN loss : 3.4563629627227783\n",
      "AE loss : 0.0692119151353836, ANN loss : 3.601019859313965, Total loss : 10.522212028503418\n",
      "learning rate A :  tf.Tensor(0.008368927, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008316189, shape=(), dtype=float32)\n",
      "Time for epoch 344 is 0.1713 sec\n",
      "train AE loss : 0.06738023459911346, train ANN loss : 3.4975743293762207\n",
      "AE loss : 0.08589701354503632, ANN loss : 3.5705440044403076, Total loss : 12.160244941711426\n",
      "learning rate A :  tf.Tensor(0.008368927, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008307432, shape=(), dtype=float32)\n",
      "Time for epoch 345 is 0.1722 sec\n",
      "train AE loss : 0.08374171704053879, train ANN loss : 3.4549851417541504\n",
      "AE loss : 0.06374552845954895, ANN loss : 3.62701153755188, Total loss : 10.001564025878906\n",
      "learning rate A :  tf.Tensor(0.008360115, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008307432, shape=(), dtype=float32)\n",
      "Time for epoch 346 is 0.1791 sec\n",
      "train AE loss : 0.062011126428842545, train ANN loss : 3.508385181427002\n",
      "AE loss : 0.09187443554401398, ANN loss : 3.5787951946258545, Total loss : 12.76623821258545\n",
      "learning rate A :  tf.Tensor(0.008360115, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008298684, shape=(), dtype=float32)\n",
      "Time for epoch 347 is 0.1755 sec\n",
      "train AE loss : 0.08954324573278427, train ANN loss : 3.4666056632995605\n",
      "AE loss : 0.13149972259998322, ANN loss : 3.5831058025360107, Total loss : 16.733078002929688\n",
      "learning rate A :  tf.Tensor(0.008360115, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008289945, shape=(), dtype=float32)\n",
      "Time for epoch 348 is 0.1750 sec\n",
      "train AE loss : 0.12828020751476288, train ANN loss : 3.5301437377929688\n",
      "AE loss : 0.07852904498577118, ANN loss : 3.604968786239624, Total loss : 11.457873344421387\n",
      "learning rate A :  tf.Tensor(0.00835131, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008289945, shape=(), dtype=float32)\n",
      "Time for epoch 349 is 0.1711 sec\n",
      "train AE loss : 0.07648715376853943, train ANN loss : 3.4832403659820557\n",
      "AE loss : 0.11571450531482697, ANN loss : 3.566709280014038, Total loss : 15.13815975189209\n",
      "learning rate A :  tf.Tensor(0.00835131, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008281215, shape=(), dtype=float32)\n",
      "Time for epoch 350 is 0.1803 sec\n",
      "train AE loss : 0.11287236958742142, train ANN loss : 3.487948179244995\n",
      "AE loss : 0.13760629296302795, ANN loss : 3.530522108078003, Total loss : 17.29115104675293\n",
      "learning rate A :  tf.Tensor(0.00835131, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008272494, shape=(), dtype=float32)\n",
      "Time for epoch 351 is 0.1722 sec\n",
      "train AE loss : 0.13442514836788177, train ANN loss : 3.4706220626831055\n",
      "AE loss : 0.1290682852268219, ANN loss : 3.5091960430145264, Total loss : 16.416025161743164\n",
      "learning rate A :  tf.Tensor(0.00835131, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0082637835, shape=(), dtype=float32)\n",
      "Time for epoch 352 is 0.1725 sec\n",
      "train AE loss : 0.1263691782951355, train ANN loss : 3.4138753414154053\n",
      "AE loss : 0.11367237567901611, ANN loss : 3.5212645530700684, Total loss : 14.88850212097168\n",
      "learning rate A :  tf.Tensor(0.00835131, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008255081, shape=(), dtype=float32)\n",
      "Time for epoch 353 is 0.1736 sec\n",
      "train AE loss : 0.11157318204641342, train ANN loss : 3.375704288482666\n",
      "AE loss : 0.07352866977453232, ANN loss : 3.6197926998138428, Total loss : 10.97265911102295\n",
      "learning rate A :  tf.Tensor(0.008342517, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008255081, shape=(), dtype=float32)\n",
      "Time for epoch 354 is 0.1702 sec\n",
      "train AE loss : 0.07207699865102768, train ANN loss : 3.451502561569214\n",
      "AE loss : 0.0575840100646019, ANN loss : 3.710087537765503, Total loss : 9.468487739562988\n",
      "learning rate A :  tf.Tensor(0.008333731, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008255081, shape=(), dtype=float32)\n",
      "Time for epoch 355 is 0.1693 sec\n",
      "train AE loss : 0.05631471425294876, train ANN loss : 3.545325756072998\n",
      "AE loss : 0.04924514517188072, ANN loss : 3.789290189743042, Total loss : 8.713805198669434\n",
      "learning rate A :  tf.Tensor(0.008324956, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008255081, shape=(), dtype=float32)\n",
      "Time for epoch 356 is 0.1687 sec\n",
      "train AE loss : 0.04809211939573288, train ANN loss : 3.624760389328003\n",
      "AE loss : 0.05944393575191498, ANN loss : 3.673837184906006, Total loss : 9.618229866027832\n",
      "learning rate A :  tf.Tensor(0.008324956, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008246387, shape=(), dtype=float32)\n",
      "Time for epoch 357 is 0.1707 sec\n",
      "train AE loss : 0.0581781268119812, train ANN loss : 3.518627405166626\n",
      "AE loss : 0.05025748908519745, ANN loss : 3.7521564960479736, Total loss : 8.777905464172363\n",
      "learning rate A :  tf.Tensor(0.008316189, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008246387, shape=(), dtype=float32)\n",
      "Time for epoch 358 is 0.1819 sec\n",
      "train AE loss : 0.04908467456698418, train ANN loss : 3.5981481075286865\n",
      "AE loss : 0.04492728039622307, ANN loss : 3.81976056098938, Total loss : 8.312488555908203\n",
      "learning rate A :  tf.Tensor(0.008307432, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008246387, shape=(), dtype=float32)\n",
      "Time for epoch 359 is 0.1705 sec\n",
      "train AE loss : 0.04382225126028061, train ANN loss : 3.6656460762023926\n",
      "AE loss : 0.04136960208415985, ANN loss : 3.8841378688812256, Total loss : 8.021098136901855\n",
      "learning rate A :  tf.Tensor(0.008298684, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008246387, shape=(), dtype=float32)\n",
      "Time for epoch 360 is 0.1821 sec\n",
      "train AE loss : 0.04032384976744652, train ANN loss : 3.7366597652435303\n",
      "AE loss : 0.0708322674036026, ANN loss : 3.585803747177124, Total loss : 10.66903018951416\n",
      "learning rate A :  tf.Tensor(0.008298684, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008237704, shape=(), dtype=float32)\n",
      "Time for epoch 361 is 0.1816 sec\n",
      "train AE loss : 0.06938581168651581, train ANN loss : 3.4736969470977783\n",
      "AE loss : 0.15266872942447662, ANN loss : 3.5227208137512207, Total loss : 18.789592742919922\n",
      "learning rate A :  tf.Tensor(0.008298684, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008229029, shape=(), dtype=float32)\n",
      "Time for epoch 362 is 0.1728 sec\n",
      "train AE loss : 0.14985023438930511, train ANN loss : 3.5697684288024902\n",
      "AE loss : 0.07909754663705826, ANN loss : 3.5871903896331787, Total loss : 11.496944427490234\n",
      "learning rate A :  tf.Tensor(0.008289945, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008229029, shape=(), dtype=float32)\n",
      "Time for epoch 363 is 0.1692 sec\n",
      "train AE loss : 0.07734916359186172, train ANN loss : 3.5240304470062256\n",
      "AE loss : 0.0594579353928566, ANN loss : 3.6361002922058105, Total loss : 9.581893920898438\n",
      "learning rate A :  tf.Tensor(0.008281215, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008229029, shape=(), dtype=float32)\n",
      "Time for epoch 364 is 0.1815 sec\n",
      "train AE loss : 0.05799664929509163, train ANN loss : 3.571105718612671\n",
      "AE loss : 0.050574421882629395, ANN loss : 3.675333261489868, Total loss : 8.732775688171387\n",
      "learning rate A :  tf.Tensor(0.008272494, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008229029, shape=(), dtype=float32)\n",
      "Time for epoch 365 is 0.1695 sec\n",
      "train AE loss : 0.049327343702316284, train ANN loss : 3.604926586151123\n",
      "AE loss : 0.10314469039440155, ANN loss : 3.5762758255004883, Total loss : 13.890746116638184\n",
      "learning rate A :  tf.Tensor(0.008272494, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008220363, shape=(), dtype=float32)\n",
      "Time for epoch 366 is 0.1731 sec\n",
      "train AE loss : 0.10092378407716751, train ANN loss : 3.580979585647583\n",
      "AE loss : 0.06282546371221542, ANN loss : 3.656365156173706, Total loss : 9.938911437988281\n",
      "learning rate A :  tf.Tensor(0.0082637835, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008220363, shape=(), dtype=float32)\n",
      "Time for epoch 367 is 0.1704 sec\n",
      "train AE loss : 0.06110591068863869, train ANN loss : 3.6327104568481445\n",
      "AE loss : 0.1284712702035904, ANN loss : 3.5120060443878174, Total loss : 16.359132766723633\n",
      "learning rate A :  tf.Tensor(0.0082637835, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008211707, shape=(), dtype=float32)\n",
      "Time for epoch 368 is 0.1745 sec\n",
      "train AE loss : 0.12552161514759064, train ANN loss : 3.6122024059295654\n",
      "AE loss : 0.06114734709262848, ANN loss : 3.671651840209961, Total loss : 9.786386489868164\n",
      "learning rate A :  tf.Tensor(0.008255081, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008211707, shape=(), dtype=float32)\n",
      "Time for epoch 369 is 0.1708 sec\n",
      "train AE loss : 0.05899413302540779, train ANN loss : 3.676161289215088\n",
      "AE loss : 0.10263732075691223, ANN loss : 3.527928590774536, Total loss : 13.791661262512207\n",
      "learning rate A :  tf.Tensor(0.008255081, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008203059, shape=(), dtype=float32)\n",
      "Time for epoch 370 is 0.1735 sec\n",
      "train AE loss : 0.099627785384655, train ANN loss : 3.5786232948303223\n",
      "AE loss : 0.15466336905956268, ANN loss : 3.4555938243865967, Total loss : 18.92193031311035\n",
      "learning rate A :  tf.Tensor(0.008255081, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008194421, shape=(), dtype=float32)\n",
      "Time for epoch 371 is 0.1750 sec\n",
      "train AE loss : 0.15108457207679749, train ANN loss : 3.598785638809204\n",
      "AE loss : 0.11914738267660141, ANN loss : 3.5358901023864746, Total loss : 15.450628280639648\n",
      "learning rate A :  tf.Tensor(0.008255081, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008185792, shape=(), dtype=float32)\n",
      "Time for epoch 372 is 0.1740 sec\n",
      "train AE loss : 0.11696530878543854, train ANN loss : 3.435657501220703\n",
      "AE loss : 0.06658963114023209, ANN loss : 3.7619104385375977, Total loss : 10.420873641967773\n",
      "learning rate A :  tf.Tensor(0.008246387, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008185792, shape=(), dtype=float32)\n",
      "Time for epoch 373 is 0.1682 sec\n",
      "train AE loss : 0.06486980617046356, train ANN loss : 3.5914320945739746\n",
      "AE loss : 0.07612873613834381, ANN loss : 3.7442991733551025, Total loss : 11.357172966003418\n",
      "learning rate A :  tf.Tensor(0.008246387, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008177172, shape=(), dtype=float32)\n",
      "Time for epoch 374 is 0.1700 sec\n",
      "train AE loss : 0.0745258778333664, train ANN loss : 3.5496981143951416\n",
      "AE loss : 0.05435071140527725, ANN loss : 3.9270052909851074, Total loss : 9.362076759338379\n",
      "learning rate A :  tf.Tensor(0.008237704, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008177172, shape=(), dtype=float32)\n",
      "Time for epoch 375 is 0.1801 sec\n",
      "train AE loss : 0.05295095965266228, train ANN loss : 3.716883659362793\n",
      "AE loss : 0.09156592935323715, ANN loss : 3.604182481765747, Total loss : 12.760775566101074\n",
      "learning rate A :  tf.Tensor(0.008237704, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008168561, shape=(), dtype=float32)\n",
      "Time for epoch 376 is 0.1767 sec\n",
      "train AE loss : 0.08985543251037598, train ANN loss : 3.4295740127563477\n",
      "AE loss : 0.1919541209936142, ANN loss : 3.4831175804138184, Total loss : 22.678531646728516\n",
      "learning rate A :  tf.Tensor(0.008237704, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00815996, shape=(), dtype=float32)\n",
      "Time for epoch 377 is 0.1725 sec\n",
      "train AE loss : 0.18816864490509033, train ANN loss : 3.484656810760498\n",
      "AE loss : 0.05994492396712303, ANN loss : 3.6670162677764893, Total loss : 9.661508560180664\n",
      "learning rate A :  tf.Tensor(0.008229029, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00815996, shape=(), dtype=float32)\n",
      "Time for epoch 378 is 0.1695 sec\n",
      "train AE loss : 0.05860264226794243, train ANN loss : 3.5189149379730225\n",
      "AE loss : 0.125698521733284, ANN loss : 3.5322585105895996, Total loss : 16.102109909057617\n",
      "learning rate A :  tf.Tensor(0.008229029, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008151366, shape=(), dtype=float32)\n",
      "Time for epoch 379 is 0.1877 sec\n",
      "train AE loss : 0.12338653206825256, train ANN loss : 3.4751899242401123\n",
      "AE loss : 0.05299105495214462, ANN loss : 3.6581509113311768, Total loss : 8.957256317138672\n",
      "learning rate A :  tf.Tensor(0.008220363, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008151366, shape=(), dtype=float32)\n",
      "Time for epoch 380 is 0.1730 sec\n",
      "train AE loss : 0.051562756299972534, train ANN loss : 3.5602879524230957\n",
      "AE loss : 0.04836646467447281, ANN loss : 3.6787872314453125, Total loss : 8.515434265136719\n",
      "learning rate A :  tf.Tensor(0.008211707, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008151366, shape=(), dtype=float32)\n",
      "Time for epoch 381 is 0.1699 sec\n",
      "train AE loss : 0.047095827758312225, train ANN loss : 3.5679304599761963\n",
      "AE loss : 0.043921127915382385, ANN loss : 3.6967549324035645, Total loss : 8.088868141174316\n",
      "learning rate A :  tf.Tensor(0.008203059, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008151366, shape=(), dtype=float32)\n",
      "Time for epoch 382 is 0.1707 sec\n",
      "train AE loss : 0.04273630306124687, train ANN loss : 3.585498332977295\n",
      "AE loss : 0.040716640651226044, ANN loss : 3.7192368507385254, Total loss : 7.790900707244873\n",
      "learning rate A :  tf.Tensor(0.008194421, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008151366, shape=(), dtype=float32)\n",
      "Time for epoch 383 is 0.1814 sec\n",
      "train AE loss : 0.039636678993701935, train ANN loss : 3.5938639640808105\n",
      "AE loss : 0.03822977840900421, ANN loss : 3.7393245697021484, Total loss : 7.562302589416504\n",
      "learning rate A :  tf.Tensor(0.008185792, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008151366, shape=(), dtype=float32)\n",
      "Time for epoch 384 is 0.1713 sec\n",
      "train AE loss : 0.03723185881972313, train ANN loss : 3.605067491531372\n",
      "AE loss : 0.03619756922125816, ANN loss : 3.75559663772583, Total loss : 7.3753533363342285\n",
      "learning rate A :  tf.Tensor(0.008177172, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008151366, shape=(), dtype=float32)\n",
      "Time for epoch 385 is 0.1692 sec\n",
      "train AE loss : 0.03526952117681503, train ANN loss : 3.6211681365966797\n",
      "AE loss : 0.05710800737142563, ANN loss : 3.6220104694366455, Total loss : 9.33281135559082\n",
      "learning rate A :  tf.Tensor(0.008177172, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008142783, shape=(), dtype=float32)\n",
      "Time for epoch 386 is 0.1746 sec\n",
      "train AE loss : 0.05606818571686745, train ANN loss : 3.5795295238494873\n",
      "AE loss : 0.12323249876499176, ANN loss : 3.6861040592193604, Total loss : 16.009353637695312\n",
      "learning rate A :  tf.Tensor(0.008177172, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008134208, shape=(), dtype=float32)\n",
      "Time for epoch 387 is 0.1869 sec\n",
      "train AE loss : 0.12140235304832458, train ANN loss : 3.748800754547119\n",
      "AE loss : 0.05105584114789963, ANN loss : 3.724177360534668, Total loss : 8.829761505126953\n",
      "learning rate A :  tf.Tensor(0.008168561, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008134208, shape=(), dtype=float32)\n",
      "Time for epoch 388 is 0.1817 sec\n",
      "train AE loss : 0.05003600940108299, train ANN loss : 3.74949049949646\n",
      "AE loss : 0.044584970921278, ANN loss : 3.7185888290405273, Total loss : 8.177085876464844\n",
      "learning rate A :  tf.Tensor(0.00815996, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008134208, shape=(), dtype=float32)\n",
      "Time for epoch 389 is 0.1691 sec\n",
      "train AE loss : 0.04369611665606499, train ANN loss : 3.722186326980591\n",
      "AE loss : 0.04019699618220329, ANN loss : 3.7221362590789795, Total loss : 7.741836071014404\n",
      "learning rate A :  tf.Tensor(0.008151366, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008134208, shape=(), dtype=float32)\n",
      "Time for epoch 390 is 0.1814 sec\n",
      "train AE loss : 0.03941519558429718, train ANN loss : 3.7177348136901855\n",
      "AE loss : 0.05820475146174431, ANN loss : 3.6451430320739746, Total loss : 9.465618133544922\n",
      "learning rate A :  tf.Tensor(0.008151366, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008125642, shape=(), dtype=float32)\n",
      "Time for epoch 391 is 0.2933 sec\n",
      "train AE loss : 0.05718978866934776, train ANN loss : 3.638066291809082\n",
      "AE loss : 0.04627436026930809, ANN loss : 3.661806583404541, Total loss : 8.2892427444458\n",
      "learning rate A :  tf.Tensor(0.008142783, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008125642, shape=(), dtype=float32)\n",
      "Time for epoch 392 is 0.1815 sec\n",
      "train AE loss : 0.04543832689523697, train ANN loss : 3.6440205574035645\n",
      "AE loss : 0.0734579935669899, ANN loss : 3.5835723876953125, Total loss : 10.929370880126953\n",
      "learning rate A :  tf.Tensor(0.008142783, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008117086, shape=(), dtype=float32)\n",
      "Time for epoch 393 is 0.1750 sec\n",
      "train AE loss : 0.0722278282046318, train ANN loss : 3.568499803543091\n",
      "AE loss : 0.13333886861801147, ANN loss : 3.5497987270355225, Total loss : 16.883686065673828\n",
      "learning rate A :  tf.Tensor(0.008142783, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008108539, shape=(), dtype=float32)\n",
      "Time for epoch 394 is 0.1717 sec\n",
      "train AE loss : 0.13056506216526031, train ANN loss : 3.5993876457214355\n",
      "AE loss : 0.04360616207122803, ANN loss : 3.633674383163452, Total loss : 7.994289875030518\n",
      "learning rate A :  tf.Tensor(0.008134208, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008108539, shape=(), dtype=float32)\n",
      "Time for epoch 395 is 0.1810 sec\n",
      "train AE loss : 0.042506489902734756, train ANN loss : 3.5600392818450928\n",
      "AE loss : 0.03883527219295502, ANN loss : 3.6584713459014893, Total loss : 7.541998863220215\n",
      "learning rate A :  tf.Tensor(0.008125642, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008108539, shape=(), dtype=float32)\n",
      "Time for epoch 396 is 0.1709 sec\n",
      "train AE loss : 0.037899747490882874, train ANN loss : 3.5721893310546875\n",
      "AE loss : 0.05108669027686119, ANN loss : 3.6366262435913086, Total loss : 8.745295524597168\n",
      "learning rate A :  tf.Tensor(0.008125642, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008099999, shape=(), dtype=float32)\n",
      "Time for epoch 397 is 0.1854 sec\n",
      "train AE loss : 0.04958450421690941, train ANN loss : 3.542468547821045\n",
      "AE loss : 0.043368078768253326, ANN loss : 3.6848676204681396, Total loss : 8.021676063537598\n",
      "learning rate A :  tf.Tensor(0.008117086, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008099999, shape=(), dtype=float32)\n",
      "Time for epoch 398 is 0.1703 sec\n",
      "train AE loss : 0.042154401540756226, train ANN loss : 3.578108072280884\n",
      "AE loss : 0.06579498946666718, ANN loss : 3.5742876529693604, Total loss : 10.153785705566406\n",
      "learning rate A :  tf.Tensor(0.008117086, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00809147, shape=(), dtype=float32)\n",
      "Time for epoch 399 is 0.1840 sec\n",
      "train AE loss : 0.06405837088823318, train ANN loss : 3.485167980194092\n",
      "AE loss : 0.13198408484458923, ANN loss : 3.5027239322662354, Total loss : 16.70113182067871\n",
      "learning rate A :  tf.Tensor(0.008117086, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008082949, shape=(), dtype=float32)\n",
      "Time for epoch 400 is 0.1832 sec\n",
      "train AE loss : 0.1287849396467209, train ANN loss : 3.5686419010162354\n",
      "AE loss : 0.04669701308012009, ANN loss : 3.671079635620117, Total loss : 8.340780258178711\n",
      "learning rate A :  tf.Tensor(0.008108539, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008082949, shape=(), dtype=float32)\n",
      "Time for epoch 401 is 0.1847 sec\n",
      "train AE loss : 0.045266564935445786, train ANN loss : 3.5691213607788086\n",
      "AE loss : 0.04116172343492508, ANN loss : 3.718137741088867, Total loss : 7.834310054779053\n",
      "learning rate A :  tf.Tensor(0.008099999, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008082949, shape=(), dtype=float32)\n",
      "Time for epoch 402 is 0.1767 sec\n",
      "train AE loss : 0.03991740942001343, train ANN loss : 3.5996828079223633\n",
      "AE loss : 0.06719987094402313, ANN loss : 3.5438435077667236, Total loss : 10.263830184936523\n",
      "learning rate A :  tf.Tensor(0.008099999, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008074437, shape=(), dtype=float32)\n",
      "Time for epoch 403 is 0.1743 sec\n",
      "train AE loss : 0.06527641415596008, train ANN loss : 3.4949207305908203\n",
      "AE loss : 0.12443173676729202, ANN loss : 3.511307954788208, Total loss : 15.954480171203613\n",
      "learning rate A :  tf.Tensor(0.008099999, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008065934, shape=(), dtype=float32)\n",
      "Time for epoch 404 is 0.1736 sec\n",
      "train AE loss : 0.12148319184780121, train ANN loss : 3.642655372619629\n",
      "AE loss : 0.05206817388534546, ANN loss : 3.5856683254241943, Total loss : 8.792485237121582\n",
      "learning rate A :  tf.Tensor(0.00809147, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008065934, shape=(), dtype=float32)\n",
      "Time for epoch 405 is 0.1717 sec\n",
      "train AE loss : 0.05080216005444527, train ANN loss : 3.573087692260742\n",
      "AE loss : 0.044167667627334595, ANN loss : 3.6012632846832275, Total loss : 8.018030166625977\n",
      "learning rate A :  tf.Tensor(0.008082949, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008065934, shape=(), dtype=float32)\n",
      "Time for epoch 406 is 0.1803 sec\n",
      "train AE loss : 0.04314297437667847, train ANN loss : 3.575319528579712\n",
      "AE loss : 0.058358993381261826, ANN loss : 3.5393192768096924, Total loss : 9.375218391418457\n",
      "learning rate A :  tf.Tensor(0.008082949, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008057441, shape=(), dtype=float32)\n",
      "Time for epoch 407 is 0.1736 sec\n",
      "train AE loss : 0.05680980160832405, train ANN loss : 3.5257978439331055\n",
      "AE loss : 0.07461900264024734, ANN loss : 3.5084502696990967, Total loss : 10.97035026550293\n",
      "learning rate A :  tf.Tensor(0.008082949, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008048955, shape=(), dtype=float32)\n",
      "Time for epoch 408 is 0.1921 sec\n",
      "train AE loss : 0.07253693044185638, train ANN loss : 3.4769387245178223\n",
      "AE loss : 0.05032418295741081, ANN loss : 3.5582008361816406, Total loss : 8.590619087219238\n",
      "learning rate A :  tf.Tensor(0.008074437, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008048955, shape=(), dtype=float32)\n",
      "Time for epoch 409 is 0.1716 sec\n",
      "train AE loss : 0.048850152641534805, train ANN loss : 3.4761948585510254\n",
      "AE loss : 0.06783197075128555, ANN loss : 3.536344289779663, Total loss : 10.319540977478027\n",
      "learning rate A :  tf.Tensor(0.008074437, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008040479, shape=(), dtype=float32)\n",
      "Time for epoch 410 is 0.1721 sec\n",
      "train AE loss : 0.0659150555729866, train ANN loss : 3.472687005996704\n",
      "AE loss : 0.09775281697511673, ANN loss : 3.473912477493286, Total loss : 13.249195098876953\n",
      "learning rate A :  tf.Tensor(0.008074437, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008032013, shape=(), dtype=float32)\n",
      "Time for epoch 411 is 0.1715 sec\n",
      "train AE loss : 0.09534690529108047, train ANN loss : 3.4842655658721924\n",
      "AE loss : 0.12996356189250946, ANN loss : 3.4422972202301025, Total loss : 16.43865394592285\n",
      "learning rate A :  tf.Tensor(0.008074437, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008023555, shape=(), dtype=float32)\n",
      "Time for epoch 412 is 0.1739 sec\n",
      "train AE loss : 0.12714993953704834, train ANN loss : 3.5313267707824707\n",
      "AE loss : 0.042154841125011444, ANN loss : 3.763864278793335, Total loss : 7.979348182678223\n",
      "learning rate A :  tf.Tensor(0.008065934, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008023555, shape=(), dtype=float32)\n",
      "Time for epoch 413 is 0.1819 sec\n",
      "train AE loss : 0.040560703724622726, train ANN loss : 3.6469061374664307\n",
      "AE loss : 0.05861514434218407, ANN loss : 3.502474308013916, Total loss : 9.36398983001709\n",
      "learning rate A :  tf.Tensor(0.008065934, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0080151055, shape=(), dtype=float32)\n",
      "Time for epoch 414 is 0.1814 sec\n",
      "train AE loss : 0.05693071708083153, train ANN loss : 3.4463565349578857\n",
      "AE loss : 0.08857297152280807, ANN loss : 3.4905426502227783, Total loss : 12.347838401794434\n",
      "learning rate A :  tf.Tensor(0.008065934, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.008006666, shape=(), dtype=float32)\n",
      "Time for epoch 415 is 0.1726 sec\n",
      "train AE loss : 0.08675717562437057, train ANN loss : 3.540925979614258\n",
      "AE loss : 0.1089923232793808, ANN loss : 3.47003173828125, Total loss : 14.369263648986816\n",
      "learning rate A :  tf.Tensor(0.008065934, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007998234, shape=(), dtype=float32)\n",
      "Time for epoch 416 is 0.1721 sec\n",
      "train AE loss : 0.10688354820013046, train ANN loss : 3.522407054901123\n",
      "AE loss : 0.05020953714847565, ANN loss : 3.536785364151001, Total loss : 8.5577392578125\n",
      "learning rate A :  tf.Tensor(0.008057441, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007998234, shape=(), dtype=float32)\n",
      "Time for epoch 417 is 0.1712 sec\n",
      "train AE loss : 0.048700325191020966, train ANN loss : 3.4898576736450195\n",
      "AE loss : 0.05490797385573387, ANN loss : 3.542316436767578, Total loss : 9.033113479614258\n",
      "learning rate A :  tf.Tensor(0.008057441, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007989811, shape=(), dtype=float32)\n",
      "Time for epoch 418 is 0.1721 sec\n",
      "train AE loss : 0.0532541498541832, train ANN loss : 3.456137180328369\n",
      "AE loss : 0.04408834129571915, ANN loss : 3.5889546871185303, Total loss : 7.997788429260254\n",
      "learning rate A :  tf.Tensor(0.008048955, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007989811, shape=(), dtype=float32)\n",
      "Time for epoch 419 is 0.1693 sec\n",
      "train AE loss : 0.0426320880651474, train ANN loss : 3.4992918968200684\n",
      "AE loss : 0.051271289587020874, ANN loss : 3.5614030361175537, Total loss : 8.688531875610352\n",
      "learning rate A :  tf.Tensor(0.008048955, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007981397, shape=(), dtype=float32)\n",
      "Time for epoch 420 is 0.1721 sec\n",
      "train AE loss : 0.049660492688417435, train ANN loss : 3.4690921306610107\n",
      "AE loss : 0.07005652040243149, ANN loss : 3.4869346618652344, Total loss : 10.492586135864258\n",
      "learning rate A :  tf.Tensor(0.008048955, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007972992, shape=(), dtype=float32)\n",
      "Time for epoch 421 is 0.1739 sec\n",
      "train AE loss : 0.06819656491279602, train ANN loss : 3.4200689792633057\n",
      "AE loss : 0.09662918746471405, ANN loss : 3.4519355297088623, Total loss : 13.114852905273438\n",
      "learning rate A :  tf.Tensor(0.008048955, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007964596, shape=(), dtype=float32)\n",
      "Time for epoch 422 is 0.1704 sec\n",
      "train AE loss : 0.09435050934553146, train ANN loss : 3.417639970779419\n",
      "AE loss : 0.052749890834093094, ANN loss : 3.5373849868774414, Total loss : 8.812374114990234\n",
      "learning rate A :  tf.Tensor(0.008040479, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007964596, shape=(), dtype=float32)\n",
      "Time for epoch 423 is 0.1694 sec\n",
      "train AE loss : 0.050930798053741455, train ANN loss : 3.4466378688812256\n",
      "AE loss : 0.046562645584344864, ANN loss : 3.592817544937134, Total loss : 8.249082565307617\n",
      "learning rate A :  tf.Tensor(0.008032013, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007964596, shape=(), dtype=float32)\n",
      "Time for epoch 424 is 0.1713 sec\n",
      "train AE loss : 0.04490990191698074, train ANN loss : 3.4852490425109863\n",
      "AE loss : 0.06467625498771667, ANN loss : 3.4978415966033936, Total loss : 9.96546745300293\n",
      "learning rate A :  tf.Tensor(0.008032013, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00795621, shape=(), dtype=float32)\n",
      "Time for epoch 425 is 0.1834 sec\n",
      "train AE loss : 0.06260588765144348, train ANN loss : 3.433345317840576\n",
      "AE loss : 0.08838923275470734, ANN loss : 3.4646685123443604, Total loss : 12.303592681884766\n",
      "learning rate A :  tf.Tensor(0.008032013, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007947831, shape=(), dtype=float32)\n",
      "Time for epoch 426 is 0.1694 sec\n",
      "train AE loss : 0.0857967734336853, train ANN loss : 3.4617228507995605\n",
      "AE loss : 0.057396359741687775, ANN loss : 3.505178928375244, Total loss : 9.2448148727417\n",
      "learning rate A :  tf.Tensor(0.008023555, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007947831, shape=(), dtype=float32)\n",
      "Time for epoch 427 is 0.1900 sec\n",
      "train AE loss : 0.05525405332446098, train ANN loss : 3.4416894912719727\n",
      "AE loss : 0.06458601355552673, ANN loss : 3.478374719619751, Total loss : 9.936976432800293\n",
      "learning rate A :  tf.Tensor(0.008023555, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007939462, shape=(), dtype=float32)\n",
      "Time for epoch 428 is 0.1725 sec\n",
      "train AE loss : 0.062306515872478485, train ANN loss : 3.396005392074585\n",
      "AE loss : 0.08038776367902756, ANN loss : 3.4400038719177246, Total loss : 11.478779792785645\n",
      "learning rate A :  tf.Tensor(0.008023555, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007931101, shape=(), dtype=float32)\n",
      "Time for epoch 429 is 0.1812 sec\n",
      "train AE loss : 0.07777127623558044, train ANN loss : 3.3839361667633057\n",
      "AE loss : 0.10228506475687027, ANN loss : 3.4093923568725586, Total loss : 13.637897491455078\n",
      "learning rate A :  tf.Tensor(0.008023555, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007922749, shape=(), dtype=float32)\n",
      "Time for epoch 430 is 0.1814 sec\n",
      "train AE loss : 0.09909222275018692, train ANN loss : 3.412229061126709\n",
      "AE loss : 0.05690941587090492, ANN loss : 3.5026865005493164, Total loss : 9.193628311157227\n",
      "learning rate A :  tf.Tensor(0.0080151055, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007922749, shape=(), dtype=float32)\n",
      "Time for epoch 431 is 0.1691 sec\n",
      "train AE loss : 0.0544661246240139, train ANN loss : 3.419825792312622\n",
      "AE loss : 0.07335851341485977, ANN loss : 3.4250967502593994, Total loss : 10.760948181152344\n",
      "learning rate A :  tf.Tensor(0.0080151055, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007914406, shape=(), dtype=float32)\n",
      "Time for epoch 432 is 0.1714 sec\n",
      "train AE loss : 0.07033450156450272, train ANN loss : 3.390484571456909\n",
      "AE loss : 0.08402731269598007, ANN loss : 3.397005558013916, Total loss : 11.799736976623535\n",
      "learning rate A :  tf.Tensor(0.0080151055, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007906071, shape=(), dtype=float32)\n",
      "Time for epoch 433 is 0.1749 sec\n",
      "train AE loss : 0.08077478408813477, train ANN loss : 3.377598762512207\n",
      "AE loss : 0.09279021620750427, ANN loss : 3.387166738510132, Total loss : 12.66618824005127\n",
      "learning rate A :  tf.Tensor(0.0080151055, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007897747, shape=(), dtype=float32)\n",
      "Time for epoch 434 is 0.1700 sec\n",
      "train AE loss : 0.08932258188724518, train ANN loss : 3.3754210472106934\n",
      "AE loss : 0.05609587952494621, ANN loss : 3.6420867443084717, Total loss : 9.25167465209961\n",
      "learning rate A :  tf.Tensor(0.008006666, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007897747, shape=(), dtype=float32)\n",
      "Time for epoch 435 is 0.1697 sec\n",
      "train AE loss : 0.05328953266143799, train ANN loss : 3.525986671447754\n",
      "AE loss : 0.07693161815404892, ANN loss : 3.407109022140503, Total loss : 11.100271224975586\n",
      "learning rate A :  tf.Tensor(0.008006666, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007889429, shape=(), dtype=float32)\n",
      "Time for epoch 436 is 0.1750 sec\n",
      "train AE loss : 0.07378856837749481, train ANN loss : 3.3743481636047363\n",
      "AE loss : 0.11299581080675125, ANN loss : 3.3687527179718018, Total loss : 14.668334007263184\n",
      "learning rate A :  tf.Tensor(0.008006666, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007881122, shape=(), dtype=float32)\n",
      "Time for epoch 437 is 0.1759 sec\n",
      "train AE loss : 0.10907542705535889, train ANN loss : 3.488769292831421\n",
      "AE loss : 0.06613893806934357, ANN loss : 3.407461643218994, Total loss : 10.021355628967285\n",
      "learning rate A :  tf.Tensor(0.007998234, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007881122, shape=(), dtype=float32)\n",
      "Time for epoch 438 is 0.2005 sec\n",
      "train AE loss : 0.06307268142700195, train ANN loss : 3.3921191692352295\n",
      "AE loss : 0.08436763286590576, ANN loss : 3.385622978210449, Total loss : 11.822386741638184\n",
      "learning rate A :  tf.Tensor(0.007998234, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007872822, shape=(), dtype=float32)\n",
      "Time for epoch 439 is 0.2275 sec\n",
      "train AE loss : 0.08075257390737534, train ANN loss : 3.410332202911377\n",
      "AE loss : 0.07280774414539337, ANN loss : 3.459381580352783, Total loss : 10.740156173706055\n",
      "learning rate A :  tf.Tensor(0.007998234, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007864532, shape=(), dtype=float32)\n",
      "Time for epoch 440 is 0.1754 sec\n",
      "train AE loss : 0.06979089230298996, train ANN loss : 3.364551544189453\n",
      "AE loss : 0.05474228039383888, ANN loss : 3.6024458408355713, Total loss : 9.07667350769043\n",
      "learning rate A :  tf.Tensor(0.007989811, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007864532, shape=(), dtype=float32)\n",
      "Time for epoch 441 is 0.1734 sec\n",
      "train AE loss : 0.052016403526067734, train ANN loss : 3.4819746017456055\n",
      "AE loss : 0.06794475018978119, ANN loss : 3.5170648097991943, Total loss : 10.311539649963379\n",
      "learning rate A :  tf.Tensor(0.007989811, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00785625, shape=(), dtype=float32)\n",
      "Time for epoch 442 is 0.1733 sec\n",
      "train AE loss : 0.06503601372241974, train ANN loss : 3.4030773639678955\n",
      "AE loss : 0.10562774538993835, ANN loss : 3.374448299407959, Total loss : 13.937223434448242\n",
      "learning rate A :  tf.Tensor(0.007989811, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007847977, shape=(), dtype=float32)\n",
      "Time for epoch 443 is 0.2266 sec\n",
      "train AE loss : 0.10248228907585144, train ANN loss : 3.372297525405884\n",
      "AE loss : 0.06576915085315704, ANN loss : 3.4780938625335693, Total loss : 10.055008888244629\n",
      "learning rate A :  tf.Tensor(0.007981397, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007847977, shape=(), dtype=float32)\n",
      "Time for epoch 444 is 0.1745 sec\n",
      "train AE loss : 0.06301482021808624, train ANN loss : 3.3906023502349854\n",
      "AE loss : 0.11892069876194, ANN loss : 3.3766329288482666, Total loss : 15.268702507019043\n",
      "learning rate A :  tf.Tensor(0.007981397, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007839713, shape=(), dtype=float32)\n",
      "Time for epoch 445 is 0.1723 sec\n",
      "train AE loss : 0.11548803746700287, train ANN loss : 3.4425735473632812\n",
      "AE loss : 0.14939503371715546, ANN loss : 3.3854689598083496, Total loss : 18.32497215270996\n",
      "learning rate A :  tf.Tensor(0.007981397, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007831457, shape=(), dtype=float32)\n",
      "Time for epoch 446 is 0.1718 sec\n",
      "train AE loss : 0.14570927619934082, train ANN loss : 3.508380174636841\n",
      "AE loss : 0.07096286118030548, ANN loss : 3.4395837783813477, Total loss : 10.535869598388672\n",
      "learning rate A :  tf.Tensor(0.007972992, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007831457, shape=(), dtype=float32)\n",
      "Time for epoch 447 is 0.1688 sec\n",
      "train AE loss : 0.06810326129198074, train ANN loss : 3.37052583694458\n",
      "AE loss : 0.049972109496593475, ANN loss : 3.539661407470703, Total loss : 8.536872863769531\n",
      "learning rate A :  tf.Tensor(0.007964596, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007831457, shape=(), dtype=float32)\n",
      "Time for epoch 448 is 0.1697 sec\n",
      "train AE loss : 0.04763989523053169, train ANN loss : 3.4411582946777344\n",
      "AE loss : 0.08045786619186401, ANN loss : 3.4164223670959473, Total loss : 11.462209701538086\n",
      "learning rate A :  tf.Tensor(0.007964596, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00782321, shape=(), dtype=float32)\n",
      "Time for epoch 449 is 0.1712 sec\n",
      "train AE loss : 0.07764171063899994, train ANN loss : 3.348917245864868\n",
      "AE loss : 0.0518529899418354, ANN loss : 3.5008668899536133, Total loss : 8.686165809631348\n",
      "learning rate A :  tf.Tensor(0.00795621, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00782321, shape=(), dtype=float32)\n",
      "Time for epoch 450 is 0.1678 sec\n",
      "train AE loss : 0.049555838108062744, train ANN loss : 3.3966832160949707\n",
      "AE loss : 0.09382990747690201, ANN loss : 3.414200782775879, Total loss : 12.797192573547363\n",
      "learning rate A :  tf.Tensor(0.00795621, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007814972, shape=(), dtype=float32)\n",
      "Time for epoch 451 is 0.1737 sec\n",
      "train AE loss : 0.09069549292325974, train ANN loss : 3.402369737625122\n",
      "AE loss : 0.05319641903042793, ANN loss : 3.4871134757995605, Total loss : 8.806756019592285\n",
      "learning rate A :  tf.Tensor(0.007947831, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007814972, shape=(), dtype=float32)\n",
      "Time for epoch 452 is 0.1685 sec\n",
      "train AE loss : 0.05096181482076645, train ANN loss : 3.3800625801086426\n",
      "AE loss : 0.10251406580209732, ANN loss : 3.457595109939575, Total loss : 13.709000587463379\n",
      "learning rate A :  tf.Tensor(0.007947831, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0078067426, shape=(), dtype=float32)\n",
      "Time for epoch 453 is 0.1700 sec\n",
      "train AE loss : 0.09898393601179123, train ANN loss : 3.5380592346191406\n",
      "AE loss : 0.10367811471223831, ANN loss : 3.391615629196167, Total loss : 13.75942611694336\n",
      "learning rate A :  tf.Tensor(0.007947831, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0077985213, shape=(), dtype=float32)\n",
      "Time for epoch 454 is 0.1699 sec\n",
      "train AE loss : 0.10037456452846527, train ANN loss : 3.378481149673462\n",
      "AE loss : 0.08527352660894394, ANN loss : 3.4548680782318115, Total loss : 11.982220649719238\n",
      "learning rate A :  tf.Tensor(0.007947831, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.00779031, shape=(), dtype=float32)\n",
      "Time for epoch 455 is 0.1847 sec\n",
      "train AE loss : 0.08234389871358871, train ANN loss : 3.329261064529419\n",
      "AE loss : 0.08253706991672516, ANN loss : 3.5329554080963135, Total loss : 11.786663055419922\n",
      "learning rate A :  tf.Tensor(0.007947831, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007782106, shape=(), dtype=float32)\n",
      "Time for epoch 456 is 0.1772 sec\n",
      "train AE loss : 0.0794607400894165, train ANN loss : 3.3768978118896484\n",
      "AE loss : 0.09967783093452454, ANN loss : 3.4407615661621094, Total loss : 13.408544540405273\n",
      "learning rate A :  tf.Tensor(0.007947831, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0077739106, shape=(), dtype=float32)\n",
      "Time for epoch 457 is 0.1702 sec\n",
      "train AE loss : 0.09623073786497116, train ANN loss : 3.3359763622283936\n",
      "AE loss : 0.1336880922317505, ANN loss : 3.3525350093841553, Total loss : 16.721343994140625\n",
      "learning rate A :  tf.Tensor(0.007947831, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0077657243, shape=(), dtype=float32)\n",
      "Time for epoch 458 is 0.1729 sec\n",
      "train AE loss : 0.12999548017978668, train ANN loss : 3.3244621753692627\n",
      "AE loss : 0.06821145117282867, ANN loss : 3.4759325981140137, Total loss : 10.297077178955078\n",
      "learning rate A :  tf.Tensor(0.007939462, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0077657243, shape=(), dtype=float32)\n",
      "Time for epoch 459 is 0.1695 sec\n",
      "train AE loss : 0.06549713760614395, train ANN loss : 3.3806161880493164\n",
      "AE loss : 0.04805575683712959, ANN loss : 3.573667049407959, Total loss : 8.379242897033691\n",
      "learning rate A :  tf.Tensor(0.007931101, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0077657243, shape=(), dtype=float32)\n",
      "Time for epoch 460 is 0.1713 sec\n",
      "train AE loss : 0.045962344855070114, train ANN loss : 3.4655535221099854\n",
      "AE loss : 0.07679169625043869, ANN loss : 3.42106556892395, Total loss : 11.100234985351562\n",
      "learning rate A :  tf.Tensor(0.007931101, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007757547, shape=(), dtype=float32)\n",
      "Time for epoch 461 is 0.1717 sec\n",
      "train AE loss : 0.074289970099926, train ANN loss : 3.381051540374756\n",
      "AE loss : 0.12575040757656097, ANN loss : 3.3509609699249268, Total loss : 15.926002502441406\n",
      "learning rate A :  tf.Tensor(0.007931101, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0077493773, shape=(), dtype=float32)\n",
      "Time for epoch 462 is 0.1708 sec\n",
      "train AE loss : 0.12259526550769806, train ANN loss : 3.4058499336242676\n",
      "AE loss : 0.052599046379327774, ANN loss : 3.552018404006958, Total loss : 8.811923027038574\n",
      "learning rate A :  tf.Tensor(0.007922749, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0077493773, shape=(), dtype=float32)\n",
      "Time for epoch 463 is 0.1698 sec\n",
      "train AE loss : 0.050582289695739746, train ANN loss : 3.545879364013672\n",
      "AE loss : 0.07903768122196198, ANN loss : 3.404374837875366, Total loss : 11.308141708374023\n",
      "learning rate A :  tf.Tensor(0.007922749, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007741217, shape=(), dtype=float32)\n",
      "Time for epoch 464 is 0.1715 sec\n",
      "train AE loss : 0.07650616019964218, train ANN loss : 3.3206732273101807\n",
      "AE loss : 0.04742353782057762, ANN loss : 3.5376780033111572, Total loss : 8.28003215789795\n",
      "learning rate A :  tf.Tensor(0.007914406, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007741217, shape=(), dtype=float32)\n",
      "Time for epoch 465 is 0.1785 sec\n",
      "train AE loss : 0.04561689496040344, train ANN loss : 3.4801089763641357\n",
      "AE loss : 0.07254292070865631, ANN loss : 3.6494991779327393, Total loss : 10.903790473937988\n",
      "learning rate A :  tf.Tensor(0.007914406, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0077330647, shape=(), dtype=float32)\n",
      "Time for epoch 466 is 0.1714 sec\n",
      "train AE loss : 0.07013964653015137, train ANN loss : 3.455167770385742\n",
      "AE loss : 0.043364960700273514, ANN loss : 3.7455942630767822, Total loss : 8.082090377807617\n",
      "learning rate A :  tf.Tensor(0.007906071, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0077330647, shape=(), dtype=float32)\n",
      "Time for epoch 467 is 0.1707 sec\n",
      "train AE loss : 0.041744258254766464, train ANN loss : 3.543842315673828\n",
      "AE loss : 0.06515589356422424, ANN loss : 3.4815151691436768, Total loss : 9.99710464477539\n",
      "learning rate A :  tf.Tensor(0.007906071, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007724922, shape=(), dtype=float32)\n",
      "Time for epoch 468 is 0.1829 sec\n",
      "train AE loss : 0.06263884902000427, train ANN loss : 3.372295618057251\n",
      "AE loss : 0.1099301353096962, ANN loss : 3.3368887901306152, Total loss : 14.329901695251465\n",
      "learning rate A :  tf.Tensor(0.007906071, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0077167875, shape=(), dtype=float32)\n",
      "Time for epoch 469 is 0.1714 sec\n",
      "train AE loss : 0.10625391453504562, train ANN loss : 3.469756841659546\n",
      "AE loss : 0.118765689432621, ANN loss : 3.3246564865112305, Total loss : 15.201225280761719\n",
      "learning rate A :  tf.Tensor(0.007906071, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007708661, shape=(), dtype=float32)\n",
      "Time for epoch 470 is 0.1821 sec\n",
      "train AE loss : 0.1149422898888588, train ANN loss : 3.301429033279419\n",
      "AE loss : 0.12031219154596329, ANN loss : 3.423290729522705, Total loss : 15.454509735107422\n",
      "learning rate A :  tf.Tensor(0.007906071, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0077005434, shape=(), dtype=float32)\n",
      "Time for epoch 471 is 0.1730 sec\n",
      "train AE loss : 0.1157352551817894, train ANN loss : 3.3625638484954834\n",
      "AE loss : 0.14106258749961853, ANN loss : 3.3587536811828613, Total loss : 17.46501350402832\n",
      "learning rate A :  tf.Tensor(0.007906071, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007692435, shape=(), dtype=float32)\n",
      "Time for epoch 472 is 0.1746 sec\n",
      "train AE loss : 0.1354503035545349, train ANN loss : 3.382728099822998\n",
      "AE loss : 0.06511922180652618, ANN loss : 3.6210029125213623, Total loss : 10.132925033569336\n",
      "learning rate A :  tf.Tensor(0.007897747, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007692435, shape=(), dtype=float32)\n",
      "Time for epoch 473 is 0.1690 sec\n",
      "train AE loss : 0.06200248748064041, train ANN loss : 3.548046350479126\n",
      "AE loss : 0.04730137437582016, ANN loss : 3.6189517974853516, Total loss : 8.349089622497559\n",
      "learning rate A :  tf.Tensor(0.007889429, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007692435, shape=(), dtype=float32)\n",
      "Time for epoch 474 is 0.1708 sec\n",
      "train AE loss : 0.0450439453125, train ANN loss : 3.5568864345550537\n",
      "AE loss : 0.04072927311062813, ANN loss : 3.6235437393188477, Total loss : 7.696471214294434\n",
      "learning rate A :  tf.Tensor(0.007881122, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007692435, shape=(), dtype=float32)\n",
      "Time for epoch 475 is 0.1787 sec\n",
      "train AE loss : 0.03880108520388603, train ANN loss : 3.5687143802642822\n",
      "AE loss : 0.03685632348060608, ANN loss : 3.640634298324585, Total loss : 7.326266288757324\n",
      "learning rate A :  tf.Tensor(0.007872822, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007692435, shape=(), dtype=float32)\n",
      "Time for epoch 476 is 0.1800 sec\n",
      "train AE loss : 0.035172607749700546, train ANN loss : 3.576361894607544\n",
      "AE loss : 0.04304105043411255, ANN loss : 3.5520241260528564, Total loss : 7.856129169464111\n",
      "learning rate A :  tf.Tensor(0.007872822, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007684334, shape=(), dtype=float32)\n",
      "Time for epoch 477 is 0.1724 sec\n",
      "train AE loss : 0.04122665524482727, train ANN loss : 3.594226598739624\n",
      "AE loss : 0.0390515960752964, ANN loss : 3.558654308319092, Total loss : 7.463813781738281\n",
      "learning rate A :  tf.Tensor(0.007864532, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007684334, shape=(), dtype=float32)\n",
      "Time for epoch 478 is 0.1709 sec\n",
      "train AE loss : 0.037451859563589096, train ANN loss : 3.5953385829925537\n",
      "AE loss : 0.03616494685411453, ANN loss : 3.5687122344970703, Total loss : 7.185206890106201\n",
      "learning rate A :  tf.Tensor(0.00785625, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007684334, shape=(), dtype=float32)\n",
      "Time for epoch 479 is 0.1696 sec\n",
      "train AE loss : 0.03473798558115959, train ANN loss : 3.601651906967163\n",
      "AE loss : 0.033826299011707306, ANN loss : 3.5786216259002686, Total loss : 6.961251735687256\n",
      "learning rate A :  tf.Tensor(0.007847977, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007684334, shape=(), dtype=float32)\n",
      "Time for epoch 480 is 0.1687 sec\n",
      "train AE loss : 0.032536596059799194, train ANN loss : 3.611018657684326\n",
      "AE loss : 0.039785128086805344, ANN loss : 3.628009557723999, Total loss : 7.606522560119629\n",
      "learning rate A :  tf.Tensor(0.007847977, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076762424, shape=(), dtype=float32)\n",
      "Time for epoch 481 is 0.1701 sec\n",
      "train AE loss : 0.038329944014549255, train ANN loss : 3.673495054244995\n",
      "AE loss : 0.03597162291407585, ANN loss : 3.6045262813568115, Total loss : 7.201688766479492\n",
      "learning rate A :  tf.Tensor(0.007839713, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076762424, shape=(), dtype=float32)\n",
      "Time for epoch 482 is 0.1680 sec\n",
      "train AE loss : 0.034689199179410934, train ANN loss : 3.6362202167510986\n",
      "AE loss : 0.03340470790863037, ANN loss : 3.608097553253174, Total loss : 6.948568820953369\n",
      "learning rate A :  tf.Tensor(0.007831457, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076762424, shape=(), dtype=float32)\n",
      "Time for epoch 483 is 0.1784 sec\n",
      "train AE loss : 0.03226132318377495, train ANN loss : 3.6371378898620605\n",
      "AE loss : 0.03135653957724571, ANN loss : 3.6182661056518555, Total loss : 6.753920078277588\n",
      "learning rate A :  tf.Tensor(0.00782321, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076762424, shape=(), dtype=float32)\n",
      "Time for epoch 484 is 0.1726 sec\n",
      "train AE loss : 0.030333515256643295, train ANN loss : 3.646690607070923\n",
      "AE loss : 0.02958579733967781, ANN loss : 3.6350696086883545, Total loss : 6.593649387359619\n",
      "learning rate A :  tf.Tensor(0.007814972, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076762424, shape=(), dtype=float32)\n",
      "Time for epoch 485 is 0.1679 sec\n",
      "train AE loss : 0.02867729216814041, train ANN loss : 3.659843683242798\n",
      "AE loss : 0.03414459899067879, ANN loss : 3.5644571781158447, Total loss : 6.978917121887207\n",
      "learning rate A :  tf.Tensor(0.007814972, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.007668159, shape=(), dtype=float32)\n",
      "Time for epoch 486 is 0.1719 sec\n",
      "train AE loss : 0.0328768752515316, train ANN loss : 3.4922356605529785\n",
      "AE loss : 0.04080832004547119, ANN loss : 3.561954975128174, Total loss : 7.642786979675293\n",
      "learning rate A :  tf.Tensor(0.007814972, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076600835, shape=(), dtype=float32)\n",
      "Time for epoch 487 is 0.1728 sec\n",
      "train AE loss : 0.03915296867489815, train ANN loss : 3.4421491622924805\n",
      "AE loss : 0.03748929128050804, ANN loss : 3.5821845531463623, Total loss : 7.331113815307617\n",
      "learning rate A :  tf.Tensor(0.0078067426, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076600835, shape=(), dtype=float32)\n",
      "Time for epoch 488 is 0.1714 sec\n",
      "train AE loss : 0.03604493290185928, train ANN loss : 3.4591052532196045\n",
      "AE loss : 0.05588831007480621, ANN loss : 3.5333476066589355, Total loss : 9.122178077697754\n",
      "learning rate A :  tf.Tensor(0.0078067426, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076520173, shape=(), dtype=float32)\n",
      "Time for epoch 489 is 0.1748 sec\n",
      "train AE loss : 0.053716469556093216, train ANN loss : 3.574270725250244\n",
      "AE loss : 0.05334096774458885, ANN loss : 4.081434726715088, Total loss : 9.415531158447266\n",
      "learning rate A :  tf.Tensor(0.0078067426, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076439595, shape=(), dtype=float32)\n",
      "Time for epoch 490 is 0.1759 sec\n",
      "train AE loss : 0.05080036073923111, train ANN loss : 3.8650028705596924\n",
      "AE loss : 0.04587215557694435, ANN loss : 3.952474355697632, Total loss : 8.539689064025879\n",
      "learning rate A :  tf.Tensor(0.0077985213, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076439595, shape=(), dtype=float32)\n",
      "Time for epoch 491 is 0.1702 sec\n",
      "train AE loss : 0.04380330815911293, train ANN loss : 3.743572235107422\n",
      "AE loss : 0.0416438952088356, ANN loss : 3.9343183040618896, Total loss : 8.098708152770996\n",
      "learning rate A :  tf.Tensor(0.00779031, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076439595, shape=(), dtype=float32)\n",
      "Time for epoch 492 is 0.1710 sec\n",
      "train AE loss : 0.039857327938079834, train ANN loss : 3.7241106033325195\n",
      "AE loss : 0.03818552941083908, ANN loss : 3.9288864135742188, Total loss : 7.747438907623291\n",
      "learning rate A :  tf.Tensor(0.007782106, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076439595, shape=(), dtype=float32)\n",
      "Time for epoch 493 is 0.1662 sec\n",
      "train AE loss : 0.036652516573667526, train ANN loss : 3.712423086166382\n",
      "AE loss : 0.03536462038755417, ANN loss : 3.943378448486328, Total loss : 7.4798407554626465\n",
      "learning rate A :  tf.Tensor(0.0077739106, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076439595, shape=(), dtype=float32)\n",
      "Time for epoch 494 is 0.1810 sec\n",
      "train AE loss : 0.03405352309346199, train ANN loss : 3.7286932468414307\n",
      "AE loss : 0.033132683485746384, ANN loss : 3.9618747234344482, Total loss : 7.275143146514893\n",
      "learning rate A :  tf.Tensor(0.0077657243, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076439595, shape=(), dtype=float32)\n",
      "Time for epoch 495 is 0.1718 sec\n",
      "train AE loss : 0.03200559690594673, train ANN loss : 3.7504308223724365\n",
      "AE loss : 0.03140907362103462, ANN loss : 4.000181198120117, Total loss : 7.141088008880615\n",
      "learning rate A :  tf.Tensor(0.007757547, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076439595, shape=(), dtype=float32)\n",
      "Time for epoch 496 is 0.1692 sec\n",
      "train AE loss : 0.030425654724240303, train ANN loss : 3.7850208282470703\n",
      "AE loss : 0.04272674769163132, ANN loss : 3.4961533546447754, Total loss : 7.768827438354492\n",
      "learning rate A :  tf.Tensor(0.007757547, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076359096, shape=(), dtype=float32)\n",
      "Time for epoch 497 is 0.1702 sec\n",
      "train AE loss : 0.041213925927877426, train ANN loss : 3.4305291175842285\n",
      "AE loss : 0.03678947314620018, ANN loss : 3.52990984916687, Total loss : 7.208857536315918\n",
      "learning rate A :  tf.Tensor(0.0077493773, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076359096, shape=(), dtype=float32)\n",
      "Time for epoch 498 is 0.1692 sec\n",
      "train AE loss : 0.03550264239311218, train ANN loss : 3.4265055656433105\n",
      "AE loss : 0.05696383863687515, ANN loss : 3.517472982406616, Total loss : 9.21385669708252\n",
      "learning rate A :  tf.Tensor(0.0077493773, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076278686, shape=(), dtype=float32)\n",
      "Time for epoch 499 is 0.1724 sec\n",
      "train AE loss : 0.05486040562391281, train ANN loss : 3.6607306003570557\n",
      "AE loss : 0.05544140562415123, ANN loss : 3.502763509750366, Total loss : 9.046903610229492\n",
      "learning rate A :  tf.Tensor(0.0077493773, shape=(), dtype=float32)\n",
      "learning rate B :  tf.Tensor(0.0076198364, shape=(), dtype=float32)\n",
      "Time for epoch 500 is 0.1734 sec\n"
     ]
    }
   ],
   "source": [
    "train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model_encoder_decoder.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-15, momentum=1e-17)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_encoder_decoder.compile(optimizer = optimizer, loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_encoder_decoder.fit(train_dataset,\n",
    "                epochs=500,\n",
    "                validation_data=valid_dataset,\n",
    "                callbacks = callbacks\n",
    "                         )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_latent = model_down(val_x)\n",
    "from sklearn import manifold, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "train_val_split = np.random.rand(len(val_latent)) < 0.4\n",
    "X_tsne = manifold.TSNE(n_components=2, init='pca', n_iter=5000, method='exact').fit_transform(val_latent[train_val_split])\n",
    "y = val_y[train_val_split].argmax(axis=1).reshape([-1,1])\n",
    "x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
    "X_norm = (X_tsne - x_min) / (x_max - x_min)\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(X_norm.shape[0]):\n",
    "    plt.text(X_norm[i, 0], X_norm[i, 1], str(y[i,0]), color=plt.cm.Set1(y[i,0]), \n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig(\"result/gps_pca_latent16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "valid_dataset,val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_latent = model_down(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=627077, shape=(6443, 16), dtype=float32, numpy=\n",
       "array([[0.00282131, 0.04940335, 0.        , ..., 0.        , 0.        ,\n",
       "        0.04733327],\n",
       "       [0.01026605, 0.06043176, 0.        , ..., 0.        , 0.        ,\n",
       "        0.0680249 ],\n",
       "       [0.00181532, 0.049019  , 0.        , ..., 0.        , 0.        ,\n",
       "        0.05388596],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold, datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = np.random.rand(len(val_latent)) < 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = manifold.TSNE(n_components=2, init='pca', n_iter=5000, method='exact').fit_transform(val_latent[train_val_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = val_y[train_val_split].argmax(axis=1).reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2554, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6443, 16])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-62.12664 , -50.942444], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_tsne.min(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50.860874, 49.19238 ], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_tsne.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
    "X_norm = (X_tsne - x_min) / (x_max - x_min)\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(X_norm.shape[0]):\n",
    "    plt.text(X_norm[i, 0], X_norm[i, 1], str(y[i,0]), color=plt.cm.Set1(y[i,0]), \n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig(\"result/gps_pca_latent16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DNN = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.Dense(256, activation='relu'),\n",
    "      layers.Dense(labels.shape[1], activation='softmax'),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_encoded = np.array(autoencoder.encoder(train_x))\n",
    "val_encoded = np.array(autoencoder.encoder(val_x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DNN.compile(optimizer='adam', loss=losses.CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DNN.fit(train_encoded, train_y,\n",
    "                epochs=1000,\n",
    "                shuffle=True,\n",
    "                validation_data=(val_encoded, val_y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_encoder = np.array(autoencoder.encoder(test_features))\n",
    "y_ = DNN(test_encoder)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.argmax(np.array(y_[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gps_wifi",
   "language": "python",
   "name": "gps_wifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
