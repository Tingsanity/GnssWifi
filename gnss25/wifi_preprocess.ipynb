{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#File = 1\n",
    "#prFileName = 'dataRssi_at_%d.txt'%File\n",
    "#dirName = '/home/lyt/gnss_wifi/gnss/20201022/indoor/wifi/'\n",
    "#outputname = 'outplot/1out_in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = glob.glob('20201203/wifi/dataRssi_at_*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'45'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[0].split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "origin_data = []\n",
    "train_label = []\n",
    "for data in datas:\n",
    "    load_data = np.loadtxt(data)#f.read()\n",
    "    load_data = load_data[:200,:15].astype(int)\n",
    "    load_data[load_data==0] = -100\n",
    "    load_data = load_data/100 + 1\n",
    "    #print(load_data.shape)\n",
    "    for i in range(len(load_data)):\n",
    "        origin_data.append(load_data[i])\n",
    "        mask = np.random.rand((15))\n",
    "        mask = (mask>0.1).astype(int)\n",
    "        noise = np.random.normal(scale = 0.05,size=(15))\n",
    "        load_data[i]*mask\n",
    "        load_data[i]+noise\n",
    "        train_data.append(load_data[i])\n",
    "        train_label.append(data.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0])\n",
    "    #print(load_data.shape)    \n",
    "    \n",
    "    #print(np.array(train_data).shape)\n",
    "origin_data = np.array(origin_data).astype('float32') \n",
    "train_data = np.array(train_data).astype('float32')\n",
    "train_label = np.array(pd.get_dummies(train_label)).astype('float32')\n",
    "#train_label = train_label.reshape(len(train_label),1)\n",
    "train_val_split = np.random.rand(len(train_data)) < 0.70\n",
    "train_x = train_data[train_val_split]\n",
    "train_o = origin_data[train_val_split]\n",
    "train_y = train_label[train_val_split]\n",
    "val_x = train_data[~train_val_split]\n",
    "val_o = origin_data[~train_val_split]\n",
    "val_y = train_label[~train_val_split]\n",
    "BUFFER_SIZE = train_x.shape[0]\n",
    "BATCH_SIZE = train_x.shape[0]\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x,train_o,train_y))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_x,val_o)).batch(len(val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_label).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.array(pd.get_dummies(train_label)).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for a,b,c in train_dataset:\n",
    "    print(a.dtype)\n",
    "    print(b.dtype)\n",
    "    print(c.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        self.dense = dense\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name='bias',shape=[self.dense.input_shape[-1]],initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b = True)\n",
    "        return self.activation(z + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\"])\n",
    "#with mirrored_strategy.scope():\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(15), name='input_layer1')\n",
    "#flatten = tf.keras.layers.Flatten()(input)\n",
    "dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "dense2 = tf.keras.layers.Dense(24, activation='relu')\n",
    "dense3 = tf.keras.layers.Dense(16, activation='relu')\n",
    "model_encoder = dense1(input)\n",
    "model_encoder = dense2(model_encoder)\n",
    "model_encoder = dense3(model_encoder)\n",
    "model_down = tf.keras.Model(inputs=[input], outputs=model_encoder,name = \"encoder\")#input1, input2,input3,input4,input5,input6,input7,input8,input9,input10\n",
    "#model_down.summary()\n",
    "#input_encoder = tf.keras.layers.Input(shape=(15), name='input_layer2')\n",
    "input2 = tf.keras.layers.Input(shape=(16), name='input_layer2')\n",
    "model_decoder = DenseTranspose(dense3, activation = 'relu')(input2)\n",
    "model_decoder = DenseTranspose(dense2, activation = 'relu')(model_decoder)\n",
    "model_decoder = DenseTranspose(dense1, activation = 'relu')(model_decoder)\n",
    "model_up = tf.keras.Model(inputs=[input2], outputs=model_decoder,name = \"decoder\")\n",
    "\n",
    "#model_encoder_decoder.summary()\n",
    "input3 = tf.keras.layers.Input(shape=(16), name='input_layer3')\n",
    "model_ann = tf.keras.layers.Dense(16, activation='relu')(input3)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "#model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "#model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(50, activation='softmax')(model_ann)\n",
    "model_ANN = tf.keras.Model(inputs=[input3], outputs=model_ann,name = \"ann\")\n",
    "\n",
    "input_full = tf.keras.layers.Input(shape=(15), name='input_layer4')\n",
    "encoder_out = model_down(input_full)\n",
    "decoder_out = model_up(encoder_out)\n",
    "ann_out = model_ANN(encoder_out)\n",
    "#model_encoder_decoder_ann = tf.keras.Model(inputs=[input_full],outputs=[decoder_out],name = 'encoder_decoder_ann')\n",
    "model_encoder_decoder_ann = tf.keras.Model(inputs=[input_full],outputs=[decoder_out,ann_out],name = 'encoder_decoder_ann')\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=1e-5)\n",
    "#model_encoder_decoder_ann.compile(optimizer = 'sgd', \n",
    "#                                  loss={'AE_out1': 'mean_squared_error','place_out2': 'categorical_crossentropy'}),\n",
    "#                                  loss_weights={'AE_out1': 0.5,'place_out2': 0.5},\n",
    "#                                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\"])\n",
    "#with mirrored_strategy.scope():\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(15), name='input_layer1')\n",
    "#flatten = tf.keras.layers.Flatten()(input)\n",
    "dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "dense2 = tf.keras.layers.Dense(24, activation='relu')\n",
    "dense3 = tf.keras.layers.Dense(16, activation='relu')\n",
    "model_encoder = dense1(input)\n",
    "model_encoder = dense2(model_encoder)\n",
    "model_encoder = dense3(model_encoder)\n",
    "model_down = tf.keras.Model(inputs=[input], outputs=model_encoder,name = \"encoder\")#input1, input2,input3,input4,input5,input6,input7,input8,input9,input10\n",
    "#model_down.summary()\n",
    "input_encoder = tf.keras.layers.Input(shape=(15), name='input_layer2')\n",
    "input_decoder = model_down(input_encoder)\n",
    "model_decoder = DenseTranspose(dense3, activation = 'relu')(input_decoder)\n",
    "model_decoder = DenseTranspose(dense2, activation = 'relu')(model_decoder)\n",
    "model_decoder = DenseTranspose(dense1, activation = 'relu')(model_decoder)\n",
    "\n",
    "#model_encoder_decoder.summary()\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(input_decoder)\n",
    "model_ann = tf.keras.layers.Dense(9, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(9, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(9, activation='softmax')(model_ann)\n",
    "model_encoder_decoder_ann = tf.keras.Model(inputs=[input_encoder],outputs=[model_decoder,model_ann],name = 'encoder_decoder')\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=1e-5)\n",
    "#model_encoder_decoder_ann.compile(optimizer = 'sgd', \n",
    "#                                  loss={'AE_out1': 'mean_squared_error','place_out2': 'categorical_crossentropy'}),\n",
    "#                                  loss_weights={'AE_out1': 0.5,'place_out2': 0.5},\n",
    "#                                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_decoder_ann\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer4 (InputLayer)       [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 (None, 16)           1704        input_layer4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 15)           1775        encoder[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ann (Model)                     (None, 50)           1666        encoder[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,441\n",
      "Trainable params: 3,441\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_encoder_decoder_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_encoder_decoder_ann.trainable_variables)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(15):\n",
    "    print(model_encoder_decoder_ann.trainable_variables[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vars = model_encoder_decoder_ann.trainable_variables[:6]\n",
    "decode_vars = model_encoder_decoder_ann.trainable_variables[6:9]\n",
    "ann_vars = model_encoder_decoder_ann.trainable_variables[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(output,t_o,t_y):\n",
    "    #print(\"in loss\")\n",
    "    output_AE , output_label = output\n",
    "    #output_AE = output\n",
    "    mse = losses.MeanSquaredError()\n",
    "    AE_loss = mse(t_o,output_AE)\n",
    "    ann_loss =losses.categorical_crossentropy(t_y,output_label)\n",
    "    total_loss = AE_loss*100 + ann_loss\n",
    "    \n",
    "    return AE_loss,ann_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_A = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "initial_learning_rate=1e-4, decay_steps=5000, decay_rate=0.9)\n",
    "optimizer_A = tf.optimizers.SGD(learning_rate=learning_rate_A , momentum=1e-5)\n",
    "learning_rate_B = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.8)\n",
    "optimizer_B = tf.optimizers.Adam(learning_rate=1e-3)#learning_rate_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "\n",
    "def train_step(t_x,t_o,t_y):\n",
    "  \n",
    "    with tf.GradientTape() as AE_tape,tf.GradientTape() as ANN_tape:\n",
    "        output = model_encoder_decoder_ann(t_x, training=True)\n",
    "        #AE_loss = model_loss(output,t_o,t_y)\n",
    "        AE_loss,ANN_loss = model_loss(output,t_o,t_y)\n",
    "        #gradients = tape.gradient(total_loss, shared_vars+decode_vars)\n",
    "        #optimizer_A.apply_gradients(zip(gradients, shared_vars+decode_vars))\n",
    "\n",
    "    if np.random.rand() < 0.5:\n",
    "        #print(\"AE\")\n",
    "        gradients_AE = AE_tape.gradient(AE_loss, shared_vars+decode_vars)\n",
    "        #gradients_AE = [tf.clip_by_value(g, -1,1) for g in gradients_AE]\n",
    "\n",
    "        #print(\"AE gradient : \",gradients_AE)\n",
    "        optimizer_A.apply_gradients(zip(gradients_AE, shared_vars+decode_vars))\n",
    "    \n",
    "    else:\n",
    "        #print(\"ANN\")\n",
    "        gradients_ANN = ANN_tape.gradient(ANN_loss, shared_vars+ann_vars)\n",
    "        #print(\"ANN gradient : \",gradients_ANN)\n",
    "        #gradients_ANN = [tf.clip_by_value(g, -1,1) for g in gradients_ANN] \n",
    "        optimizer_B.apply_gradients(zip(gradients_ANN, shared_vars+ann_vars))\n",
    "    \n",
    "    return np.array(AE_loss).mean(),np.array(ANN_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './wifi_checkpoints/checkpoints_1222'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_dropout_model_{epoch}\")\n",
    "checkpoint = tf.train.Checkpoint(optimizerA=optimizer_A,\n",
    "                                 optimizerB=optimizer_B,\n",
    "                                 model_encoder_decoder_ann=model_encoder_decoder_ann,\n",
    "                                 model_down=model_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model_encoder_decoder.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(v_x,v_o,v_y):\n",
    "    output = model_encoder_decoder_ann(v_x)\n",
    "    #output_AE = output\n",
    "    output_AE , output_label = output\n",
    "    mse = losses.MeanSquaredError()\n",
    "    AE_loss = mse(v_o,output_AE)\n",
    "    ann_loss = losses.categorical_crossentropy(v_y,output_label)\n",
    "    total_loss = AE_loss*100 + ann_loss\n",
    "    #print(output_label[200])\n",
    "    #print(v_y[200])\n",
    "    #print(ann_loss)\n",
    "    #print(\"AE loss : {},\".format(np.array(AE_loss).mean()))\n",
    "    print(\"AE loss : {}, ANN loss : {}, Total loss : {}\".format(np.array(AE_loss).mean(),np.array(ann_loss).mean(),np.array(total_loss).mean()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "validation(val_x,val_o,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3039, 15)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    all_AE = []\n",
    "    all_ANN =[]\n",
    "    for x,o,y in train_dataset:\n",
    "        #AE_loss = train_step(x,o,y)\n",
    "        AE_loss,ANN_loss = train_step(x,o,y)\n",
    "        all_AE.append(AE_loss)\n",
    "        all_ANN.append(ANN_loss)\n",
    "    #print(\"train AE loss : {}\".format(np.array(all_AE).mean()))\n",
    "    print(\"train AE loss : {}, train ANN loss : {}\".format(np.array(all_AE).mean(),np.array(all_ANN).mean()))\n",
    "    validation(val_x,val_o,val_y)\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"learning rate A : \",optimizer_A._decayed_lr(tf.float32))\n",
    "    print(\"learning rate B : \",optimizer_B._decayed_lr(tf.float32))\n",
    "    print(f'Time for epoch {epoch + 1} is {time.time() - start:.4f} sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=475, shape=(), dtype=float32, numpy=2.908749e-05>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0., 0., 1., 0., 0., 0., 0., 0., 0.]#[0.11685812, 0.11185785, 0.11026918, 0.10764945, 0.10728354, 0.11020868,\n",
    " #0.10912149, 0.11363789, 0.11311384]#[0, 1, 0,0,0,0,0,0,0]\n",
    "b = [2.4371104e-08, 4.2344610e-11, 9.9997091e-01, 4.9084689e-13, 6.4827432e-06,\n",
    " 2.1712562e-08, 3.4163491e-11, 3.0665456e-11, 2.2567538e-05]\n",
    "tf.keras.losses.categorical_crossentropy(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train AE loss : 0.07074526697397232, train ANN loss : 3.9126532077789307\n",
      "AE loss : 0.07167715579271317, ANN loss : 3.9124388694763184, Total loss : 11.080153465270996\n",
      "learning rate A :  tf.Tensor(1e-04, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1 is 0.4852 sec\n",
      "train AE loss : 0.07177841663360596, train ANN loss : 3.9125783443450928\n",
      "AE loss : 0.07167460024356842, ANN loss : 3.9124388694763184, Total loss : 11.079898834228516\n",
      "learning rate A :  tf.Tensor(9.999789e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 2 is 0.1063 sec\n",
      "train AE loss : 0.07177584618330002, train ANN loss : 3.912745714187622\n",
      "AE loss : 0.07263150066137314, ANN loss : 3.912421703338623, Total loss : 11.17557144165039\n",
      "learning rate A :  tf.Tensor(9.999789e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 3 is 0.0901 sec\n",
      "train AE loss : 0.07272699475288391, train ANN loss : 3.9124062061309814\n",
      "AE loss : 0.07353558391332626, ANN loss : 3.91241717338562, Total loss : 11.265976905822754\n",
      "learning rate A :  tf.Tensor(9.999789e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 4 is 0.0869 sec\n",
      "train AE loss : 0.07362814247608185, train ANN loss : 3.912180185317993\n",
      "AE loss : 0.07353304326534271, ANN loss : 3.91241717338562, Total loss : 11.265722274780273\n",
      "learning rate A :  tf.Tensor(9.999578e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 5 is 0.0855 sec\n",
      "train AE loss : 0.0736256018280983, train ANN loss : 3.912022590637207\n",
      "AE loss : 0.07353051006793976, ANN loss : 3.9124176502227783, Total loss : 11.26546859741211\n",
      "learning rate A :  tf.Tensor(9.9993675e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 6 is 0.0842 sec\n",
      "train AE loss : 0.07362306118011475, train ANN loss : 3.9123222827911377\n",
      "AE loss : 0.07423001527786255, ANN loss : 3.9124228954315186, Total loss : 11.335423469543457\n",
      "learning rate A :  tf.Tensor(9.9993675e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 7 is 0.0858 sec\n",
      "train AE loss : 0.07431928813457489, train ANN loss : 3.911979913711548\n",
      "AE loss : 0.07422750443220139, ANN loss : 3.9124228954315186, Total loss : 11.335172653198242\n",
      "learning rate A :  tf.Tensor(9.9991565e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 8 is 0.0882 sec\n",
      "train AE loss : 0.07431676983833313, train ANN loss : 3.911736011505127\n",
      "AE loss : 0.07422497868537903, ANN loss : 3.9124228954315186, Total loss : 11.334920883178711\n",
      "learning rate A :  tf.Tensor(9.998946e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 9 is 0.0857 sec\n",
      "train AE loss : 0.07431424409151077, train ANN loss : 3.9119415283203125\n",
      "AE loss : 0.07422245293855667, ANN loss : 3.9124228954315186, Total loss : 11.33466911315918\n",
      "learning rate A :  tf.Tensor(9.998735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 10 is 0.0859 sec\n",
      "train AE loss : 0.07431171089410782, train ANN loss : 3.9121336936950684\n",
      "AE loss : 0.07468566298484802, ANN loss : 3.9124467372894287, Total loss : 11.381012916564941\n",
      "learning rate A :  tf.Tensor(9.998735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 11 is 0.0873 sec\n",
      "train AE loss : 0.07477302104234695, train ANN loss : 3.9119479656219482\n",
      "AE loss : 0.07510776072740555, ANN loss : 3.912475824356079, Total loss : 11.423253059387207\n",
      "learning rate A :  tf.Tensor(9.998735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 12 is 0.0870 sec\n",
      "train AE loss : 0.07519692927598953, train ANN loss : 3.9118638038635254\n",
      "AE loss : 0.07550312578678131, ANN loss : 3.9125046730041504, Total loss : 11.462817192077637\n",
      "learning rate A :  tf.Tensor(9.998735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 13 is 0.0881 sec\n",
      "train AE loss : 0.0755932480096817, train ANN loss : 3.911357879638672\n",
      "AE loss : 0.07550065219402313, ANN loss : 3.9125046730041504, Total loss : 11.462570190429688\n",
      "learning rate A :  tf.Tensor(9.998524e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 14 is 0.0845 sec\n",
      "train AE loss : 0.07559077441692352, train ANN loss : 3.9115443229675293\n",
      "AE loss : 0.07585925608873367, ANN loss : 3.9125399589538574, Total loss : 11.498465538024902\n",
      "learning rate A :  tf.Tensor(9.998524e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 15 is 0.0882 sec\n",
      "train AE loss : 0.07594906538724899, train ANN loss : 3.9115076065063477\n",
      "AE loss : 0.0761909931898117, ANN loss : 3.912588119506836, Total loss : 11.531686782836914\n",
      "learning rate A :  tf.Tensor(9.998524e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 16 is 0.0885 sec\n",
      "train AE loss : 0.07628247141838074, train ANN loss : 3.911094903945923\n",
      "AE loss : 0.07642620801925659, ANN loss : 3.9126288890838623, Total loss : 11.55525016784668\n",
      "learning rate A :  tf.Tensor(9.998524e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 17 is 0.0890 sec\n",
      "train AE loss : 0.07651984691619873, train ANN loss : 3.9112532138824463\n",
      "AE loss : 0.0764237716794014, ANN loss : 3.9126288890838623, Total loss : 11.55500602722168\n",
      "learning rate A :  tf.Tensor(9.998313e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 18 is 0.0865 sec\n",
      "train AE loss : 0.07651740312576294, train ANN loss : 3.9111831188201904\n",
      "AE loss : 0.07660764455795288, ANN loss : 3.912656545639038, Total loss : 11.573421478271484\n",
      "learning rate A :  tf.Tensor(9.998313e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 19 is 0.0882 sec\n",
      "train AE loss : 0.07670353353023529, train ANN loss : 3.9110069274902344\n",
      "AE loss : 0.07660523056983948, ANN loss : 3.912656545639038, Total loss : 11.573179244995117\n",
      "learning rate A :  tf.Tensor(9.998103e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 20 is 0.0881 sec\n",
      "train AE loss : 0.0767011046409607, train ANN loss : 3.911083936691284\n",
      "AE loss : 0.07660280913114548, ANN loss : 3.912656545639038, Total loss : 11.57293701171875\n",
      "learning rate A :  tf.Tensor(9.9978926e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 21 is 0.0956 sec\n",
      "train AE loss : 0.0766986757516861, train ANN loss : 3.911116123199463\n",
      "AE loss : 0.07660037279129028, ANN loss : 3.912656545639038, Total loss : 11.572693824768066\n",
      "learning rate A :  tf.Tensor(9.9976816e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 22 is 0.0884 sec\n",
      "train AE loss : 0.0766962468624115, train ANN loss : 3.911245584487915\n",
      "AE loss : 0.07679559290409088, ANN loss : 3.912670373916626, Total loss : 11.592230796813965\n",
      "learning rate A :  tf.Tensor(9.9976816e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 23 is 0.0867 sec\n",
      "train AE loss : 0.07689373195171356, train ANN loss : 3.9108939170837402\n",
      "AE loss : 0.07695113122463226, ANN loss : 3.9126763343811035, Total loss : 11.607789039611816\n",
      "learning rate A :  tf.Tensor(9.9976816e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 24 is 0.0860 sec\n",
      "train AE loss : 0.07705169171094894, train ANN loss : 3.9108974933624268\n",
      "AE loss : 0.07694875448942184, ANN loss : 3.9126763343811035, Total loss : 11.607552528381348\n",
      "learning rate A :  tf.Tensor(9.997471e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 25 is 0.0855 sec\n",
      "train AE loss : 0.07704931497573853, train ANN loss : 3.9109675884246826\n",
      "AE loss : 0.07694637030363083, ANN loss : 3.9126763343811035, Total loss : 11.60731315612793\n",
      "learning rate A :  tf.Tensor(9.99726e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 26 is 0.0848 sec\n",
      "train AE loss : 0.07704693078994751, train ANN loss : 3.9110255241394043\n",
      "AE loss : 0.07694400101900101, ANN loss : 3.9126763343811035, Total loss : 11.607076644897461\n",
      "learning rate A :  tf.Tensor(9.99705e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 27 is 0.0847 sec\n",
      "train AE loss : 0.07704455405473709, train ANN loss : 3.9106955528259277\n",
      "AE loss : 0.07702885568141937, ANN loss : 3.9126851558685303, Total loss : 11.615570068359375\n",
      "learning rate A :  tf.Tensor(9.99705e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 28 is 0.0919 sec\n",
      "train AE loss : 0.07713118195533752, train ANN loss : 3.9106035232543945\n",
      "AE loss : 0.07706064730882645, ANN loss : 3.9126858711242676, Total loss : 11.61875057220459\n",
      "learning rate A :  tf.Tensor(9.99705e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 29 is 0.0916 sec\n",
      "train AE loss : 0.07716140896081924, train ANN loss : 3.910597801208496\n",
      "AE loss : 0.07705828547477722, ANN loss : 3.9126858711242676, Total loss : 11.618515014648438\n",
      "learning rate A :  tf.Tensor(9.996839e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 30 is 0.0918 sec\n",
      "train AE loss : 0.07715904712677002, train ANN loss : 3.9105732440948486\n",
      "AE loss : 0.077055923640728, ANN loss : 3.9126858711242676, Total loss : 11.618277549743652\n",
      "learning rate A :  tf.Tensor(9.996629e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 31 is 0.0880 sec\n",
      "train AE loss : 0.0771566778421402, train ANN loss : 3.9106976985931396\n",
      "AE loss : 0.07702235877513885, ANN loss : 3.91268253326416, Total loss : 11.614917755126953\n",
      "learning rate A :  tf.Tensor(9.996629e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 32 is 0.1212 sec\n",
      "train AE loss : 0.07712279260158539, train ANN loss : 3.9107072353363037\n",
      "AE loss : 0.07694268226623535, ANN loss : 3.912672281265259, Total loss : 11.606940269470215\n",
      "learning rate A :  tf.Tensor(9.996629e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 33 is 0.1143 sec\n",
      "train AE loss : 0.07704298198223114, train ANN loss : 3.909712076187134\n",
      "AE loss : 0.07674392312765121, ANN loss : 3.912653684616089, Total loss : 11.58704662322998\n",
      "learning rate A :  tf.Tensor(9.996629e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 34 is 0.1188 sec\n",
      "train AE loss : 0.07684586942195892, train ANN loss : 3.91011118888855\n",
      "AE loss : 0.0765102356672287, ANN loss : 3.9126241207122803, Total loss : 11.56364631652832\n",
      "learning rate A :  tf.Tensor(9.996629e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 35 is 0.1170 sec\n",
      "train AE loss : 0.07661526650190353, train ANN loss : 3.9097633361816406\n",
      "AE loss : 0.07650778442621231, ANN loss : 3.912623405456543, Total loss : 11.56340217590332\n",
      "learning rate A :  tf.Tensor(9.996418e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 36 is 0.1041 sec\n",
      "train AE loss : 0.07661280781030655, train ANN loss : 3.910336971282959\n",
      "AE loss : 0.07650533318519592, ANN loss : 3.912623405456543, Total loss : 11.563157081604004\n",
      "learning rate A :  tf.Tensor(9.996207e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 37 is 0.0908 sec\n",
      "train AE loss : 0.07661035656929016, train ANN loss : 3.910015344619751\n",
      "AE loss : 0.07650288194417953, ANN loss : 3.912623882293701, Total loss : 11.562911033630371\n",
      "learning rate A :  tf.Tensor(9.9959965e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 38 is 0.1043 sec\n",
      "train AE loss : 0.07660789787769318, train ANN loss : 3.910125494003296\n",
      "AE loss : 0.07625812292098999, ANN loss : 3.912580728530884, Total loss : 11.538393020629883\n",
      "learning rate A :  tf.Tensor(9.9959965e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 39 is 0.0971 sec\n",
      "train AE loss : 0.07636336237192154, train ANN loss : 3.9099674224853516\n",
      "AE loss : 0.07602079212665558, ANN loss : 3.91251802444458, Total loss : 11.514596939086914\n",
      "learning rate A :  tf.Tensor(9.9959965e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 40 is 0.0870 sec\n",
      "train AE loss : 0.0761258602142334, train ANN loss : 3.9095568656921387\n",
      "AE loss : 0.07601834088563919, ANN loss : 3.91251802444458, Total loss : 11.514352798461914\n",
      "learning rate A :  tf.Tensor(9.995786e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 41 is 0.0929 sec\n",
      "train AE loss : 0.07612339407205582, train ANN loss : 3.9096078872680664\n",
      "AE loss : 0.0760158896446228, ANN loss : 3.91251802444458, Total loss : 11.514108657836914\n",
      "learning rate A :  tf.Tensor(9.995575e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 42 is 0.0859 sec\n",
      "train AE loss : 0.07612093538045883, train ANN loss : 3.9099502563476562\n",
      "AE loss : 0.07601345330476761, ANN loss : 3.91251802444458, Total loss : 11.513863563537598\n",
      "learning rate A :  tf.Tensor(9.995365e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 43 is 0.0840 sec\n",
      "train AE loss : 0.07611846923828125, train ANN loss : 3.9094631671905518\n",
      "AE loss : 0.07577313482761383, ANN loss : 3.9124372005462646, Total loss : 11.489750862121582\n",
      "learning rate A :  tf.Tensor(9.995365e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 44 is 0.0870 sec\n",
      "train AE loss : 0.07587923109531403, train ANN loss : 3.9091248512268066\n",
      "AE loss : 0.07577072083950043, ANN loss : 3.9124372005462646, Total loss : 11.489508628845215\n",
      "learning rate A :  tf.Tensor(9.995155e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 45 is 0.1009 sec\n",
      "train AE loss : 0.07587679475545883, train ANN loss : 3.9097235202789307\n",
      "AE loss : 0.07552099227905273, ANN loss : 3.912329912185669, Total loss : 11.464428901672363\n",
      "learning rate A :  tf.Tensor(9.995155e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 46 is 0.0888 sec\n",
      "train AE loss : 0.075625479221344, train ANN loss : 3.9094560146331787\n",
      "AE loss : 0.07551857829093933, ANN loss : 3.912330150604248, Total loss : 11.464188575744629\n",
      "learning rate A :  tf.Tensor(9.994944e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 47 is 0.0848 sec\n",
      "train AE loss : 0.07562305778265, train ANN loss : 3.9093987941741943\n",
      "AE loss : 0.07551617175340652, ANN loss : 3.912329912185669, Total loss : 11.463946342468262\n",
      "learning rate A :  tf.Tensor(9.9947334e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 48 is 0.0855 sec\n",
      "train AE loss : 0.075620636343956, train ANN loss : 3.909423828125\n",
      "AE loss : 0.07551375776529312, ANN loss : 3.912329912185669, Total loss : 11.463706016540527\n",
      "learning rate A :  tf.Tensor(9.9945224e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 49 is 0.0861 sec\n",
      "train AE loss : 0.07561822980642319, train ANN loss : 3.9095711708068848\n",
      "AE loss : 0.07522226870059967, ANN loss : 3.9122085571289062, Total loss : 11.434435844421387\n",
      "learning rate A :  tf.Tensor(9.9945224e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 50 is 0.0859 sec\n",
      "train AE loss : 0.07532498240470886, train ANN loss : 3.909299373626709\n",
      "AE loss : 0.07491414994001389, ANN loss : 3.912065029144287, Total loss : 11.40347957611084\n",
      "learning rate A :  tf.Tensor(9.9945224e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 51 is 0.0880 sec\n",
      "train AE loss : 0.07501564174890518, train ANN loss : 3.9088237285614014\n",
      "AE loss : 0.07455819100141525, ANN loss : 3.911897897720337, Total loss : 11.367716789245605\n",
      "learning rate A :  tf.Tensor(9.9945224e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 52 is 0.0857 sec\n",
      "train AE loss : 0.07465717196464539, train ANN loss : 3.908543109893799\n",
      "AE loss : 0.07455582171678543, ANN loss : 3.911897897720337, Total loss : 11.367480278015137\n",
      "learning rate A :  tf.Tensor(9.9943114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 53 is 0.0824 sec\n",
      "train AE loss : 0.07465481013059616, train ANN loss : 3.908860206604004\n",
      "AE loss : 0.07420455664396286, ANN loss : 3.9117045402526855, Total loss : 11.332159996032715\n",
      "learning rate A :  tf.Tensor(9.9943114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 54 is 0.0864 sec\n",
      "train AE loss : 0.07430105656385422, train ANN loss : 3.908193826675415\n",
      "AE loss : 0.07380682975053787, ANN loss : 3.911489248275757, Total loss : 11.292171478271484\n",
      "learning rate A :  tf.Tensor(9.9943114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 55 is 0.0858 sec\n",
      "train AE loss : 0.07389841228723526, train ANN loss : 3.9081223011016846\n",
      "AE loss : 0.07335589081048965, ANN loss : 3.9112539291381836, Total loss : 11.246843338012695\n",
      "learning rate A :  tf.Tensor(9.9943114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 56 is 0.0874 sec\n",
      "train AE loss : 0.07344178855419159, train ANN loss : 3.907550096511841\n",
      "AE loss : 0.07282029092311859, ANN loss : 3.9110052585601807, Total loss : 11.193035125732422\n",
      "learning rate A :  tf.Tensor(9.9943114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 57 is 0.0874 sec\n",
      "train AE loss : 0.07290083169937134, train ANN loss : 3.90736985206604\n",
      "AE loss : 0.07281788438558578, ANN loss : 3.9110054969787598, Total loss : 11.192792892456055\n",
      "learning rate A :  tf.Tensor(9.994101e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 58 is 0.0842 sec\n",
      "train AE loss : 0.07289841771125793, train ANN loss : 3.9072136878967285\n",
      "AE loss : 0.0721970796585083, ANN loss : 3.91072940826416, Total loss : 11.130436897277832\n",
      "learning rate A :  tf.Tensor(9.994101e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 59 is 0.0876 sec\n",
      "train AE loss : 0.07227183133363724, train ANN loss : 3.907325267791748\n",
      "AE loss : 0.07219463586807251, ANN loss : 3.91072940826416, Total loss : 11.130192756652832\n",
      "learning rate A :  tf.Tensor(9.993891e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 60 is 0.0842 sec\n",
      "train AE loss : 0.07226937264204025, train ANN loss : 3.907266616821289\n",
      "AE loss : 0.07219219952821732, ANN loss : 3.91072940826416, Total loss : 11.129949569702148\n",
      "learning rate A :  tf.Tensor(9.99368e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 61 is 0.1003 sec\n",
      "train AE loss : 0.07226692140102386, train ANN loss : 3.907062530517578\n",
      "AE loss : 0.07218976318836212, ANN loss : 3.91072940826416, Total loss : 11.129707336425781\n",
      "learning rate A :  tf.Tensor(9.9934696e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 62 is 0.0853 sec\n",
      "train AE loss : 0.07226447761058807, train ANN loss : 3.9064087867736816\n",
      "AE loss : 0.07218732684850693, ANN loss : 3.91072940826416, Total loss : 11.129463195800781\n",
      "learning rate A :  tf.Tensor(9.993259e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 63 is 0.0816 sec\n",
      "train AE loss : 0.07226203382015228, train ANN loss : 3.907503604888916\n",
      "AE loss : 0.07218489050865173, ANN loss : 3.910729169845581, Total loss : 11.129219055175781\n",
      "learning rate A :  tf.Tensor(9.993048e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 64 is 0.0852 sec\n",
      "train AE loss : 0.0722595825791359, train ANN loss : 3.9066643714904785\n",
      "AE loss : 0.07218245416879654, ANN loss : 3.91072940826416, Total loss : 11.128973960876465\n",
      "learning rate A :  tf.Tensor(9.992837e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 65 is 0.0837 sec\n",
      "train AE loss : 0.0722571387887001, train ANN loss : 3.9079954624176025\n",
      "AE loss : 0.07158701121807098, ANN loss : 3.9104361534118652, Total loss : 11.069137573242188\n",
      "learning rate A :  tf.Tensor(9.992837e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 66 is 0.0846 sec\n",
      "train AE loss : 0.07165580987930298, train ANN loss : 3.9069113731384277\n",
      "AE loss : 0.0715845450758934, ANN loss : 3.9104363918304443, Total loss : 11.068890571594238\n",
      "learning rate A :  tf.Tensor(9.992627e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 67 is 0.0826 sec\n",
      "train AE loss : 0.0716533362865448, train ANN loss : 3.9066646099090576\n",
      "AE loss : 0.07090099155902863, ANN loss : 3.910109758377075, Total loss : 11.000207901000977\n",
      "learning rate A :  tf.Tensor(9.992627e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 68 is 0.0849 sec\n",
      "train AE loss : 0.0709647685289383, train ANN loss : 3.9069366455078125\n",
      "AE loss : 0.07020777463912964, ANN loss : 3.909761905670166, Total loss : 10.930540084838867\n",
      "learning rate A :  tf.Tensor(9.992627e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 69 is 0.0878 sec\n",
      "train AE loss : 0.07026544958353043, train ANN loss : 3.9058947563171387\n",
      "AE loss : 0.07020524889230728, ANN loss : 3.909762382507324, Total loss : 10.930286407470703\n",
      "learning rate A :  tf.Tensor(9.992417e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 70 is 0.0842 sec\n",
      "train AE loss : 0.07026290893554688, train ANN loss : 3.905336618423462\n",
      "AE loss : 0.07020273059606552, ANN loss : 3.909761905670166, Total loss : 10.930034637451172\n",
      "learning rate A :  tf.Tensor(9.9922065e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 71 is 0.0851 sec\n",
      "train AE loss : 0.07026037573814392, train ANN loss : 3.906792402267456\n",
      "AE loss : 0.06952844560146332, ANN loss : 3.909386396408081, Total loss : 10.862231254577637\n",
      "learning rate A :  tf.Tensor(9.9922065e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 72 is 0.0886 sec\n",
      "train AE loss : 0.06958207488059998, train ANN loss : 3.9058027267456055\n",
      "AE loss : 0.06952589005231857, ANN loss : 3.909386396408081, Total loss : 10.86197566986084\n",
      "learning rate A :  tf.Tensor(9.9919955e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 73 is 0.0851 sec\n",
      "train AE loss : 0.06957950443029404, train ANN loss : 3.904350757598877\n",
      "AE loss : 0.06952333450317383, ANN loss : 3.909386396408081, Total loss : 10.861719131469727\n",
      "learning rate A :  tf.Tensor(9.991785e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 74 is 0.0876 sec\n",
      "train AE loss : 0.0695769414305687, train ANN loss : 3.9055888652801514\n",
      "AE loss : 0.06952079385519028, ANN loss : 3.909386396408081, Total loss : 10.861466407775879\n",
      "learning rate A :  tf.Tensor(9.991575e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 75 is 0.0853 sec\n",
      "train AE loss : 0.06957437098026276, train ANN loss : 3.9050967693328857\n",
      "AE loss : 0.06951823085546494, ANN loss : 3.909386396408081, Total loss : 10.861207962036133\n",
      "learning rate A :  tf.Tensor(9.991364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 76 is 0.0839 sec\n",
      "train AE loss : 0.06957180798053741, train ANN loss : 3.9056591987609863\n",
      "AE loss : 0.06883832067251205, ANN loss : 3.908972978591919, Total loss : 10.792805671691895\n",
      "learning rate A :  tf.Tensor(9.991364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 77 is 0.0853 sec\n",
      "train AE loss : 0.06888919323682785, train ANN loss : 3.903806209564209\n",
      "AE loss : 0.06798802316188812, ANN loss : 3.9084861278533936, Total loss : 10.70728874206543\n",
      "learning rate A :  tf.Tensor(9.991364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 78 is 0.0848 sec\n",
      "train AE loss : 0.06803561747074127, train ANN loss : 3.9027748107910156\n",
      "AE loss : 0.06694626808166504, ANN loss : 3.907924175262451, Total loss : 10.602550506591797\n",
      "learning rate A :  tf.Tensor(9.991364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 79 is 0.0854 sec\n",
      "train AE loss : 0.06699109822511673, train ANN loss : 3.9038338661193848\n",
      "AE loss : 0.06694360822439194, ANN loss : 3.907923936843872, Total loss : 10.60228443145752\n",
      "learning rate A :  tf.Tensor(9.991154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 80 is 0.0801 sec\n",
      "train AE loss : 0.06698843091726303, train ANN loss : 3.903866767883301\n",
      "AE loss : 0.06694095581769943, ANN loss : 3.907923936843872, Total loss : 10.602018356323242\n",
      "learning rate A :  tf.Tensor(9.9909426e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 81 is 0.0799 sec\n",
      "train AE loss : 0.06698577105998993, train ANN loss : 3.9033026695251465\n",
      "AE loss : 0.06693829596042633, ANN loss : 3.907923460006714, Total loss : 10.601752281188965\n",
      "learning rate A :  tf.Tensor(9.990732e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 82 is 0.0784 sec\n",
      "train AE loss : 0.06698310375213623, train ANN loss : 3.9023256301879883\n",
      "AE loss : 0.06693564355373383, ANN loss : 3.907923460006714, Total loss : 10.60148811340332\n",
      "learning rate A :  tf.Tensor(9.990522e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 83 is 0.0970 sec\n",
      "train AE loss : 0.06698043644428253, train ANN loss : 3.9034783840179443\n",
      "AE loss : 0.06693298369646072, ANN loss : 3.907923460006714, Total loss : 10.601222038269043\n",
      "learning rate A :  tf.Tensor(9.990312e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 84 is 0.0843 sec\n",
      "train AE loss : 0.06697777658700943, train ANN loss : 3.903411626815796\n",
      "AE loss : 0.06693033874034882, ANN loss : 3.9079232215881348, Total loss : 10.600956916809082\n",
      "learning rate A :  tf.Tensor(9.9901015e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 85 is 0.0789 sec\n",
      "train AE loss : 0.06697510927915573, train ANN loss : 3.9044153690338135\n",
      "AE loss : 0.06692767888307571, ANN loss : 3.9079229831695557, Total loss : 10.600690841674805\n",
      "learning rate A :  tf.Tensor(9.9898905e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 86 is 0.0784 sec\n",
      "train AE loss : 0.06697244197130203, train ANN loss : 3.902660846710205\n",
      "AE loss : 0.06579262763261795, ANN loss : 3.90729022026062, Total loss : 10.486552238464355\n",
      "learning rate A :  tf.Tensor(9.9898905e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 87 is 0.0826 sec\n",
      "train AE loss : 0.06583599001169205, train ANN loss : 3.903381586074829\n",
      "AE loss : 0.06463934481143951, ANN loss : 3.906597375869751, Total loss : 10.370532035827637\n",
      "learning rate A :  tf.Tensor(9.9898905e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 88 is 0.0814 sec\n",
      "train AE loss : 0.06468260288238525, train ANN loss : 3.901951789855957\n",
      "AE loss : 0.06348736584186554, ANN loss : 3.9058268070220947, Total loss : 10.254563331604004\n",
      "learning rate A :  tf.Tensor(9.9898905e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 89 is 0.0805 sec\n",
      "train AE loss : 0.06352967768907547, train ANN loss : 3.90100359916687\n",
      "AE loss : 0.06227992847561836, ANN loss : 3.904984474182129, Total loss : 10.132977485656738\n",
      "learning rate A :  tf.Tensor(9.9898905e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 90 is 0.0908 sec\n",
      "train AE loss : 0.0623202808201313, train ANN loss : 3.9002175331115723\n",
      "AE loss : 0.06104469671845436, ANN loss : 3.904085397720337, Total loss : 10.00855541229248\n",
      "learning rate A :  tf.Tensor(9.9898905e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 91 is 0.0981 sec\n",
      "train AE loss : 0.06108015775680542, train ANN loss : 3.900205135345459\n",
      "AE loss : 0.06104188412427902, ANN loss : 3.904085397720337, Total loss : 10.00827407836914\n",
      "learning rate A :  tf.Tensor(9.9896795e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 92 is 0.0786 sec\n",
      "train AE loss : 0.06107733026146889, train ANN loss : 3.8995919227600098\n",
      "AE loss : 0.061039071530103683, ANN loss : 3.904085159301758, Total loss : 10.0079927444458\n",
      "learning rate A :  tf.Tensor(9.989469e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 93 is 0.0781 sec\n",
      "train AE loss : 0.06107450649142265, train ANN loss : 3.900590181350708\n",
      "AE loss : 0.061036255210638046, ANN loss : 3.9040846824645996, Total loss : 10.007710456848145\n",
      "learning rate A :  tf.Tensor(9.989259e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 94 is 0.0785 sec\n",
      "train AE loss : 0.06107168272137642, train ANN loss : 3.9002294540405273\n",
      "AE loss : 0.06103344261646271, ANN loss : 3.9040846824645996, Total loss : 10.007429122924805\n",
      "learning rate A :  tf.Tensor(9.989049e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 95 is 0.0787 sec\n",
      "train AE loss : 0.061068855226039886, train ANN loss : 3.898937225341797\n",
      "AE loss : 0.06103062629699707, ANN loss : 3.9040844440460205, Total loss : 10.007147789001465\n",
      "learning rate A :  tf.Tensor(9.988838e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 96 is 0.0783 sec\n",
      "train AE loss : 0.061066027730703354, train ANN loss : 3.898533344268799\n",
      "AE loss : 0.05978851020336151, ANN loss : 3.9031052589416504, Total loss : 9.881956100463867\n",
      "learning rate A :  tf.Tensor(9.988838e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 97 is 0.0794 sec\n",
      "train AE loss : 0.05981538072228432, train ANN loss : 3.8987574577331543\n",
      "AE loss : 0.058718811720609665, ANN loss : 3.9021248817443848, Total loss : 9.774005889892578\n",
      "learning rate A :  tf.Tensor(9.988838e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 98 is 0.0803 sec\n",
      "train AE loss : 0.05873548984527588, train ANN loss : 3.8965256214141846\n",
      "AE loss : 0.05871596932411194, ANN loss : 3.9021248817443848, Total loss : 9.773721694946289\n",
      "learning rate A :  tf.Tensor(9.988627e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 99 is 0.0773 sec\n",
      "train AE loss : 0.05873265117406845, train ANN loss : 3.897365093231201\n",
      "AE loss : 0.05780579522252083, ANN loss : 3.901111125946045, Total loss : 9.68169116973877\n",
      "learning rate A :  tf.Tensor(9.988627e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 100 is 0.0792 sec\n",
      "train AE loss : 0.057812340557575226, train ANN loss : 3.8970727920532227\n",
      "AE loss : 0.057802964001894, ANN loss : 3.901111125946045, Total loss : 9.68140697479248\n",
      "learning rate A :  tf.Tensor(9.9884164e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 101 is 0.0781 sec\n",
      "train AE loss : 0.0578094981610775, train ANN loss : 3.8953607082366943\n",
      "AE loss : 0.056915152817964554, ANN loss : 3.900035858154297, Total loss : 9.591551780700684\n",
      "learning rate A :  tf.Tensor(9.9884164e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 102 is 0.0880 sec\n",
      "train AE loss : 0.056909870356321335, train ANN loss : 3.895416736602783\n",
      "AE loss : 0.056912340223789215, ANN loss : 3.900035858154297, Total loss : 9.591269493103027\n",
      "learning rate A :  tf.Tensor(9.9882054e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 103 is 0.0783 sec\n",
      "train AE loss : 0.0569070503115654, train ANN loss : 3.895207166671753\n",
      "AE loss : 0.05690952390432358, ANN loss : 3.9000351428985596, Total loss : 9.590988159179688\n",
      "learning rate A :  tf.Tensor(9.987995e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 104 is 0.0778 sec\n",
      "train AE loss : 0.056904230266809464, train ANN loss : 3.893617630004883\n",
      "AE loss : 0.056906718760728836, ANN loss : 3.9000349044799805, Total loss : 9.590707778930664\n",
      "learning rate A :  tf.Tensor(9.987785e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 105 is 0.0767 sec\n",
      "train AE loss : 0.05690140649676323, train ANN loss : 3.896296739578247\n",
      "AE loss : 0.0569039024412632, ANN loss : 3.9000346660614014, Total loss : 9.590424537658691\n",
      "learning rate A :  tf.Tensor(9.9875746e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 106 is 0.0786 sec\n",
      "train AE loss : 0.05689859017729759, train ANN loss : 3.8948209285736084\n",
      "AE loss : 0.05690108984708786, ANN loss : 3.900034189224243, Total loss : 9.590143203735352\n",
      "learning rate A :  tf.Tensor(9.987364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 107 is 0.0776 sec\n",
      "train AE loss : 0.056895770132541656, train ANN loss : 3.8960418701171875\n",
      "AE loss : 0.05689828842878342, ANN loss : 3.900033950805664, Total loss : 9.589862823486328\n",
      "learning rate A :  tf.Tensor(9.987154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 108 is 0.0781 sec\n",
      "train AE loss : 0.05689294636249542, train ANN loss : 3.8970065116882324\n",
      "AE loss : 0.056139059364795685, ANN loss : 3.8989670276641846, Total loss : 9.512872695922852\n",
      "learning rate A :  tf.Tensor(9.987154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 109 is 0.0805 sec\n",
      "train AE loss : 0.05612298473715782, train ANN loss : 3.8923842906951904\n",
      "AE loss : 0.05531524494290352, ANN loss : 3.897794723510742, Total loss : 9.429320335388184\n",
      "learning rate A :  tf.Tensor(9.987154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 110 is 0.0796 sec\n",
      "train AE loss : 0.055288828909397125, train ANN loss : 3.8913838863372803\n",
      "AE loss : 0.05440898239612579, ANN loss : 3.896496534347534, Total loss : 9.337394714355469\n",
      "learning rate A :  tf.Tensor(9.987154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 111 is 0.0802 sec\n",
      "train AE loss : 0.05437014624476433, train ANN loss : 3.8906779289245605\n",
      "AE loss : 0.05351220816373825, ANN loss : 3.895087718963623, Total loss : 9.246308326721191\n",
      "learning rate A :  tf.Tensor(9.987154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 112 is 0.0799 sec\n",
      "train AE loss : 0.05346103757619858, train ANN loss : 3.8924434185028076\n",
      "AE loss : 0.05290863290429115, ANN loss : 3.8937771320343018, Total loss : 9.184640884399414\n",
      "learning rate A :  tf.Tensor(9.987154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 113 is 0.0780 sec\n",
      "train AE loss : 0.05284677445888519, train ANN loss : 3.8891749382019043\n",
      "AE loss : 0.05290604382753372, ANN loss : 3.8937766551971436, Total loss : 9.184380531311035\n",
      "learning rate A :  tf.Tensor(9.986943e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 114 is 0.0770 sec\n",
      "train AE loss : 0.05284417048096657, train ANN loss : 3.886807918548584\n",
      "AE loss : 0.05219808220863342, ANN loss : 3.8922877311706543, Total loss : 9.112095832824707\n",
      "learning rate A :  tf.Tensor(9.986943e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 115 is 0.0789 sec\n",
      "train AE loss : 0.0521242655813694, train ANN loss : 3.8843531608581543\n",
      "AE loss : 0.052195578813552856, ANN loss : 3.892287254333496, Total loss : 9.111845016479492\n",
      "learning rate A :  tf.Tensor(9.986733e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 116 is 0.0793 sec\n",
      "train AE loss : 0.05212174728512764, train ANN loss : 3.884631872177124\n",
      "AE loss : 0.05219307169318199, ANN loss : 3.892286777496338, Total loss : 9.111593246459961\n",
      "learning rate A :  tf.Tensor(9.9865225e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 117 is 0.0808 sec\n",
      "train AE loss : 0.052119236439466476, train ANN loss : 3.887830972671509\n",
      "AE loss : 0.05219056084752083, ANN loss : 3.8922860622406006, Total loss : 9.111342430114746\n",
      "learning rate A :  tf.Tensor(9.986312e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 118 is 0.0779 sec\n",
      "train AE loss : 0.052116718143224716, train ANN loss : 3.8868660926818848\n",
      "AE loss : 0.051529452204704285, ANN loss : 3.8906311988830566, Total loss : 9.043575286865234\n",
      "learning rate A :  tf.Tensor(9.986312e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 119 is 0.0795 sec\n",
      "train AE loss : 0.051443010568618774, train ANN loss : 3.8859269618988037\n",
      "AE loss : 0.05092308297753334, ANN loss : 3.88871431350708, Total loss : 8.981022834777832\n",
      "learning rate A :  tf.Tensor(9.986312e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 120 is 0.0781 sec\n",
      "train AE loss : 0.05082723870873451, train ANN loss : 3.8844106197357178\n",
      "AE loss : 0.05044299736618996, ANN loss : 3.88667368888855, Total loss : 8.930973052978516\n",
      "learning rate A :  tf.Tensor(9.986312e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 121 is 0.0805 sec\n",
      "train AE loss : 0.05033959820866585, train ANN loss : 3.882899522781372\n",
      "AE loss : 0.05009769648313522, ANN loss : 3.884526491165161, Total loss : 8.894295692443848\n",
      "learning rate A :  tf.Tensor(9.986312e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 122 is 0.0796 sec\n",
      "train AE loss : 0.04998694732785225, train ANN loss : 3.8778350353240967\n",
      "AE loss : 0.05009566247463226, ANN loss : 3.884526014328003, Total loss : 8.894092559814453\n",
      "learning rate A :  tf.Tensor(9.986102e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 123 is 0.0872 sec\n",
      "train AE loss : 0.04998490959405899, train ANN loss : 3.87845516204834\n",
      "AE loss : 0.0500936321914196, ANN loss : 3.8845252990722656, Total loss : 8.893888473510742\n",
      "learning rate A :  tf.Tensor(9.985892e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 124 is 0.0779 sec\n",
      "train AE loss : 0.04998286813497543, train ANN loss : 3.879401206970215\n",
      "AE loss : 0.05009160563349724, ANN loss : 3.8845245838165283, Total loss : 8.893685340881348\n",
      "learning rate A :  tf.Tensor(9.985681e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 125 is 0.0766 sec\n",
      "train AE loss : 0.04998083412647247, train ANN loss : 3.879024028778076\n",
      "AE loss : 0.050089579075574875, ANN loss : 3.884523868560791, Total loss : 8.893482208251953\n",
      "learning rate A :  tf.Tensor(9.9854704e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 126 is 0.0780 sec\n",
      "train AE loss : 0.049978796392679214, train ANN loss : 3.8797945976257324\n",
      "AE loss : 0.05008754879236221, ANN loss : 3.884523391723633, Total loss : 8.893278121948242\n",
      "learning rate A :  tf.Tensor(9.98526e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 127 is 0.0760 sec\n",
      "train AE loss : 0.049976762384176254, train ANN loss : 3.8781468868255615\n",
      "AE loss : 0.05008552223443985, ANN loss : 3.8845226764678955, Total loss : 8.893074989318848\n",
      "learning rate A :  tf.Tensor(9.98505e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 128 is 0.0765 sec\n",
      "train AE loss : 0.049974728375673294, train ANN loss : 3.8777682781219482\n",
      "AE loss : 0.05008349195122719, ANN loss : 3.884521961212158, Total loss : 8.892871856689453\n",
      "learning rate A :  tf.Tensor(9.9848396e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 129 is 0.0793 sec\n",
      "train AE loss : 0.049972690641880035, train ANN loss : 3.8786346912384033\n",
      "AE loss : 0.049684036523103714, ANN loss : 3.882011651992798, Total loss : 8.850415229797363\n",
      "learning rate A :  tf.Tensor(9.9848396e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 130 is 0.0797 sec\n",
      "train AE loss : 0.04956578463315964, train ANN loss : 3.8758039474487305\n",
      "AE loss : 0.04968217387795448, ANN loss : 3.8820111751556396, Total loss : 8.850228309631348\n",
      "learning rate A :  tf.Tensor(9.9846286e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 131 is 0.0770 sec\n",
      "train AE loss : 0.04956391453742981, train ANN loss : 3.8748209476470947\n",
      "AE loss : 0.04968031868338585, ANN loss : 3.8820104598999023, Total loss : 8.850042343139648\n",
      "learning rate A :  tf.Tensor(9.984418e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 132 is 0.0771 sec\n",
      "train AE loss : 0.04956204444169998, train ANN loss : 3.876042127609253\n",
      "AE loss : 0.04967845603823662, ANN loss : 3.882009744644165, Total loss : 8.849855422973633\n",
      "learning rate A :  tf.Tensor(9.984208e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 133 is 0.0763 sec\n",
      "train AE loss : 0.049560174345970154, train ANN loss : 3.8762917518615723\n",
      "AE loss : 0.04942956939339638, ANN loss : 3.8794126510620117, Total loss : 8.822368621826172\n",
      "learning rate A :  tf.Tensor(9.984208e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 134 is 0.0790 sec\n",
      "train AE loss : 0.0493009015917778, train ANN loss : 3.8727290630340576\n",
      "AE loss : 0.049265045672655106, ANN loss : 3.876549243927002, Total loss : 8.803053855895996\n",
      "learning rate A :  tf.Tensor(9.984208e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 135 is 0.0795 sec\n",
      "train AE loss : 0.04912363365292549, train ANN loss : 3.869093656539917\n",
      "AE loss : 0.04926352947950363, ANN loss : 3.8765487670898438, Total loss : 8.802901268005371\n",
      "learning rate A :  tf.Tensor(9.983998e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 136 is 0.0769 sec\n",
      "train AE loss : 0.049122102558612823, train ANN loss : 3.8744118213653564\n",
      "AE loss : 0.04926200211048126, ANN loss : 3.8765482902526855, Total loss : 8.802748680114746\n",
      "learning rate A :  tf.Tensor(9.9837875e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 137 is 0.0774 sec\n",
      "train AE loss : 0.049120575189590454, train ANN loss : 3.8724935054779053\n",
      "AE loss : 0.04926048591732979, ANN loss : 3.8765482902526855, Total loss : 8.802597045898438\n",
      "learning rate A :  tf.Tensor(9.983577e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 138 is 0.0833 sec\n",
      "train AE loss : 0.049119047820568085, train ANN loss : 3.871812582015991\n",
      "AE loss : 0.04932147637009621, ANN loss : 3.8736419677734375, Total loss : 8.80578899383545\n",
      "learning rate A :  tf.Tensor(9.983577e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 139 is 0.0796 sec\n",
      "train AE loss : 0.04916423559188843, train ANN loss : 3.868931293487549\n",
      "AE loss : 0.049320075660943985, ANN loss : 3.8736419677734375, Total loss : 8.805649757385254\n",
      "learning rate A :  tf.Tensor(9.983366e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 140 is 0.0779 sec\n",
      "train AE loss : 0.049162834882736206, train ANN loss : 3.8711726665496826\n",
      "AE loss : 0.049567487090826035, ANN loss : 3.870922088623047, Total loss : 8.82767105102539\n",
      "learning rate A :  tf.Tensor(9.983366e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 141 is 0.0805 sec\n",
      "train AE loss : 0.04939328506588936, train ANN loss : 3.8664159774780273\n",
      "AE loss : 0.04956616088747978, ANN loss : 3.8709218502044678, Total loss : 8.827537536621094\n",
      "learning rate A :  tf.Tensor(9.983156e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 142 is 0.0780 sec\n",
      "train AE loss : 0.04939195141196251, train ANN loss : 3.867971181869507\n",
      "AE loss : 0.04956483468413353, ANN loss : 3.8709216117858887, Total loss : 8.827404975891113\n",
      "learning rate A :  tf.Tensor(9.9829456e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 143 is 0.0773 sec\n",
      "train AE loss : 0.04939062148332596, train ANN loss : 3.867478847503662\n",
      "AE loss : 0.049563515931367874, ANN loss : 3.8709216117858887, Total loss : 8.827272415161133\n",
      "learning rate A :  tf.Tensor(9.982735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 144 is 0.0786 sec\n",
      "train AE loss : 0.04938928410410881, train ANN loss : 3.8659684658050537\n",
      "AE loss : 0.04991693049669266, ANN loss : 3.8681702613830566, Total loss : 8.85986328125\n",
      "learning rate A :  tf.Tensor(9.982735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 145 is 0.0787 sec\n",
      "train AE loss : 0.04972691461443901, train ANN loss : 3.863036632537842\n",
      "AE loss : 0.05037260055541992, ANN loss : 3.865161180496216, Total loss : 8.902421951293945\n",
      "learning rate A :  tf.Tensor(9.982735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 146 is 0.0789 sec\n",
      "train AE loss : 0.05016384273767471, train ANN loss : 3.861362934112549\n",
      "AE loss : 0.050371285527944565, ANN loss : 3.865161657333374, Total loss : 8.902290344238281\n",
      "learning rate A :  tf.Tensor(9.982526e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 147 is 0.0778 sec\n",
      "train AE loss : 0.05016252398490906, train ANN loss : 3.8601107597351074\n",
      "AE loss : 0.0503699816763401, ANN loss : 3.865161657333374, Total loss : 8.902159690856934\n",
      "learning rate A :  tf.Tensor(9.982315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 148 is 0.0772 sec\n",
      "train AE loss : 0.0501612089574337, train ANN loss : 3.8601958751678467\n",
      "AE loss : 0.05100938305258751, ANN loss : 3.8617677688598633, Total loss : 8.962706565856934\n",
      "learning rate A :  tf.Tensor(9.982315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 149 is 0.0782 sec\n",
      "train AE loss : 0.05077752843499184, train ANN loss : 3.85801362991333\n",
      "AE loss : 0.051885176450014114, ANN loss : 3.8580877780914307, Total loss : 9.046605110168457\n",
      "learning rate A :  tf.Tensor(9.982315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 150 is 0.0800 sec\n",
      "train AE loss : 0.0516265444457531, train ANN loss : 3.854759693145752\n",
      "AE loss : 0.05302761495113373, ANN loss : 3.854283571243286, Total loss : 9.157045364379883\n",
      "learning rate A :  tf.Tensor(9.982315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 151 is 0.0790 sec\n",
      "train AE loss : 0.05273792892694473, train ANN loss : 3.853764057159424\n",
      "AE loss : 0.05302533134818077, ANN loss : 3.8542850017547607, Total loss : 9.156818389892578\n",
      "learning rate A :  tf.Tensor(9.9821045e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 152 is 0.0774 sec\n",
      "train AE loss : 0.05273565649986267, train ANN loss : 3.855220079421997\n",
      "AE loss : 0.05401614308357239, ANN loss : 3.8510725498199463, Total loss : 9.252686500549316\n",
      "learning rate A :  tf.Tensor(9.9821045e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 153 is 0.0789 sec\n",
      "train AE loss : 0.05369800329208374, train ANN loss : 3.8527274131774902\n",
      "AE loss : 0.0550013966858387, ANN loss : 3.847841739654541, Total loss : 9.347981452941895\n",
      "learning rate A :  tf.Tensor(9.9821045e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 154 is 0.0797 sec\n",
      "train AE loss : 0.05464949831366539, train ANN loss : 3.843672752380371\n",
      "AE loss : 0.054998185485601425, ANN loss : 3.847843885421753, Total loss : 9.347662925720215\n",
      "learning rate A :  tf.Tensor(9.981894e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 155 is 0.0782 sec\n",
      "train AE loss : 0.05464630573987961, train ANN loss : 3.848268985748291\n",
      "AE loss : 0.05499497428536415, ANN loss : 3.847846508026123, Total loss : 9.347343444824219\n",
      "learning rate A :  tf.Tensor(9.981684e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 156 is 0.0775 sec\n",
      "train AE loss : 0.054643116891384125, train ANN loss : 3.8458409309387207\n",
      "AE loss : 0.056474871933460236, ANN loss : 3.84375262260437, Total loss : 9.491239547729492\n",
      "learning rate A :  tf.Tensor(9.981684e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 157 is 0.0790 sec\n",
      "train AE loss : 0.056082408875226974, train ANN loss : 3.844536781311035\n",
      "AE loss : 0.05647064000368118, ANN loss : 3.8437557220458984, Total loss : 9.490819931030273\n",
      "learning rate A :  tf.Tensor(9.981474e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 158 is 0.0771 sec\n",
      "train AE loss : 0.05607821047306061, train ANN loss : 3.8420469760894775\n",
      "AE loss : 0.05646641179919243, ANN loss : 3.843759059906006, Total loss : 9.490400314331055\n",
      "learning rate A :  tf.Tensor(9.981263e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 159 is 0.0787 sec\n",
      "train AE loss : 0.05607400834560394, train ANN loss : 3.841881275177002\n",
      "AE loss : 0.056462179869413376, ANN loss : 3.843762159347534, Total loss : 9.489980697631836\n",
      "learning rate A :  tf.Tensor(9.981053e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 160 is 0.0771 sec\n",
      "train AE loss : 0.056069809943437576, train ANN loss : 3.8445591926574707\n",
      "AE loss : 0.05853140354156494, ANN loss : 3.839010238647461, Total loss : 9.692150115966797\n",
      "learning rate A :  tf.Tensor(9.981053e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 161 is 0.0790 sec\n",
      "train AE loss : 0.058094535022974014, train ANN loss : 3.833040714263916\n",
      "AE loss : 0.06103169173002243, ANN loss : 3.8340988159179688, Total loss : 9.937268257141113\n",
      "learning rate A :  tf.Tensor(9.981053e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 162 is 0.0796 sec\n",
      "train AE loss : 0.06054291874170303, train ANN loss : 3.8367533683776855\n",
      "AE loss : 0.06102325767278671, ANN loss : 3.83410382270813, Total loss : 9.936429977416992\n",
      "learning rate A :  tf.Tensor(9.980843e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 163 is 0.0808 sec\n",
      "train AE loss : 0.060534533113241196, train ANN loss : 3.8358206748962402\n",
      "AE loss : 0.06101484224200249, ANN loss : 3.83410906791687, Total loss : 9.935593605041504\n",
      "learning rate A :  tf.Tensor(9.9806326e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 164 is 0.0782 sec\n",
      "train AE loss : 0.06052615866065025, train ANN loss : 3.839437484741211\n",
      "AE loss : 0.06254943460226059, ANN loss : 3.8301548957824707, Total loss : 10.085098266601562\n",
      "learning rate A :  tf.Tensor(9.9806326e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 165 is 0.0793 sec\n",
      "train AE loss : 0.06201111525297165, train ANN loss : 3.830345630645752\n",
      "AE loss : 0.06380767375230789, ANN loss : 3.826404094696045, Total loss : 10.207171440124512\n",
      "learning rate A :  tf.Tensor(9.9806326e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 166 is 0.0799 sec\n",
      "train AE loss : 0.06322096288204193, train ANN loss : 3.8308165073394775\n",
      "AE loss : 0.06379646062850952, ANN loss : 3.826411008834839, Total loss : 10.20605754852295\n",
      "learning rate A :  tf.Tensor(9.980422e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 167 is 0.0776 sec\n",
      "train AE loss : 0.06320983916521072, train ANN loss : 3.8309264183044434\n",
      "AE loss : 0.06378525495529175, ANN loss : 3.826417922973633, Total loss : 10.204943656921387\n",
      "learning rate A :  tf.Tensor(9.980211e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 168 is 0.0787 sec\n",
      "train AE loss : 0.06319871544837952, train ANN loss : 3.829418659210205\n",
      "AE loss : 0.06470107287168503, ANN loss : 3.822876214981079, Total loss : 10.292983055114746\n",
      "learning rate A :  tf.Tensor(9.980211e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 169 is 0.0789 sec\n",
      "train AE loss : 0.06406820565462112, train ANN loss : 3.825894594192505\n",
      "AE loss : 0.06468889117240906, ANN loss : 3.8228845596313477, Total loss : 10.291773796081543\n",
      "learning rate A :  tf.Tensor(9.980001e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 170 is 0.0779 sec\n",
      "train AE loss : 0.06405612081289291, train ANN loss : 3.8213725090026855\n",
      "AE loss : 0.06663724780082703, ANN loss : 3.818357467651367, Total loss : 10.48208236694336\n",
      "learning rate A :  tf.Tensor(9.980001e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 171 is 0.0814 sec\n",
      "train AE loss : 0.06594709306955338, train ANN loss : 3.8201777935028076\n",
      "AE loss : 0.06662239134311676, ANN loss : 3.8183670043945312, Total loss : 10.480606079101562\n",
      "learning rate A :  tf.Tensor(9.9797915e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 172 is 0.0773 sec\n",
      "train AE loss : 0.06593236327171326, train ANN loss : 3.8209707736968994\n",
      "AE loss : 0.06660754233598709, ANN loss : 3.8183767795562744, Total loss : 10.479130744934082\n",
      "learning rate A :  tf.Tensor(9.979581e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 173 is 0.0777 sec\n",
      "train AE loss : 0.06591764092445374, train ANN loss : 3.8262524604797363\n",
      "AE loss : 0.06659272313117981, ANN loss : 3.8183863162994385, Total loss : 10.477659225463867\n",
      "learning rate A :  tf.Tensor(9.97937e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 174 is 0.0781 sec\n",
      "train AE loss : 0.06590293347835541, train ANN loss : 3.818690061569214\n",
      "AE loss : 0.07020550966262817, ANN loss : 3.8125579357147217, Total loss : 10.833106994628906\n",
      "learning rate A :  tf.Tensor(9.97937e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 175 is 0.0807 sec\n",
      "train AE loss : 0.06944287568330765, train ANN loss : 3.8199703693389893\n",
      "AE loss : 0.0701851174235344, ANN loss : 3.8125686645507812, Total loss : 10.83108139038086\n",
      "learning rate A :  tf.Tensor(9.97916e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 176 is 0.0770 sec\n",
      "train AE loss : 0.069422647356987, train ANN loss : 3.8178164958953857\n",
      "AE loss : 0.0701647400856018, ANN loss : 3.81257963180542, Total loss : 10.829052925109863\n",
      "learning rate A :  tf.Tensor(9.9789504e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 177 is 0.0765 sec\n",
      "train AE loss : 0.06940244138240814, train ANN loss : 3.8221535682678223\n",
      "AE loss : 0.07225742936134338, ANN loss : 3.8078951835632324, Total loss : 11.033637046813965\n",
      "learning rate A :  tf.Tensor(9.9789504e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 178 is 0.0789 sec\n",
      "train AE loss : 0.07142718136310577, train ANN loss : 3.814570426940918\n",
      "AE loss : 0.0722334012389183, ANN loss : 3.8079073429107666, Total loss : 11.03124713897705\n",
      "learning rate A :  tf.Tensor(9.97874e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 179 is 0.0825 sec\n",
      "train AE loss : 0.07140333205461502, train ANN loss : 3.8138132095336914\n",
      "AE loss : 0.07220941036939621, ANN loss : 3.807920217514038, Total loss : 11.028861999511719\n",
      "learning rate A :  tf.Tensor(9.97853e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 180 is 0.0998 sec\n",
      "train AE loss : 0.07137954235076904, train ANN loss : 3.812664747238159\n",
      "AE loss : 0.07325303554534912, ANN loss : 3.803919553756714, Total loss : 11.129223823547363\n",
      "learning rate A :  tf.Tensor(9.97853e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 181 is 0.0851 sec\n",
      "train AE loss : 0.07235435396432877, train ANN loss : 3.808178186416626\n",
      "AE loss : 0.07322689145803452, ANN loss : 3.803934335708618, Total loss : 11.126625061035156\n",
      "learning rate A :  tf.Tensor(9.978319e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 182 is 0.0920 sec\n",
      "train AE loss : 0.07232842594385147, train ANN loss : 3.8106892108917236\n",
      "AE loss : 0.07425041496753693, ANN loss : 3.7998766899108887, Total loss : 11.224918365478516\n",
      "learning rate A :  tf.Tensor(9.978319e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 183 is 0.1027 sec\n",
      "train AE loss : 0.07328112423419952, train ANN loss : 3.8072335720062256\n",
      "AE loss : 0.07422199845314026, ANN loss : 3.799894332885742, Total loss : 11.222094535827637\n",
      "learning rate A :  tf.Tensor(9.978109e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 184 is 0.0828 sec\n",
      "train AE loss : 0.07325292378664017, train ANN loss : 3.80627703666687\n",
      "AE loss : 0.07419362664222717, ANN loss : 3.799912452697754, Total loss : 11.219274520874023\n",
      "learning rate A :  tf.Tensor(9.977899e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 185 is 0.0861 sec\n",
      "train AE loss : 0.07322476804256439, train ANN loss : 3.8034489154815674\n",
      "AE loss : 0.07416531443595886, ANN loss : 3.7999300956726074, Total loss : 11.216462135314941\n",
      "learning rate A :  tf.Tensor(9.977688e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 186 is 0.0867 sec\n",
      "train AE loss : 0.07319667935371399, train ANN loss : 3.8036274909973145\n",
      "AE loss : 0.07413703203201294, ANN loss : 3.799947738647461, Total loss : 11.213650703430176\n",
      "learning rate A :  tf.Tensor(9.977478e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 187 is 0.0961 sec\n",
      "train AE loss : 0.07316862046718597, train ANN loss : 3.8098905086517334\n",
      "AE loss : 0.07629584521055222, ANN loss : 3.7949931621551514, Total loss : 11.424577713012695\n",
      "learning rate A :  tf.Tensor(9.977478e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 188 is 0.0967 sec\n",
      "train AE loss : 0.07524821907281876, train ANN loss : 3.8038368225097656\n",
      "AE loss : 0.0801934152841568, ANN loss : 3.7887990474700928, Total loss : 11.80813980102539\n",
      "learning rate A :  tf.Tensor(9.977478e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 189 is 0.0937 sec\n",
      "train AE loss : 0.07905127108097076, train ANN loss : 3.799269199371338\n",
      "AE loss : 0.08373407274484634, ANN loss : 3.782928705215454, Total loss : 12.15633487701416\n",
      "learning rate A :  tf.Tensor(9.977478e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 190 is 0.0902 sec\n",
      "train AE loss : 0.08249294757843018, train ANN loss : 3.7966601848602295\n",
      "AE loss : 0.08368270099163055, ANN loss : 3.7829530239105225, Total loss : 12.151224136352539\n",
      "learning rate A :  tf.Tensor(9.9772675e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 191 is 0.0988 sec\n",
      "train AE loss : 0.08244191855192184, train ANN loss : 3.7956674098968506\n",
      "AE loss : 0.08363144099712372, ANN loss : 3.7829782962799072, Total loss : 12.14612102508545\n",
      "learning rate A :  tf.Tensor(9.977057e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 192 is 0.0830 sec\n",
      "train AE loss : 0.08239100873470306, train ANN loss : 3.7921218872070312\n",
      "AE loss : 0.08358030021190643, ANN loss : 3.7830028533935547, Total loss : 12.141034126281738\n",
      "learning rate A :  tf.Tensor(9.976847e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 193 is 0.0829 sec\n",
      "train AE loss : 0.08234021067619324, train ANN loss : 3.7940311431884766\n",
      "AE loss : 0.08599655330181122, ANN loss : 3.7778091430664062, Total loss : 12.37746524810791\n",
      "learning rate A :  tf.Tensor(9.976847e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 194 is 0.0864 sec\n",
      "train AE loss : 0.08465718477964401, train ANN loss : 3.788010358810425\n",
      "AE loss : 0.08593814074993134, ANN loss : 3.7778372764587402, Total loss : 12.371650695800781\n",
      "learning rate A :  tf.Tensor(9.976637e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 195 is 0.0863 sec\n",
      "train AE loss : 0.08459919691085815, train ANN loss : 3.7873499393463135\n",
      "AE loss : 0.0858798697590828, ANN loss : 3.7778656482696533, Total loss : 12.365853309631348\n",
      "learning rate A :  tf.Tensor(9.9764264e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 196 is 0.0847 sec\n",
      "train AE loss : 0.08454136550426483, train ANN loss : 3.789520025253296\n",
      "AE loss : 0.08637286722660065, ANN loss : 3.773709774017334, Total loss : 12.410996437072754\n",
      "learning rate A :  tf.Tensor(9.9764264e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 197 is 0.1011 sec\n",
      "train AE loss : 0.08495046943426132, train ANN loss : 3.786729097366333\n",
      "AE loss : 0.08724503964185715, ANN loss : 3.7691922187805176, Total loss : 12.493696212768555\n",
      "learning rate A :  tf.Tensor(9.9764264e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 198 is 0.0892 sec\n",
      "train AE loss : 0.08573542535305023, train ANN loss : 3.7775955200195312\n",
      "AE loss : 0.09136354178190231, ANN loss : 3.762471914291382, Total loss : 12.898825645446777\n",
      "learning rate A :  tf.Tensor(9.9764264e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 199 is 0.0874 sec\n",
      "train AE loss : 0.08973895013332367, train ANN loss : 3.7731027603149414\n",
      "AE loss : 0.0957590788602829, ANN loss : 3.7557294368743896, Total loss : 13.331636428833008\n",
      "learning rate A :  tf.Tensor(9.9764264e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 200 is 0.0901 sec\n",
      "train AE loss : 0.09401480108499527, train ANN loss : 3.7688887119293213\n",
      "AE loss : 0.09566263854503632, ANN loss : 3.755772352218628, Total loss : 13.322037696838379\n",
      "learning rate A :  tf.Tensor(9.976216e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 201 is 0.0850 sec\n",
      "train AE loss : 0.09391923248767853, train ANN loss : 3.7676546573638916\n",
      "AE loss : 0.09685263782739639, ANN loss : 3.7508225440979004, Total loss : 13.43608570098877\n",
      "learning rate A :  tf.Tensor(9.976216e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 202 is 0.0868 sec\n",
      "train AE loss : 0.09501747041940689, train ANN loss : 3.7623651027679443\n",
      "AE loss : 0.09674879908561707, ANN loss : 3.750871419906616, Total loss : 13.425751686096191\n",
      "learning rate A :  tf.Tensor(9.976006e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 203 is 0.0823 sec\n",
      "train AE loss : 0.09491470456123352, train ANN loss : 3.7639009952545166\n",
      "AE loss : 0.09664525091648102, ANN loss : 3.750920295715332, Total loss : 13.415445327758789\n",
      "learning rate A :  tf.Tensor(9.975796e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 204 is 0.0872 sec\n",
      "train AE loss : 0.09481224417686462, train ANN loss : 3.767333984375\n",
      "AE loss : 0.09593826532363892, ANN loss : 3.747083902359009, Total loss : 13.340909004211426\n",
      "learning rate A :  tf.Tensor(9.975796e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 205 is 0.0889 sec\n",
      "train AE loss : 0.09402356296777725, train ANN loss : 3.7604477405548096\n",
      "AE loss : 0.09647500514984131, ANN loss : 3.7425405979156494, Total loss : 13.390042304992676\n",
      "learning rate A :  tf.Tensor(9.975796e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 206 is 0.0847 sec\n",
      "train AE loss : 0.09446065872907639, train ANN loss : 3.75785231590271\n",
      "AE loss : 0.10460441559553146, ANN loss : 3.7334976196289062, Total loss : 14.193940162658691\n",
      "learning rate A :  tf.Tensor(9.975796e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 207 is 0.0882 sec\n",
      "train AE loss : 0.10240902751684189, train ANN loss : 3.7495062351226807\n",
      "AE loss : 0.11098884791135788, ANN loss : 3.7259645462036133, Total loss : 14.824848175048828\n",
      "learning rate A :  tf.Tensor(9.975796e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 208 is 0.0860 sec\n",
      "train AE loss : 0.10863196849822998, train ANN loss : 3.7488341331481934\n",
      "AE loss : 0.11081019043922424, ANN loss : 3.7260284423828125, Total loss : 14.807048797607422\n",
      "learning rate A :  tf.Tensor(9.975586e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 209 is 0.0894 sec\n",
      "train AE loss : 0.10845524817705154, train ANN loss : 3.7452232837677\n",
      "AE loss : 0.1106322631239891, ANN loss : 3.726093292236328, Total loss : 14.78931999206543\n",
      "learning rate A :  tf.Tensor(9.975375e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 210 is 0.0855 sec\n",
      "train AE loss : 0.1082792580127716, train ANN loss : 3.749242067337036\n",
      "AE loss : 0.11045512557029724, ANN loss : 3.7261576652526855, Total loss : 14.771668434143066\n",
      "learning rate A :  tf.Tensor(9.9751654e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 211 is 0.0898 sec\n",
      "train AE loss : 0.10810402035713196, train ANN loss : 3.7457261085510254\n",
      "AE loss : 0.11027872562408447, ANN loss : 3.726222515106201, Total loss : 14.754094123840332\n",
      "learning rate A :  tf.Tensor(9.974955e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 212 is 0.0959 sec\n",
      "train AE loss : 0.107929527759552, train ANN loss : 3.74782133102417\n",
      "AE loss : 0.11010304093360901, ANN loss : 3.726287841796875, Total loss : 14.736592292785645\n",
      "learning rate A :  tf.Tensor(9.974745e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 213 is 0.1128 sec\n",
      "train AE loss : 0.10775575041770935, train ANN loss : 3.7427897453308105\n",
      "AE loss : 0.11065111309289932, ANN loss : 3.721447706222534, Total loss : 14.786558151245117\n",
      "learning rate A :  tf.Tensor(9.974745e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 214 is 0.0859 sec\n",
      "train AE loss : 0.1081949844956398, train ANN loss : 3.743399143218994\n",
      "AE loss : 0.10521265864372253, ANN loss : 3.720010280609131, Total loss : 14.241275787353516\n",
      "learning rate A :  tf.Tensor(9.974745e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 215 is 0.0893 sec\n",
      "train AE loss : 0.10272803157567978, train ANN loss : 3.7356691360473633\n",
      "AE loss : 0.1113920658826828, ANN loss : 3.712174654006958, Total loss : 14.851381301879883\n",
      "learning rate A :  tf.Tensor(9.974745e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 216 is 0.0869 sec\n",
      "train AE loss : 0.10872280597686768, train ANN loss : 3.7362985610961914\n",
      "AE loss : 0.11119066923856735, ANN loss : 3.712270498275757, Total loss : 14.831338882446289\n",
      "learning rate A :  tf.Tensor(9.9745346e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 217 is 0.1019 sec\n",
      "train AE loss : 0.10852450877428055, train ANN loss : 3.7330551147460938\n",
      "AE loss : 0.11099017411470413, ANN loss : 3.712366819381714, Total loss : 14.811384201049805\n",
      "learning rate A :  tf.Tensor(9.974324e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 218 is 0.0905 sec\n",
      "train AE loss : 0.10832709819078445, train ANN loss : 3.731100082397461\n",
      "AE loss : 0.12087437510490417, ANN loss : 3.703402042388916, Total loss : 15.790840148925781\n",
      "learning rate A :  tf.Tensor(9.974324e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 219 is 0.0908 sec\n",
      "train AE loss : 0.117976114153862, train ANN loss : 3.729984998703003\n",
      "AE loss : 0.12061843276023865, ANN loss : 3.703490972518921, Total loss : 15.765334129333496\n",
      "learning rate A :  tf.Tensor(9.974115e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 220 is 0.1001 sec\n",
      "train AE loss : 0.11772368848323822, train ANN loss : 3.727400779724121\n",
      "AE loss : 0.12175104767084122, ANN loss : 3.6986160278320312, Total loss : 15.8737211227417\n",
      "learning rate A :  tf.Tensor(9.974115e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 221 is 0.0856 sec\n",
      "train AE loss : 0.11873118579387665, train ANN loss : 3.7225182056427\n",
      "AE loss : 0.11602534353733063, ANN loss : 3.6972312927246094, Total loss : 15.299764633178711\n",
      "learning rate A :  tf.Tensor(9.974115e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 222 is 0.0876 sec\n",
      "train AE loss : 0.11298763751983643, train ANN loss : 3.718921184539795\n",
      "AE loss : 0.1183852106332779, ANN loss : 3.6920390129089355, Total loss : 15.530559539794922\n",
      "learning rate A :  tf.Tensor(9.974115e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 223 is 0.0946 sec\n",
      "train AE loss : 0.11519961804151535, train ANN loss : 3.7193808555603027\n",
      "AE loss : 0.11811535805463791, ANN loss : 3.6921746730804443, Total loss : 15.50370979309082\n",
      "learning rate A :  tf.Tensor(9.9739045e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 224 is 0.1015 sec\n",
      "train AE loss : 0.11493485420942307, train ANN loss : 3.7158453464508057\n",
      "AE loss : 0.1301741749048233, ANN loss : 3.682612419128418, Total loss : 16.700029373168945\n",
      "learning rate A :  tf.Tensor(9.9739045e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 225 is 0.0893 sec\n",
      "train AE loss : 0.12666234374046326, train ANN loss : 3.712843656539917\n",
      "AE loss : 0.13243527710437775, ANN loss : 3.677682876586914, Total loss : 16.92120933532715\n",
      "learning rate A :  tf.Tensor(9.9739045e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 226 is 0.0891 sec\n",
      "train AE loss : 0.1287604421377182, train ANN loss : 3.709928035736084\n",
      "AE loss : 0.12937210500240326, ANN loss : 3.6751160621643066, Total loss : 16.61232566833496\n",
      "learning rate A :  tf.Tensor(9.9739045e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 227 is 0.0851 sec\n",
      "train AE loss : 0.12562863528728485, train ANN loss : 3.6992177963256836\n",
      "AE loss : 0.13744279742240906, ANN loss : 3.667877435684204, Total loss : 17.41215705871582\n",
      "learning rate A :  tf.Tensor(9.9739045e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 228 is 0.0875 sec\n",
      "train AE loss : 0.13339854776859283, train ANN loss : 3.702474594116211\n",
      "AE loss : 0.1370031088590622, ANN loss : 3.668038845062256, Total loss : 17.368349075317383\n",
      "learning rate A :  tf.Tensor(9.9736935e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 229 is 0.0937 sec\n",
      "train AE loss : 0.13296781480312347, train ANN loss : 3.7008185386657715\n",
      "AE loss : 0.13656648993492126, ANN loss : 3.668200969696045, Total loss : 17.32485008239746\n",
      "learning rate A :  tf.Tensor(9.973484e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 230 is 0.0875 sec\n",
      "train AE loss : 0.13254013657569885, train ANN loss : 3.698112726211548\n",
      "AE loss : 0.13613300025463104, ANN loss : 3.668365240097046, Total loss : 17.281665802001953\n",
      "learning rate A :  tf.Tensor(9.973274e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 231 is 0.0863 sec\n",
      "train AE loss : 0.13211560249328613, train ANN loss : 3.7003703117370605\n",
      "AE loss : 0.13570253551006317, ANN loss : 3.6685304641723633, Total loss : 17.23878288269043\n",
      "learning rate A :  tf.Tensor(9.973064e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 232 is 0.0823 sec\n",
      "train AE loss : 0.13169409334659576, train ANN loss : 3.7015621662139893\n",
      "AE loss : 0.13527514040470123, ANN loss : 3.668696880340576, Total loss : 17.196212768554688\n",
      "learning rate A :  tf.Tensor(9.972853e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 233 is 0.0820 sec\n",
      "train AE loss : 0.13127563893795013, train ANN loss : 3.702942371368408\n",
      "AE loss : 0.13485072553157806, ANN loss : 3.6688642501831055, Total loss : 17.1539363861084\n",
      "learning rate A :  tf.Tensor(9.972643e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 234 is 0.0824 sec\n",
      "train AE loss : 0.13086020946502686, train ANN loss : 3.7013325691223145\n",
      "AE loss : 0.13442929089069366, ANN loss : 3.6690332889556885, Total loss : 17.111963272094727\n",
      "learning rate A :  tf.Tensor(9.972433e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 235 is 0.0838 sec\n",
      "train AE loss : 0.13044777512550354, train ANN loss : 3.7026994228363037\n",
      "AE loss : 0.1340106874704361, ANN loss : 3.669203281402588, Total loss : 17.07027244567871\n",
      "learning rate A :  tf.Tensor(9.972223e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 236 is 0.0823 sec\n",
      "train AE loss : 0.13003818690776825, train ANN loss : 3.70356822013855\n",
      "AE loss : 0.13359490036964417, ANN loss : 3.669374942779541, Total loss : 17.028865814208984\n",
      "learning rate A :  tf.Tensor(9.972013e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 237 is 0.0822 sec\n",
      "train AE loss : 0.1296314150094986, train ANN loss : 3.7005038261413574\n",
      "AE loss : 0.13318195939064026, ANN loss : 3.669546604156494, Total loss : 16.987743377685547\n",
      "learning rate A :  tf.Tensor(9.971803e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 238 is 0.0834 sec\n",
      "train AE loss : 0.12922747433185577, train ANN loss : 3.693408250808716\n",
      "AE loss : 0.15315274894237518, ANN loss : 3.658903121948242, Total loss : 18.974178314208984\n",
      "learning rate A :  tf.Tensor(9.971803e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 239 is 0.0825 sec\n",
      "train AE loss : 0.14867784082889557, train ANN loss : 3.6954658031463623\n",
      "AE loss : 0.1525810956954956, ANN loss : 3.6590282917022705, Total loss : 18.917139053344727\n",
      "learning rate A :  tf.Tensor(9.971593e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 240 is 0.0826 sec\n",
      "train AE loss : 0.14811652898788452, train ANN loss : 3.695542812347412\n",
      "AE loss : 0.1520138680934906, ANN loss : 3.6591556072235107, Total loss : 18.86054229736328\n",
      "learning rate A :  tf.Tensor(9.971383e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 241 is 0.0811 sec\n",
      "train AE loss : 0.14755967259407043, train ANN loss : 3.695415735244751\n",
      "AE loss : 0.1479111760854721, ANN loss : 3.6564743518829346, Total loss : 18.44759178161621\n",
      "learning rate A :  tf.Tensor(9.971383e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 242 is 0.1132 sec\n",
      "train AE loss : 0.14341747760772705, train ANN loss : 3.691819429397583\n",
      "AE loss : 0.14736169576644897, ANN loss : 3.656641960144043, Total loss : 18.392810821533203\n",
      "learning rate A :  tf.Tensor(9.9711724e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 243 is 0.1106 sec\n",
      "train AE loss : 0.14287927746772766, train ANN loss : 3.692310094833374\n",
      "AE loss : 0.13262756168842316, ANN loss : 3.6598060131073, Total loss : 16.922561645507812\n",
      "learning rate A :  tf.Tensor(9.9711724e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 244 is 0.1146 sec\n",
      "train AE loss : 0.128397136926651, train ANN loss : 3.687683582305908\n",
      "AE loss : 0.14440999925136566, ANN loss : 3.650981903076172, Total loss : 18.091981887817383\n",
      "learning rate A :  tf.Tensor(9.9711724e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 245 is 0.1181 sec\n",
      "train AE loss : 0.13982298970222473, train ANN loss : 3.6855249404907227\n",
      "AE loss : 0.16281913220882416, ANN loss : 3.641676425933838, Total loss : 19.9235897064209\n",
      "learning rate A :  tf.Tensor(9.9711724e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 246 is 0.0975 sec\n",
      "train AE loss : 0.1577232927083969, train ANN loss : 3.680311441421509\n",
      "AE loss : 0.16208809614181519, ANN loss : 3.6418492794036865, Total loss : 19.850658416748047\n",
      "learning rate A :  tf.Tensor(9.970962e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 247 is 0.1172 sec\n",
      "train AE loss : 0.15700763463974, train ANN loss : 3.6812429428100586\n",
      "AE loss : 0.1613634079694748, ANN loss : 3.6420249938964844, Total loss : 19.778366088867188\n",
      "learning rate A :  tf.Tensor(9.9707526e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 248 is 0.0822 sec\n",
      "train AE loss : 0.15629856288433075, train ANN loss : 3.6842992305755615\n",
      "AE loss : 0.1606450080871582, ANN loss : 3.6422033309936523, Total loss : 19.70670509338379\n",
      "learning rate A :  tf.Tensor(9.970542e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 249 is 0.0785 sec\n",
      "train AE loss : 0.15559588372707367, train ANN loss : 3.6825084686279297\n",
      "AE loss : 0.1599329710006714, ANN loss : 3.6423847675323486, Total loss : 19.63568115234375\n",
      "learning rate A :  tf.Tensor(9.970332e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 250 is 0.0774 sec\n",
      "train AE loss : 0.15489955246448517, train ANN loss : 3.6790547370910645\n",
      "AE loss : 0.15922728180885315, ANN loss : 3.642568826675415, Total loss : 19.565296173095703\n",
      "learning rate A :  tf.Tensor(9.9701225e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 251 is 0.0767 sec\n",
      "train AE loss : 0.15420953929424286, train ANN loss : 3.6803512573242188\n",
      "AE loss : 0.16614720225334167, ANN loss : 3.6375014781951904, Total loss : 20.252222061157227\n",
      "learning rate A :  tf.Tensor(9.9701225e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 252 is 0.0794 sec\n",
      "train AE loss : 0.1608896702528, train ANN loss : 3.6772587299346924\n",
      "AE loss : 0.1653641164302826, ANN loss : 3.6376798152923584, Total loss : 20.174091339111328\n",
      "learning rate A :  tf.Tensor(9.969912e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 253 is 0.0785 sec\n",
      "train AE loss : 0.16012391448020935, train ANN loss : 3.6775572299957275\n",
      "AE loss : 0.16458842158317566, ANN loss : 3.6378607749938965, Total loss : 20.096702575683594\n",
      "learning rate A :  tf.Tensor(9.969702e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 254 is 0.0770 sec\n",
      "train AE loss : 0.1593654602766037, train ANN loss : 3.680525779724121\n",
      "AE loss : 0.15479344129562378, ANN loss : 3.6378724575042725, Total loss : 19.117216110229492\n",
      "learning rate A :  tf.Tensor(9.969702e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 255 is 0.0790 sec\n",
      "train AE loss : 0.14973396062850952, train ANN loss : 3.673830986022949\n",
      "AE loss : 0.15409433841705322, ANN loss : 3.6381173133850098, Total loss : 19.047550201416016\n",
      "learning rate A :  tf.Tensor(9.969492e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 256 is 0.0766 sec\n",
      "train AE loss : 0.1490519791841507, train ANN loss : 3.678107738494873\n",
      "AE loss : 0.1534017026424408, ANN loss : 3.638364315032959, Total loss : 18.978534698486328\n",
      "learning rate A :  tf.Tensor(9.969282e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 257 is 0.0766 sec\n",
      "train AE loss : 0.14837630093097687, train ANN loss : 3.669895887374878\n",
      "AE loss : 0.15271542966365814, ANN loss : 3.638613700866699, Total loss : 18.91015625\n",
      "learning rate A :  tf.Tensor(9.969072e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 258 is 0.0778 sec\n",
      "train AE loss : 0.14770683646202087, train ANN loss : 3.674724578857422\n",
      "AE loss : 0.15207315981388092, ANN loss : 3.636075496673584, Total loss : 18.843393325805664\n",
      "learning rate A :  tf.Tensor(9.969072e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 259 is 0.0788 sec\n",
      "train AE loss : 0.14701715111732483, train ANN loss : 3.666449785232544\n",
      "AE loss : 0.17268362641334534, ANN loss : 3.6266074180603027, Total loss : 20.894969940185547\n",
      "learning rate A :  tf.Tensor(9.969072e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 260 is 0.0788 sec\n",
      "train AE loss : 0.1670588254928589, train ANN loss : 3.6668918132781982\n",
      "AE loss : 0.17176610231399536, ANN loss : 3.626814842224121, Total loss : 20.803424835205078\n",
      "learning rate A :  tf.Tensor(9.9688616e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 261 is 0.0781 sec\n",
      "train AE loss : 0.16616284847259521, train ANN loss : 3.6657350063323975\n",
      "AE loss : 0.1805325150489807, ANN loss : 3.621903419494629, Total loss : 21.675151824951172\n",
      "learning rate A :  tf.Tensor(9.9688616e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 262 is 0.0796 sec\n",
      "train AE loss : 0.1746511459350586, train ANN loss : 3.6658241748809814\n",
      "AE loss : 0.17950738966464996, ANN loss : 3.62209153175354, Total loss : 21.572830200195312\n",
      "learning rate A :  tf.Tensor(9.968652e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 263 is 0.0779 sec\n",
      "train AE loss : 0.17364932596683502, train ANN loss : 3.666565179824829\n",
      "AE loss : 0.1630115658044815, ANN loss : 3.6237943172454834, Total loss : 19.924951553344727\n",
      "learning rate A :  tf.Tensor(9.968652e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 264 is 0.0821 sec\n",
      "train AE loss : 0.15747754275798798, train ANN loss : 3.6565968990325928\n",
      "AE loss : 0.16449570655822754, ANN loss : 3.6206252574920654, Total loss : 20.070194244384766\n",
      "learning rate A :  tf.Tensor(9.968652e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 265 is 0.0801 sec\n",
      "train AE loss : 0.15883997082710266, train ANN loss : 3.6583874225616455\n",
      "AE loss : 0.16359543800354004, ANN loss : 3.62093186378479, Total loss : 19.9804744720459\n",
      "learning rate A :  tf.Tensor(9.968442e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 266 is 0.0777 sec\n",
      "train AE loss : 0.15796175599098206, train ANN loss : 3.6545042991638184\n",
      "AE loss : 0.16270455718040466, ANN loss : 3.621243476867676, Total loss : 19.891698837280273\n",
      "learning rate A :  tf.Tensor(9.9682315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 267 is 0.0778 sec\n",
      "train AE loss : 0.15709282457828522, train ANN loss : 3.6556458473205566\n",
      "AE loss : 0.16182300448417664, ANN loss : 3.6215577125549316, Total loss : 19.803857803344727\n",
      "learning rate A :  tf.Tensor(9.968022e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 268 is 0.0773 sec\n",
      "train AE loss : 0.15623322129249573, train ANN loss : 3.653208017349243\n",
      "AE loss : 0.18554282188415527, ANN loss : 3.6121344566345215, Total loss : 22.166419982910156\n",
      "learning rate A :  tf.Tensor(9.968022e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 269 is 0.0788 sec\n",
      "train AE loss : 0.17929857969284058, train ANN loss : 3.652250051498413\n",
      "AE loss : 0.18438220024108887, ANN loss : 3.6123545169830322, Total loss : 22.050575256347656\n",
      "learning rate A :  tf.Tensor(9.967812e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 270 is 0.0782 sec\n",
      "train AE loss : 0.1781647801399231, train ANN loss : 3.653710126876831\n",
      "AE loss : 0.18323533236980438, ANN loss : 3.6125807762145996, Total loss : 21.93611717224121\n",
      "learning rate A :  tf.Tensor(9.967601e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 271 is 0.0776 sec\n",
      "train AE loss : 0.17704451084136963, train ANN loss : 3.6522130966186523\n",
      "AE loss : 0.18210211396217346, ANN loss : 3.612812042236328, Total loss : 21.823022842407227\n",
      "learning rate A :  tf.Tensor(9.967391e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 272 is 0.0781 sec\n",
      "train AE loss : 0.17593754827976227, train ANN loss : 3.6442651748657227\n",
      "AE loss : 0.18098169565200806, ANN loss : 3.6130495071411133, Total loss : 21.711219787597656\n",
      "learning rate A :  tf.Tensor(9.9671815e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 273 is 0.0782 sec\n",
      "train AE loss : 0.17484313249588013, train ANN loss : 3.6493661403656006\n",
      "AE loss : 0.17987383902072906, ANN loss : 3.613293170928955, Total loss : 21.600675582885742\n",
      "learning rate A :  tf.Tensor(9.966971e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 274 is 0.0778 sec\n",
      "train AE loss : 0.1737612634897232, train ANN loss : 3.652674913406372\n",
      "AE loss : 0.17877863347530365, ANN loss : 3.613542079925537, Total loss : 21.491405487060547\n",
      "learning rate A :  tf.Tensor(9.966761e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 275 is 0.0781 sec\n",
      "train AE loss : 0.1726919710636139, train ANN loss : 3.654008150100708\n",
      "AE loss : 0.17769595980644226, ANN loss : 3.613797903060913, Total loss : 21.383394241333008\n",
      "learning rate A :  tf.Tensor(9.966551e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 276 is 0.0796 sec\n",
      "train AE loss : 0.17163479328155518, train ANN loss : 3.6556687355041504\n",
      "AE loss : 0.19822297990322113, ANN loss : 3.6071462631225586, Total loss : 23.429443359375\n",
      "learning rate A :  tf.Tensor(9.966551e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 277 is 0.0800 sec\n",
      "train AE loss : 0.19161170721054077, train ANN loss : 3.6503732204437256\n",
      "AE loss : 0.19688113033771515, ANN loss : 3.607308864593506, Total loss : 23.295421600341797\n",
      "learning rate A :  tf.Tensor(9.9663404e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 278 is 0.0776 sec\n",
      "train AE loss : 0.19029857218265533, train ANN loss : 3.655155658721924\n",
      "AE loss : 0.1955554336309433, ANN loss : 3.6074788570404053, Total loss : 23.163021087646484\n",
      "learning rate A :  tf.Tensor(9.966131e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 279 is 0.0786 sec\n",
      "train AE loss : 0.18900132179260254, train ANN loss : 3.6488037109375\n",
      "AE loss : 0.19424563646316528, ANN loss : 3.6076581478118896, Total loss : 23.0322208404541\n",
      "learning rate A :  tf.Tensor(9.9659206e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 280 is 0.0774 sec\n",
      "train AE loss : 0.1877201795578003, train ANN loss : 3.649097442626953\n",
      "AE loss : 0.19295170903205872, ANN loss : 3.607844829559326, Total loss : 22.90301513671875\n",
      "learning rate A :  tf.Tensor(9.96571e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 281 is 0.0769 sec\n",
      "train AE loss : 0.1864548772573471, train ANN loss : 3.646862506866455\n",
      "AE loss : 0.18489603698253632, ANN loss : 3.6067593097686768, Total loss : 22.09636116027832\n",
      "learning rate A :  tf.Tensor(9.96571e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 282 is 0.0792 sec\n",
      "train AE loss : 0.17849454283714294, train ANN loss : 3.6453495025634766\n",
      "AE loss : 0.18368765711784363, ANN loss : 3.607029438018799, Total loss : 21.975793838500977\n",
      "learning rate A :  tf.Tensor(9.965501e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 283 is 0.0780 sec\n",
      "train AE loss : 0.17731444537639618, train ANN loss : 3.6466140747070312\n",
      "AE loss : 0.1750725507736206, ANN loss : 3.6071085929870605, Total loss : 21.114362716674805\n",
      "learning rate A :  tf.Tensor(9.965501e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 284 is 0.0788 sec\n",
      "train AE loss : 0.1688033789396286, train ANN loss : 3.6444172859191895\n",
      "AE loss : 0.18819516897201538, ANN loss : 3.601135015487671, Total loss : 22.420650482177734\n",
      "learning rate A :  tf.Tensor(9.965501e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 285 is 0.0802 sec\n",
      "train AE loss : 0.1815004050731659, train ANN loss : 3.642613649368286\n",
      "AE loss : 0.19917574524879456, ANN loss : 3.596428632736206, Total loss : 23.51400375366211\n",
      "learning rate A :  tf.Tensor(9.965501e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 286 is 0.0787 sec\n",
      "train AE loss : 0.1921103447675705, train ANN loss : 3.6357765197753906\n",
      "AE loss : 0.19769801199436188, ANN loss : 3.5966975688934326, Total loss : 23.366500854492188\n",
      "learning rate A :  tf.Tensor(9.9652905e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 287 is 0.0773 sec\n",
      "train AE loss : 0.19066685438156128, train ANN loss : 3.6383583545684814\n",
      "AE loss : 0.1962396502494812, ANN loss : 3.596975326538086, Total loss : 23.22093963623047\n",
      "learning rate A :  tf.Tensor(9.965081e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 288 is 0.0793 sec\n",
      "train AE loss : 0.18924260139465332, train ANN loss : 3.6374926567077637\n",
      "AE loss : 0.19480036199092865, ANN loss : 3.597261667251587, Total loss : 23.07729721069336\n",
      "learning rate A :  tf.Tensor(9.964871e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 289 is 0.0773 sec\n",
      "train AE loss : 0.1878369003534317, train ANN loss : 3.6378931999206543\n",
      "AE loss : 0.19338002800941467, ANN loss : 3.597555637359619, Total loss : 22.935558319091797\n",
      "learning rate A :  tf.Tensor(9.9646604e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 290 is 0.0772 sec\n",
      "train AE loss : 0.18644960224628448, train ANN loss : 3.636385202407837\n",
      "AE loss : 0.1985241323709488, ANN loss : 3.594085454940796, Total loss : 23.446496963500977\n",
      "learning rate A :  tf.Tensor(9.9646604e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 291 is 0.0795 sec\n",
      "train AE loss : 0.19134847819805145, train ANN loss : 3.637028455734253\n",
      "AE loss : 0.1970231682062149, ANN loss : 3.5943844318389893, Total loss : 23.29669952392578\n",
      "learning rate A :  tf.Tensor(9.964451e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 292 is 0.0782 sec\n",
      "train AE loss : 0.18988217413425446, train ANN loss : 3.636035203933716\n",
      "AE loss : 0.19886142015457153, ANN loss : 3.5916833877563477, Total loss : 23.477827072143555\n",
      "learning rate A :  tf.Tensor(9.964451e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 293 is 0.0775 sec\n",
      "train AE loss : 0.19154666364192963, train ANN loss : 3.6301677227020264\n",
      "AE loss : 0.19732531905174255, ANN loss : 3.5920140743255615, Total loss : 23.324546813964844\n",
      "learning rate A :  tf.Tensor(9.964241e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 294 is 0.0769 sec\n",
      "train AE loss : 0.190046489238739, train ANN loss : 3.6322567462921143\n",
      "AE loss : 0.20578789710998535, ANN loss : 3.5879623889923096, Total loss : 24.166751861572266\n",
      "learning rate A :  tf.Tensor(9.964241e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 295 is 0.0778 sec\n",
      "train AE loss : 0.19817979633808136, train ANN loss : 3.6277031898498535\n",
      "AE loss : 0.20412184298038483, ANN loss : 3.5882794857025146, Total loss : 24.000463485717773\n",
      "learning rate A :  tf.Tensor(9.964031e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 296 is 0.0759 sec\n",
      "train AE loss : 0.1965518593788147, train ANN loss : 3.6244287490844727\n",
      "AE loss : 0.21313653886318207, ANN loss : 3.5843334197998047, Total loss : 24.897987365722656\n",
      "learning rate A :  tf.Tensor(9.964031e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 297 is 0.0786 sec\n",
      "train AE loss : 0.2052251100540161, train ANN loss : 3.6269915103912354\n",
      "AE loss : 0.21132872998714447, ANN loss : 3.584637403488159, Total loss : 24.717510223388672\n",
      "learning rate A :  tf.Tensor(9.963821e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 298 is 0.0766 sec\n",
      "train AE loss : 0.20345750451087952, train ANN loss : 3.6270971298217773\n",
      "AE loss : 0.20954754948616028, ANN loss : 3.584954261779785, Total loss : 24.539710998535156\n",
      "learning rate A :  tf.Tensor(9.963611e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 299 is 0.0763 sec\n",
      "train AE loss : 0.20171625912189484, train ANN loss : 3.62388014793396\n",
      "AE loss : 0.207792267203331, ANN loss : 3.585282564163208, Total loss : 24.36450958251953\n",
      "learning rate A :  tf.Tensor(9.963401e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 300 is 0.0782 sec\n",
      "train AE loss : 0.20000091195106506, train ANN loss : 3.622446060180664\n",
      "AE loss : 0.20606264472007751, ANN loss : 3.585622787475586, Total loss : 24.191884994506836\n",
      "learning rate A :  tf.Tensor(9.963191e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 301 is 0.0778 sec\n",
      "train AE loss : 0.19831089675426483, train ANN loss : 3.626654863357544\n",
      "AE loss : 0.20435766875743866, ANN loss : 3.585972785949707, Total loss : 24.021738052368164\n",
      "learning rate A :  tf.Tensor(9.962981e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 302 is 0.0780 sec\n",
      "train AE loss : 0.19664539396762848, train ANN loss : 3.622821569442749\n",
      "AE loss : 0.2026771605014801, ANN loss : 3.586332082748413, Total loss : 23.854047775268555\n",
      "learning rate A :  tf.Tensor(9.9627716e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 303 is 0.0785 sec\n",
      "train AE loss : 0.19500429928302765, train ANN loss : 3.621239185333252\n",
      "AE loss : 0.22917138040065765, ANN loss : 3.5797066688537598, Total loss : 26.496843338012695\n",
      "learning rate A :  tf.Tensor(9.9627716e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 304 is 0.0816 sec\n",
      "train AE loss : 0.22077123820781708, train ANN loss : 3.6286582946777344\n",
      "AE loss : 0.22708012163639069, ANN loss : 3.579928398132324, Total loss : 26.287939071655273\n",
      "learning rate A :  tf.Tensor(9.962561e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 305 is 0.0776 sec\n",
      "train AE loss : 0.21872548758983612, train ANN loss : 3.6240696907043457\n",
      "AE loss : 0.2250215709209442, ANN loss : 3.580166816711426, Total loss : 26.082324981689453\n",
      "learning rate A :  tf.Tensor(9.962352e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 306 is 0.0766 sec\n",
      "train AE loss : 0.21671178936958313, train ANN loss : 3.624650716781616\n",
      "AE loss : 0.22823433578014374, ANN loss : 3.577568531036377, Total loss : 26.4010009765625\n",
      "learning rate A :  tf.Tensor(9.962352e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 307 is 0.0785 sec\n",
      "train AE loss : 0.21970534324645996, train ANN loss : 3.621311664581299\n",
      "AE loss : 0.22611860930919647, ANN loss : 3.5778398513793945, Total loss : 26.189697265625\n",
      "learning rate A :  tf.Tensor(9.962142e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 308 is 0.0765 sec\n",
      "train AE loss : 0.2176368683576584, train ANN loss : 3.6207592487335205\n",
      "AE loss : 0.2240355908870697, ANN loss : 3.57812762260437, Total loss : 25.981685638427734\n",
      "learning rate A :  tf.Tensor(9.961931e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 309 is 0.0776 sec\n",
      "train AE loss : 0.2156008929014206, train ANN loss : 3.6248204708099365\n",
      "AE loss : 0.213003009557724, ANN loss : 3.5782485008239746, Total loss : 24.87854766845703\n",
      "learning rate A :  tf.Tensor(9.961931e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 310 is 0.0778 sec\n",
      "train AE loss : 0.2046808898448944, train ANN loss : 3.6191213130950928\n",
      "AE loss : 0.2265920639038086, ANN loss : 3.573732614517212, Total loss : 26.232938766479492\n",
      "learning rate A :  tf.Tensor(9.961931e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 311 is 0.0786 sec\n",
      "train AE loss : 0.21782878041267395, train ANN loss : 3.613337278366089\n",
      "AE loss : 0.2244260013103485, ANN loss : 3.5740928649902344, Total loss : 26.016695022583008\n",
      "learning rate A :  tf.Tensor(9.961722e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 312 is 0.0775 sec\n",
      "train AE loss : 0.21571151912212372, train ANN loss : 3.6168458461761475\n",
      "AE loss : 0.22229447960853577, ANN loss : 3.574467420578003, Total loss : 25.803916931152344\n",
      "learning rate A :  tf.Tensor(9.961512e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 313 is 0.0763 sec\n",
      "train AE loss : 0.21362866461277008, train ANN loss : 3.6184191703796387\n",
      "AE loss : 0.24056661128997803, ANN loss : 3.5697238445281982, Total loss : 27.626384735107422\n",
      "learning rate A :  tf.Tensor(9.961512e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 314 is 0.0794 sec\n",
      "train AE loss : 0.23137710988521576, train ANN loss : 3.6135289669036865\n",
      "AE loss : 0.24309305846691132, ANN loss : 3.5673892498016357, Total loss : 27.876697540283203\n",
      "learning rate A :  tf.Tensor(9.961512e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 315 is 0.0814 sec\n",
      "train AE loss : 0.23374691605567932, train ANN loss : 3.6088624000549316\n",
      "AE loss : 0.2405662089586258, ANN loss : 3.567692279815674, Total loss : 27.624313354492188\n",
      "learning rate A :  tf.Tensor(9.961302e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 316 is 0.0785 sec\n",
      "train AE loss : 0.23127616941928864, train ANN loss : 3.6149837970733643\n",
      "AE loss : 0.238082617521286, ANN loss : 3.568016767501831, Total loss : 27.376279830932617\n",
      "learning rate A :  tf.Tensor(9.961092e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 317 is 0.0772 sec\n",
      "train AE loss : 0.22884804010391235, train ANN loss : 3.612312078475952\n",
      "AE loss : 0.23564182221889496, ANN loss : 3.5683624744415283, Total loss : 27.13254165649414\n",
      "learning rate A :  tf.Tensor(9.960883e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 318 is 0.0793 sec\n",
      "train AE loss : 0.2264622300863266, train ANN loss : 3.611999034881592\n",
      "AE loss : 0.23324242234230042, ANN loss : 3.5687270164489746, Total loss : 26.892969131469727\n",
      "learning rate A :  tf.Tensor(9.960672e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 319 is 0.0796 sec\n",
      "train AE loss : 0.22411790490150452, train ANN loss : 3.6068265438079834\n",
      "AE loss : 0.23993423581123352, ANN loss : 3.5657808780670166, Total loss : 27.5592041015625\n",
      "learning rate A :  tf.Tensor(9.960672e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 320 is 0.0796 sec\n",
      "train AE loss : 0.23057082295417786, train ANN loss : 3.6087229251861572\n",
      "AE loss : 0.24237821996212006, ANN loss : 3.5634939670562744, Total loss : 27.801315307617188\n",
      "learning rate A :  tf.Tensor(9.960672e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 321 is 0.0832 sec\n",
      "train AE loss : 0.2329002469778061, train ANN loss : 3.602254629135132\n",
      "AE loss : 0.2535615563392639, ANN loss : 3.5601015090942383, Total loss : 28.916255950927734\n",
      "learning rate A :  tf.Tensor(9.960672e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 322 is 0.0798 sec\n",
      "train AE loss : 0.2437484711408615, train ANN loss : 3.6040642261505127\n",
      "AE loss : 0.25070831179618835, ANN loss : 3.560427665710449, Total loss : 28.631258010864258\n",
      "learning rate A :  tf.Tensor(9.960462e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 323 is 0.0784 sec\n",
      "train AE loss : 0.24095909297466278, train ANN loss : 3.6037471294403076\n",
      "AE loss : 0.24790817499160767, ANN loss : 3.5607802867889404, Total loss : 28.35159683227539\n",
      "learning rate A :  tf.Tensor(9.960253e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 324 is 0.0790 sec\n",
      "train AE loss : 0.23822246491909027, train ANN loss : 3.6025657653808594\n",
      "AE loss : 0.2591472864151001, ANN loss : 3.557559013366699, Total loss : 29.472288131713867\n",
      "learning rate A :  tf.Tensor(9.960253e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 325 is 0.0792 sec\n",
      "train AE loss : 0.24910157918930054, train ANN loss : 3.596468210220337\n",
      "AE loss : 0.2476711869239807, ANN loss : 3.557363271713257, Total loss : 28.324481964111328\n",
      "learning rate A :  tf.Tensor(9.960253e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 326 is 0.0787 sec\n",
      "train AE loss : 0.2377699315547943, train ANN loss : 3.603012800216675\n",
      "AE loss : 0.24483537673950195, ANN loss : 3.557812213897705, Total loss : 28.041351318359375\n",
      "learning rate A :  tf.Tensor(9.9600424e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 327 is 0.0780 sec\n",
      "train AE loss : 0.2350025475025177, train ANN loss : 3.5994718074798584\n",
      "AE loss : 0.2420535385608673, ANN loss : 3.5582849979400635, Total loss : 27.763639450073242\n",
      "learning rate A :  tf.Tensor(9.959833e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 328 is 0.0797 sec\n",
      "train AE loss : 0.23228862881660461, train ANN loss : 3.5984199047088623\n",
      "AE loss : 0.25079384446144104, ANN loss : 3.555293321609497, Total loss : 28.634675979614258\n",
      "learning rate A :  tf.Tensor(9.959833e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 329 is 0.0796 sec\n",
      "train AE loss : 0.2406982034444809, train ANN loss : 3.596757173538208\n",
      "AE loss : 0.24785205721855164, ANN loss : 3.555753469467163, Total loss : 28.340957641601562\n",
      "learning rate A :  tf.Tensor(9.959623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 330 is 0.0785 sec\n",
      "train AE loss : 0.23782944679260254, train ANN loss : 3.5955147743225098\n",
      "AE loss : 0.24496738612651825, ANN loss : 3.5562376976013184, Total loss : 28.052974700927734\n",
      "learning rate A :  tf.Tensor(9.959413e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 331 is 0.0777 sec\n",
      "train AE loss : 0.23501679301261902, train ANN loss : 3.594968795776367\n",
      "AE loss : 0.24213728308677673, ANN loss : 3.556746006011963, Total loss : 27.77047348022461\n",
      "learning rate A :  tf.Tensor(9.9592035e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 332 is 0.0776 sec\n",
      "train AE loss : 0.23225773870944977, train ANN loss : 3.592543601989746\n",
      "AE loss : 0.23936031758785248, ANN loss : 3.5572750568389893, Total loss : 27.493305206298828\n",
      "learning rate A :  tf.Tensor(9.958993e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 333 is 0.0780 sec\n",
      "train AE loss : 0.22955146431922913, train ANN loss : 3.5975775718688965\n",
      "AE loss : 0.28902286291122437, ANN loss : 3.550016403198242, Total loss : 32.452301025390625\n",
      "learning rate A :  tf.Tensor(9.958993e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 334 is 0.0788 sec\n",
      "train AE loss : 0.27790457010269165, train ANN loss : 3.5931529998779297\n",
      "AE loss : 0.28526216745376587, ANN loss : 3.5501463413238525, Total loss : 32.07636260986328\n",
      "learning rate A :  tf.Tensor(9.9587836e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 335 is 0.0775 sec\n",
      "train AE loss : 0.274223268032074, train ANN loss : 3.595752239227295\n",
      "AE loss : 0.2732599377632141, ANN loss : 3.5493757724761963, Total loss : 30.875370025634766\n",
      "learning rate A :  tf.Tensor(9.9587836e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 336 is 0.1066 sec\n",
      "train AE loss : 0.26233991980552673, train ANN loss : 3.5890228748321533\n",
      "AE loss : 0.2697751820087433, ANN loss : 3.5497093200683594, Total loss : 30.5272274017334\n",
      "learning rate A :  tf.Tensor(9.958574e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 337 is 0.0935 sec\n",
      "train AE loss : 0.2589368224143982, train ANN loss : 3.59409236907959\n",
      "AE loss : 0.26636049151420593, ANN loss : 3.5500736236572266, Total loss : 30.18612289428711\n",
      "learning rate A :  tf.Tensor(9.958364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 338 is 0.0790 sec\n",
      "train AE loss : 0.2556047737598419, train ANN loss : 3.593038558959961\n",
      "AE loss : 0.26301509141921997, ANN loss : 3.5504701137542725, Total loss : 29.85198211669922\n",
      "learning rate A :  tf.Tensor(9.958154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 339 is 0.0772 sec\n",
      "train AE loss : 0.2523413896560669, train ANN loss : 3.594572067260742\n",
      "AE loss : 0.2555861473083496, ANN loss : 3.550238609313965, Total loss : 29.10885238647461\n",
      "learning rate A :  tf.Tensor(9.958154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 340 is 0.0898 sec\n",
      "train AE loss : 0.24496209621429443, train ANN loss : 3.589797258377075\n",
      "AE loss : 0.2524145841598511, ANN loss : 3.550767421722412, Total loss : 28.792226791381836\n",
      "learning rate A :  tf.Tensor(9.957945e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 341 is 0.0772 sec\n",
      "train AE loss : 0.24187253415584564, train ANN loss : 3.587279796600342\n",
      "AE loss : 0.2493060827255249, ANN loss : 3.5513229370117188, Total loss : 28.48193359375\n",
      "learning rate A :  tf.Tensor(9.9577344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 342 is 0.0771 sec\n",
      "train AE loss : 0.2388463020324707, train ANN loss : 3.5918335914611816\n",
      "AE loss : 0.246260404586792, ANN loss : 3.5519039630889893, Total loss : 28.177947998046875\n",
      "learning rate A :  tf.Tensor(9.957525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 343 is 0.0770 sec\n",
      "train AE loss : 0.23588208854198456, train ANN loss : 3.5917892456054688\n",
      "AE loss : 0.27227500081062317, ANN loss : 3.5464789867401123, Total loss : 30.77397918701172\n",
      "learning rate A :  tf.Tensor(9.957525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 344 is 0.0816 sec\n",
      "train AE loss : 0.26108258962631226, train ANN loss : 3.5836973190307617\n",
      "AE loss : 0.3016243875026703, ANN loss : 3.542809247970581, Total loss : 33.70524978637695\n",
      "learning rate A :  tf.Tensor(9.957525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 345 is 0.0882 sec\n",
      "train AE loss : 0.28959161043167114, train ANN loss : 3.5909476280212402\n",
      "AE loss : 0.2641305923461914, ANN loss : 3.5449039936065674, Total loss : 29.957963943481445\n",
      "learning rate A :  tf.Tensor(9.957525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 346 is 0.0807 sec\n",
      "train AE loss : 0.25288575887680054, train ANN loss : 3.586158037185669\n",
      "AE loss : 0.2543264329433441, ANN loss : 3.5454769134521484, Total loss : 28.978120803833008\n",
      "learning rate A :  tf.Tensor(9.957525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 347 is 0.0790 sec\n",
      "train AE loss : 0.24322740733623505, train ANN loss : 3.5785534381866455\n",
      "AE loss : 0.301727294921875, ANN loss : 3.537795066833496, Total loss : 33.71052551269531\n",
      "learning rate A :  tf.Tensor(9.957525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 348 is 0.0796 sec\n",
      "train AE loss : 0.28930485248565674, train ANN loss : 3.5795788764953613\n",
      "AE loss : 0.32156863808631897, ANN loss : 3.5353448390960693, Total loss : 35.69221115112305\n",
      "learning rate A :  tf.Tensor(9.957525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 349 is 0.0813 sec\n",
      "train AE loss : 0.30860215425491333, train ANN loss : 3.582388401031494\n",
      "AE loss : 0.27964359521865845, ANN loss : 3.5369484424591064, Total loss : 31.50130844116211\n",
      "learning rate A :  tf.Tensor(9.957525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 350 is 0.0796 sec\n",
      "train AE loss : 0.26762455701828003, train ANN loss : 3.577390432357788\n",
      "AE loss : 0.2602294385433197, ANN loss : 3.539149284362793, Total loss : 29.56209373474121\n",
      "learning rate A :  tf.Tensor(9.957525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 351 is 0.0892 sec\n",
      "train AE loss : 0.2486899197101593, train ANN loss : 3.5772528648376465\n",
      "AE loss : 0.2968229353427887, ANN loss : 3.5319478511810303, Total loss : 33.21424102783203\n",
      "learning rate A :  tf.Tensor(9.957525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 352 is 0.0839 sec\n",
      "train AE loss : 0.284259557723999, train ANN loss : 3.5705058574676514\n",
      "AE loss : 0.2923356890678406, ANN loss : 3.532417058944702, Total loss : 32.76598358154297\n",
      "learning rate A :  tf.Tensor(9.957315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 353 is 0.0786 sec\n",
      "train AE loss : 0.2798840403556824, train ANN loss : 3.570657968521118\n",
      "AE loss : 0.2879534959793091, ANN loss : 3.5329294204711914, Total loss : 32.328277587890625\n",
      "learning rate A :  tf.Tensor(9.957105e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 354 is 0.0850 sec\n",
      "train AE loss : 0.27561163902282715, train ANN loss : 3.573996067047119\n",
      "AE loss : 0.34110718965530396, ANN loss : 3.528320074081421, Total loss : 37.6390380859375\n",
      "learning rate A :  tf.Tensor(9.957105e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 355 is 0.0824 sec\n",
      "train AE loss : 0.32743772864341736, train ANN loss : 3.5712685585021973\n",
      "AE loss : 0.3158615827560425, ANN loss : 3.5273430347442627, Total loss : 35.113502502441406\n",
      "learning rate A :  tf.Tensor(9.957105e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 356 is 0.0803 sec\n",
      "train AE loss : 0.302659273147583, train ANN loss : 3.567152261734009\n",
      "AE loss : 0.27635976672172546, ANN loss : 3.5307629108428955, Total loss : 31.166738510131836\n",
      "learning rate A :  tf.Tensor(9.957105e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 357 is 0.0800 sec\n",
      "train AE loss : 0.26406046748161316, train ANN loss : 3.5656447410583496\n",
      "AE loss : 0.28615260124206543, ANN loss : 3.5277769565582275, Total loss : 32.143035888671875\n",
      "learning rate A :  tf.Tensor(9.957105e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 358 is 0.0794 sec\n",
      "train AE loss : 0.2734445631504059, train ANN loss : 3.5624806880950928\n",
      "AE loss : 0.2817634642124176, ANN loss : 3.5284857749938965, Total loss : 31.704832077026367\n",
      "learning rate A :  tf.Tensor(9.9568955e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 359 is 0.0776 sec\n",
      "train AE loss : 0.26917746663093567, train ANN loss : 3.563342809677124\n",
      "AE loss : 0.329023152589798, ANN loss : 3.521944284439087, Total loss : 36.424259185791016\n",
      "learning rate A :  tf.Tensor(9.9568955e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 360 is 0.0804 sec\n",
      "train AE loss : 0.31507691740989685, train ANN loss : 3.557997465133667\n",
      "AE loss : 0.32351264357566833, ANN loss : 3.522235631942749, Total loss : 35.87350082397461\n",
      "learning rate A :  tf.Tensor(9.956686e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 361 is 0.0946 sec\n",
      "train AE loss : 0.3096955120563507, train ANN loss : 3.564333915710449\n",
      "AE loss : 0.35537073016166687, ANN loss : 3.519836187362671, Total loss : 39.056907653808594\n",
      "learning rate A :  tf.Tensor(9.956686e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 362 is 0.0810 sec\n",
      "train AE loss : 0.3406912684440613, train ANN loss : 3.558926582336426\n",
      "AE loss : 0.34911680221557617, ANN loss : 3.519824743270874, Total loss : 38.4315071105957\n",
      "learning rate A :  tf.Tensor(9.956476e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 363 is 0.0769 sec\n",
      "train AE loss : 0.33457374572753906, train ANN loss : 3.5614492893218994\n",
      "AE loss : 0.32882076501846313, ANN loss : 3.5190908908843994, Total loss : 36.40116882324219\n",
      "learning rate A :  tf.Tensor(9.956476e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 364 is 0.0792 sec\n",
      "train AE loss : 0.3146355152130127, train ANN loss : 3.557849168777466\n",
      "AE loss : 0.2965632975101471, ANN loss : 3.5212466716766357, Total loss : 33.17757797241211\n",
      "learning rate A :  tf.Tensor(9.956476e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 365 is 0.0815 sec\n",
      "train AE loss : 0.28312668204307556, train ANN loss : 3.556122303009033\n",
      "AE loss : 0.2917654514312744, ANN loss : 3.5219759941101074, Total loss : 32.69852066040039\n",
      "learning rate A :  tf.Tensor(9.956266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 366 is 0.0776 sec\n",
      "train AE loss : 0.2784646451473236, train ANN loss : 3.5571606159210205\n",
      "AE loss : 0.2870904505252838, ANN loss : 3.5227489471435547, Total loss : 32.23179244995117\n",
      "learning rate A :  tf.Tensor(9.956056e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 367 is 0.0782 sec\n",
      "train AE loss : 0.2739233672618866, train ANN loss : 3.557295560836792\n",
      "AE loss : 0.30923354625701904, ANN loss : 3.5182673931121826, Total loss : 34.44162368774414\n",
      "learning rate A :  tf.Tensor(9.956056e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 368 is 0.0803 sec\n",
      "train AE loss : 0.29533860087394714, train ANN loss : 3.5574185848236084\n",
      "AE loss : 0.3040671944618225, ANN loss : 3.518913745880127, Total loss : 33.92563247680664\n",
      "learning rate A :  tf.Tensor(9.955846e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 369 is 0.0772 sec\n",
      "train AE loss : 0.2903127372264862, train ANN loss : 3.5564382076263428\n",
      "AE loss : 0.3588440716266632, ANN loss : 3.5136847496032715, Total loss : 39.39809036254883\n",
      "learning rate A :  tf.Tensor(9.955846e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 370 is 0.0794 sec\n",
      "train AE loss : 0.343563050031662, train ANN loss : 3.552725076675415\n",
      "AE loss : 0.35228991508483887, ANN loss : 3.5137875080108643, Total loss : 38.74277877807617\n",
      "learning rate A :  tf.Tensor(9.955637e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 371 is 0.0785 sec\n",
      "train AE loss : 0.33716216683387756, train ANN loss : 3.5502989292144775\n",
      "AE loss : 0.36785417795181274, ANN loss : 3.5123846530914307, Total loss : 40.29780578613281\n",
      "learning rate A :  tf.Tensor(9.955637e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 372 is 0.0790 sec\n",
      "train AE loss : 0.3522443473339081, train ANN loss : 3.5514068603515625\n",
      "AE loss : 0.3255143165588379, ANN loss : 3.513195753097534, Total loss : 36.06462478637695\n",
      "learning rate A :  tf.Tensor(9.955637e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 373 is 0.0789 sec\n",
      "train AE loss : 0.31087687611579895, train ANN loss : 3.5494751930236816\n",
      "AE loss : 0.31979167461395264, ANN loss : 3.5137534141540527, Total loss : 35.492923736572266\n",
      "learning rate A :  tf.Tensor(9.955426e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 374 is 0.0771 sec\n",
      "train AE loss : 0.3053100109100342, train ANN loss : 3.549217462539673\n",
      "AE loss : 0.3142225742340088, ANN loss : 3.5143721103668213, Total loss : 34.93663024902344\n",
      "learning rate A :  tf.Tensor(9.955216e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 375 is 0.0766 sec\n",
      "train AE loss : 0.2998964488506317, train ANN loss : 3.5525994300842285\n",
      "AE loss : 0.3084680438041687, ANN loss : 3.514136552810669, Total loss : 34.360939025878906\n",
      "learning rate A :  tf.Tensor(9.955216e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 376 is 0.0789 sec\n",
      "train AE loss : 0.29423558712005615, train ANN loss : 3.548372745513916\n",
      "AE loss : 0.3416537940502167, ANN loss : 3.509631395339966, Total loss : 37.675010681152344\n",
      "learning rate A :  tf.Tensor(9.955216e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 377 is 0.1109 sec\n",
      "train AE loss : 0.32640889286994934, train ANN loss : 3.544752359390259\n",
      "AE loss : 0.3749447166919708, ANN loss : 3.5073041915893555, Total loss : 41.00177764892578\n",
      "learning rate A :  tf.Tensor(9.955216e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 378 is 0.1002 sec\n",
      "train AE loss : 0.35877367854118347, train ANN loss : 3.547466278076172\n",
      "AE loss : 0.3586410582065582, ANN loss : 3.506359100341797, Total loss : 39.37046432495117\n",
      "learning rate A :  tf.Tensor(9.955216e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 379 is 0.0833 sec\n",
      "train AE loss : 0.3428196310997009, train ANN loss : 3.542963743209839\n",
      "AE loss : 0.35181376338005066, ANN loss : 3.506645441055298, Total loss : 38.68802261352539\n",
      "learning rate A :  tf.Tensor(9.955007e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 380 is 0.0772 sec\n",
      "train AE loss : 0.33617067337036133, train ANN loss : 3.5438475608825684\n",
      "AE loss : 0.3265782594680786, ANN loss : 3.5074734687805176, Total loss : 36.16529846191406\n",
      "learning rate A :  tf.Tensor(9.955007e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 381 is 0.0796 sec\n",
      "train AE loss : 0.31160974502563477, train ANN loss : 3.5394256114959717\n",
      "AE loss : 0.33676016330718994, ANN loss : 3.5051748752593994, Total loss : 37.18119430541992\n",
      "learning rate A :  tf.Tensor(9.955007e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 382 is 0.0800 sec\n",
      "train AE loss : 0.32148319482803345, train ANN loss : 3.5400781631469727\n",
      "AE loss : 0.37723106145858765, ANN loss : 3.5016040802001953, Total loss : 41.224708557128906\n",
      "learning rate A :  tf.Tensor(9.955007e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 383 is 0.0792 sec\n",
      "train AE loss : 0.36084720492362976, train ANN loss : 3.5379250049591064\n",
      "AE loss : 0.3776980936527252, ANN loss : 3.5000648498535156, Total loss : 41.269874572753906\n",
      "learning rate A :  tf.Tensor(9.955007e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 384 is 0.0791 sec\n",
      "train AE loss : 0.3612912893295288, train ANN loss : 3.533046245574951\n",
      "AE loss : 0.37011006474494934, ANN loss : 3.5002353191375732, Total loss : 40.51123809814453\n",
      "learning rate A :  tf.Tensor(9.954797e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 385 is 0.0788 sec\n",
      "train AE loss : 0.35389718413352966, train ANN loss : 3.536425828933716\n",
      "AE loss : 0.36274945735931396, ANN loss : 3.5005013942718506, Total loss : 39.77544403076172\n",
      "learning rate A :  tf.Tensor(9.954587e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 386 is 0.0779 sec\n",
      "train AE loss : 0.34672871232032776, train ANN loss : 3.539175271987915\n",
      "AE loss : 0.35458120703697205, ANN loss : 3.499647855758667, Total loss : 38.957767486572266\n",
      "learning rate A :  tf.Tensor(9.954587e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 387 is 0.0806 sec\n",
      "train AE loss : 0.3387865126132965, train ANN loss : 3.5337908267974854\n",
      "AE loss : 0.36044102907180786, ANN loss : 3.4980013370513916, Total loss : 39.5421028137207\n",
      "learning rate A :  tf.Tensor(9.954587e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 388 is 0.0805 sec\n",
      "train AE loss : 0.3444889485836029, train ANN loss : 3.536189317703247\n",
      "AE loss : 0.36728575825691223, ANN loss : 3.4963924884796143, Total loss : 40.22496795654297\n",
      "learning rate A :  tf.Tensor(9.954587e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 389 is 0.0809 sec\n",
      "train AE loss : 0.3511486351490021, train ANN loss : 3.5314133167266846\n",
      "AE loss : 0.3598673641681671, ANN loss : 3.496809482574463, Total loss : 39.48354721069336\n",
      "learning rate A :  tf.Tensor(9.954377e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 390 is 0.0776 sec\n",
      "train AE loss : 0.34393075108528137, train ANN loss : 3.5362415313720703\n",
      "AE loss : 0.3701428472995758, ANN loss : 3.4950313568115234, Total loss : 40.509315490722656\n",
      "learning rate A :  tf.Tensor(9.954377e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 391 is 0.0803 sec\n",
      "train AE loss : 0.3539462089538574, train ANN loss : 3.5341296195983887\n",
      "AE loss : 0.362589955329895, ANN loss : 3.4954490661621094, Total loss : 39.75444412231445\n",
      "learning rate A :  tf.Tensor(9.954167e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 392 is 0.0791 sec\n",
      "train AE loss : 0.34659573435783386, train ANN loss : 3.532681465148926\n",
      "AE loss : 0.37747201323509216, ANN loss : 3.493410587310791, Total loss : 41.2406120300293\n",
      "learning rate A :  tf.Tensor(9.954167e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 393 is 0.0799 sec\n",
      "train AE loss : 0.36108720302581787, train ANN loss : 3.528458595275879\n",
      "AE loss : 0.3696393370628357, ANN loss : 3.493781328201294, Total loss : 40.45771026611328\n",
      "learning rate A :  tf.Tensor(9.9539575e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 394 is 0.0787 sec\n",
      "train AE loss : 0.353463739156723, train ANN loss : 3.5300235748291016\n",
      "AE loss : 0.38674160838127136, ANN loss : 3.491685628890991, Total loss : 42.16584777832031\n",
      "learning rate A :  tf.Tensor(9.9539575e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 395 is 0.0805 sec\n",
      "train AE loss : 0.3700971305370331, train ANN loss : 3.5223729610443115\n",
      "AE loss : 0.38745740056037903, ANN loss : 3.4902331829071045, Total loss : 42.23596954345703\n",
      "learning rate A :  tf.Tensor(9.9539575e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 396 is 0.0806 sec\n",
      "train AE loss : 0.370774507522583, train ANN loss : 3.5249414443969727\n",
      "AE loss : 0.3792131245136261, ANN loss : 3.4905717372894287, Total loss : 41.411888122558594\n",
      "learning rate A :  tf.Tensor(9.953748e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 397 is 0.0787 sec\n",
      "train AE loss : 0.36275017261505127, train ANN loss : 3.5242910385131836\n",
      "AE loss : 0.37792497873306274, ANN loss : 3.4893741607666016, Total loss : 41.28187561035156\n",
      "learning rate A :  tf.Tensor(9.953748e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 398 is 0.0806 sec\n",
      "train AE loss : 0.36148419976234436, train ANN loss : 3.5229685306549072\n",
      "AE loss : 0.36995401978492737, ANN loss : 3.4898860454559326, Total loss : 40.485286712646484\n",
      "learning rate A :  tf.Tensor(9.953538e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 399 is 0.0780 sec\n",
      "train AE loss : 0.35373252630233765, train ANN loss : 3.522390365600586\n",
      "AE loss : 0.36224275827407837, ANN loss : 3.490490198135376, Total loss : 39.71476745605469\n",
      "learning rate A :  tf.Tensor(9.953329e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 400 is 0.0903 sec\n",
      "train AE loss : 0.34623485803604126, train ANN loss : 3.5237669944763184\n",
      "AE loss : 0.3893628716468811, ANN loss : 3.48738694190979, Total loss : 42.423675537109375\n",
      "learning rate A :  tf.Tensor(9.953329e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 401 is 0.0805 sec\n",
      "train AE loss : 0.3725869953632355, train ANN loss : 3.5250604152679443\n",
      "AE loss : 0.400962769985199, ANN loss : 3.4855666160583496, Total loss : 43.58184051513672\n",
      "learning rate A :  tf.Tensor(9.953329e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 402 is 0.0821 sec\n",
      "train AE loss : 0.38384324312210083, train ANN loss : 3.5202338695526123\n",
      "AE loss : 0.3921337127685547, ANN loss : 3.4858551025390625, Total loss : 42.69922637939453\n",
      "learning rate A :  tf.Tensor(9.9531186e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 403 is 0.0841 sec\n",
      "train AE loss : 0.3752460479736328, train ANN loss : 3.518908977508545\n",
      "AE loss : 0.398617684841156, ANN loss : 3.484251022338867, Total loss : 43.34601593017578\n",
      "learning rate A :  tf.Tensor(9.9531186e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 404 is 0.0809 sec\n",
      "train AE loss : 0.3815094530582428, train ANN loss : 3.5187747478485107\n",
      "AE loss : 0.3976042568683624, ANN loss : 3.4829001426696777, Total loss : 43.24332809448242\n",
      "learning rate A :  tf.Tensor(9.9531186e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 405 is 0.0820 sec\n",
      "train AE loss : 0.38046014308929443, train ANN loss : 3.5124528408050537\n",
      "AE loss : 0.38880494236946106, ANN loss : 3.483332633972168, Total loss : 42.363826751708984\n",
      "learning rate A :  tf.Tensor(9.952909e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 406 is 0.0794 sec\n",
      "train AE loss : 0.37189456820487976, train ANN loss : 3.5160562992095947\n",
      "AE loss : 0.4059138000011444, ANN loss : 3.4811248779296875, Total loss : 44.072505950927734\n",
      "learning rate A :  tf.Tensor(9.952909e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 407 is 0.0794 sec\n",
      "train AE loss : 0.3884683847427368, train ANN loss : 3.518704891204834\n",
      "AE loss : 0.4085449278354645, ANN loss : 3.479642868041992, Total loss : 44.334136962890625\n",
      "learning rate A :  tf.Tensor(9.952909e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 408 is 0.0811 sec\n",
      "train AE loss : 0.39094772934913635, train ANN loss : 3.514233112335205\n",
      "AE loss : 0.3992804288864136, ANN loss : 3.480041265487671, Total loss : 43.4080810546875\n",
      "learning rate A :  tf.Tensor(9.9526995e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 409 is 0.0776 sec\n",
      "train AE loss : 0.3819272518157959, train ANN loss : 3.5110654830932617\n",
      "AE loss : 0.39033108949661255, ANN loss : 3.480557441711426, Total loss : 42.51366424560547\n",
      "learning rate A :  tf.Tensor(9.952489e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 410 is 0.0786 sec\n",
      "train AE loss : 0.37321925163269043, train ANN loss : 3.517009973526001\n",
      "AE loss : 0.3816847503185272, ANN loss : 3.4811878204345703, Total loss : 41.649662017822266\n",
      "learning rate A :  tf.Tensor(9.95228e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 411 is 0.0795 sec\n",
      "train AE loss : 0.36481231451034546, train ANN loss : 3.5168814659118652\n",
      "AE loss : 0.4230569005012512, ANN loss : 3.477667808532715, Total loss : 45.78335952758789\n",
      "learning rate A :  tf.Tensor(9.95228e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 412 is 0.0799 sec\n",
      "train AE loss : 0.4049930274486542, train ANN loss : 3.5143349170684814\n",
      "AE loss : 0.43788760900497437, ANN loss : 3.4760947227478027, Total loss : 47.26485824584961\n",
      "learning rate A :  tf.Tensor(9.95228e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 413 is 0.0805 sec\n",
      "train AE loss : 0.41936028003692627, train ANN loss : 3.5152413845062256\n",
      "AE loss : 0.4029201567173004, ANN loss : 3.4760689735412598, Total loss : 43.768089294433594\n",
      "learning rate A :  tf.Tensor(9.95228e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 414 is 0.0818 sec\n",
      "train AE loss : 0.38525763154029846, train ANN loss : 3.510331869125366\n",
      "AE loss : 0.3961610198020935, ANN loss : 3.475358724594116, Total loss : 43.091461181640625\n",
      "learning rate A :  tf.Tensor(9.95228e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 415 is 0.0804 sec\n",
      "train AE loss : 0.3786441385746002, train ANN loss : 3.5084805488586426\n",
      "AE loss : 0.38716253638267517, ANN loss : 3.4761178493499756, Total loss : 42.1923713684082\n",
      "learning rate A :  tf.Tensor(9.95207e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 416 is 0.0789 sec\n",
      "train AE loss : 0.36990877985954285, train ANN loss : 3.509458065032959\n",
      "AE loss : 0.42894500494003296, ANN loss : 3.4722166061401367, Total loss : 46.3667106628418\n",
      "learning rate A :  tf.Tensor(9.95207e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 417 is 0.0823 sec\n",
      "train AE loss : 0.4104278087615967, train ANN loss : 3.505666732788086\n",
      "AE loss : 0.4692535102367401, ANN loss : 3.47055721282959, Total loss : 50.395904541015625\n",
      "learning rate A :  tf.Tensor(9.95207e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 418 is 0.0790 sec\n",
      "train AE loss : 0.44958582520484924, train ANN loss : 3.5041463375091553\n",
      "AE loss : 0.4574652314186096, ANN loss : 3.4703760147094727, Total loss : 49.21690368652344\n",
      "learning rate A :  tf.Tensor(9.9518606e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 419 is 0.0775 sec\n",
      "train AE loss : 0.4380880296230316, train ANN loss : 3.5065701007843018\n",
      "AE loss : 0.4273345470428467, ANN loss : 3.469541072845459, Total loss : 46.20299530029297\n",
      "learning rate A :  tf.Tensor(9.9518606e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 420 is 0.0811 sec\n",
      "train AE loss : 0.4086887538433075, train ANN loss : 3.501081705093384\n",
      "AE loss : 0.4170817732810974, ANN loss : 3.4700229167938232, Total loss : 45.17820358276367\n",
      "learning rate A :  tf.Tensor(9.951651e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 421 is 0.0778 sec\n",
      "train AE loss : 0.39872613549232483, train ANN loss : 3.503786087036133\n",
      "AE loss : 0.40865421295166016, ANN loss : 3.469312906265259, Total loss : 44.33473587036133\n",
      "learning rate A :  tf.Tensor(9.951651e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 422 is 0.0790 sec\n",
      "train AE loss : 0.3904808759689331, train ANN loss : 3.501512289047241\n",
      "AE loss : 0.3990601897239685, ANN loss : 3.4701054096221924, Total loss : 43.376121520996094\n",
      "learning rate A :  tf.Tensor(9.951441e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 423 is 0.0778 sec\n",
      "train AE loss : 0.38117432594299316, train ANN loss : 3.500779628753662\n",
      "AE loss : 0.4492727816104889, ANN loss : 3.4658427238464355, Total loss : 48.39311981201172\n",
      "learning rate A :  tf.Tensor(9.951441e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 424 is 0.0795 sec\n",
      "train AE loss : 0.4298451244831085, train ANN loss : 3.502887725830078\n",
      "AE loss : 0.43810853362083435, ANN loss : 3.4661381244659424, Total loss : 47.27699279785156\n",
      "learning rate A :  tf.Tensor(9.951231e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 425 is 0.0784 sec\n",
      "train AE loss : 0.4189903438091278, train ANN loss : 3.500265121459961\n",
      "AE loss : 0.4911186695098877, ANN loss : 3.4644575119018555, Total loss : 52.57632064819336\n",
      "learning rate A :  tf.Tensor(9.951231e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 426 is 0.0794 sec\n",
      "train AE loss : 0.47048795223236084, train ANN loss : 3.4966821670532227\n",
      "AE loss : 0.47826895117759705, ANN loss : 3.464162588119507, Total loss : 51.29106140136719\n",
      "learning rate A :  tf.Tensor(9.9510224e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 427 is 0.0779 sec\n",
      "train AE loss : 0.457966148853302, train ANN loss : 3.5008955001831055\n",
      "AE loss : 0.4659239947795868, ANN loss : 3.4640841484069824, Total loss : 50.05648422241211\n",
      "learning rate A :  tf.Tensor(9.950812e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 428 is 0.0769 sec\n",
      "train AE loss : 0.44594264030456543, train ANN loss : 3.503274917602539\n",
      "AE loss : 0.4647769033908844, ANN loss : 3.4626927375793457, Total loss : 49.94038772583008\n",
      "learning rate A :  tf.Tensor(9.950812e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 429 is 0.0797 sec\n",
      "train AE loss : 0.44475048780441284, train ANN loss : 3.495363712310791\n",
      "AE loss : 0.45291778445243835, ANN loss : 3.46289324760437, Total loss : 48.754669189453125\n",
      "learning rate A :  tf.Tensor(9.9506025e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 430 is 0.0768 sec\n",
      "train AE loss : 0.4332190454006195, train ANN loss : 3.4946398735046387\n",
      "AE loss : 0.4433163106441498, ANN loss : 3.4619510173797607, Total loss : 47.7935791015625\n",
      "learning rate A :  tf.Tensor(9.9506025e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 431 is 0.0784 sec\n",
      "train AE loss : 0.42382070422172546, train ANN loss : 3.493055582046509\n",
      "AE loss : 0.4322650730609894, ANN loss : 3.4625346660614014, Total loss : 46.68904113769531\n",
      "learning rate A :  tf.Tensor(9.950392e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 432 is 0.0773 sec\n",
      "train AE loss : 0.4130948483943939, train ANN loss : 3.4976682662963867\n",
      "AE loss : 0.42163071036338806, ANN loss : 3.463256359100342, Total loss : 45.62632751464844\n",
      "learning rate A :  tf.Tensor(9.9501834e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 433 is 0.0770 sec\n",
      "train AE loss : 0.4027821123600006, train ANN loss : 3.4950084686279297\n",
      "AE loss : 0.4631565511226654, ANN loss : 3.4597272872924805, Total loss : 49.7753791809082\n",
      "learning rate A :  tf.Tensor(9.9501834e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 434 is 0.0795 sec\n",
      "train AE loss : 0.44300612807273865, train ANN loss : 3.500131845474243\n",
      "AE loss : 0.4512774348258972, ANN loss : 3.460080862045288, Total loss : 48.587825775146484\n",
      "learning rate A :  tf.Tensor(9.949974e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 435 is 0.0776 sec\n",
      "train AE loss : 0.43146300315856934, train ANN loss : 3.4956793785095215\n",
      "AE loss : 0.43986254930496216, ANN loss : 3.4606032371520996, Total loss : 47.44685745239258\n",
      "learning rate A :  tf.Tensor(9.9497636e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 436 is 0.0774 sec\n",
      "train AE loss : 0.4203800559043884, train ANN loss : 3.4906980991363525\n",
      "AE loss : 0.5118197202682495, ANN loss : 3.4581661224365234, Total loss : 54.64013671875\n",
      "learning rate A :  tf.Tensor(9.9497636e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 437 is 0.0795 sec\n",
      "train AE loss : 0.49025318026542664, train ANN loss : 3.4913811683654785\n",
      "AE loss : 0.484637051820755, ANN loss : 3.4560678005218506, Total loss : 51.919776916503906\n",
      "learning rate A :  tf.Tensor(9.9497636e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 438 is 0.0795 sec\n",
      "train AE loss : 0.4637130796909332, train ANN loss : 3.4947092533111572\n",
      "AE loss : 0.471802681684494, ANN loss : 3.4562485218048096, Total loss : 50.63651657104492\n",
      "learning rate A :  tf.Tensor(9.949554e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 439 is 0.0768 sec\n",
      "train AE loss : 0.4512331187725067, train ANN loss : 3.490469217300415\n",
      "AE loss : 0.4594841003417969, ANN loss : 3.456604480743408, Total loss : 49.4050178527832\n",
      "learning rate A :  tf.Tensor(9.9493445e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 440 is 0.0774 sec\n",
      "train AE loss : 0.43926113843917847, train ANN loss : 3.4900248050689697\n",
      "AE loss : 0.4395731985569, ANN loss : 3.4563465118408203, Total loss : 47.413665771484375\n",
      "learning rate A :  tf.Tensor(9.9493445e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 441 is 0.0797 sec\n",
      "train AE loss : 0.41990455985069275, train ANN loss : 3.491328477859497\n",
      "AE loss : 0.42851126194000244, ANN loss : 3.4572300910949707, Total loss : 46.30835723876953\n",
      "learning rate A :  tf.Tensor(9.949135e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 442 is 0.0776 sec\n",
      "train AE loss : 0.40918222069740295, train ANN loss : 3.491773843765259\n",
      "AE loss : 0.4178808629512787, ANN loss : 3.458265781402588, Total loss : 45.2463493347168\n",
      "learning rate A :  tf.Tensor(9.9489254e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 443 is 0.0782 sec\n",
      "train AE loss : 0.3988860845565796, train ANN loss : 3.491780996322632\n",
      "AE loss : 0.4789341688156128, ANN loss : 3.452817678451538, Total loss : 51.34623718261719\n",
      "learning rate A :  tf.Tensor(9.9489254e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 444 is 0.0799 sec\n",
      "train AE loss : 0.4580274224281311, train ANN loss : 3.48760724067688\n",
      "AE loss : 0.46621665358543396, ANN loss : 3.4531853199005127, Total loss : 50.07484817504883\n",
      "learning rate A :  tf.Tensor(9.948715e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 445 is 0.0774 sec\n",
      "train AE loss : 0.44567134976387024, train ANN loss : 3.4884092807769775\n",
      "AE loss : 0.5290868878364563, ANN loss : 3.4512014389038086, Total loss : 56.35988998413086\n",
      "learning rate A :  tf.Tensor(9.948715e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 446 is 0.0797 sec\n",
      "train AE loss : 0.5067185163497925, train ANN loss : 3.487558126449585\n",
      "AE loss : 0.4939049482345581, ANN loss : 3.4492900371551514, Total loss : 52.839778900146484\n",
      "learning rate A :  tf.Tensor(9.948715e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 447 is 0.0801 sec\n",
      "train AE loss : 0.47243592143058777, train ANN loss : 3.490626335144043\n",
      "AE loss : 0.4804788827896118, ANN loss : 3.449587106704712, Total loss : 51.497474670410156\n",
      "learning rate A :  tf.Tensor(9.948506e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 448 is 0.0777 sec\n",
      "train AE loss : 0.45938801765441895, train ANN loss : 3.4855968952178955\n",
      "AE loss : 0.44678428769111633, ANN loss : 3.450300455093384, Total loss : 48.128726959228516\n",
      "learning rate A :  tf.Tensor(9.948506e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 449 is 0.0804 sec\n",
      "train AE loss : 0.4266606867313385, train ANN loss : 3.485366106033325\n",
      "AE loss : 0.43529218435287476, ANN loss : 3.451326847076416, Total loss : 46.98054504394531\n",
      "learning rate A :  tf.Tensor(9.948296e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 450 is 0.0788 sec\n",
      "train AE loss : 0.41552817821502686, train ANN loss : 3.485832452774048\n",
      "AE loss : 0.47267138957977295, ANN loss : 3.4471569061279297, Total loss : 50.714298248291016\n",
      "learning rate A :  tf.Tensor(9.948296e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 451 is 0.0901 sec\n",
      "train AE loss : 0.45168155431747437, train ANN loss : 3.4806270599365234\n",
      "AE loss : 0.46006953716278076, ANN loss : 3.447873592376709, Total loss : 49.45482635498047\n",
      "learning rate A :  tf.Tensor(9.9480865e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 452 is 0.0786 sec\n",
      "train AE loss : 0.43945810198783875, train ANN loss : 3.481461524963379\n",
      "AE loss : 0.4479863941669464, ANN loss : 3.448756694793701, Total loss : 48.247398376464844\n",
      "learning rate A :  tf.Tensor(9.947877e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 453 is 0.0842 sec\n",
      "train AE loss : 0.42775189876556396, train ANN loss : 3.4804322719573975\n",
      "AE loss : 0.5460957288742065, ANN loss : 3.44513201713562, Total loss : 58.05470657348633\n",
      "learning rate A :  tf.Tensor(9.947877e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 454 is 0.0801 sec\n",
      "train AE loss : 0.522934079170227, train ANN loss : 3.483301877975464\n",
      "AE loss : 0.5302519798278809, ANN loss : 3.444676160812378, Total loss : 56.46987533569336\n",
      "learning rate A :  tf.Tensor(9.9476674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 455 is 0.0794 sec\n",
      "train AE loss : 0.5075076818466187, train ANN loss : 3.4816477298736572\n",
      "AE loss : 0.5529624223709106, ANN loss : 3.443960666656494, Total loss : 58.74020004272461\n",
      "learning rate A :  tf.Tensor(9.9476674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 456 is 0.0796 sec\n",
      "train AE loss : 0.5295149087905884, train ANN loss : 3.4817254543304443\n",
      "AE loss : 0.468109130859375, ANN loss : 3.4437410831451416, Total loss : 50.25465774536133\n",
      "learning rate A :  tf.Tensor(9.9476674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 457 is 0.0799 sec\n",
      "train AE loss : 0.44704991579055786, train ANN loss : 3.4812090396881104\n",
      "AE loss : 0.4314877986907959, ANN loss : 3.4462485313415527, Total loss : 46.59503173828125\n",
      "learning rate A :  tf.Tensor(9.9476674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 458 is 0.0804 sec\n",
      "train AE loss : 0.4115665853023529, train ANN loss : 3.482616662979126\n",
      "AE loss : 0.42048996686935425, ANN loss : 3.447671890258789, Total loss : 45.49666976928711\n",
      "learning rate A :  tf.Tensor(9.947458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 459 is 0.0781 sec\n",
      "train AE loss : 0.4009335935115814, train ANN loss : 3.479506731033325\n",
      "AE loss : 0.48282939195632935, ANN loss : 3.4405393600463867, Total loss : 51.723480224609375\n",
      "learning rate A :  tf.Tensor(9.947458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 460 is 0.0803 sec\n",
      "train AE loss : 0.46118229627609253, train ANN loss : 3.4688305854797363\n",
      "AE loss : 0.5816908478736877, ANN loss : 3.439579963684082, Total loss : 61.60866928100586\n",
      "learning rate A :  tf.Tensor(9.947458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 461 is 0.0803 sec\n",
      "train AE loss : 0.5571942329406738, train ANN loss : 3.479635715484619\n",
      "AE loss : 0.5420178174972534, ANN loss : 3.43630051612854, Total loss : 57.63808059692383\n",
      "learning rate A :  tf.Tensor(9.947458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 462 is 0.0797 sec\n",
      "train AE loss : 0.518586277961731, train ANN loss : 3.4682321548461914\n",
      "AE loss : 0.46607303619384766, ANN loss : 3.4374032020568848, Total loss : 50.04471206665039\n",
      "learning rate A :  tf.Tensor(9.947458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 463 is 0.0799 sec\n",
      "train AE loss : 0.44495436549186707, train ANN loss : 3.4730749130249023\n",
      "AE loss : 0.4535403251647949, ANN loss : 3.4385437965393066, Total loss : 48.79258346557617\n",
      "learning rate A :  tf.Tensor(9.947248e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 464 is 0.0802 sec\n",
      "train AE loss : 0.43282121419906616, train ANN loss : 3.4719457626342773\n",
      "AE loss : 0.4616028368473053, ANN loss : 3.4362363815307617, Total loss : 49.596519470214844\n",
      "learning rate A :  tf.Tensor(9.947248e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 465 is 0.0810 sec\n",
      "train AE loss : 0.44063451886177063, train ANN loss : 3.4707236289978027\n",
      "AE loss : 0.5233463048934937, ANN loss : 3.4309091567993164, Total loss : 55.76553726196289\n",
      "learning rate A :  tf.Tensor(9.947248e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 466 is 0.0797 sec\n",
      "train AE loss : 0.5004761815071106, train ANN loss : 3.4714627265930176\n",
      "AE loss : 0.5597690343856812, ANN loss : 3.4290213584899902, Total loss : 59.405921936035156\n",
      "learning rate A :  tf.Tensor(9.947248e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 467 is 0.0802 sec\n",
      "train AE loss : 0.5358502864837646, train ANN loss : 3.463360548019409\n",
      "AE loss : 0.5428951382637024, ANN loss : 3.4288623332977295, Total loss : 57.71837615966797\n",
      "learning rate A :  tf.Tensor(9.947039e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 468 is 0.0855 sec\n",
      "train AE loss : 0.5194365382194519, train ANN loss : 3.46781849861145\n",
      "AE loss : 0.5268012285232544, ANN loss : 3.428985357284546, Total loss : 56.109107971191406\n",
      "learning rate A :  tf.Tensor(9.946829e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 469 is 0.1215 sec\n",
      "train AE loss : 0.5037978887557983, train ANN loss : 3.4682302474975586\n",
      "AE loss : 0.5114417672157288, ANN loss : 3.4293510913848877, Total loss : 54.573524475097656\n",
      "learning rate A :  tf.Tensor(9.94662e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 470 is 0.1083 sec\n",
      "train AE loss : 0.48888373374938965, train ANN loss : 3.4692389965057373\n",
      "AE loss : 0.49677520990371704, ANN loss : 3.429949998855591, Total loss : 53.10747146606445\n",
      "learning rate A :  tf.Tensor(9.94641e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 471 is 0.1108 sec\n",
      "train AE loss : 0.4746495187282562, train ANN loss : 3.467585563659668\n",
      "AE loss : 0.48276567459106445, ANN loss : 3.4307658672332764, Total loss : 51.707332611083984\n",
      "learning rate A :  tf.Tensor(9.9462006e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 472 is 0.1190 sec\n",
      "train AE loss : 0.46106189489364624, train ANN loss : 3.462759494781494\n",
      "AE loss : 0.5609495043754578, ANN loss : 3.427164316177368, Total loss : 59.522117614746094\n",
      "learning rate A :  tf.Tensor(9.9462006e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 473 is 0.0836 sec\n",
      "train AE loss : 0.5369020700454712, train ANN loss : 3.4647371768951416\n",
      "AE loss : 0.5439158082008362, ANN loss : 3.426974296569824, Total loss : 57.81855773925781\n",
      "learning rate A :  tf.Tensor(9.945991e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 474 is 0.0769 sec\n",
      "train AE loss : 0.5203361511230469, train ANN loss : 3.4686388969421387\n",
      "AE loss : 0.5276702642440796, ANN loss : 3.4270858764648438, Total loss : 56.19411087036133\n",
      "learning rate A :  tf.Tensor(9.945781e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 475 is 0.0792 sec\n",
      "train AE loss : 0.504555344581604, train ANN loss : 3.463987112045288\n",
      "AE loss : 0.5121738910675049, ANN loss : 3.4274325370788574, Total loss : 54.64482116699219\n",
      "learning rate A :  tf.Tensor(9.945572e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 476 is 0.0831 sec\n",
      "train AE loss : 0.48950812220573425, train ANN loss : 3.463653802871704\n",
      "AE loss : 0.4973772466182709, ANN loss : 3.428018093109131, Total loss : 53.165740966796875\n",
      "learning rate A :  tf.Tensor(9.945362e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 477 is 0.0802 sec\n",
      "train AE loss : 0.4751516282558441, train ANN loss : 3.462683916091919\n",
      "AE loss : 0.48324233293533325, ANN loss : 3.428828477859497, Total loss : 51.7530632019043\n",
      "learning rate A :  tf.Tensor(9.945153e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 478 is 0.0854 sec\n",
      "train AE loss : 0.4614465832710266, train ANN loss : 3.459050416946411\n",
      "AE loss : 0.46972978115081787, ANN loss : 3.4298384189605713, Total loss : 50.40281677246094\n",
      "learning rate A :  tf.Tensor(9.9449426e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 479 is 0.0797 sec\n",
      "train AE loss : 0.44836103916168213, train ANN loss : 3.465373992919922\n",
      "AE loss : 0.45681998133659363, ANN loss : 3.4310436248779297, Total loss : 49.113040924072266\n",
      "learning rate A :  tf.Tensor(9.944734e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 480 is 0.0759 sec\n",
      "train AE loss : 0.43586283922195435, train ANN loss : 3.4668731689453125\n",
      "AE loss : 0.5942481756210327, ANN loss : 3.4265685081481934, Total loss : 62.85138702392578\n",
      "learning rate A :  tf.Tensor(9.944734e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 481 is 0.0803 sec\n",
      "train AE loss : 0.5691444277763367, train ANN loss : 3.463679790496826\n",
      "AE loss : 0.6133909225463867, ANN loss : 3.4260377883911133, Total loss : 64.76512908935547\n",
      "learning rate A :  tf.Tensor(9.944734e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 482 is 0.0794 sec\n",
      "train AE loss : 0.5877469778060913, train ANN loss : 3.471665620803833\n",
      "AE loss : 0.5002837777137756, ANN loss : 3.4239816665649414, Total loss : 53.45235824584961\n",
      "learning rate A :  tf.Tensor(9.944734e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 483 is 0.0792 sec\n",
      "train AE loss : 0.4779212176799774, train ANN loss : 3.4621148109436035\n",
      "AE loss : 0.4352531135082245, ANN loss : 3.430499315261841, Total loss : 46.955810546875\n",
      "learning rate A :  tf.Tensor(9.944734e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 484 is 0.0792 sec\n",
      "train AE loss : 0.41506603360176086, train ANN loss : 3.462671995162964\n",
      "AE loss : 0.47588425874710083, ANN loss : 3.4244933128356934, Total loss : 51.01292037963867\n",
      "learning rate A :  tf.Tensor(9.944734e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 485 is 0.0802 sec\n",
      "train AE loss : 0.4543013274669647, train ANN loss : 3.459503173828125\n",
      "AE loss : 0.5710926651954651, ANN loss : 3.4194488525390625, Total loss : 60.52872085571289\n",
      "learning rate A :  tf.Tensor(9.944734e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 486 is 0.0789 sec\n",
      "train AE loss : 0.5465942621231079, train ANN loss : 3.454378604888916\n",
      "AE loss : 0.5532510280609131, ANN loss : 3.419342279434204, Total loss : 58.74444580078125\n",
      "learning rate A :  tf.Tensor(9.9445235e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 487 is 0.0782 sec\n",
      "train AE loss : 0.5292589068412781, train ANN loss : 3.4609694480895996\n",
      "AE loss : 0.5362692475318909, ANN loss : 3.4195544719696045, Total loss : 57.046478271484375\n",
      "learning rate A :  tf.Tensor(9.944313e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 488 is 0.0780 sec\n",
      "train AE loss : 0.512763261795044, train ANN loss : 3.4571444988250732\n",
      "AE loss : 0.5200979113578796, ANN loss : 3.4200336933135986, Total loss : 55.42982864379883\n",
      "learning rate A :  tf.Tensor(9.9441044e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 489 is 0.0775 sec\n",
      "train AE loss : 0.49706539511680603, train ANN loss : 3.456193208694458\n",
      "AE loss : 0.6338514089584351, ANN loss : 3.420680284500122, Total loss : 66.80581665039062\n",
      "learning rate A :  tf.Tensor(9.9441044e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 490 is 0.0804 sec\n",
      "train AE loss : 0.6075179576873779, train ANN loss : 3.461808204650879\n",
      "AE loss : 0.6127623319625854, ANN loss : 3.419307231903076, Total loss : 64.69554138183594\n",
      "learning rate A :  tf.Tensor(9.943895e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 491 is 0.0769 sec\n",
      "train AE loss : 0.5869874358177185, train ANN loss : 3.4565343856811523\n",
      "AE loss : 0.6103417277336121, ANN loss : 3.417377471923828, Total loss : 64.45154571533203\n",
      "learning rate A :  tf.Tensor(9.943895e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 492 is 0.0791 sec\n",
      "train AE loss : 0.5846083760261536, train ANN loss : 3.4589622020721436\n",
      "AE loss : 0.5155750513076782, ANN loss : 3.4166805744171143, Total loss : 54.97418975830078\n",
      "learning rate A :  tf.Tensor(9.943895e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 493 is 0.0792 sec\n",
      "train AE loss : 0.49265870451927185, train ANN loss : 3.449183225631714\n",
      "AE loss : 0.5003088116645813, ANN loss : 3.417752742767334, Total loss : 53.448631286621094\n",
      "learning rate A :  tf.Tensor(9.943685e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 494 is 0.0770 sec\n",
      "train AE loss : 0.4778600335121155, train ANN loss : 3.4542627334594727\n",
      "AE loss : 0.4652981162071228, ANN loss : 3.4208602905273438, Total loss : 49.9506721496582\n",
      "learning rate A :  tf.Tensor(9.943685e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 495 is 0.0800 sec\n",
      "train AE loss : 0.4440147876739502, train ANN loss : 3.4558584690093994\n",
      "AE loss : 0.5113363265991211, ANN loss : 3.4146318435668945, Total loss : 54.54826354980469\n",
      "learning rate A :  tf.Tensor(9.943685e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 496 is 0.0868 sec\n",
      "train AE loss : 0.48851701617240906, train ANN loss : 3.4519784450531006\n",
      "AE loss : 0.6012783646583557, ANN loss : 3.41025972366333, Total loss : 63.5380973815918\n",
      "learning rate A :  tf.Tensor(9.943685e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 497 is 0.0787 sec\n",
      "train AE loss : 0.5756495594978333, train ANN loss : 3.446537971496582\n",
      "AE loss : 0.6290105581283569, ANN loss : 3.409358024597168, Total loss : 66.31040954589844\n",
      "learning rate A :  tf.Tensor(9.943685e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 498 is 0.0783 sec\n",
      "train AE loss : 0.6025331616401672, train ANN loss : 3.4507482051849365\n",
      "AE loss : 0.5726969242095947, ANN loss : 3.407461166381836, Total loss : 60.677154541015625\n",
      "learning rate A :  tf.Tensor(9.943685e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 499 is 0.0798 sec\n",
      "train AE loss : 0.5478301048278809, train ANN loss : 3.441272020339966\n",
      "AE loss : 0.5544054508209229, ANN loss : 3.4079315662384033, Total loss : 58.84847640991211\n",
      "learning rate A :  tf.Tensor(9.943476e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 500 is 0.0766 sec\n",
      "train AE loss : 0.5300767421722412, train ANN loss : 3.444559097290039\n",
      "AE loss : 0.5121176242828369, ANN loss : 3.409423351287842, Total loss : 54.621185302734375\n",
      "learning rate A :  tf.Tensor(9.943476e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 501 is 0.0814 sec\n",
      "train AE loss : 0.4891056418418884, train ANN loss : 3.4473776817321777\n",
      "AE loss : 0.5314540266990662, ANN loss : 3.406554698944092, Total loss : 56.55195617675781\n",
      "learning rate A :  tf.Tensor(9.943476e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 502 is 0.0795 sec\n",
      "train AE loss : 0.5078080296516418, train ANN loss : 3.4427402019500732\n",
      "AE loss : 0.5762311816215515, ANN loss : 3.402841806411743, Total loss : 61.025962829589844\n",
      "learning rate A :  tf.Tensor(9.943476e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 503 is 0.0783 sec\n",
      "train AE loss : 0.5511754751205444, train ANN loss : 3.438328266143799\n",
      "AE loss : 0.6221067309379578, ANN loss : 3.4011600017547607, Total loss : 65.61183166503906\n",
      "learning rate A :  tf.Tensor(9.943476e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 504 is 0.0793 sec\n",
      "train AE loss : 0.5956732630729675, train ANN loss : 3.4409966468811035\n",
      "AE loss : 0.6011341214179993, ANN loss : 3.400954008102417, Total loss : 63.514366149902344\n",
      "learning rate A :  tf.Tensor(9.943266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 505 is 0.0777 sec\n",
      "train AE loss : 0.5752934217453003, train ANN loss : 3.4414820671081543\n",
      "AE loss : 0.5937181711196899, ANN loss : 3.3997132778167725, Total loss : 62.77153396606445\n",
      "learning rate A :  tf.Tensor(9.943266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 506 is 0.0786 sec\n",
      "train AE loss : 0.56810063123703, train ANN loss : 3.436359167098999\n",
      "AE loss : 0.5700352787971497, ANN loss : 3.399193286895752, Total loss : 60.40271759033203\n",
      "learning rate A :  tf.Tensor(9.943266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 507 is 0.0792 sec\n",
      "train AE loss : 0.5451555252075195, train ANN loss : 3.4349730014801025\n",
      "AE loss : 0.5517398715019226, ANN loss : 3.4001107215881348, Total loss : 58.574100494384766\n",
      "learning rate A :  tf.Tensor(9.943057e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 508 is 0.0788 sec\n",
      "train AE loss : 0.5274113416671753, train ANN loss : 3.437136650085449\n",
      "AE loss : 0.5685401558876038, ANN loss : 3.398141860961914, Total loss : 60.25215530395508\n",
      "learning rate A :  tf.Tensor(9.943057e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 509 is 0.0794 sec\n",
      "train AE loss : 0.5436696410179138, train ANN loss : 3.4366910457611084\n",
      "AE loss : 0.5502855777740479, ANN loss : 3.3991289138793945, Total loss : 58.42768859863281\n",
      "learning rate A :  tf.Tensor(9.942847e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 510 is 0.0786 sec\n",
      "train AE loss : 0.5259612798690796, train ANN loss : 3.4332456588745117\n",
      "AE loss : 0.6084463000297546, ANN loss : 3.395859956741333, Total loss : 64.24048614501953\n",
      "learning rate A :  tf.Tensor(9.942847e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 511 is 0.0811 sec\n",
      "train AE loss : 0.5822733044624329, train ANN loss : 3.4335646629333496\n",
      "AE loss : 0.6313735842704773, ANN loss : 3.3947060108184814, Total loss : 66.53206634521484\n",
      "learning rate A :  tf.Tensor(9.942847e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 512 is 0.0798 sec\n",
      "train AE loss : 0.6044375896453857, train ANN loss : 3.4290337562561035\n",
      "AE loss : 0.6060364246368408, ANN loss : 3.393268585205078, Total loss : 63.996910095214844\n",
      "learning rate A :  tf.Tensor(9.942847e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 513 is 0.0802 sec\n",
      "train AE loss : 0.5798007845878601, train ANN loss : 3.4300220012664795\n",
      "AE loss : 0.5758268237113953, ANN loss : 3.392913579940796, Total loss : 60.9755973815918\n",
      "learning rate A :  tf.Tensor(9.942847e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 514 is 0.0924 sec\n",
      "train AE loss : 0.5504925847053528, train ANN loss : 3.4269001483917236\n",
      "AE loss : 0.5570549368858337, ANN loss : 3.394141435623169, Total loss : 59.099632263183594\n",
      "learning rate A :  tf.Tensor(9.9426376e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 515 is 0.0772 sec\n",
      "train AE loss : 0.532279372215271, train ANN loss : 3.4300687313079834\n",
      "AE loss : 0.5810602307319641, ANN loss : 3.3908801078796387, Total loss : 61.49689865112305\n",
      "learning rate A :  tf.Tensor(9.9426376e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 516 is 0.0799 sec\n",
      "train AE loss : 0.5555481910705566, train ANN loss : 3.424978017807007\n",
      "AE loss : 0.6330117583274841, ANN loss : 3.3872897624969482, Total loss : 66.68846893310547\n",
      "learning rate A :  tf.Tensor(9.9426376e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 517 is 0.0801 sec\n",
      "train AE loss : 0.6059162020683289, train ANN loss : 3.4290096759796143\n",
      "AE loss : 0.6554577946662903, ANN loss : 3.3856565952301025, Total loss : 68.93143463134766\n",
      "learning rate A :  tf.Tensor(9.9426376e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 518 is 0.0797 sec\n",
      "train AE loss : 0.627668023109436, train ANN loss : 3.42753267288208\n",
      "AE loss : 0.6225258111953735, ANN loss : 3.3850483894348145, Total loss : 65.63762664794922\n",
      "learning rate A :  tf.Tensor(9.9426376e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 519 is 0.0809 sec\n",
      "train AE loss : 0.5956829190254211, train ANN loss : 3.4199728965759277\n",
      "AE loss : 0.5956692099571228, ANN loss : 3.3852343559265137, Total loss : 62.9521598815918\n",
      "learning rate A :  tf.Tensor(9.9426376e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 520 is 0.0793 sec\n",
      "train AE loss : 0.5696668028831482, train ANN loss : 3.42317271232605\n",
      "AE loss : 0.6155319213867188, ANN loss : 3.3827836513519287, Total loss : 64.93598175048828\n",
      "learning rate A :  tf.Tensor(9.9426376e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 521 is 0.0788 sec\n",
      "train AE loss : 0.5889259576797485, train ANN loss : 3.4201314449310303\n",
      "AE loss : 0.5944440960884094, ANN loss : 3.383924961090088, Total loss : 62.82833480834961\n",
      "learning rate A :  tf.Tensor(9.942428e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 522 is 0.0775 sec\n",
      "train AE loss : 0.5684616565704346, train ANN loss : 3.417005777359009\n",
      "AE loss : 0.6511068344116211, ANN loss : 3.3800880908966064, Total loss : 68.49077606201172\n",
      "learning rate A :  tf.Tensor(9.942428e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 523 is 0.0792 sec\n",
      "train AE loss : 0.623414158821106, train ANN loss : 3.420950174331665\n",
      "AE loss : 0.6712872982025146, ANN loss : 3.378530979156494, Total loss : 70.50726318359375\n",
      "learning rate A :  tf.Tensor(9.942428e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 524 is 0.0786 sec\n",
      "train AE loss : 0.6430256366729736, train ANN loss : 3.4210305213928223\n",
      "AE loss : 0.6469656825065613, ANN loss : 3.378699541091919, Total loss : 68.07527160644531\n",
      "learning rate A :  tf.Tensor(9.9422185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 525 is 0.1814 sec\n",
      "train AE loss : 0.6194087862968445, train ANN loss : 3.4186782836914062\n",
      "AE loss : 0.653842031955719, ANN loss : 3.377145290374756, Total loss : 68.76134490966797\n",
      "learning rate A :  tf.Tensor(9.9422185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 526 is 0.0899 sec\n",
      "train AE loss : 0.6261129379272461, train ANN loss : 3.415600061416626\n",
      "AE loss : 0.6398276686668396, ANN loss : 3.376441240310669, Total loss : 67.35920715332031\n",
      "learning rate A :  tf.Tensor(9.9422185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 527 is 0.0787 sec\n",
      "train AE loss : 0.6125457286834717, train ANN loss : 3.410010814666748\n",
      "AE loss : 0.6298556327819824, ANN loss : 3.376004934310913, Total loss : 66.361572265625\n",
      "learning rate A :  tf.Tensor(9.9422185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 528 is 0.0782 sec\n",
      "train AE loss : 0.6028617024421692, train ANN loss : 3.414240837097168\n",
      "AE loss : 0.6369228959083557, ANN loss : 3.374772548675537, Total loss : 67.06706237792969\n",
      "learning rate A :  tf.Tensor(9.9422185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 529 is 0.0804 sec\n",
      "train AE loss : 0.6096756458282471, train ANN loss : 3.416496753692627\n",
      "AE loss : 0.6144781708717346, ANN loss : 3.3758482933044434, Total loss : 64.82366943359375\n",
      "learning rate A :  tf.Tensor(9.942009e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 530 is 0.0769 sec\n",
      "train AE loss : 0.5878822803497314, train ANN loss : 3.4127936363220215\n",
      "AE loss : 0.5932985544204712, ANN loss : 3.377276659011841, Total loss : 62.70713424682617\n",
      "learning rate A :  tf.Tensor(9.941799e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 531 is 0.0773 sec\n",
      "train AE loss : 0.5673388838768005, train ANN loss : 3.417787551879883\n",
      "AE loss : 0.6621814966201782, ANN loss : 3.3725948333740234, Total loss : 69.59074401855469\n",
      "learning rate A :  tf.Tensor(9.941799e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 532 is 0.0798 sec\n",
      "train AE loss : 0.6341013312339783, train ANN loss : 3.410449743270874\n",
      "AE loss : 0.6381610035896301, ANN loss : 3.3732333183288574, Total loss : 67.1893310546875\n",
      "learning rate A :  tf.Tensor(9.94159e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 533 is 0.0771 sec\n",
      "train AE loss : 0.6107833981513977, train ANN loss : 3.41264271736145\n",
      "AE loss : 0.6155322790145874, ANN loss : 3.3742852210998535, Total loss : 64.9275131225586\n",
      "learning rate A :  tf.Tensor(9.94138e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 534 is 0.0773 sec\n",
      "train AE loss : 0.5888081192970276, train ANN loss : 3.4072515964508057\n",
      "AE loss : 0.7265017032623291, ANN loss : 3.371194362640381, Total loss : 76.0213623046875\n",
      "learning rate A :  tf.Tensor(9.94138e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 535 is 0.0801 sec\n",
      "train AE loss : 0.6965305209159851, train ANN loss : 3.412109136581421\n",
      "AE loss : 0.733642041683197, ANN loss : 3.3694040775299072, Total loss : 76.73361206054688\n",
      "learning rate A :  tf.Tensor(9.94138e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 536 is 0.0798 sec\n",
      "train AE loss : 0.7034600377082825, train ANN loss : 3.4122965335845947\n",
      "AE loss : 0.645587146282196, ANN loss : 3.3688783645629883, Total loss : 67.9275894165039\n",
      "learning rate A :  tf.Tensor(9.94138e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 537 is 0.0789 sec\n",
      "train AE loss : 0.6180811524391174, train ANN loss : 3.40740704536438\n",
      "AE loss : 0.5986162424087524, ANN loss : 3.3716964721679688, Total loss : 63.23332214355469\n",
      "learning rate A :  tf.Tensor(9.94138e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 538 is 0.0796 sec\n",
      "train AE loss : 0.5725774168968201, train ANN loss : 3.406428337097168\n",
      "AE loss : 0.5783067345619202, ANN loss : 3.373828411102295, Total loss : 61.204498291015625\n",
      "learning rate A :  tf.Tensor(9.9411714e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 539 is 0.0775 sec\n",
      "train AE loss : 0.5529200434684753, train ANN loss : 3.4087119102478027\n",
      "AE loss : 0.638045072555542, ANN loss : 3.3670477867126465, Total loss : 67.17155456542969\n",
      "learning rate A :  tf.Tensor(9.9411714e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 540 is 0.0807 sec\n",
      "train AE loss : 0.6106975078582764, train ANN loss : 3.4013679027557373\n",
      "AE loss : 0.7382624745368958, ANN loss : 3.3631205558776855, Total loss : 77.18936920166016\n",
      "learning rate A :  tf.Tensor(9.9411714e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 541 is 0.0791 sec\n",
      "train AE loss : 0.7078284025192261, train ANN loss : 3.405255079269409\n",
      "AE loss : 0.7093511819839478, ANN loss : 3.3629162311553955, Total loss : 74.29803466796875\n",
      "learning rate A :  tf.Tensor(9.940962e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 542 is 0.0771 sec\n",
      "train AE loss : 0.6797482371330261, train ANN loss : 3.402147054672241\n",
      "AE loss : 0.7590625286102295, ANN loss : 3.362532615661621, Total loss : 79.2687759399414\n",
      "learning rate A :  tf.Tensor(9.940962e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 543 is 0.0790 sec\n",
      "train AE loss : 0.727877140045166, train ANN loss : 3.403449773788452\n",
      "AE loss : 0.7066600918769836, ANN loss : 3.3608691692352295, Total loss : 74.0268783569336\n",
      "learning rate A :  tf.Tensor(9.940962e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 544 is 0.0786 sec\n",
      "train AE loss : 0.6769354343414307, train ANN loss : 3.39959454536438\n",
      "AE loss : 0.679494321346283, ANN loss : 3.3614065647125244, Total loss : 71.31084442138672\n",
      "learning rate A :  tf.Tensor(9.940752e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 545 is 0.0765 sec\n",
      "train AE loss : 0.6505747437477112, train ANN loss : 3.397972583770752\n",
      "AE loss : 0.6539922952651978, ANN loss : 3.362422227859497, Total loss : 68.76165008544922\n",
      "learning rate A :  tf.Tensor(9.940543e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 546 is 0.0772 sec\n",
      "train AE loss : 0.6258302330970764, train ANN loss : 3.4003689289093018\n",
      "AE loss : 0.6409434676170349, ANN loss : 3.3617844581604004, Total loss : 67.45613098144531\n",
      "learning rate A :  tf.Tensor(9.940543e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 547 is 0.0794 sec\n",
      "train AE loss : 0.6132205128669739, train ANN loss : 3.3978219032287598\n",
      "AE loss : 0.6177802085876465, ANN loss : 3.363555431365967, Total loss : 65.1415786743164\n",
      "learning rate A :  tf.Tensor(9.940333e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 548 is 0.0762 sec\n",
      "train AE loss : 0.5907713770866394, train ANN loss : 3.4037539958953857\n",
      "AE loss : 0.5959866046905518, ANN loss : 3.3656527996063232, Total loss : 62.96430587768555\n",
      "learning rate A :  tf.Tensor(9.940124e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 549 is 0.0768 sec\n",
      "train AE loss : 0.5696780681610107, train ANN loss : 3.4064249992370605\n",
      "AE loss : 0.575465738773346, ANN loss : 3.36802339553833, Total loss : 60.91460037231445\n",
      "learning rate A :  tf.Tensor(9.939915e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 550 is 0.0775 sec\n",
      "train AE loss : 0.5498369336128235, train ANN loss : 3.40657639503479\n",
      "AE loss : 0.6763900518417358, ANN loss : 3.357836961746216, Total loss : 70.99684143066406\n",
      "learning rate A :  tf.Tensor(9.939915e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 551 is 0.0788 sec\n",
      "train AE loss : 0.6475545167922974, train ANN loss : 3.3948872089385986\n",
      "AE loss : 0.6509779691696167, ANN loss : 3.3590149879455566, Total loss : 68.4568099975586\n",
      "learning rate A :  tf.Tensor(9.9397046e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 552 is 0.0778 sec\n",
      "train AE loss : 0.622915506362915, train ANN loss : 3.394254684448242\n",
      "AE loss : 0.786218523979187, ANN loss : 3.3564341068267822, Total loss : 81.97827911376953\n",
      "learning rate A :  tf.Tensor(9.9397046e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 553 is 0.0794 sec\n",
      "train AE loss : 0.7540796995162964, train ANN loss : 3.398996353149414\n",
      "AE loss : 0.775180459022522, ANN loss : 3.3546910285949707, Total loss : 80.87273406982422\n",
      "learning rate A :  tf.Tensor(9.9397046e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 554 is 0.0774 sec\n",
      "train AE loss : 0.743392288684845, train ANN loss : 3.401064872741699\n",
      "AE loss : 0.6659127473831177, ANN loss : 3.355271339416504, Total loss : 69.94654083251953\n",
      "learning rate A :  tf.Tensor(9.9397046e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 555 is 0.0771 sec\n",
      "train AE loss : 0.6374796628952026, train ANN loss : 3.397880792617798\n",
      "AE loss : 0.641115128993988, ANN loss : 3.356874704360962, Total loss : 67.46839141845703\n",
      "learning rate A :  tf.Tensor(9.939496e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 556 is 0.0766 sec\n",
      "train AE loss : 0.6134580373764038, train ANN loss : 3.3922858238220215\n",
      "AE loss : 0.6084319949150085, ANN loss : 3.3595004081726074, Total loss : 64.20269775390625\n",
      "learning rate A :  tf.Tensor(9.939496e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 557 is 0.0871 sec\n",
      "train AE loss : 0.5818338394165039, train ANN loss : 3.3968958854675293\n",
      "AE loss : 0.6753672957420349, ANN loss : 3.3527138233184814, Total loss : 70.88943481445312\n",
      "learning rate A :  tf.Tensor(9.939496e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 558 is 0.0772 sec\n",
      "train AE loss : 0.6465511322021484, train ANN loss : 3.394005537033081\n",
      "AE loss : 0.772972583770752, ANN loss : 3.3497238159179688, Total loss : 80.64698791503906\n",
      "learning rate A :  tf.Tensor(9.939496e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 559 is 0.0782 sec\n",
      "train AE loss : 0.7411196231842041, train ANN loss : 3.394399881362915\n",
      "AE loss : 0.7775366902351379, ANN loss : 3.34818959236145, Total loss : 81.10186004638672\n",
      "learning rate A :  tf.Tensor(9.939496e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 560 is 0.0771 sec\n",
      "train AE loss : 0.7455602884292603, train ANN loss : 3.3918039798736572\n",
      "AE loss : 0.7452269196510315, ANN loss : 3.347839117050171, Total loss : 77.87052917480469\n",
      "learning rate A :  tf.Tensor(9.9392855e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 561 is 0.0759 sec\n",
      "train AE loss : 0.7141867280006409, train ANN loss : 3.3901686668395996\n",
      "AE loss : 0.7150216102600098, ANN loss : 3.3481929302215576, Total loss : 74.85034942626953\n",
      "learning rate A :  tf.Tensor(9.939077e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 562 is 0.0787 sec\n",
      "train AE loss : 0.6848797798156738, train ANN loss : 3.3912618160247803\n",
      "AE loss : 0.6867537498474121, ANN loss : 3.3491222858428955, Total loss : 72.02450561523438\n",
      "learning rate A :  tf.Tensor(9.938867e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 563 is 0.0793 sec\n",
      "train AE loss : 0.6574720740318298, train ANN loss : 3.3841283321380615\n",
      "AE loss : 0.7075328826904297, ANN loss : 3.346827268600464, Total loss : 74.10011291503906\n",
      "learning rate A :  tf.Tensor(9.938867e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 564 is 0.0806 sec\n",
      "train AE loss : 0.6775991320610046, train ANN loss : 3.3905155658721924\n",
      "AE loss : 0.679696798324585, ANN loss : 3.347991704940796, Total loss : 71.31767272949219\n",
      "learning rate A :  tf.Tensor(9.9386576e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 565 is 0.0788 sec\n",
      "train AE loss : 0.6506181955337524, train ANN loss : 3.390049695968628\n",
      "AE loss : 0.7179608345031738, ANN loss : 3.344741106033325, Total loss : 75.14082336425781\n",
      "learning rate A :  tf.Tensor(9.9386576e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 566 is 0.0797 sec\n",
      "train AE loss : 0.6877414584159851, train ANN loss : 3.3870627880096436\n",
      "AE loss : 0.7436308264732361, ANN loss : 3.342785358428955, Total loss : 77.70587158203125\n",
      "learning rate A :  tf.Tensor(9.9386576e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 567 is 0.0798 sec\n",
      "train AE loss : 0.7126756906509399, train ANN loss : 3.3883845806121826\n",
      "AE loss : 0.7134304642677307, ANN loss : 3.3433895111083984, Total loss : 74.68643951416016\n",
      "learning rate A :  tf.Tensor(9.938448e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 568 is 0.0786 sec\n",
      "train AE loss : 0.6833723783493042, train ANN loss : 3.385668992996216\n",
      "AE loss : 0.6851792335510254, ANN loss : 3.3445684909820557, Total loss : 71.86248779296875\n",
      "learning rate A :  tf.Tensor(9.938239e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 569 is 0.0784 sec\n",
      "train AE loss : 0.6559942364692688, train ANN loss : 3.3849539756774902\n",
      "AE loss : 0.7255451083183289, ANN loss : 3.341536521911621, Total loss : 75.89604949951172\n",
      "learning rate A :  tf.Tensor(9.938239e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 570 is 0.0798 sec\n",
      "train AE loss : 0.6951178312301636, train ANN loss : 3.3851120471954346\n",
      "AE loss : 0.696434736251831, ANN loss : 3.3424761295318604, Total loss : 72.98594665527344\n",
      "learning rate A :  tf.Tensor(9.93803e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 571 is 0.0780 sec\n",
      "train AE loss : 0.6669020652770996, train ANN loss : 3.3845651149749756\n",
      "AE loss : 0.7435120940208435, ANN loss : 3.339872360229492, Total loss : 77.69108581542969\n",
      "learning rate A :  tf.Tensor(9.93803e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 572 is 0.0816 sec\n",
      "train AE loss : 0.7125038504600525, train ANN loss : 3.3822429180145264\n",
      "AE loss : 0.750832736492157, ANN loss : 3.338751792907715, Total loss : 78.42202758789062\n",
      "learning rate A :  tf.Tensor(9.93803e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 573 is 0.0811 sec\n",
      "train AE loss : 0.7196335792541504, train ANN loss : 3.3839125633239746\n",
      "AE loss : 0.7199406623840332, ANN loss : 3.339134454727173, Total loss : 75.33319854736328\n",
      "learning rate A :  tf.Tensor(9.93782e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 574 is 0.0796 sec\n",
      "train AE loss : 0.689666211605072, train ANN loss : 3.380754232406616\n",
      "AE loss : 0.6910892724990845, ANN loss : 3.34014892578125, Total loss : 72.4490737915039\n",
      "learning rate A :  tf.Tensor(9.937611e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 575 is 0.0787 sec\n",
      "train AE loss : 0.6617092490196228, train ANN loss : 3.3843343257904053\n",
      "AE loss : 0.7356145977973938, ANN loss : 3.3374691009521484, Total loss : 76.89892578125\n",
      "learning rate A :  tf.Tensor(9.937611e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 576 is 0.0809 sec\n",
      "train AE loss : 0.7048524618148804, train ANN loss : 3.381154775619507\n",
      "AE loss : 0.7056620121002197, ANN loss : 3.338184356689453, Total loss : 73.90438842773438\n",
      "learning rate A :  tf.Tensor(9.937401e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 577 is 0.0797 sec\n",
      "train AE loss : 0.6758238077163696, train ANN loss : 3.3793303966522217\n",
      "AE loss : 0.6776942014694214, ANN loss : 3.339461326599121, Total loss : 71.10887908935547\n",
      "learning rate A :  tf.Tensor(9.937192e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 578 is 0.0784 sec\n",
      "train AE loss : 0.648735761642456, train ANN loss : 3.382258176803589\n",
      "AE loss : 0.651555597782135, ANN loss : 3.3411881923675537, Total loss : 68.49674987792969\n",
      "learning rate A :  tf.Tensor(9.936983e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 579 is 0.0779 sec\n",
      "train AE loss : 0.6234256029129028, train ANN loss : 3.380363702774048\n",
      "AE loss : 0.6270906329154968, ANN loss : 3.3433380126953125, Total loss : 66.05239868164062\n",
      "learning rate A :  tf.Tensor(9.936774e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 580 is 0.0785 sec\n",
      "train AE loss : 0.5997613668441772, train ANN loss : 3.3824141025543213\n",
      "AE loss : 0.6041902899742126, ANN loss : 3.3458499908447266, Total loss : 63.7648811340332\n",
      "learning rate A :  tf.Tensor(9.9365636e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 581 is 0.0789 sec\n",
      "train AE loss : 0.5776257514953613, train ANN loss : 3.3803083896636963\n",
      "AE loss : 0.5827435851097107, ANN loss : 3.3486413955688477, Total loss : 61.62299346923828\n",
      "learning rate A :  tf.Tensor(9.936355e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 582 is 0.0786 sec\n",
      "train AE loss : 0.5568888783454895, train ANN loss : 3.3860745429992676\n",
      "AE loss : 0.5626094341278076, ANN loss : 3.3516485691070557, Total loss : 59.61259460449219\n",
      "learning rate A :  tf.Tensor(9.936145e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 583 is 0.0793 sec\n",
      "train AE loss : 0.5374370813369751, train ANN loss : 3.388906478881836\n",
      "AE loss : 0.7389894723892212, ANN loss : 3.335393190383911, Total loss : 77.23433685302734\n",
      "learning rate A :  tf.Tensor(9.936145e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 584 is 0.0793 sec\n",
      "train AE loss : 0.7079483866691589, train ANN loss : 3.3795382976531982\n",
      "AE loss : 0.7085801959037781, ANN loss : 3.3358356952667236, Total loss : 74.19385528564453\n",
      "learning rate A :  tf.Tensor(9.9359364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 585 is 0.0877 sec\n",
      "train AE loss : 0.678498387336731, train ANN loss : 3.380749225616455\n",
      "AE loss : 0.6802213191986084, ANN loss : 3.3368985652923584, Total loss : 71.3590316772461\n",
      "learning rate A :  tf.Tensor(9.935726e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 586 is 0.0878 sec\n",
      "train AE loss : 0.6510303020477295, train ANN loss : 3.37648606300354\n",
      "AE loss : 0.6537205576896667, ANN loss : 3.338449001312256, Total loss : 68.71050262451172\n",
      "learning rate A :  tf.Tensor(9.935517e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 587 is 0.0767 sec\n",
      "train AE loss : 0.6253745555877686, train ANN loss : 3.3812103271484375\n",
      "AE loss : 0.8858492970466614, ANN loss : 3.340897798538208, Total loss : 91.92582702636719\n",
      "learning rate A :  tf.Tensor(9.935517e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 588 is 0.0821 sec\n",
      "train AE loss : 0.8503069877624512, train ANN loss : 3.390009880065918\n",
      "AE loss : 0.8635974526405334, ANN loss : 3.337167501449585, Total loss : 89.6969223022461\n",
      "learning rate A :  tf.Tensor(9.935517e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 589 is 0.0802 sec\n",
      "train AE loss : 0.8286374807357788, train ANN loss : 3.3873631954193115\n",
      "AE loss : 0.6681419014930725, ANN loss : 3.334995746612549, Total loss : 70.1491928100586\n",
      "learning rate A :  tf.Tensor(9.935517e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 590 is 0.0784 sec\n",
      "train AE loss : 0.6393306255340576, train ANN loss : 3.379751443862915\n",
      "AE loss : 0.565608024597168, ANN loss : 3.348151683807373, Total loss : 59.90895462036133\n",
      "learning rate A :  tf.Tensor(9.935517e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 591 is 0.0799 sec\n",
      "train AE loss : 0.5403100848197937, train ANN loss : 3.384493350982666\n",
      "AE loss : 0.6069449782371521, ANN loss : 3.340700387954712, Total loss : 64.03520202636719\n",
      "learning rate A :  tf.Tensor(9.935517e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 592 is 0.0794 sec\n",
      "train AE loss : 0.5801386833190918, train ANN loss : 3.377115249633789\n",
      "AE loss : 0.5852965116500854, ANN loss : 3.3438782691955566, Total loss : 61.873531341552734\n",
      "learning rate A :  tf.Tensor(9.935308e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 593 is 0.0777 sec\n",
      "train AE loss : 0.5592334270477295, train ANN loss : 3.382822036743164\n",
      "AE loss : 0.7434433698654175, ANN loss : 3.3278958797454834, Total loss : 77.67222595214844\n",
      "learning rate A :  tf.Tensor(9.935308e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 594 is 0.0806 sec\n",
      "train AE loss : 0.7119837999343872, train ANN loss : 3.3718411922454834\n",
      "AE loss : 0.7125667333602905, ANN loss : 3.3289215564727783, Total loss : 74.5855941772461\n",
      "learning rate A :  tf.Tensor(9.935099e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 595 is 0.0781 sec\n",
      "train AE loss : 0.6820967197418213, train ANN loss : 3.3731212615966797\n",
      "AE loss : 0.8905162811279297, ANN loss : 3.330440044403076, Total loss : 92.38206481933594\n",
      "learning rate A :  tf.Tensor(9.935099e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 596 is 0.0854 sec\n",
      "train AE loss : 0.8544867038726807, train ANN loss : 3.3809280395507812\n",
      "AE loss : 0.8605571985244751, ANN loss : 3.327338457107544, Total loss : 89.38306427001953\n",
      "learning rate A :  tf.Tensor(9.935099e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 597 is 0.0834 sec\n",
      "train AE loss : 0.8253656029701233, train ANN loss : 3.37359881401062\n",
      "AE loss : 0.6992599368095398, ANN loss : 3.3272383213043213, Total loss : 73.25323486328125\n",
      "learning rate A :  tf.Tensor(9.935099e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 598 is 0.0839 sec\n",
      "train AE loss : 0.6691919565200806, train ANN loss : 3.367609977722168\n",
      "AE loss : 0.6055294871330261, ANN loss : 3.337232828140259, Total loss : 63.89017868041992\n",
      "learning rate A :  tf.Tensor(9.935099e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 599 is 0.0841 sec\n",
      "train AE loss : 0.5787433981895447, train ANN loss : 3.377377986907959\n",
      "AE loss : 0.6346863508224487, ANN loss : 3.3318159580230713, Total loss : 66.80044555664062\n",
      "learning rate A :  tf.Tensor(9.935099e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 600 is 0.0856 sec\n",
      "train AE loss : 0.6069298982620239, train ANN loss : 3.3685245513916016\n",
      "AE loss : 0.7510899901390076, ANN loss : 3.3209502696990967, Total loss : 78.4299545288086\n",
      "learning rate A :  tf.Tensor(9.935099e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 601 is 0.0860 sec\n",
      "train AE loss : 0.7194444537162781, train ANN loss : 3.3686752319335938\n",
      "AE loss : 0.7197431325912476, ANN loss : 3.3221566677093506, Total loss : 75.29647064208984\n",
      "learning rate A :  tf.Tensor(9.934889e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 602 is 0.0842 sec\n",
      "train AE loss : 0.6891008615493774, train ANN loss : 3.367711067199707\n",
      "AE loss : 0.690556526184082, ANN loss : 3.323917865753174, Total loss : 72.37956237792969\n",
      "learning rate A :  tf.Tensor(9.93468e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 603 is 0.0837 sec\n",
      "train AE loss : 0.6608485579490662, train ANN loss : 3.36448073387146\n",
      "AE loss : 0.663370668888092, ANN loss : 3.326158285140991, Total loss : 69.66322326660156\n",
      "learning rate A :  tf.Tensor(9.93447e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 604 is 0.0838 sec\n",
      "train AE loss : 0.6345557570457458, train ANN loss : 3.3674569129943848\n",
      "AE loss : 0.6380202770233154, ANN loss : 3.328781843185425, Total loss : 67.13081359863281\n",
      "learning rate A :  tf.Tensor(9.9342615e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 605 is 0.0809 sec\n",
      "train AE loss : 0.6100589036941528, train ANN loss : 3.3683669567108154\n",
      "AE loss : 0.8382969498634338, ANN loss : 3.3189592361450195, Total loss : 87.14865112304688\n",
      "learning rate A :  tf.Tensor(9.9342615e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 606 is 0.0842 sec\n",
      "train AE loss : 0.80372554063797, train ANN loss : 3.3682398796081543\n",
      "AE loss : 0.929251492023468, ANN loss : 3.3234314918518066, Total loss : 96.24858093261719\n",
      "learning rate A :  tf.Tensor(9.9342615e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 607 is 0.0834 sec\n",
      "train AE loss : 0.8919876217842102, train ANN loss : 3.37776517868042\n",
      "AE loss : 0.8845092058181763, ANN loss : 3.32002592086792, Total loss : 91.77093505859375\n",
      "learning rate A :  tf.Tensor(9.934052e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 608 is 0.0895 sec\n",
      "train AE loss : 0.8484829664230347, train ANN loss : 3.370877504348755\n",
      "AE loss : 0.8431397676467896, ANN loss : 3.317821979522705, Total loss : 87.63180541992188\n",
      "learning rate A :  tf.Tensor(9.933843e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 609 is 0.1148 sec\n",
      "train AE loss : 0.8083027005195618, train ANN loss : 3.370159149169922\n",
      "AE loss : 0.792820394039154, ANN loss : 3.31611967086792, Total loss : 82.59815216064453\n",
      "learning rate A :  tf.Tensor(9.933843e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 610 is 0.1163 sec\n",
      "train AE loss : 0.7594854235649109, train ANN loss : 3.361954689025879\n",
      "AE loss : 0.703669011592865, ANN loss : 3.3181545734405518, Total loss : 73.68505096435547\n",
      "learning rate A :  tf.Tensor(9.933843e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 611 is 0.1178 sec\n",
      "train AE loss : 0.67326819896698, train ANN loss : 3.3568131923675537\n",
      "AE loss : 0.6751818060874939, ANN loss : 3.3201212882995605, Total loss : 70.83830261230469\n",
      "learning rate A :  tf.Tensor(9.933633e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 612 is 0.1018 sec\n",
      "train AE loss : 0.6457216739654541, train ANN loss : 3.364295721054077\n",
      "AE loss : 0.6425897479057312, ANN loss : 3.3227016925811768, Total loss : 67.5816650390625\n",
      "learning rate A :  tf.Tensor(9.933633e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 613 is 0.1028 sec\n",
      "train AE loss : 0.6143141984939575, train ANN loss : 3.36453914642334\n",
      "AE loss : 0.6796799302101135, ANN loss : 3.317347764968872, Total loss : 71.28533935546875\n",
      "learning rate A :  tf.Tensor(9.933633e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 614 is 0.1063 sec\n",
      "train AE loss : 0.6501748561859131, train ANN loss : 3.3580729961395264\n",
      "AE loss : 0.6528782844543457, ANN loss : 3.31980037689209, Total loss : 68.60762023925781\n",
      "learning rate A :  tf.Tensor(9.933423e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 615 is 0.0796 sec\n",
      "train AE loss : 0.6242800354957581, train ANN loss : 3.3594465255737305\n",
      "AE loss : 0.7576736807823181, ANN loss : 3.311394214630127, Total loss : 79.0787582397461\n",
      "learning rate A :  tf.Tensor(9.933423e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 616 is 0.0788 sec\n",
      "train AE loss : 0.7255819439888, train ANN loss : 3.3588762283325195\n",
      "AE loss : 0.725263237953186, ANN loss : 3.3123109340667725, Total loss : 75.83863067626953\n",
      "learning rate A :  tf.Tensor(9.933214e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 617 is 0.0761 sec\n",
      "train AE loss : 0.6942168474197388, train ANN loss : 3.3562583923339844\n",
      "AE loss : 0.6951624155044556, ANN loss : 3.3138744831085205, Total loss : 72.83011627197266\n",
      "learning rate A :  tf.Tensor(9.933005e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 618 is 0.0769 sec\n",
      "train AE loss : 0.6651090383529663, train ANN loss : 3.357891798019409\n",
      "AE loss : 0.6671838164329529, ANN loss : 3.3159942626953125, Total loss : 70.03437805175781\n",
      "learning rate A :  tf.Tensor(9.932795e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 619 is 0.0762 sec\n",
      "train AE loss : 0.6380646824836731, train ANN loss : 3.360661745071411\n",
      "AE loss : 0.8157163858413696, ANN loss : 3.309568405151367, Total loss : 84.88121032714844\n",
      "learning rate A :  tf.Tensor(9.932795e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 620 is 0.0781 sec\n",
      "train AE loss : 0.7816438674926758, train ANN loss : 3.359687089920044\n",
      "AE loss : 0.77886563539505, ANN loss : 3.309105396270752, Total loss : 81.19567108154297\n",
      "learning rate A :  tf.Tensor(9.932586e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 621 is 0.0769 sec\n",
      "train AE loss : 0.7459720373153687, train ANN loss : 3.3588781356811523\n",
      "AE loss : 0.7447282075881958, ANN loss : 3.3094213008880615, Total loss : 77.7822494506836\n",
      "learning rate A :  tf.Tensor(9.932376e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 622 is 0.0761 sec\n",
      "train AE loss : 0.7129555344581604, train ANN loss : 3.3579037189483643\n",
      "AE loss : 0.7130758166313171, ANN loss : 3.31044340133667, Total loss : 74.61802673339844\n",
      "learning rate A :  tf.Tensor(9.9321674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 623 is 0.0768 sec\n",
      "train AE loss : 0.6823393702507019, train ANN loss : 3.35383677482605\n",
      "AE loss : 0.8648901581764221, ANN loss : 3.3101747035980225, Total loss : 89.7991943359375\n",
      "learning rate A :  tf.Tensor(9.9321674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 624 is 0.0797 sec\n",
      "train AE loss : 0.8291386961936951, train ANN loss : 3.3635127544403076\n",
      "AE loss : 0.8485081195831299, ANN loss : 3.3080737590789795, Total loss : 88.15888977050781\n",
      "learning rate A :  tf.Tensor(9.9321674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 625 is 0.0786 sec\n",
      "train AE loss : 0.8133119940757751, train ANN loss : 3.352327346801758\n",
      "AE loss : 0.7148326635360718, ANN loss : 3.3077280521392822, Total loss : 74.79100036621094\n",
      "learning rate A :  tf.Tensor(9.9321674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 626 is 0.0776 sec\n",
      "train AE loss : 0.6841331124305725, train ANN loss : 3.3504579067230225\n",
      "AE loss : 0.6192213296890259, ANN loss : 3.317502021789551, Total loss : 65.23963165283203\n",
      "learning rate A :  tf.Tensor(9.9321674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 627 is 0.0791 sec\n",
      "train AE loss : 0.5919250845909119, train ANN loss : 3.3593356609344482\n",
      "AE loss : 0.6311782002449036, ANN loss : 3.315321445465088, Total loss : 66.43314361572266\n",
      "learning rate A :  tf.Tensor(9.9321674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 628 is 0.0795 sec\n",
      "train AE loss : 0.6034972667694092, train ANN loss : 3.3553335666656494\n",
      "AE loss : 0.7268401384353638, ANN loss : 3.304671049118042, Total loss : 75.98868560791016\n",
      "learning rate A :  tf.Tensor(9.9321674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 629 is 0.0790 sec\n",
      "train AE loss : 0.6958996057510376, train ANN loss : 3.3498404026031494\n",
      "AE loss : 0.8394461870193481, ANN loss : 3.301344871520996, Total loss : 87.2459716796875\n",
      "learning rate A :  tf.Tensor(9.9321674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 630 is 0.0792 sec\n",
      "train AE loss : 0.8048287034034729, train ANN loss : 3.3559458255767822\n",
      "AE loss : 0.8003122210502625, ANN loss : 3.300795316696167, Total loss : 83.33201599121094\n",
      "learning rate A :  tf.Tensor(9.931958e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 631 is 0.0769 sec\n",
      "train AE loss : 0.7669469714164734, train ANN loss : 3.344771385192871\n",
      "AE loss : 0.7641216516494751, ANN loss : 3.3011598587036133, Total loss : 79.71331787109375\n",
      "learning rate A :  tf.Tensor(9.931749e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 632 is 0.0762 sec\n",
      "train AE loss : 0.7319501042366028, train ANN loss : 3.348909854888916\n",
      "AE loss : 0.839706540107727, ANN loss : 3.299715042114258, Total loss : 87.27037048339844\n",
      "learning rate A :  tf.Tensor(9.931749e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 633 is 0.0794 sec\n",
      "train AE loss : 0.8050947785377502, train ANN loss : 3.3457818031311035\n",
      "AE loss : 0.8215764760971069, ANN loss : 3.2983686923980713, Total loss : 85.45601654052734\n",
      "learning rate A :  tf.Tensor(9.931749e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 634 is 0.0789 sec\n",
      "train AE loss : 0.7876778244972229, train ANN loss : 3.346846580505371\n",
      "AE loss : 0.7325375080108643, ANN loss : 3.30029034614563, Total loss : 76.55404663085938\n",
      "learning rate A :  tf.Tensor(9.931749e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 635 is 0.0790 sec\n",
      "train AE loss : 0.7016769051551819, train ANN loss : 3.343654155731201\n",
      "AE loss : 0.7014139294624329, ANN loss : 3.3023526668548584, Total loss : 73.4437484741211\n",
      "learning rate A :  tf.Tensor(9.9315395e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 636 is 0.0778 sec\n",
      "train AE loss : 0.6715726256370544, train ANN loss : 3.3463101387023926\n",
      "AE loss : 0.6683197617530823, ANN loss : 3.305483818054199, Total loss : 70.13746643066406\n",
      "learning rate A :  tf.Tensor(9.9315395e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 637 is 0.0794 sec\n",
      "train AE loss : 0.6397371292114258, train ANN loss : 3.342595100402832\n",
      "AE loss : 0.6418830156326294, ANN loss : 3.308663845062256, Total loss : 67.49696350097656\n",
      "learning rate A :  tf.Tensor(9.931331e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 638 is 0.1149 sec\n",
      "train AE loss : 0.6142324805259705, train ANN loss : 3.3491551876068115\n",
      "AE loss : 0.697128415107727, ANN loss : 3.3013641834259033, Total loss : 73.01421356201172\n",
      "learning rate A :  tf.Tensor(9.931331e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 639 is 0.0785 sec\n",
      "train AE loss : 0.6675213575363159, train ANN loss : 3.339855432510376\n",
      "AE loss : 0.7958998084068298, ANN loss : 3.2949681282043457, Total loss : 82.88494873046875\n",
      "learning rate A :  tf.Tensor(9.931331e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 640 is 0.0788 sec\n",
      "train AE loss : 0.7629987597465515, train ANN loss : 3.3421192169189453\n",
      "AE loss : 0.8360587358474731, ANN loss : 3.2941269874572754, Total loss : 86.9000015258789\n",
      "learning rate A :  tf.Tensor(9.931331e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 641 is 0.0790 sec\n",
      "train AE loss : 0.8018198013305664, train ANN loss : 3.344268560409546\n",
      "AE loss : 0.7814693450927734, ANN loss : 3.2928197383880615, Total loss : 81.43975067138672\n",
      "learning rate A :  tf.Tensor(9.931331e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 642 is 0.0794 sec\n",
      "train AE loss : 0.7491302490234375, train ANN loss : 3.341888904571533\n",
      "AE loss : 0.7462725639343262, ANN loss : 3.294053316116333, Total loss : 77.92131805419922\n",
      "learning rate A :  tf.Tensor(9.931121e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 643 is 0.0778 sec\n",
      "train AE loss : 0.7150787115097046, train ANN loss : 3.3337411880493164\n",
      "AE loss : 0.7137614488601685, ANN loss : 3.2959933280944824, Total loss : 74.67213439941406\n",
      "learning rate A :  tf.Tensor(9.930912e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 644 is 0.0783 sec\n",
      "train AE loss : 0.683647632598877, train ANN loss : 3.3407487869262695\n",
      "AE loss : 0.700203001499176, ANN loss : 3.2953975200653076, Total loss : 73.3156967163086\n",
      "learning rate A :  tf.Tensor(9.930912e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 645 is 0.0885 sec\n",
      "train AE loss : 0.6706985235214233, train ANN loss : 3.3384439945220947\n",
      "AE loss : 0.7300019264221191, ANN loss : 3.291508197784424, Total loss : 76.29169464111328\n",
      "learning rate A :  tf.Tensor(9.930912e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 646 is 0.0803 sec\n",
      "train AE loss : 0.6995909214019775, train ANN loss : 3.338610887527466\n",
      "AE loss : 0.7633781433105469, ANN loss : 3.288492441177368, Total loss : 79.62630462646484\n",
      "learning rate A :  tf.Tensor(9.930912e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 647 is 0.0788 sec\n",
      "train AE loss : 0.7318593263626099, train ANN loss : 3.3329806327819824\n",
      "AE loss : 0.7296410202980042, ANN loss : 3.290186643600464, Total loss : 76.25428771972656\n",
      "learning rate A :  tf.Tensor(9.930702e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 648 is 0.0771 sec\n",
      "train AE loss : 0.6992293000221252, train ANN loss : 3.329226493835449\n",
      "AE loss : 0.6984660029411316, ANN loss : 3.2925491333007812, Total loss : 73.13915252685547\n",
      "learning rate A :  tf.Tensor(9.930493e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 649 is 0.0792 sec\n",
      "train AE loss : 0.6691039800643921, train ANN loss : 3.3369991779327393\n",
      "AE loss : 0.7734689712524414, ANN loss : 3.2868082523345947, Total loss : 80.63369750976562\n",
      "learning rate A :  tf.Tensor(9.930493e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 650 is 0.0788 sec\n",
      "train AE loss : 0.7415212392807007, train ANN loss : 3.3347668647766113\n",
      "AE loss : 0.8283841609954834, ANN loss : 3.285562753677368, Total loss : 86.12398529052734\n",
      "learning rate A :  tf.Tensor(9.930493e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 651 is 0.0786 sec\n",
      "train AE loss : 0.7945618629455566, train ANN loss : 3.3350675106048584\n",
      "AE loss : 0.7892671227455139, ANN loss : 3.2856922149658203, Total loss : 82.21240234375\n",
      "learning rate A :  tf.Tensor(9.930284e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 652 is 0.0781 sec\n",
      "train AE loss : 0.7567029595375061, train ANN loss : 3.333282947540283\n",
      "AE loss : 0.7532188296318054, ANN loss : 3.286661386489868, Total loss : 78.60855102539062\n",
      "learning rate A :  tf.Tensor(9.930075e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 653 is 0.0764 sec\n",
      "train AE loss : 0.7218283414840698, train ANN loss : 3.330721855163574\n",
      "AE loss : 0.7862832546234131, ANN loss : 3.2849819660186768, Total loss : 81.9133071899414\n",
      "learning rate A :  tf.Tensor(9.930075e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 654 is 0.0793 sec\n",
      "train AE loss : 0.7537904977798462, train ANN loss : 3.3340446949005127\n",
      "AE loss : 0.7719902396202087, ANN loss : 3.2846150398254395, Total loss : 80.48363494873047\n",
      "learning rate A :  tf.Tensor(9.930075e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 655 is 0.0793 sec\n",
      "train AE loss : 0.7399776577949524, train ANN loss : 3.32816481590271\n",
      "AE loss : 0.7372617721557617, ANN loss : 3.286036491394043, Total loss : 77.01221466064453\n",
      "learning rate A :  tf.Tensor(9.929865e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 656 is 0.0773 sec\n",
      "train AE loss : 0.7063917517662048, train ANN loss : 3.3307688236236572\n",
      "AE loss : 0.7052112221717834, ANN loss : 3.2881689071655273, Total loss : 73.80928802490234\n",
      "learning rate A :  tf.Tensor(9.929656e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 657 is 0.0767 sec\n",
      "train AE loss : 0.675435483455658, train ANN loss : 3.33215069770813\n",
      "AE loss : 0.6755892634391785, ANN loss : 3.290863037109375, Total loss : 70.84980010986328\n",
      "learning rate A :  tf.Tensor(9.929447e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 658 is 0.0783 sec\n",
      "train AE loss : 0.6468413472175598, train ANN loss : 3.3296120166778564\n",
      "AE loss : 0.6481834650039673, ANN loss : 3.2939887046813965, Total loss : 68.1123275756836\n",
      "learning rate A :  tf.Tensor(9.9292374e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 659 is 0.0772 sec\n",
      "train AE loss : 0.6204230189323425, train ANN loss : 3.332334280014038\n",
      "AE loss : 0.7195985317230225, ANN loss : 3.2855329513549805, Total loss : 75.2453842163086\n",
      "learning rate A :  tf.Tensor(9.9292374e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 660 is 0.0804 sec\n",
      "train AE loss : 0.6893066763877869, train ANN loss : 3.3359296321868896\n",
      "AE loss : 0.688912034034729, ANN loss : 3.2879204750061035, Total loss : 72.17913055419922\n",
      "learning rate A :  tf.Tensor(9.9290286e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 661 is 0.0788 sec\n",
      "train AE loss : 0.6596754789352417, train ANN loss : 3.3315815925598145\n",
      "AE loss : 0.8039914965629578, ANN loss : 3.281449556350708, Total loss : 83.68060302734375\n",
      "learning rate A :  tf.Tensor(9.9290286e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 662 is 0.0793 sec\n",
      "train AE loss : 0.7707231640815735, train ANN loss : 3.331617593765259\n",
      "AE loss : 0.7665976285934448, ANN loss : 3.2820193767547607, Total loss : 79.94178009033203\n",
      "learning rate A :  tf.Tensor(9.928819e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 663 is 0.0770 sec\n",
      "train AE loss : 0.7345619201660156, train ANN loss : 3.3259527683258057\n",
      "AE loss : 0.8433995842933655, ANN loss : 3.2807207107543945, Total loss : 87.62068176269531\n",
      "learning rate A :  tf.Tensor(9.928819e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 664 is 0.0810 sec\n",
      "train AE loss : 0.8086261749267578, train ANN loss : 3.3284804821014404\n",
      "AE loss : 0.808465301990509, ANN loss : 3.2795355319976807, Total loss : 84.12606048583984\n",
      "learning rate A :  tf.Tensor(9.928819e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 665 is 0.0777 sec\n",
      "train AE loss : 0.7747237682342529, train ANN loss : 3.322537422180176\n",
      "AE loss : 0.7201797962188721, ANN loss : 3.28287672996521, Total loss : 75.30085754394531\n",
      "learning rate A :  tf.Tensor(9.928819e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 666 is 0.0786 sec\n",
      "train AE loss : 0.689376175403595, train ANN loss : 3.3238394260406494\n",
      "AE loss : 0.6650471091270447, ANN loss : 3.288451671600342, Total loss : 69.79315948486328\n",
      "learning rate A :  tf.Tensor(9.928819e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 667 is 0.0786 sec\n",
      "train AE loss : 0.6361808180809021, train ANN loss : 3.3291091918945312\n",
      "AE loss : 0.6873967051506042, ANN loss : 3.2838892936706543, Total loss : 72.0235595703125\n",
      "learning rate A :  tf.Tensor(9.928819e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 668 is 0.0777 sec\n",
      "train AE loss : 0.6577327847480774, train ANN loss : 3.321065664291382\n",
      "AE loss : 0.6587048768997192, ANN loss : 3.287431478500366, Total loss : 69.15792083740234\n",
      "learning rate A :  tf.Tensor(9.92861e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 669 is 0.0761 sec\n",
      "train AE loss : 0.6300715804100037, train ANN loss : 3.3223254680633545\n",
      "AE loss : 0.7686246037483215, ANN loss : 3.2743847370147705, Total loss : 80.1368408203125\n",
      "learning rate A :  tf.Tensor(9.92861e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 670 is 0.0789 sec\n",
      "train AE loss : 0.7360382676124573, train ANN loss : 3.3264365196228027\n",
      "AE loss : 0.8650211095809937, ANN loss : 3.2706637382507324, Total loss : 89.77277374267578\n",
      "learning rate A :  tf.Tensor(9.92861e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 671 is 0.0778 sec\n",
      "train AE loss : 0.8291042447090149, train ANN loss : 3.3215975761413574\n",
      "AE loss : 0.8352838158607483, ANN loss : 3.269310235977173, Total loss : 86.79769134521484\n",
      "learning rate A :  tf.Tensor(9.92861e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 672 is 0.0796 sec\n",
      "train AE loss : 0.8004177808761597, train ANN loss : 3.3223955631256104\n",
      "AE loss : 0.7946334481239319, ANN loss : 3.269984245300293, Total loss : 82.73332977294922\n",
      "learning rate A :  tf.Tensor(9.928401e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 673 is 0.0789 sec\n",
      "train AE loss : 0.7611464858055115, train ANN loss : 3.319943428039551\n",
      "AE loss : 0.7572953104972839, ANN loss : 3.2715749740600586, Total loss : 79.0010986328125\n",
      "learning rate A :  tf.Tensor(9.928192e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 674 is 0.0786 sec\n",
      "train AE loss : 0.7250744104385376, train ANN loss : 3.322381019592285\n",
      "AE loss : 0.7314083576202393, ANN loss : 3.2731149196624756, Total loss : 76.41395568847656\n",
      "learning rate A :  tf.Tensor(9.928192e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 675 is 0.0801 sec\n",
      "train AE loss : 0.7001011967658997, train ANN loss : 3.313232421875\n",
      "AE loss : 0.6990600824356079, ANN loss : 3.2760825157165527, Total loss : 73.18208312988281\n",
      "learning rate A :  tf.Tensor(9.927982e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 676 is 0.0780 sec\n",
      "train AE loss : 0.6688734889030457, train ANN loss : 3.317087411880493\n",
      "AE loss : 0.7213537096977234, ANN loss : 3.2736868858337402, Total loss : 75.4090576171875\n",
      "learning rate A :  tf.Tensor(9.927982e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 677 is 0.0805 sec\n",
      "train AE loss : 0.6902951002120972, train ANN loss : 3.3153977394104004\n",
      "AE loss : 0.6896104216575623, ANN loss : 3.2767882347106934, Total loss : 72.23783874511719\n",
      "learning rate A :  tf.Tensor(9.9277735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 678 is 0.0770 sec\n",
      "train AE loss : 0.659659743309021, train ANN loss : 3.314828872680664\n",
      "AE loss : 0.7700772285461426, ANN loss : 3.2690889835357666, Total loss : 80.27680969238281\n",
      "learning rate A :  tf.Tensor(9.9277735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 679 is 0.0799 sec\n",
      "train AE loss : 0.7372432351112366, train ANN loss : 3.3119115829467773\n",
      "AE loss : 0.7342650890350342, ANN loss : 3.271131753921509, Total loss : 76.69763946533203\n",
      "learning rate A :  tf.Tensor(9.927565e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 680 is 0.0766 sec\n",
      "train AE loss : 0.7026719450950623, train ANN loss : 3.3217153549194336\n",
      "AE loss : 0.701346218585968, ANN loss : 3.273904800415039, Total loss : 73.40852355957031\n",
      "learning rate A :  tf.Tensor(9.927355e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 681 is 0.0768 sec\n",
      "train AE loss : 0.6708837747573853, train ANN loss : 3.3153834342956543\n",
      "AE loss : 0.8218722939491272, ANN loss : 3.2655529975891113, Total loss : 85.45278930664062\n",
      "learning rate A :  tf.Tensor(9.927355e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 682 is 0.0804 sec\n",
      "train AE loss : 0.7871990203857422, train ANN loss : 3.317446708679199\n",
      "AE loss : 0.8587626218795776, ANN loss : 3.2641801834106445, Total loss : 89.14043426513672\n",
      "learning rate A :  tf.Tensor(9.927355e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 683 is 0.0785 sec\n",
      "train AE loss : 0.8229178190231323, train ANN loss : 3.3196234703063965\n",
      "AE loss : 0.81540846824646, ANN loss : 3.263967514038086, Total loss : 84.80481719970703\n",
      "learning rate A :  tf.Tensor(9.927146e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 684 is 0.0768 sec\n",
      "train AE loss : 0.7810283303260803, train ANN loss : 3.313483238220215\n",
      "AE loss : 0.7678927779197693, ANN loss : 3.2641448974609375, Total loss : 80.05342102050781\n",
      "learning rate A :  tf.Tensor(9.927146e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 685 is 0.0796 sec\n",
      "train AE loss : 0.7352602481842041, train ANN loss : 3.312668800354004\n",
      "AE loss : 0.7140706777572632, ANN loss : 3.266880512237549, Total loss : 74.6739501953125\n",
      "learning rate A :  tf.Tensor(9.927146e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 686 is 0.0797 sec\n",
      "train AE loss : 0.683468759059906, train ANN loss : 3.310751438140869\n",
      "AE loss : 0.6828930377960205, ANN loss : 3.2702414989471436, Total loss : 71.5595474243164\n",
      "learning rate A :  tf.Tensor(9.926937e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 687 is 0.0779 sec\n",
      "train AE loss : 0.6534044742584229, train ANN loss : 3.315868854522705\n",
      "AE loss : 0.6541411876678467, ANN loss : 3.274033546447754, Total loss : 68.68814849853516\n",
      "learning rate A :  tf.Tensor(9.926728e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 688 is 0.0779 sec\n",
      "train AE loss : 0.6257147789001465, train ANN loss : 3.316781759262085\n",
      "AE loss : 0.6933373808860779, ANN loss : 3.266896963119507, Total loss : 72.60063934326172\n",
      "learning rate A :  tf.Tensor(9.926728e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 689 is 0.0798 sec\n",
      "train AE loss : 0.6635284423828125, train ANN loss : 3.312190055847168\n",
      "AE loss : 0.7972376346588135, ANN loss : 3.2572906017303467, Total loss : 82.9810562133789\n",
      "learning rate A :  tf.Tensor(9.926728e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 690 is 0.0804 sec\n",
      "train AE loss : 0.7637543678283691, train ANN loss : 3.313385486602783\n",
      "AE loss : 0.8538124561309814, ANN loss : 3.255805730819702, Total loss : 88.63705444335938\n",
      "learning rate A :  tf.Tensor(9.926728e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 691 is 0.0922 sec\n",
      "train AE loss : 0.8182113766670227, train ANN loss : 3.3046417236328125\n",
      "AE loss : 0.8203692436218262, ANN loss : 3.255767583847046, Total loss : 85.29269409179688\n",
      "learning rate A :  tf.Tensor(9.926728e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 692 is 0.0918 sec\n",
      "train AE loss : 0.7857925295829773, train ANN loss : 3.3133535385131836\n",
      "AE loss : 0.780029833316803, ANN loss : 3.2568767070770264, Total loss : 81.2598648071289\n",
      "learning rate A :  tf.Tensor(9.9265184e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 693 is 0.0801 sec\n",
      "train AE loss : 0.7468135952949524, train ANN loss : 3.3095717430114746\n",
      "AE loss : 0.7390626668930054, ANN loss : 3.258735418319702, Total loss : 77.16500091552734\n",
      "learning rate A :  tf.Tensor(9.9265184e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 694 is 0.0804 sec\n",
      "train AE loss : 0.7072794437408447, train ANN loss : 3.3054044246673584\n",
      "AE loss : 0.7243273258209229, ANN loss : 3.258949041366577, Total loss : 75.69168090820312\n",
      "learning rate A :  tf.Tensor(9.9265184e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 695 is 0.0800 sec\n",
      "train AE loss : 0.6932037472724915, train ANN loss : 3.3075966835021973\n",
      "AE loss : 0.7546748518943787, ANN loss : 3.25483775138855, Total loss : 78.72232055664062\n",
      "learning rate A :  tf.Tensor(9.9265184e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 696 is 0.0811 sec\n",
      "train AE loss : 0.7225728034973145, train ANN loss : 3.300281524658203\n",
      "AE loss : 0.7201206684112549, ANN loss : 3.257962226867676, Total loss : 75.27003479003906\n",
      "learning rate A :  tf.Tensor(9.9263096e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 697 is 0.0777 sec\n",
      "train AE loss : 0.6892507076263428, train ANN loss : 3.3060784339904785\n",
      "AE loss : 0.6883651614189148, ANN loss : 3.261693000793457, Total loss : 72.09820556640625\n",
      "learning rate A :  tf.Tensor(9.9261e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 698 is 0.0785 sec\n",
      "train AE loss : 0.6586710810661316, train ANN loss : 3.308741807937622\n",
      "AE loss : 0.659136176109314, ANN loss : 3.2658560276031494, Total loss : 69.17948150634766\n",
      "learning rate A :  tf.Tensor(9.925891e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 699 is 0.0807 sec\n",
      "train AE loss : 0.6305580735206604, train ANN loss : 3.3129968643188477\n",
      "AE loss : 0.6321992874145508, ANN loss : 3.27038836479187, Total loss : 66.49031829833984\n",
      "learning rate A :  tf.Tensor(9.925682e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 700 is 0.0781 sec\n",
      "train AE loss : 0.6046749949455261, train ANN loss : 3.315854549407959\n",
      "AE loss : 0.7741017937660217, ANN loss : 3.2507550716400146, Total loss : 80.66093444824219\n",
      "learning rate A :  tf.Tensor(9.925682e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 701 is 0.0790 sec\n",
      "train AE loss : 0.7412612438201904, train ANN loss : 3.304868221282959\n",
      "AE loss : 0.7378467917442322, ANN loss : 3.253201961517334, Total loss : 77.03787231445312\n",
      "learning rate A :  tf.Tensor(9.925473e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 702 is 0.0784 sec\n",
      "train AE loss : 0.7062900066375732, train ANN loss : 3.301234722137451\n",
      "AE loss : 0.7045851945877075, ANN loss : 3.2564384937286377, Total loss : 73.71495819091797\n",
      "learning rate A :  tf.Tensor(9.925264e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 703 is 0.0784 sec\n",
      "train AE loss : 0.6742477416992188, train ANN loss : 3.3048384189605713\n",
      "AE loss : 0.6740136742591858, ANN loss : 3.2601802349090576, Total loss : 70.66154479980469\n",
      "learning rate A :  tf.Tensor(9.9250545e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 704 is 0.0786 sec\n",
      "train AE loss : 0.6448362469673157, train ANN loss : 3.3002352714538574\n",
      "AE loss : 0.6458722949028015, ANN loss : 3.264397144317627, Total loss : 67.85162353515625\n",
      "learning rate A :  tf.Tensor(9.924846e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 705 is 0.0959 sec\n",
      "train AE loss : 0.617790699005127, train ANN loss : 3.3066155910491943\n",
      "AE loss : 0.8875558376312256, ANN loss : 3.2471976280212402, Total loss : 92.00277709960938\n",
      "learning rate A :  tf.Tensor(9.924846e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 706 is 0.0791 sec\n",
      "train AE loss : 0.8507531881332397, train ANN loss : 3.3046958446502686\n",
      "AE loss : 0.8413683772087097, ANN loss : 3.24626088142395, Total loss : 87.38310241699219\n",
      "learning rate A :  tf.Tensor(9.924636e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 707 is 0.0769 sec\n",
      "train AE loss : 0.8061103224754333, train ANN loss : 3.304872751235962\n",
      "AE loss : 0.9892461895942688, ANN loss : 3.2533369064331055, Total loss : 102.1779556274414\n",
      "learning rate A :  tf.Tensor(9.924636e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 708 is 0.0794 sec\n",
      "train AE loss : 0.9489864706993103, train ANN loss : 3.3209493160247803\n",
      "AE loss : 0.9334700107574463, ANN loss : 3.248518705368042, Total loss : 96.59552001953125\n",
      "learning rate A :  tf.Tensor(9.924427e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 709 is 0.0765 sec\n",
      "train AE loss : 0.8949770331382751, train ANN loss : 3.3175323009490967\n",
      "AE loss : 0.8467379212379456, ANN loss : 3.244967460632324, Total loss : 87.91876220703125\n",
      "learning rate A :  tf.Tensor(9.924427e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 710 is 0.0789 sec\n",
      "train AE loss : 0.8110355138778687, train ANN loss : 3.30719256401062\n",
      "AE loss : 0.6925855875015259, ANN loss : 3.2556028366088867, Total loss : 72.51416015625\n",
      "learning rate A :  tf.Tensor(9.924427e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 711 is 0.0788 sec\n",
      "train AE loss : 0.66232830286026, train ANN loss : 3.3013455867767334\n",
      "AE loss : 0.6349332332611084, ANN loss : 3.2661311626434326, Total loss : 66.75946044921875\n",
      "learning rate A :  tf.Tensor(9.924427e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 712 is 0.0808 sec\n",
      "train AE loss : 0.606927752494812, train ANN loss : 3.3057775497436523\n",
      "AE loss : 0.6092542409896851, ANN loss : 3.2711880207061768, Total loss : 64.19660949707031\n",
      "learning rate A :  tf.Tensor(9.924218e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 713 is 0.0763 sec\n",
      "train AE loss : 0.582285463809967, train ANN loss : 3.3035264015197754\n",
      "AE loss : 0.6863282322883606, ANN loss : 3.255246162414551, Total loss : 71.88806915283203\n",
      "learning rate A :  tf.Tensor(9.924218e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 714 is 0.0792 sec\n",
      "train AE loss : 0.6564124822616577, train ANN loss : 3.3003625869750977\n",
      "AE loss : 0.6566546559333801, ANN loss : 3.2596371173858643, Total loss : 68.92510223388672\n",
      "learning rate A :  tf.Tensor(9.924009e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 715 is 0.0763 sec\n",
      "train AE loss : 0.6278873682022095, train ANN loss : 3.3087964057922363\n",
      "AE loss : 0.6293454766273499, ANN loss : 3.2644224166870117, Total loss : 66.19896697998047\n",
      "learning rate A :  tf.Tensor(9.9238e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 716 is 0.0764 sec\n",
      "train AE loss : 0.6016546487808228, train ANN loss : 3.305460214614868\n",
      "AE loss : 0.821686863899231, ANN loss : 3.241060256958008, Total loss : 85.40974426269531\n",
      "learning rate A :  tf.Tensor(9.9238e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 717 is 0.0794 sec\n",
      "train AE loss : 0.7869586944580078, train ANN loss : 3.29030179977417\n",
      "AE loss : 0.7806587815284729, ANN loss : 3.242414951324463, Total loss : 81.30829620361328\n",
      "learning rate A :  tf.Tensor(9.9235906e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 718 is 0.0771 sec\n",
      "train AE loss : 0.7473474144935608, train ANN loss : 3.296325445175171\n",
      "AE loss : 0.7431706786155701, ANN loss : 3.2447705268859863, Total loss : 77.56184387207031\n",
      "learning rate A :  tf.Tensor(9.923382e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 719 is 0.0773 sec\n",
      "train AE loss : 0.7111880779266357, train ANN loss : 3.293208360671997\n",
      "AE loss : 0.7088319063186646, ANN loss : 3.2479357719421387, Total loss : 74.1311264038086\n",
      "learning rate A :  tf.Tensor(9.923172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 720 is 0.0777 sec\n",
      "train AE loss : 0.6781065464019775, train ANN loss : 3.3004539012908936\n",
      "AE loss : 0.6773285269737244, ANN loss : 3.25173020362854, Total loss : 70.98457336425781\n",
      "learning rate A :  tf.Tensor(9.922963e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 721 is 0.0768 sec\n",
      "train AE loss : 0.647807240486145, train ANN loss : 3.302905321121216\n",
      "AE loss : 0.9410814642906189, ANN loss : 3.2429001331329346, Total loss : 97.35104370117188\n",
      "learning rate A :  tf.Tensor(9.922963e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 722 is 0.0794 sec\n",
      "train AE loss : 0.9022124409675598, train ANN loss : 3.3080317974090576\n",
      "AE loss : 0.9974765777587891, ANN loss : 3.2468559741973877, Total loss : 102.99452209472656\n",
      "learning rate A :  tf.Tensor(9.922963e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 723 is 0.0796 sec\n",
      "train AE loss : 0.9567407369613647, train ANN loss : 3.310286045074463\n",
      "AE loss : 0.9399793148040771, ANN loss : 3.2418270111083984, Total loss : 97.23976135253906\n",
      "learning rate A :  tf.Tensor(9.9227545e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 724 is 0.0773 sec\n",
      "train AE loss : 0.9011039137840271, train ANN loss : 3.3103442192077637\n",
      "AE loss : 0.8879300355911255, ANN loss : 3.238903284072876, Total loss : 92.03190612792969\n",
      "learning rate A :  tf.Tensor(9.922545e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 725 is 0.0775 sec\n",
      "train AE loss : 0.8507790565490723, train ANN loss : 3.2981107234954834\n",
      "AE loss : 0.7926086783409119, ANN loss : 3.237898826599121, Total loss : 82.49876403808594\n",
      "learning rate A :  tf.Tensor(9.922545e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 726 is 0.0795 sec\n",
      "train AE loss : 0.7585941553115845, train ANN loss : 3.286252021789551\n",
      "AE loss : 0.6895177364349365, ANN loss : 3.246683120727539, Total loss : 72.1984634399414\n",
      "learning rate A :  tf.Tensor(9.922545e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 727 is 0.0807 sec\n",
      "train AE loss : 0.6591675281524658, train ANN loss : 3.2926247119903564\n",
      "AE loss : 0.6605278849601746, ANN loss : 3.2513978481292725, Total loss : 69.30418395996094\n",
      "learning rate A :  tf.Tensor(9.922545e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 728 is 0.0774 sec\n",
      "train AE loss : 0.6311726570129395, train ANN loss : 3.294144868850708\n",
      "AE loss : 0.6323626041412354, ANN loss : 3.2561967372894287, Total loss : 66.4924545288086\n",
      "learning rate A :  tf.Tensor(9.9223354e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 729 is 0.0763 sec\n",
      "train AE loss : 0.6041263937950134, train ANN loss : 3.2979421615600586\n",
      "AE loss : 0.7078179121017456, ANN loss : 3.2430288791656494, Total loss : 74.02482604980469\n",
      "learning rate A :  tf.Tensor(9.9223354e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 730 is 0.0779 sec\n",
      "train AE loss : 0.6765196919441223, train ANN loss : 3.294111728668213\n",
      "AE loss : 0.833845853805542, ANN loss : 3.2330963611602783, Total loss : 86.61767578125\n",
      "learning rate A :  tf.Tensor(9.9223354e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 731 is 0.0774 sec\n",
      "train AE loss : 0.797976553440094, train ANN loss : 3.2873380184173584\n",
      "AE loss : 0.9090704321861267, ANN loss : 3.2320523262023926, Total loss : 94.13909912109375\n",
      "learning rate A :  tf.Tensor(9.9223354e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 732 is 0.0791 sec\n",
      "train AE loss : 0.8706037402153015, train ANN loss : 3.2864468097686768\n",
      "AE loss : 0.8486859798431396, ANN loss : 3.2299442291259766, Total loss : 88.09854125976562\n",
      "learning rate A :  tf.Tensor(9.9223354e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 733 is 0.0780 sec\n",
      "train AE loss : 0.8123236298561096, train ANN loss : 3.286836862564087\n",
      "AE loss : 0.8043490648269653, ANN loss : 3.2309720516204834, Total loss : 83.66587829589844\n",
      "learning rate A :  tf.Tensor(9.9221266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 734 is 0.0757 sec\n",
      "train AE loss : 0.7695059776306152, train ANN loss : 3.290313243865967\n",
      "AE loss : 0.733443558216095, ANN loss : 3.2363481521606445, Total loss : 76.58070373535156\n",
      "learning rate A :  tf.Tensor(9.9221266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 735 is 0.0785 sec\n",
      "train AE loss : 0.7011762857437134, train ANN loss : 3.2867112159729004\n",
      "AE loss : 0.7167247533798218, ANN loss : 3.2380261421203613, Total loss : 74.91050720214844\n",
      "learning rate A :  tf.Tensor(9.9221266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 736 is 0.0778 sec\n",
      "train AE loss : 0.6849943399429321, train ANN loss : 3.281938076019287\n",
      "AE loss : 0.7687603235244751, ANN loss : 3.2312209606170654, Total loss : 80.10725402832031\n",
      "learning rate A :  tf.Tensor(9.9221266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 737 is 0.0788 sec\n",
      "train AE loss : 0.7349538207054138, train ANN loss : 3.2833173274993896\n",
      "AE loss : 0.8428476452827454, ANN loss : 3.2262473106384277, Total loss : 87.51102447509766\n",
      "learning rate A :  tf.Tensor(9.9221266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 738 is 0.0806 sec\n",
      "train AE loss : 0.806311309337616, train ANN loss : 3.281958818435669\n",
      "AE loss : 0.8657744526863098, ANN loss : 3.2253353595733643, Total loss : 89.80278015136719\n",
      "learning rate A :  tf.Tensor(9.9221266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 739 is 0.0777 sec\n",
      "train AE loss : 0.8283652067184448, train ANN loss : 3.2878646850585938\n",
      "AE loss : 0.8093182444572449, ANN loss : 3.226092576980591, Total loss : 84.15791320800781\n",
      "learning rate A :  tf.Tensor(9.9221266e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 740 is 0.0778 sec\n",
      "train AE loss : 0.7738984227180481, train ANN loss : 3.2767159938812256\n",
      "AE loss : 0.7680520415306091, ANN loss : 3.2286550998687744, Total loss : 80.03385925292969\n",
      "learning rate A :  tf.Tensor(9.921917e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 741 is 0.0766 sec\n",
      "train AE loss : 0.7340940237045288, train ANN loss : 3.2784476280212402\n",
      "AE loss : 0.7304546236991882, ANN loss : 3.232163190841675, Total loss : 76.27762603759766\n",
      "learning rate A :  tf.Tensor(9.921708e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 742 is 0.0764 sec\n",
      "train AE loss : 0.6978664994239807, train ANN loss : 3.2814812660217285\n",
      "AE loss : 0.6961259245872498, ANN loss : 3.2364206314086914, Total loss : 72.84901428222656\n",
      "learning rate A :  tf.Tensor(9.9214994e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 743 is 0.0764 sec\n",
      "train AE loss : 0.6648248434066772, train ANN loss : 3.2843422889709473\n",
      "AE loss : 0.6647120118141174, ANN loss : 3.2412800788879395, Total loss : 69.71247863769531\n",
      "learning rate A :  tf.Tensor(9.92129e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 744 is 0.0773 sec\n",
      "train AE loss : 0.634621798992157, train ANN loss : 3.289362668991089\n",
      "AE loss : 0.7187032699584961, ANN loss : 3.2315938472747803, Total loss : 75.1019287109375\n",
      "learning rate A :  tf.Tensor(9.92129e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 745 is 0.0779 sec\n",
      "train AE loss : 0.6866430640220642, train ANN loss : 3.2769248485565186\n",
      "AE loss : 0.8329748511314392, ANN loss : 3.221942663192749, Total loss : 86.5194320678711\n",
      "learning rate A :  tf.Tensor(9.92129e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 746 is 0.0772 sec\n",
      "train AE loss : 0.7968016266822815, train ANN loss : 3.279151201248169\n",
      "AE loss : 0.7896228432655334, ANN loss : 3.2235684394836426, Total loss : 82.18585205078125\n",
      "learning rate A :  tf.Tensor(9.921081e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 747 is 0.0769 sec\n",
      "train AE loss : 0.7549753189086914, train ANN loss : 3.279439687728882\n",
      "AE loss : 0.7501887679100037, ANN loss : 3.2263100147247314, Total loss : 78.24518585205078\n",
      "learning rate A :  tf.Tensor(9.920872e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 748 is 0.0753 sec\n",
      "train AE loss : 0.7169687747955322, train ANN loss : 3.275277614593506\n",
      "AE loss : 0.8871687650680542, ANN loss : 3.220363140106201, Total loss : 91.9372329711914\n",
      "learning rate A :  tf.Tensor(9.920872e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 749 is 0.0773 sec\n",
      "train AE loss : 0.8491575717926025, train ANN loss : 3.287940263748169\n",
      "AE loss : 0.9117757678031921, ANN loss : 3.220215082168579, Total loss : 94.39778900146484\n",
      "learning rate A :  tf.Tensor(9.920872e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 750 is 0.0798 sec\n",
      "train AE loss : 0.8728393316268921, train ANN loss : 3.2820379734039307\n",
      "AE loss : 0.8093055486679077, ANN loss : 3.221247911453247, Total loss : 84.15180969238281\n",
      "learning rate A :  tf.Tensor(9.920872e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 751 is 0.0783 sec\n",
      "train AE loss : 0.773723840713501, train ANN loss : 3.2756893634796143\n",
      "AE loss : 0.7090317606925964, ANN loss : 3.2316017150878906, Total loss : 74.13478088378906\n",
      "learning rate A :  tf.Tensor(9.920872e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 752 is 0.0839 sec\n",
      "train AE loss : 0.6771349310874939, train ANN loss : 3.2765300273895264\n",
      "AE loss : 0.6763470768928528, ANN loss : 3.23663330078125, Total loss : 70.871337890625\n",
      "learning rate A :  tf.Tensor(9.920663e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 753 is 0.0792 sec\n",
      "train AE loss : 0.6457059979438782, train ANN loss : 3.285243272781372\n",
      "AE loss : 0.6886557340621948, ANN loss : 3.234041452407837, Total loss : 72.099609375\n",
      "learning rate A :  tf.Tensor(9.920663e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 754 is 0.0783 sec\n",
      "train AE loss : 0.6575608849525452, train ANN loss : 3.284975051879883\n",
      "AE loss : 0.6577023863792419, ANN loss : 3.239464521408081, Total loss : 69.00970458984375\n",
      "learning rate A :  tf.Tensor(9.920454e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 755 is 0.0765 sec\n",
      "train AE loss : 0.627810537815094, train ANN loss : 3.282742977142334\n",
      "AE loss : 0.629349946975708, ANN loss : 3.245201587677002, Total loss : 66.1801986694336\n",
      "learning rate A :  tf.Tensor(9.920245e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 756 is 0.0775 sec\n",
      "train AE loss : 0.6006203293800354, train ANN loss : 3.2858705520629883\n",
      "AE loss : 0.6032872200012207, ANN loss : 3.2512004375457764, Total loss : 63.579917907714844\n",
      "learning rate A :  tf.Tensor(9.920036e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 757 is 0.0764 sec\n",
      "train AE loss : 0.575666069984436, train ANN loss : 3.294644355773926\n",
      "AE loss : 0.745222806930542, ANN loss : 3.2229788303375244, Total loss : 77.7452621459961\n",
      "learning rate A :  tf.Tensor(9.920036e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 758 is 0.0787 sec\n",
      "train AE loss : 0.7119807600975037, train ANN loss : 3.2644927501678467\n",
      "AE loss : 0.9621715545654297, ANN loss : 3.217590570449829, Total loss : 99.43474578857422\n",
      "learning rate A :  tf.Tensor(9.920036e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 759 is 0.0799 sec\n",
      "train AE loss : 0.9214048385620117, train ANN loss : 3.274862766265869\n",
      "AE loss : 0.9057986736297607, ANN loss : 3.2150018215179443, Total loss : 93.79487609863281\n",
      "learning rate A :  tf.Tensor(9.919827e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 760 is 0.0764 sec\n",
      "train AE loss : 0.866905927658081, train ANN loss : 3.279472827911377\n",
      "AE loss : 0.8549814224243164, ANN loss : 3.2144246101379395, Total loss : 88.71256256103516\n",
      "learning rate A :  tf.Tensor(9.919618e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 761 is 0.0773 sec\n",
      "train AE loss : 0.8178286552429199, train ANN loss : 3.2730424404144287\n",
      "AE loss : 1.0012065172195435, ANN loss : 3.219679117202759, Total loss : 103.34033966064453\n",
      "learning rate A :  tf.Tensor(9.919618e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 762 is 0.0786 sec\n",
      "train AE loss : 0.9591189026832581, train ANN loss : 3.2873246669769287\n",
      "AE loss : 0.9407928586006165, ANN loss : 3.215235710144043, Total loss : 97.29451751708984\n",
      "learning rate A :  tf.Tensor(9.919409e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 763 is 0.0775 sec\n",
      "train AE loss : 0.9006730318069458, train ANN loss : 3.278244972229004\n",
      "AE loss : 0.8864383697509766, ANN loss : 3.2131097316741943, Total loss : 91.85694122314453\n",
      "learning rate A :  tf.Tensor(9.9191995e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 764 is 0.0769 sec\n",
      "train AE loss : 0.8481664657592773, train ANN loss : 3.275552988052368\n",
      "AE loss : 0.9018856883049011, ANN loss : 3.212167501449585, Total loss : 93.40074157714844\n",
      "learning rate A :  tf.Tensor(9.9191995e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 765 is 0.0793 sec\n",
      "train AE loss : 0.8629404306411743, train ANN loss : 3.270038604736328\n",
      "AE loss : 0.8511688709259033, ANN loss : 3.211568832397461, Total loss : 88.32846069335938\n",
      "learning rate A :  tf.Tensor(9.9189914e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 766 is 0.0763 sec\n",
      "train AE loss : 0.8139675855636597, train ANN loss : 3.274077892303467\n",
      "AE loss : 0.8025765419006348, ANN loss : 3.2131059169769287, Total loss : 83.47075653076172\n",
      "learning rate A :  tf.Tensor(9.9189914e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 767 is 0.0780 sec\n",
      "train AE loss : 0.7668460607528687, train ANN loss : 3.267714738845825\n",
      "AE loss : 0.7609161734580994, ANN loss : 3.215585470199585, Total loss : 79.30719757080078\n",
      "learning rate A :  tf.Tensor(9.918782e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 768 is 0.0781 sec\n",
      "train AE loss : 0.7267369627952576, train ANN loss : 3.2669897079467773\n",
      "AE loss : 0.7230523824691772, ANN loss : 3.219109535217285, Total loss : 75.52435302734375\n",
      "learning rate A :  tf.Tensor(9.918572e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 769 is 0.0764 sec\n",
      "train AE loss : 0.6903674006462097, train ANN loss : 3.2731921672821045\n",
      "AE loss : 0.7327909469604492, ANN loss : 3.2192325592041016, Total loss : 76.49833679199219\n",
      "learning rate A :  tf.Tensor(9.918572e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 770 is 0.0786 sec\n",
      "train AE loss : 0.6994296312332153, train ANN loss : 3.266815185546875\n",
      "AE loss : 0.7871890664100647, ANN loss : 3.214869737625122, Total loss : 81.93377685546875\n",
      "learning rate A :  tf.Tensor(9.918572e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 771 is 0.0792 sec\n",
      "train AE loss : 0.7515660524368286, train ANN loss : 3.26920747756958\n",
      "AE loss : 0.7464256882667542, ANN loss : 3.217916965484619, Total loss : 77.86048889160156\n",
      "learning rate A :  tf.Tensor(9.918364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 772 is 0.0769 sec\n",
      "train AE loss : 0.7123661637306213, train ANN loss : 3.265599250793457\n",
      "AE loss : 0.8456710577011108, ANN loss : 3.2108025550842285, Total loss : 87.77790832519531\n",
      "learning rate A :  tf.Tensor(9.918364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 773 is 0.0778 sec\n",
      "train AE loss : 0.8079164028167725, train ANN loss : 3.2628629207611084\n",
      "AE loss : 0.8976337909698486, ANN loss : 3.2091064453125, Total loss : 92.97249603271484\n",
      "learning rate A :  tf.Tensor(9.918364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 774 is 0.0797 sec\n",
      "train AE loss : 0.8581710457801819, train ANN loss : 3.2658443450927734\n",
      "AE loss : 0.8614450693130493, ANN loss : 3.208630323410034, Total loss : 89.35314178466797\n",
      "learning rate A :  tf.Tensor(9.918364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 775 is 0.0786 sec\n",
      "train AE loss : 0.8233395218849182, train ANN loss : 3.265925168991089\n",
      "AE loss : 0.7832836508750916, ANN loss : 3.2130870819091797, Total loss : 81.54144287109375\n",
      "learning rate A :  tf.Tensor(9.918364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 776 is 0.0782 sec\n",
      "train AE loss : 0.7479814887046814, train ANN loss : 3.266568899154663\n",
      "AE loss : 0.7368258833885193, ANN loss : 3.218679904937744, Total loss : 76.9012680053711\n",
      "learning rate A :  tf.Tensor(9.918364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 777 is 0.0814 sec\n",
      "train AE loss : 0.7030722498893738, train ANN loss : 3.2677977085113525\n",
      "AE loss : 0.7007548809051514, ANN loss : 3.2240450382232666, Total loss : 73.29953002929688\n",
      "learning rate A :  tf.Tensor(9.918155e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 778 is 0.0783 sec\n",
      "train AE loss : 0.6684496402740479, train ANN loss : 3.277980089187622\n",
      "AE loss : 0.6678897738456726, ANN loss : 3.229940414428711, Total loss : 70.0189208984375\n",
      "learning rate A :  tf.Tensor(9.917946e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 779 is 0.0762 sec\n",
      "train AE loss : 0.6369368433952332, train ANN loss : 3.274378538131714\n",
      "AE loss : 0.7532362341880798, ANN loss : 3.2150967121124268, Total loss : 78.5387191772461\n",
      "learning rate A :  tf.Tensor(9.917946e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 780 is 0.0792 sec\n",
      "train AE loss : 0.7185235023498535, train ANN loss : 3.2675511837005615\n",
      "AE loss : 0.9115926623344421, ANN loss : 3.2054355144500732, Total loss : 94.36469268798828\n",
      "learning rate A :  tf.Tensor(9.917946e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 781 is 0.0782 sec\n",
      "train AE loss : 0.8707619905471802, train ANN loss : 3.2646353244781494\n",
      "AE loss : 0.9762307405471802, ANN loss : 3.2053544521331787, Total loss : 100.82843780517578\n",
      "learning rate A :  tf.Tensor(9.917946e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 782 is 0.0782 sec\n",
      "train AE loss : 0.9330582618713379, train ANN loss : 3.271050214767456\n",
      "AE loss : 0.9156844019889832, ANN loss : 3.2036783695220947, Total loss : 94.77212524414062\n",
      "learning rate A :  tf.Tensor(9.917737e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 783 is 0.0774 sec\n",
      "train AE loss : 0.8745700120925903, train ANN loss : 3.264535665512085\n",
      "AE loss : 0.8862746357917786, ANN loss : 3.201669216156006, Total loss : 91.82913970947266\n",
      "learning rate A :  tf.Tensor(9.917737e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 784 is 0.0792 sec\n",
      "train AE loss : 0.8462722301483154, train ANN loss : 3.2548348903656006\n",
      "AE loss : 0.8350487351417542, ANN loss : 3.2031898498535156, Total loss : 86.70806884765625\n",
      "learning rate A :  tf.Tensor(9.9175275e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 785 is 0.0767 sec\n",
      "train AE loss : 0.7968898415565491, train ANN loss : 3.260338068008423\n",
      "AE loss : 0.7924706935882568, ANN loss : 3.2051703929901123, Total loss : 82.45223999023438\n",
      "learning rate A :  tf.Tensor(9.9175275e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 786 is 0.0789 sec\n",
      "train AE loss : 0.7560225129127502, train ANN loss : 3.2548186779022217\n",
      "AE loss : 0.797562301158905, ANN loss : 3.204202890396118, Total loss : 82.96043395996094\n",
      "learning rate A :  tf.Tensor(9.9175275e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 787 is 0.0779 sec\n",
      "train AE loss : 0.7610077857971191, train ANN loss : 3.2595136165618896\n",
      "AE loss : 0.8357593417167664, ANN loss : 3.2007625102996826, Total loss : 86.77669525146484\n",
      "learning rate A :  tf.Tensor(9.9175275e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 788 is 0.0779 sec\n",
      "train AE loss : 0.7978641986846924, train ANN loss : 3.2548465728759766\n",
      "AE loss : 0.7898797392845154, ANN loss : 3.20389723777771, Total loss : 82.1918716430664\n",
      "learning rate A :  tf.Tensor(9.9173194e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 789 is 0.0786 sec\n",
      "train AE loss : 0.7536821365356445, train ANN loss : 3.2527761459350586\n",
      "AE loss : 0.7484331130981445, ANN loss : 3.20820951461792, Total loss : 78.05152130126953\n",
      "learning rate A :  tf.Tensor(9.91711e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 790 is 0.0766 sec\n",
      "train AE loss : 0.7138532996177673, train ANN loss : 3.259831666946411\n",
      "AE loss : 0.7108975052833557, ANN loss : 3.213428020477295, Total loss : 74.30318450927734\n",
      "learning rate A :  tf.Tensor(9.916901e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 791 is 0.0767 sec\n",
      "train AE loss : 0.6778157949447632, train ANN loss : 3.2605814933776855\n",
      "AE loss : 0.825542151927948, ANN loss : 3.1991312503814697, Total loss : 85.75334930419922\n",
      "learning rate A :  tf.Tensor(9.916901e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 792 is 0.0812 sec\n",
      "train AE loss : 0.787958562374115, train ANN loss : 3.2514560222625732\n",
      "AE loss : 0.780430793762207, ANN loss : 3.202273368835449, Total loss : 81.24535369873047\n",
      "learning rate A :  tf.Tensor(9.916692e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 793 is 0.0757 sec\n",
      "train AE loss : 0.7445507049560547, train ANN loss : 3.256723642349243\n",
      "AE loss : 0.924807608127594, ANN loss : 3.196200132369995, Total loss : 95.67695617675781\n",
      "learning rate A :  tf.Tensor(9.916692e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 794 is 0.0780 sec\n",
      "train AE loss : 0.8834522366523743, train ANN loss : 3.261052131652832\n",
      "AE loss : 0.9565701484680176, ANN loss : 3.1965901851654053, Total loss : 98.85359954833984\n",
      "learning rate A :  tf.Tensor(9.916692e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 795 is 0.0783 sec\n",
      "train AE loss : 0.9139922261238098, train ANN loss : 3.2585408687591553\n",
      "AE loss : 0.8530901670455933, ANN loss : 3.1964592933654785, Total loss : 88.5054702758789\n",
      "learning rate A :  tf.Tensor(9.916692e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 796 is 0.0783 sec\n",
      "train AE loss : 0.8142794370651245, train ANN loss : 3.256077289581299\n",
      "AE loss : 0.8047587871551514, ANN loss : 3.199113368988037, Total loss : 83.67498779296875\n",
      "learning rate A :  tf.Tensor(9.916483e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 797 is 0.0766 sec\n",
      "train AE loss : 0.7677208185195923, train ANN loss : 3.2475881576538086\n",
      "AE loss : 0.7611935138702393, ANN loss : 3.203007936477661, Total loss : 79.3223648071289\n",
      "learning rate A :  tf.Tensor(9.9162746e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 798 is 0.0773 sec\n",
      "train AE loss : 0.7258309125900269, train ANN loss : 3.2526705265045166\n",
      "AE loss : 0.721824049949646, ANN loss : 3.2079386711120605, Total loss : 75.39034271240234\n",
      "learning rate A :  tf.Tensor(9.916065e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 799 is 0.0773 sec\n",
      "train AE loss : 0.6880143284797668, train ANN loss : 3.2593441009521484\n",
      "AE loss : 0.7187855243682861, ANN loss : 3.208566665649414, Total loss : 75.08712005615234\n",
      "learning rate A :  tf.Tensor(9.916065e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 800 is 0.0785 sec\n",
      "train AE loss : 0.6851561069488525, train ANN loss : 3.256014347076416\n",
      "AE loss : 0.81675124168396, ANN loss : 3.196272611618042, Total loss : 84.87139892578125\n",
      "learning rate A :  tf.Tensor(9.916065e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 801 is 0.0774 sec\n",
      "train AE loss : 0.779333233833313, train ANN loss : 3.2545039653778076\n",
      "AE loss : 0.7719709873199463, ANN loss : 3.1999154090881348, Total loss : 80.39701843261719\n",
      "learning rate A :  tf.Tensor(9.915856e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 802 is 0.2107 sec\n",
      "train AE loss : 0.7362759113311768, train ANN loss : 3.2475218772888184\n",
      "AE loss : 0.731566309928894, ANN loss : 3.204646348953247, Total loss : 76.36127471923828\n",
      "learning rate A :  tf.Tensor(9.9156474e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 803 is 0.0942 sec\n",
      "train AE loss : 0.6974664926528931, train ANN loss : 3.2559657096862793\n",
      "AE loss : 0.6949697732925415, ANN loss : 3.2101380825042725, Total loss : 72.70712280273438\n",
      "learning rate A :  tf.Tensor(9.9154386e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 804 is 0.0769 sec\n",
      "train AE loss : 0.6623724102973938, train ANN loss : 3.259589910507202\n",
      "AE loss : 0.6617216467857361, ANN loss : 3.2162320613861084, Total loss : 69.38839721679688\n",
      "learning rate A :  tf.Tensor(9.91523e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 805 is 0.0776 sec\n",
      "train AE loss : 0.6305602192878723, train ANN loss : 3.2584152221679688\n",
      "AE loss : 0.8731582164764404, ANN loss : 3.1912899017333984, Total loss : 90.50711822509766\n",
      "learning rate A :  tf.Tensor(9.91523e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 806 is 0.0776 sec\n",
      "train AE loss : 0.8336430191993713, train ANN loss : 3.2490530014038086\n",
      "AE loss : 1.0431861877441406, ANN loss : 3.1979994773864746, Total loss : 107.51661682128906\n",
      "learning rate A :  tf.Tensor(9.91523e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 807 is 0.0776 sec\n",
      "train AE loss : 0.9976113438606262, train ANN loss : 3.2684712409973145\n",
      "AE loss : 0.9543808102607727, ANN loss : 3.1906697750091553, Total loss : 98.62874603271484\n",
      "learning rate A :  tf.Tensor(9.91523e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 808 is 0.0775 sec\n",
      "train AE loss : 0.911865770816803, train ANN loss : 3.252316474914551\n",
      "AE loss : 0.8953624963760376, ANN loss : 3.1895713806152344, Total loss : 92.72582244873047\n",
      "learning rate A :  tf.Tensor(9.915021e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 809 is 0.0752 sec\n",
      "train AE loss : 0.8549793362617493, train ANN loss : 3.2502570152282715\n",
      "AE loss : 0.8424959778785706, ANN loss : 3.190507173538208, Total loss : 87.44010162353516\n",
      "learning rate A :  tf.Tensor(9.9148114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 810 is 0.0756 sec\n",
      "train AE loss : 0.8040485978126526, train ANN loss : 3.242074728012085\n",
      "AE loss : 0.7545379400253296, ANN loss : 3.1985960006713867, Total loss : 78.65239715576172\n",
      "learning rate A :  tf.Tensor(9.9148114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 811 is 0.0791 sec\n",
      "train AE loss : 0.7194110751152039, train ANN loss : 3.2467687129974365\n",
      "AE loss : 0.7290009260177612, ANN loss : 3.203460454940796, Total loss : 76.10355377197266\n",
      "learning rate A :  tf.Tensor(9.9148114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 812 is 0.0770 sec\n",
      "train AE loss : 0.69477379322052, train ANN loss : 3.2512900829315186\n",
      "AE loss : 0.6923225522041321, ANN loss : 3.209416151046753, Total loss : 72.4416732788086\n",
      "learning rate A :  tf.Tensor(9.914603e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 813 is 0.0772 sec\n",
      "train AE loss : 0.6596469879150391, train ANN loss : 3.2588391304016113\n",
      "AE loss : 0.7809725403785706, ANN loss : 3.1963021755218506, Total loss : 81.2935562133789\n",
      "learning rate A :  tf.Tensor(9.914603e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 814 is 0.0786 sec\n",
      "train AE loss : 0.7445845007896423, train ANN loss : 3.235600233078003\n",
      "AE loss : 0.7391840815544128, ANN loss : 3.201244592666626, Total loss : 77.1196517944336\n",
      "learning rate A :  tf.Tensor(9.914394e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 815 is 0.0757 sec\n",
      "train AE loss : 0.7044772505760193, train ANN loss : 3.2484405040740967\n",
      "AE loss : 0.7014093399047852, ANN loss : 3.2069971561431885, Total loss : 73.3479232788086\n",
      "learning rate A :  tf.Tensor(9.914185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 816 is 0.0755 sec\n",
      "train AE loss : 0.6682987213134766, train ANN loss : 3.2550466060638428\n",
      "AE loss : 0.6671651005744934, ANN loss : 3.2133231163024902, Total loss : 69.9298324584961\n",
      "learning rate A :  tf.Tensor(9.913976e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 817 is 0.0760 sec\n",
      "train AE loss : 0.6355711817741394, train ANN loss : 3.254401206970215\n",
      "AE loss : 0.882734477519989, ANN loss : 3.187978744506836, Total loss : 91.46142578125\n",
      "learning rate A :  tf.Tensor(9.913976e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 818 is 0.0776 sec\n",
      "train AE loss : 0.8423330783843994, train ANN loss : 3.244337320327759\n",
      "AE loss : 1.0742708444595337, ANN loss : 3.1953587532043457, Total loss : 110.62245178222656\n",
      "learning rate A :  tf.Tensor(9.913976e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 819 is 0.0768 sec\n",
      "train AE loss : 1.0270177125930786, train ANN loss : 3.260115146636963\n",
      "AE loss : 1.0012075901031494, ANN loss : 3.18941068649292, Total loss : 103.31016540527344\n",
      "learning rate A :  tf.Tensor(9.913767e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 820 is 0.0764 sec\n",
      "train AE loss : 0.9564814567565918, train ANN loss : 3.2589802742004395\n",
      "AE loss : 0.9363533854484558, ANN loss : 3.1866252422332764, Total loss : 96.82196044921875\n",
      "learning rate A :  tf.Tensor(9.9135585e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 821 is 0.0761 sec\n",
      "train AE loss : 0.8939552903175354, train ANN loss : 3.247706890106201\n",
      "AE loss : 0.8784623146057129, ANN loss : 3.186253070831299, Total loss : 91.03248596191406\n",
      "learning rate A :  tf.Tensor(9.91335e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 822 is 0.0754 sec\n",
      "train AE loss : 0.8381914496421814, train ANN loss : 3.240546464920044\n",
      "AE loss : 1.0018717050552368, ANN loss : 3.1881368160247803, Total loss : 103.37530517578125\n",
      "learning rate A :  tf.Tensor(9.91335e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 823 is 0.0792 sec\n",
      "train AE loss : 0.9570407867431641, train ANN loss : 3.2619471549987793\n",
      "AE loss : 0.9368645548820496, ANN loss : 3.185250997543335, Total loss : 96.87171173095703\n",
      "learning rate A :  tf.Tensor(9.913141e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 824 is 0.0764 sec\n",
      "train AE loss : 0.8943735361099243, train ANN loss : 3.244851589202881\n",
      "AE loss : 0.9472090601921082, ANN loss : 3.184319257736206, Total loss : 97.90523529052734\n",
      "learning rate A :  tf.Tensor(9.913141e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 825 is 0.0877 sec\n",
      "train AE loss : 0.904208779335022, train ANN loss : 3.2460825443267822\n",
      "AE loss : 0.8351066708564758, ANN loss : 3.186807155609131, Total loss : 86.69747161865234\n",
      "learning rate A :  tf.Tensor(9.913141e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 826 is 0.0794 sec\n",
      "train AE loss : 0.7962466478347778, train ANN loss : 3.236281156539917\n",
      "AE loss : 0.754456639289856, ANN loss : 3.1967849731445312, Total loss : 78.64244842529297\n",
      "learning rate A :  tf.Tensor(9.913141e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 827 is 0.0782 sec\n",
      "train AE loss : 0.7187411189079285, train ANN loss : 3.2396717071533203\n",
      "AE loss : 0.7645080089569092, ANN loss : 3.195979356765747, Total loss : 79.64678192138672\n",
      "learning rate A :  tf.Tensor(9.913141e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 828 is 0.0781 sec\n",
      "train AE loss : 0.7282257676124573, train ANN loss : 3.2429232597351074\n",
      "AE loss : 0.8519997596740723, ANN loss : 3.186128616333008, Total loss : 88.3861083984375\n",
      "learning rate A :  tf.Tensor(9.913141e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 829 is 0.0782 sec\n",
      "train AE loss : 0.8120391368865967, train ANN loss : 3.2335736751556396\n",
      "AE loss : 0.9626118540763855, ANN loss : 3.180973529815674, Total loss : 99.4421615600586\n",
      "learning rate A :  tf.Tensor(9.913141e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 830 is 0.0790 sec\n",
      "train AE loss : 0.9184304475784302, train ANN loss : 3.2426559925079346\n",
      "AE loss : 0.9009936451911926, ANN loss : 3.1817452907562256, Total loss : 93.28111267089844\n",
      "learning rate A :  tf.Tensor(9.912932e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 831 is 0.0766 sec\n",
      "train AE loss : 0.8591081500053406, train ANN loss : 3.2401483058929443\n",
      "AE loss : 0.9719647169113159, ANN loss : 3.1794567108154297, Total loss : 100.37592315673828\n",
      "learning rate A :  tf.Tensor(9.912932e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 832 is 0.0821 sec\n",
      "train AE loss : 0.9274288415908813, train ANN loss : 3.2385590076446533\n",
      "AE loss : 0.9092810153961182, ANN loss : 3.1799168586730957, Total loss : 94.1080093383789\n",
      "learning rate A :  tf.Tensor(9.912723e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 833 is 0.0782 sec\n",
      "train AE loss : 0.8670724630355835, train ANN loss : 3.2390899658203125\n",
      "AE loss : 0.8533589839935303, ANN loss : 3.182455539703369, Total loss : 88.51835632324219\n",
      "learning rate A :  tf.Tensor(9.9125136e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 834 is 0.0890 sec\n",
      "train AE loss : 0.8132766485214233, train ANN loss : 3.2355093955993652\n",
      "AE loss : 0.8032781481742859, ANN loss : 3.1866016387939453, Total loss : 83.51441955566406\n",
      "learning rate A :  tf.Tensor(9.9123055e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 835 is 0.0774 sec\n",
      "train AE loss : 0.7651848793029785, train ANN loss : 3.2359187602996826\n",
      "AE loss : 0.9203004240989685, ANN loss : 3.178135395050049, Total loss : 95.20817565917969\n",
      "learning rate A :  tf.Tensor(9.9123055e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 836 is 0.0794 sec\n",
      "train AE loss : 0.8776816725730896, train ANN loss : 3.229495048522949\n",
      "AE loss : 0.9936336874961853, ANN loss : 3.1776070594787598, Total loss : 102.54096984863281\n",
      "learning rate A :  tf.Tensor(9.9123055e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 837 is 0.0798 sec\n",
      "train AE loss : 0.9482728242874146, train ANN loss : 3.243154764175415\n",
      "AE loss : 0.9344902038574219, ANN loss : 3.176894426345825, Total loss : 96.62591552734375\n",
      "learning rate A :  tf.Tensor(9.9123055e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 838 is 0.0789 sec\n",
      "train AE loss : 0.8911994099617004, train ANN loss : 3.2348906993865967\n",
      "AE loss : 0.8756526708602905, ANN loss : 3.178863286972046, Total loss : 90.744140625\n",
      "learning rate A :  tf.Tensor(9.912096e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 839 is 0.0781 sec\n",
      "train AE loss : 0.8345761299133301, train ANN loss : 3.2346034049987793\n",
      "AE loss : 0.8150668144226074, ANN loss : 3.184454917907715, Total loss : 84.6911392211914\n",
      "learning rate A :  tf.Tensor(9.912096e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 840 is 0.0802 sec\n",
      "train AE loss : 0.7762049436569214, train ANN loss : 3.241900682449341\n",
      "AE loss : 0.8160476088523865, ANN loss : 3.1841793060302734, Total loss : 84.7889404296875\n",
      "learning rate A :  tf.Tensor(9.912096e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 841 is 0.0792 sec\n",
      "train AE loss : 0.7770276665687561, train ANN loss : 3.2305426597595215\n",
      "AE loss : 0.7692583203315735, ANN loss : 3.189969778060913, Total loss : 80.11579895019531\n",
      "learning rate A :  tf.Tensor(9.911887e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 842 is 0.0774 sec\n",
      "train AE loss : 0.732171356678009, train ANN loss : 3.240957736968994\n",
      "AE loss : 0.727236270904541, ANN loss : 3.1966280937194824, Total loss : 75.92025756835938\n",
      "learning rate A :  tf.Tensor(9.911679e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 843 is 0.0782 sec\n",
      "train AE loss : 0.6919606328010559, train ANN loss : 3.240591526031494\n",
      "AE loss : 0.689348578453064, ANN loss : 3.203962802886963, Total loss : 72.1388168334961\n",
      "learning rate A :  tf.Tensor(9.9114695e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 844 is 0.0774 sec\n",
      "train AE loss : 0.6557950377464294, train ANN loss : 3.245398998260498\n",
      "AE loss : 0.8247281312942505, ANN loss : 3.180617570877075, Total loss : 85.65343475341797\n",
      "learning rate A :  tf.Tensor(9.9114695e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 845 is 0.0800 sec\n",
      "train AE loss : 0.7852495312690735, train ANN loss : 3.238656759262085\n",
      "AE loss : 1.0051113367080688, ANN loss : 3.173738718032837, Total loss : 103.68486785888672\n",
      "learning rate A :  tf.Tensor(9.9114695e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 846 is 0.0796 sec\n",
      "train AE loss : 0.9587745070457458, train ANN loss : 3.240011215209961\n",
      "AE loss : 0.9373325109481812, ANN loss : 3.1727182865142822, Total loss : 96.90597534179688\n",
      "learning rate A :  tf.Tensor(9.911261e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 847 is 0.0803 sec\n",
      "train AE loss : 0.8934868574142456, train ANN loss : 3.2271318435668945\n",
      "AE loss : 0.8771232962608337, ANN loss : 3.1741855144500732, Total loss : 90.88652038574219\n",
      "learning rate A :  tf.Tensor(9.911051e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 848 is 0.0788 sec\n",
      "train AE loss : 0.8355721831321716, train ANN loss : 3.232996702194214\n",
      "AE loss : 1.0306438207626343, ANN loss : 3.17378830909729, Total loss : 106.23817443847656\n",
      "learning rate A :  tf.Tensor(9.911051e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 849 is 0.0806 sec\n",
      "train AE loss : 0.983397901058197, train ANN loss : 3.2389233112335205\n",
      "AE loss : 0.959823727607727, ANN loss : 3.1713833808898926, Total loss : 99.15376281738281\n",
      "learning rate A :  tf.Tensor(9.910842e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 850 is 0.0770 sec\n",
      "train AE loss : 0.9151726961135864, train ANN loss : 3.2319869995117188\n",
      "AE loss : 0.8970274329185486, ANN loss : 3.1717734336853027, Total loss : 92.87451934814453\n",
      "learning rate A :  tf.Tensor(9.910634e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 851 is 0.0787 sec\n",
      "train AE loss : 0.8547332882881165, train ANN loss : 3.229034185409546\n",
      "AE loss : 0.841117799282074, ANN loss : 3.17434024810791, Total loss : 87.28611755371094\n",
      "learning rate A :  tf.Tensor(9.910425e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 852 is 0.0783 sec\n",
      "train AE loss : 0.8010290861129761, train ANN loss : 3.2255353927612305\n",
      "AE loss : 0.9686399102210999, ANN loss : 3.1706902980804443, Total loss : 100.03468322753906\n",
      "learning rate A :  tf.Tensor(9.910425e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 853 is 0.0781 sec\n",
      "train AE loss : 0.923675537109375, train ANN loss : 3.2303812503814697\n",
      "AE loss : 0.9047656059265137, ANN loss : 3.170619249343872, Total loss : 93.64717864990234\n",
      "learning rate A :  tf.Tensor(9.910216e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 854 is 0.0764 sec\n",
      "train AE loss : 0.8621996641159058, train ANN loss : 3.228215456008911\n",
      "AE loss : 0.983400285243988, ANN loss : 3.1703808307647705, Total loss : 101.51040649414062\n",
      "learning rate A :  tf.Tensor(9.910216e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 855 is 0.0796 sec\n",
      "train AE loss : 0.9378733038902283, train ANN loss : 3.2373194694519043\n",
      "AE loss : 0.9177772998809814, ANN loss : 3.16996169090271, Total loss : 94.94770050048828\n",
      "learning rate A :  tf.Tensor(9.910008e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 856 is 0.0775 sec\n",
      "train AE loss : 0.8746929168701172, train ANN loss : 3.225227117538452\n",
      "AE loss : 0.8594574332237244, ANN loss : 3.171961784362793, Total loss : 89.1176986694336\n",
      "learning rate A :  tf.Tensor(9.909798e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 857 is 0.0765 sec\n",
      "train AE loss : 0.8186636567115784, train ANN loss : 3.2212257385253906\n",
      "AE loss : 0.807451605796814, ANN loss : 3.1758460998535156, Total loss : 83.92100524902344\n",
      "learning rate A :  tf.Tensor(9.9095894e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 858 is 0.0779 sec\n",
      "train AE loss : 0.7687941789627075, train ANN loss : 3.226813554763794\n",
      "AE loss : 0.7609644532203674, ANN loss : 3.1810860633850098, Total loss : 79.27752685546875\n",
      "learning rate A :  tf.Tensor(9.909381e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 859 is 0.0766 sec\n",
      "train AE loss : 0.7243571281433105, train ANN loss : 3.229534387588501\n",
      "AE loss : 0.8984469175338745, ANN loss : 3.169950246810913, Total loss : 93.01464080810547\n",
      "learning rate A :  tf.Tensor(9.909381e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 860 is 0.0792 sec\n",
      "train AE loss : 0.8559721112251282, train ANN loss : 3.220897674560547\n",
      "AE loss : 0.9958886504173279, ANN loss : 3.1699976921081543, Total loss : 102.75886535644531\n",
      "learning rate A :  tf.Tensor(9.909381e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 861 is 0.0808 sec\n",
      "train AE loss : 0.9495031237602234, train ANN loss : 3.2297263145446777\n",
      "AE loss : 0.9280949831008911, ANN loss : 3.1691300868988037, Total loss : 95.97863006591797\n",
      "learning rate A :  tf.Tensor(9.909172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 862 is 0.0783 sec\n",
      "train AE loss : 0.8842387199401855, train ANN loss : 3.2253403663635254\n",
      "AE loss : 0.9569432735443115, ANN loss : 3.168727397918701, Total loss : 98.86305236816406\n",
      "learning rate A :  tf.Tensor(9.909172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 863 is 0.0796 sec\n",
      "train AE loss : 0.9118099212646484, train ANN loss : 3.2261977195739746\n",
      "AE loss : 0.8932968378067017, ANN loss : 3.169804334640503, Total loss : 92.4994888305664\n",
      "learning rate A :  tf.Tensor(9.908963e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 864 is 0.0798 sec\n",
      "train AE loss : 0.8506283164024353, train ANN loss : 3.226630926132202\n",
      "AE loss : 0.9039680361747742, ANN loss : 3.1691982746124268, Total loss : 93.56600952148438\n",
      "learning rate A :  tf.Tensor(9.908963e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 865 is 0.0789 sec\n",
      "train AE loss : 0.8607624173164368, train ANN loss : 3.2316861152648926\n",
      "AE loss : 0.878716766834259, ANN loss : 3.1705996990203857, Total loss : 91.04227447509766\n",
      "learning rate A :  tf.Tensor(9.908963e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 866 is 0.0787 sec\n",
      "train AE loss : 0.8363671898841858, train ANN loss : 3.2254855632781982\n",
      "AE loss : 0.8235400915145874, ANN loss : 3.1750853061676025, Total loss : 85.52909088134766\n",
      "learning rate A :  tf.Tensor(9.908755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 867 is 0.0831 sec\n",
      "train AE loss : 0.7834184765815735, train ANN loss : 3.224801540374756\n",
      "AE loss : 0.774339497089386, ANN loss : 3.181023120880127, Total loss : 80.61497497558594\n",
      "learning rate A :  tf.Tensor(9.908545e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 868 is 0.0815 sec\n",
      "train AE loss : 0.736349880695343, train ANN loss : 3.2279739379882812\n",
      "AE loss : 0.8473685383796692, ANN loss : 3.1715033054351807, Total loss : 87.90835571289062\n",
      "learning rate A :  tf.Tensor(9.908545e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 869 is 0.0785 sec\n",
      "train AE loss : 0.8061427474021912, train ANN loss : 3.2233083248138428\n",
      "AE loss : 0.7954668998718262, ANN loss : 3.1768994331359863, Total loss : 82.72359466552734\n",
      "learning rate A :  tf.Tensor(9.9083365e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 870 is 0.0775 sec\n",
      "train AE loss : 0.7564228773117065, train ANN loss : 3.227309465408325\n",
      "AE loss : 0.9514451622962952, ANN loss : 3.1643412113189697, Total loss : 98.3088607788086\n",
      "learning rate A :  tf.Tensor(9.9083365e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 871 is 0.0785 sec\n",
      "train AE loss : 0.9060342311859131, train ANN loss : 3.2150957584381104\n",
      "AE loss : 0.8877895474433899, ANN loss : 3.166440725326538, Total loss : 91.94539642333984\n",
      "learning rate A :  tf.Tensor(9.9081284e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 872 is 0.0760 sec\n",
      "train AE loss : 0.8448508381843567, train ANN loss : 3.2237284183502197\n",
      "AE loss : 0.8312888741493225, ANN loss : 3.1705455780029297, Total loss : 86.29943084716797\n",
      "learning rate A :  tf.Tensor(9.9079196e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 873 is 0.0757 sec\n",
      "train AE loss : 0.7906239628791809, train ANN loss : 3.223891258239746\n",
      "AE loss : 1.034917950630188, ANN loss : 3.164259195327759, Total loss : 106.65605163574219\n",
      "learning rate A :  tf.Tensor(9.9079196e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 874 is 0.0778 sec\n",
      "train AE loss : 0.9862021207809448, train ANN loss : 3.2300360202789307\n",
      "AE loss : 0.9612192511558533, ANN loss : 3.1626510620117188, Total loss : 99.2845687866211\n",
      "learning rate A :  tf.Tensor(9.90771e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 875 is 0.0764 sec\n",
      "train AE loss : 0.9152377247810364, train ANN loss : 3.2223615646362305\n",
      "AE loss : 1.0614988803863525, ANN loss : 3.1647465229034424, Total loss : 109.31463623046875\n",
      "learning rate A :  tf.Tensor(9.90771e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 876 is 0.0780 sec\n",
      "train AE loss : 1.0116194486618042, train ANN loss : 3.234652280807495\n",
      "AE loss : 0.9843367338180542, ANN loss : 3.1619179248809814, Total loss : 101.59558868408203\n",
      "learning rate A :  tf.Tensor(9.907502e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 877 is 0.0763 sec\n",
      "train AE loss : 0.9372847676277161, train ANN loss : 3.232358694076538\n",
      "AE loss : 0.9163210391998291, ANN loss : 3.1622378826141357, Total loss : 94.79434204101562\n",
      "learning rate A :  tf.Tensor(9.907293e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 878 is 0.0771 sec\n",
      "train AE loss : 0.8718791007995605, train ANN loss : 3.216902732849121\n",
      "AE loss : 0.9684118032455444, ANN loss : 3.161742925643921, Total loss : 100.00292205810547\n",
      "learning rate A :  tf.Tensor(9.907293e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 879 is 0.0794 sec\n",
      "train AE loss : 0.9216626286506653, train ANN loss : 3.220853805541992\n",
      "AE loss : 0.9018830060958862, ANN loss : 3.1626646518707275, Total loss : 93.35096740722656\n",
      "learning rate A :  tf.Tensor(9.907084e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 880 is 0.0751 sec\n",
      "train AE loss : 0.8577254414558411, train ANN loss : 3.215653419494629\n",
      "AE loss : 0.843021035194397, ANN loss : 3.1659257411956787, Total loss : 87.468017578125\n",
      "learning rate A :  tf.Tensor(9.9068755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 881 is 0.0766 sec\n",
      "train AE loss : 0.8012434840202332, train ANN loss : 3.219188928604126\n",
      "AE loss : 0.9159114360809326, ANN loss : 3.1621954441070557, Total loss : 94.75333404541016\n",
      "learning rate A :  tf.Tensor(9.9068755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 882 is 0.0783 sec\n",
      "train AE loss : 0.8708161115646362, train ANN loss : 3.2168803215026855\n",
      "AE loss : 0.9432085752487183, ANN loss : 3.1613729000091553, Total loss : 97.48223114013672\n",
      "learning rate A :  tf.Tensor(9.9068755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 883 is 0.0774 sec\n",
      "train AE loss : 0.8968458771705627, train ANN loss : 3.219973087310791\n",
      "AE loss : 0.87901371717453, ANN loss : 3.1636061668395996, Total loss : 91.06498718261719\n",
      "learning rate A :  tf.Tensor(9.906667e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 884 is 0.0769 sec\n",
      "train AE loss : 0.8351969718933105, train ANN loss : 3.2177815437316895\n",
      "AE loss : 0.904591977596283, ANN loss : 3.1621031761169434, Total loss : 93.62129974365234\n",
      "learning rate A :  tf.Tensor(9.906667e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 885 is 0.0778 sec\n",
      "train AE loss : 0.8596317172050476, train ANN loss : 3.2130990028381348\n",
      "AE loss : 0.8448054790496826, ANN loss : 3.166048288345337, Total loss : 87.64659118652344\n",
      "learning rate A :  tf.Tensor(9.906458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 886 is 0.0762 sec\n",
      "train AE loss : 0.8022717237472534, train ANN loss : 3.217003107070923\n",
      "AE loss : 0.7917242646217346, ANN loss : 3.171736478805542, Total loss : 82.34416198730469\n",
      "learning rate A :  tf.Tensor(9.906249e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 887 is 0.0768 sec\n",
      "train AE loss : 0.7514891028404236, train ANN loss : 3.2216949462890625\n",
      "AE loss : 0.8988263010978699, ANN loss : 3.161713123321533, Total loss : 93.04434204101562\n",
      "learning rate A :  tf.Tensor(9.906249e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 888 is 0.0784 sec\n",
      "train AE loss : 0.8540306091308594, train ANN loss : 3.216787576675415\n",
      "AE loss : 0.839718759059906, ANN loss : 3.1659278869628906, Total loss : 87.13780975341797\n",
      "learning rate A :  tf.Tensor(9.906041e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 889 is 0.0757 sec\n",
      "train AE loss : 0.797346293926239, train ANN loss : 3.2178802490234375\n",
      "AE loss : 0.9808256030082703, ANN loss : 3.1587045192718506, Total loss : 101.24125671386719\n",
      "learning rate A :  tf.Tensor(9.906041e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 890 is 0.0800 sec\n",
      "train AE loss : 0.932717502117157, train ANN loss : 3.2213008403778076\n",
      "AE loss : 0.9120962023735046, ANN loss : 3.1601693630218506, Total loss : 94.36979675292969\n",
      "learning rate A :  tf.Tensor(9.905832e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 891 is 0.0761 sec\n",
      "train AE loss : 0.8666984438896179, train ANN loss : 3.214803457260132\n",
      "AE loss : 1.023834228515625, ANN loss : 3.157881736755371, Total loss : 105.54130554199219\n",
      "learning rate A :  tf.Tensor(9.905832e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 892 is 0.0775 sec\n",
      "train AE loss : 0.9738815426826477, train ANN loss : 3.2119319438934326\n",
      "AE loss : 1.0050785541534424, ANN loss : 3.156398057937622, Total loss : 103.66426086425781\n",
      "learning rate A :  tf.Tensor(9.905832e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 893 is 0.0789 sec\n",
      "train AE loss : 0.9556100964546204, train ANN loss : 3.216731548309326\n",
      "AE loss : 0.9328113794326782, ANN loss : 3.1575844287872314, Total loss : 96.43871307373047\n",
      "learning rate A :  tf.Tensor(9.905623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 894 is 0.0763 sec\n",
      "train AE loss : 0.8861523270606995, train ANN loss : 3.212214946746826\n",
      "AE loss : 0.869188666343689, ANN loss : 3.1612706184387207, Total loss : 90.08013153076172\n",
      "learning rate A :  tf.Tensor(9.9054145e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 895 is 0.0766 sec\n",
      "train AE loss : 0.8251236081123352, train ANN loss : 3.213559150695801\n",
      "AE loss : 0.8860230445861816, ANN loss : 3.159518003463745, Total loss : 91.76182556152344\n",
      "learning rate A :  tf.Tensor(9.9054145e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 896 is 0.0789 sec\n",
      "train AE loss : 0.8410218954086304, train ANN loss : 3.2196943759918213\n",
      "AE loss : 0.8274824619293213, ANN loss : 3.16503643989563, Total loss : 85.91329193115234\n",
      "learning rate A :  tf.Tensor(9.905206e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 897 is 0.0769 sec\n",
      "train AE loss : 0.7849476337432861, train ANN loss : 3.2183756828308105\n",
      "AE loss : 0.7755516171455383, ANN loss : 3.1720643043518066, Total loss : 80.72722625732422\n",
      "learning rate A :  tf.Tensor(9.904997e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 898 is 0.0765 sec\n",
      "train AE loss : 0.7353536486625671, train ANN loss : 3.2190611362457275\n",
      "AE loss : 0.9003479480743408, ANN loss : 3.157008409500122, Total loss : 93.19181060791016\n",
      "learning rate A :  tf.Tensor(9.904997e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 899 is 0.0798 sec\n",
      "train AE loss : 0.8545504212379456, train ANN loss : 3.2185099124908447\n",
      "AE loss : 1.0343717336654663, ANN loss : 3.152188539505005, Total loss : 106.58935546875\n",
      "learning rate A :  tf.Tensor(9.904997e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 900 is 0.0788 sec\n",
      "train AE loss : 0.9832699298858643, train ANN loss : 3.2210278511047363\n",
      "AE loss : 1.0245585441589355, ANN loss : 3.150754928588867, Total loss : 105.60661315917969\n",
      "learning rate A :  tf.Tensor(9.904997e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 901 is 0.0783 sec\n",
      "train AE loss : 0.973952054977417, train ANN loss : 3.2100887298583984\n",
      "AE loss : 0.9491231441497803, ANN loss : 3.1519529819488525, Total loss : 98.06427001953125\n",
      "learning rate A :  tf.Tensor(9.904789e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 902 is 0.0782 sec\n",
      "train AE loss : 0.9014601111412048, train ANN loss : 3.212789297103882\n",
      "AE loss : 0.8828781247138977, ANN loss : 3.1558704376220703, Total loss : 91.44368743896484\n",
      "learning rate A :  tf.Tensor(9.90458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 903 is 0.0776 sec\n",
      "train AE loss : 0.8379070162773132, train ANN loss : 3.2081069946289062\n",
      "AE loss : 0.8244091272354126, ANN loss : 3.1617705821990967, Total loss : 85.6026840209961\n",
      "learning rate A :  tf.Tensor(9.904371e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 904 is 0.0768 sec\n",
      "train AE loss : 0.7819523811340332, train ANN loss : 3.21250319480896\n",
      "AE loss : 0.7725847959518433, ANN loss : 3.1691689491271973, Total loss : 80.42764282226562\n",
      "learning rate A :  tf.Tensor(9.904162e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 905 is 0.0771 sec\n",
      "train AE loss : 0.7325377464294434, train ANN loss : 3.2135496139526367\n",
      "AE loss : 0.726509153842926, ANN loss : 3.1775095462799072, Total loss : 75.82843017578125\n",
      "learning rate A :  tf.Tensor(9.9039535e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 906 is 0.0770 sec\n",
      "train AE loss : 0.6886785626411438, train ANN loss : 3.224857807159424\n",
      "AE loss : 0.8716655969619751, ANN loss : 3.1556596755981445, Total loss : 90.32221984863281\n",
      "learning rate A :  tf.Tensor(9.9039535e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 907 is 0.0787 sec\n",
      "train AE loss : 0.8271763324737549, train ANN loss : 3.211919069290161\n",
      "AE loss : 0.8144311308860779, ANN loss : 3.1616456508636475, Total loss : 84.60476684570312\n",
      "learning rate A :  tf.Tensor(9.903745e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 908 is 0.0773 sec\n",
      "train AE loss : 0.7724798917770386, train ANN loss : 3.2105484008789062\n",
      "AE loss : 1.0580673217773438, ANN loss : 3.151123285293579, Total loss : 108.95784759521484\n",
      "learning rate A :  tf.Tensor(9.903745e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 909 is 0.0782 sec\n",
      "train AE loss : 1.0060991048812866, train ANN loss : 3.2165911197662354\n",
      "AE loss : 0.9781121015548706, ANN loss : 3.149975061416626, Total loss : 100.96118927001953\n",
      "learning rate A :  tf.Tensor(9.903536e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 910 is 0.0766 sec\n",
      "train AE loss : 0.9292545914649963, train ANN loss : 3.217087507247925\n",
      "AE loss : 1.1450303792953491, ANN loss : 3.156836986541748, Total loss : 117.65988159179688\n",
      "learning rate A :  tf.Tensor(9.903536e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 911 is 0.0786 sec\n",
      "train AE loss : 1.0896469354629517, train ANN loss : 3.2264692783355713\n",
      "AE loss : 1.0518749952316284, ANN loss : 3.151477336883545, Total loss : 108.3389892578125\n",
      "learning rate A :  tf.Tensor(9.903536e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 912 is 0.0780 sec\n",
      "train AE loss : 0.9998549818992615, train ANN loss : 3.2201271057128906\n",
      "AE loss : 0.8547781109809875, ANN loss : 3.163409948348999, Total loss : 88.64122772216797\n",
      "learning rate A :  tf.Tensor(9.903536e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 913 is 0.0794 sec\n",
      "train AE loss : 0.8107017278671265, train ANN loss : 3.2136104106903076\n",
      "AE loss : 0.7750087976455688, ANN loss : 3.1788904666900635, Total loss : 80.67976379394531\n",
      "learning rate A :  tf.Tensor(9.903536e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 914 is 0.0797 sec\n",
      "train AE loss : 0.7345248460769653, train ANN loss : 3.2158966064453125\n",
      "AE loss : 0.7281758189201355, ANN loss : 3.188657522201538, Total loss : 76.00623321533203\n",
      "learning rate A :  tf.Tensor(9.903328e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 915 is 0.0906 sec\n",
      "train AE loss : 0.6899279952049255, train ANN loss : 3.2266292572021484\n",
      "AE loss : 0.6864322423934937, ANN loss : 3.198868751525879, Total loss : 71.84209442138672\n",
      "learning rate A :  tf.Tensor(9.903119e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 916 is 0.0860 sec\n",
      "train AE loss : 0.6502473950386047, train ANN loss : 3.2456414699554443\n",
      "AE loss : 0.6490046381950378, ANN loss : 3.209359645843506, Total loss : 68.10982513427734\n",
      "learning rate A :  tf.Tensor(9.90291e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 917 is 0.0783 sec\n",
      "train AE loss : 0.614778995513916, train ANN loss : 3.24418568611145\n",
      "AE loss : 0.6154259443283081, ANN loss : 3.219906806945801, Total loss : 64.76250457763672\n",
      "learning rate A :  tf.Tensor(9.902702e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 918 is 0.0775 sec\n",
      "train AE loss : 0.5830272436141968, train ANN loss : 3.252730131149292\n",
      "AE loss : 0.5851200222969055, ANN loss : 3.2304539680480957, Total loss : 61.74245071411133\n",
      "learning rate A :  tf.Tensor(9.902493e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 919 is 0.0793 sec\n",
      "train AE loss : 0.5544186234474182, train ANN loss : 3.2687759399414062\n",
      "AE loss : 0.7321738004684448, ANN loss : 3.1826066970825195, Total loss : 76.39998626708984\n",
      "learning rate A :  tf.Tensor(9.902493e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 920 is 0.0804 sec\n",
      "train AE loss : 0.6938371062278748, train ANN loss : 3.2270944118499756\n",
      "AE loss : 1.0721114873886108, ANN loss : 3.1457316875457764, Total loss : 110.35688018798828\n",
      "learning rate A :  tf.Tensor(9.902493e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 921 is 0.0879 sec\n",
      "train AE loss : 1.0190470218658447, train ANN loss : 3.209963083267212\n",
      "AE loss : 0.9894485473632812, ANN loss : 3.1459639072418213, Total loss : 102.0908203125\n",
      "learning rate A :  tf.Tensor(9.9022844e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 922 is 0.0778 sec\n",
      "train AE loss : 0.9396293759346008, train ANN loss : 3.2074666023254395\n",
      "AE loss : 0.9172248244285583, ANN loss : 3.1492807865142822, Total loss : 94.87175750732422\n",
      "learning rate A :  tf.Tensor(9.9020755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 923 is 0.0775 sec\n",
      "train AE loss : 0.8703646063804626, train ANN loss : 3.2057464122772217\n",
      "AE loss : 1.303544044494629, ANN loss : 3.165611982345581, Total loss : 133.52000427246094\n",
      "learning rate A :  tf.Tensor(9.9020755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 924 is 0.0793 sec\n",
      "train AE loss : 1.2417023181915283, train ANN loss : 3.2472212314605713\n",
      "AE loss : 1.2439463138580322, ANN loss : 3.1564626693725586, Total loss : 127.55109405517578\n",
      "learning rate A :  tf.Tensor(9.9020755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 925 is 0.0810 sec\n",
      "train AE loss : 1.1841813325881958, train ANN loss : 3.23100209236145\n",
      "AE loss : 0.9057447910308838, ANN loss : 3.151257038116455, Total loss : 93.7257308959961\n",
      "learning rate A :  tf.Tensor(9.9020755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 926 is 0.0786 sec\n",
      "train AE loss : 0.8591850399971008, train ANN loss : 3.204139232635498\n",
      "AE loss : 0.7058544754981995, ANN loss : 3.190114974975586, Total loss : 73.77556610107422\n",
      "learning rate A :  tf.Tensor(9.9020755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 927 is 0.0805 sec\n",
      "train AE loss : 0.6686580181121826, train ANN loss : 3.228551149368286\n",
      "AE loss : 0.6918665766716003, ANN loss : 3.1945178508758545, Total loss : 72.38117980957031\n",
      "learning rate A :  tf.Tensor(9.9020755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 928 is 0.0807 sec\n",
      "train AE loss : 0.6554431915283203, train ANN loss : 3.231374979019165\n",
      "AE loss : 0.653940737247467, ANN loss : 3.2051162719726562, Total loss : 68.59918975830078\n",
      "learning rate A :  tf.Tensor(9.9018675e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 929 is 0.0772 sec\n",
      "train AE loss : 0.6195252537727356, train ANN loss : 3.24570369720459\n",
      "AE loss : 0.8226026296615601, ANN loss : 3.162122964859009, Total loss : 85.4223861694336\n",
      "learning rate A :  tf.Tensor(9.9018675e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 930 is 0.0787 sec\n",
      "train AE loss : 0.779853105545044, train ANN loss : 3.211865186691284\n",
      "AE loss : 0.7703009247779846, ANN loss : 3.1709513664245605, Total loss : 80.20104217529297\n",
      "learning rate A :  tf.Tensor(9.9016586e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 931 is 0.0805 sec\n",
      "train AE loss : 0.7300217747688293, train ANN loss : 3.2135050296783447\n",
      "AE loss : 1.094582200050354, ANN loss : 3.1438751220703125, Total loss : 112.60208892822266\n",
      "learning rate A :  tf.Tensor(9.9016586e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 932 is 0.0873 sec\n",
      "train AE loss : 1.04023277759552, train ANN loss : 3.200237989425659\n",
      "AE loss : 1.3254996538162231, ANN loss : 3.1631813049316406, Total loss : 135.71315002441406\n",
      "learning rate A :  tf.Tensor(9.9016586e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 933 is 0.0957 sec\n",
      "train AE loss : 1.2621809244155884, train ANN loss : 3.2380099296569824\n",
      "AE loss : 1.2066985368728638, ANN loss : 3.1497132778167725, Total loss : 123.81956481933594\n",
      "learning rate A :  tf.Tensor(9.90145e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 934 is 0.0765 sec\n",
      "train AE loss : 1.1476889848709106, train ANN loss : 3.219433546066284\n",
      "AE loss : 1.1046687364578247, ANN loss : 3.1428093910217285, Total loss : 113.60968017578125\n",
      "learning rate A :  tf.Tensor(9.901242e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 935 is 0.0762 sec\n",
      "train AE loss : 1.0495259761810303, train ANN loss : 3.215522527694702\n",
      "AE loss : 1.1342136859893799, ANN loss : 3.142904043197632, Total loss : 116.56427001953125\n",
      "learning rate A :  tf.Tensor(9.901242e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 936 is 0.0802 sec\n",
      "train AE loss : 1.0774372816085815, train ANN loss : 3.216843843460083\n",
      "AE loss : 0.9777330160140991, ANN loss : 3.1423897743225098, Total loss : 100.91570281982422\n",
      "learning rate A :  tf.Tensor(9.901242e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 937 is 0.0785 sec\n",
      "train AE loss : 0.9267680048942566, train ANN loss : 3.1961071491241455\n",
      "AE loss : 0.9047398567199707, ANN loss : 3.1471571922302246, Total loss : 93.62113189697266\n",
      "learning rate A :  tf.Tensor(9.901033e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 938 is 0.0756 sec\n",
      "train AE loss : 0.856876790523529, train ANN loss : 3.2016067504882812\n",
      "AE loss : 0.8407113552093506, ANN loss : 3.1541593074798584, Total loss : 87.22530364990234\n",
      "learning rate A :  tf.Tensor(9.900824e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 939 is 0.0771 sec\n",
      "train AE loss : 0.7958170175552368, train ANN loss : 3.2076382637023926\n",
      "AE loss : 0.7843998670578003, ANN loss : 3.1628355979919434, Total loss : 81.60282135009766\n",
      "learning rate A :  tf.Tensor(9.900615e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 940 is 0.0764 sec\n",
      "train AE loss : 0.7422184944152832, train ANN loss : 3.21307373046875\n",
      "AE loss : 0.7347066402435303, ANN loss : 3.172454595565796, Total loss : 76.64311981201172\n",
      "learning rate A :  tf.Tensor(9.900407e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 941 is 0.0777 sec\n",
      "train AE loss : 0.6949765682220459, train ANN loss : 3.215700149536133\n",
      "AE loss : 0.6905500292778015, ANN loss : 3.1827328205108643, Total loss : 72.23773193359375\n",
      "learning rate A :  tf.Tensor(9.900197e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 942 is 0.0775 sec\n",
      "train AE loss : 0.6531120538711548, train ANN loss : 3.2207186222076416\n",
      "AE loss : 0.7344496846199036, ANN loss : 3.1726343631744385, Total loss : 76.61760711669922\n",
      "learning rate A :  tf.Tensor(9.900197e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 943 is 0.0785 sec\n",
      "train AE loss : 0.6946463584899902, train ANN loss : 3.2152926921844482\n",
      "AE loss : 0.6902411580085754, ANN loss : 3.1830332279205322, Total loss : 72.2071533203125\n",
      "learning rate A :  tf.Tensor(9.899989e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 944 is 0.0771 sec\n",
      "train AE loss : 0.6527430415153503, train ANN loss : 3.224616289138794\n",
      "AE loss : 0.891165018081665, ANN loss : 3.144676923751831, Total loss : 92.26117706298828\n",
      "learning rate A :  tf.Tensor(9.899989e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 945 is 0.0802 sec\n",
      "train AE loss : 0.8438584208488464, train ANN loss : 3.196181535720825\n",
      "AE loss : 1.1579020023345947, ANN loss : 3.1400399208068848, Total loss : 118.93023681640625\n",
      "learning rate A :  tf.Tensor(9.899989e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 946 is 0.0795 sec\n",
      "train AE loss : 1.099709153175354, train ANN loss : 3.2110729217529297\n",
      "AE loss : 1.0607917308807373, ANN loss : 3.1366329193115234, Total loss : 109.2157974243164\n",
      "learning rate A :  tf.Tensor(9.89978e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 947 is 0.0773 sec\n",
      "train AE loss : 1.0063902139663696, train ANN loss : 3.198789358139038\n",
      "AE loss : 1.2142045497894287, ANN loss : 3.143982410430908, Total loss : 124.5644302368164\n",
      "learning rate A :  tf.Tensor(9.89978e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 948 is 0.0802 sec\n",
      "train AE loss : 1.153759479522705, train ANN loss : 3.2137701511383057\n",
      "AE loss : 1.1090240478515625, ANN loss : 3.137651205062866, Total loss : 114.04005432128906\n",
      "learning rate A :  tf.Tensor(9.899571e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 949 is 0.0770 sec\n",
      "train AE loss : 1.0526373386383057, train ANN loss : 3.198737382888794\n",
      "AE loss : 1.1037400960922241, ANN loss : 3.1374051570892334, Total loss : 113.51141357421875\n",
      "learning rate A :  tf.Tensor(9.899571e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 950 is 0.0782 sec\n",
      "train AE loss : 1.0473661422729492, train ANN loss : 3.194838762283325\n",
      "AE loss : 1.013494849205017, ANN loss : 3.1370797157287598, Total loss : 104.486572265625\n",
      "learning rate A :  tf.Tensor(9.899362e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 951 is 0.0774 sec\n",
      "train AE loss : 0.9607577919960022, train ANN loss : 3.1990020275115967\n",
      "AE loss : 0.9713454246520996, ANN loss : 3.1398043632507324, Total loss : 100.27435302734375\n",
      "learning rate A :  tf.Tensor(9.899362e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 952 is 0.0786 sec\n",
      "train AE loss : 0.9200856685638428, train ANN loss : 3.1967294216156006\n",
      "AE loss : 0.9260232448577881, ANN loss : 3.144425868988037, Total loss : 95.74675750732422\n",
      "learning rate A :  tf.Tensor(9.899362e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 953 is 0.0804 sec\n",
      "train AE loss : 0.8764331936836243, train ANN loss : 3.19869065284729\n",
      "AE loss : 0.858009397983551, ANN loss : 3.151711940765381, Total loss : 88.9526596069336\n",
      "learning rate A :  tf.Tensor(9.899155e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 954 is 0.0778 sec\n",
      "train AE loss : 0.811601996421814, train ANN loss : 3.195793390274048\n",
      "AE loss : 0.9075575470924377, ANN loss : 3.1461739540100098, Total loss : 93.90193176269531\n",
      "learning rate A :  tf.Tensor(9.899155e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 955 is 0.0797 sec\n",
      "train AE loss : 0.8585516214370728, train ANN loss : 3.1912131309509277\n",
      "AE loss : 0.8415077328681946, ANN loss : 3.1541759967803955, Total loss : 87.3049545288086\n",
      "learning rate A :  tf.Tensor(9.898946e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 956 is 0.0770 sec\n",
      "train AE loss : 0.7956620454788208, train ANN loss : 3.1956188678741455\n",
      "AE loss : 0.7836729288101196, ANN loss : 3.1636602878570557, Total loss : 81.53095245361328\n",
      "learning rate A :  tf.Tensor(9.898737e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 957 is 0.0787 sec\n",
      "train AE loss : 0.7406792640686035, train ANN loss : 3.199085235595703\n",
      "AE loss : 0.9522503018379211, ANN loss : 3.139934778213501, Total loss : 98.3649673461914\n",
      "learning rate A :  tf.Tensor(9.898737e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 958 is 0.0786 sec\n",
      "train AE loss : 0.9010605216026306, train ANN loss : 3.1947031021118164\n",
      "AE loss : 1.1400353908538818, ANN loss : 3.135303497314453, Total loss : 117.13883972167969\n",
      "learning rate A :  tf.Tensor(9.898737e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 959 is 0.0782 sec\n",
      "train AE loss : 1.0809781551361084, train ANN loss : 3.203605890274048\n",
      "AE loss : 1.1774780750274658, ANN loss : 3.1347427368164062, Total loss : 120.88256072998047\n",
      "learning rate A :  tf.Tensor(9.898737e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 960 is 0.0892 sec\n",
      "train AE loss : 1.1169363260269165, train ANN loss : 3.2018959522247314\n",
      "AE loss : 1.07509183883667, ANN loss : 3.1322782039642334, Total loss : 110.64147186279297\n",
      "learning rate A :  tf.Tensor(9.8985285e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 961 is 0.0774 sec\n",
      "train AE loss : 1.0186232328414917, train ANN loss : 3.1948394775390625\n",
      "AE loss : 0.986893355846405, ANN loss : 3.134272575378418, Total loss : 101.8236083984375\n",
      "learning rate A :  tf.Tensor(9.8983204e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 962 is 0.0789 sec\n",
      "train AE loss : 0.9340959787368774, train ANN loss : 3.1825129985809326\n",
      "AE loss : 1.0073479413986206, ANN loss : 3.133002758026123, Total loss : 103.86780548095703\n",
      "learning rate A :  tf.Tensor(9.8983204e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 963 is 0.1175 sec\n",
      "train AE loss : 0.9536652565002441, train ANN loss : 3.19408917427063\n",
      "AE loss : 0.9282302856445312, ANN loss : 3.137866258621216, Total loss : 95.96090698242188\n",
      "learning rate A :  tf.Tensor(9.8981116e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 964 is 0.0759 sec\n",
      "train AE loss : 0.8779670596122742, train ANN loss : 3.1944632530212402\n",
      "AE loss : 0.9752628803253174, ANN loss : 3.134312152862549, Total loss : 100.66060638427734\n",
      "learning rate A :  tf.Tensor(9.8981116e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 965 is 0.0806 sec\n",
      "train AE loss : 0.9227082133293152, train ANN loss : 3.1871349811553955\n",
      "AE loss : 0.9000990986824036, ANN loss : 3.1404881477355957, Total loss : 93.15039825439453\n",
      "learning rate A :  tf.Tensor(9.897903e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 966 is 0.0781 sec\n",
      "train AE loss : 0.8508864641189575, train ANN loss : 3.193358898162842\n",
      "AE loss : 1.0082403421401978, ANN loss : 3.1317288875579834, Total loss : 103.95576477050781\n",
      "learning rate A :  tf.Tensor(9.897903e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 967 is 0.0789 sec\n",
      "train AE loss : 0.9539191126823425, train ANN loss : 3.188729763031006\n",
      "AE loss : 0.9281436204910278, ANN loss : 3.1368260383605957, Total loss : 95.95118713378906\n",
      "learning rate A :  tf.Tensor(9.897695e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 968 is 0.0783 sec\n",
      "train AE loss : 0.8772798776626587, train ANN loss : 3.191187858581543\n",
      "AE loss : 1.0803388357162476, ANN loss : 3.129741668701172, Total loss : 111.16361999511719\n",
      "learning rate A :  tf.Tensor(9.897695e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 969 is 0.0873 sec\n",
      "train AE loss : 1.0226497650146484, train ANN loss : 3.1869449615478516\n",
      "AE loss : 0.9897994995117188, ANN loss : 3.1319499015808105, Total loss : 102.11190032958984\n",
      "learning rate A :  tf.Tensor(9.897486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 970 is 0.0844 sec\n",
      "train AE loss : 0.9358605146408081, train ANN loss : 3.1829938888549805\n",
      "AE loss : 1.1206378936767578, ANN loss : 3.130065679550171, Total loss : 115.19385528564453\n",
      "learning rate A :  tf.Tensor(9.897486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 971 is 0.0809 sec\n",
      "train AE loss : 1.0609517097473145, train ANN loss : 3.1933956146240234\n",
      "AE loss : 1.125043511390686, ANN loss : 3.1292099952697754, Total loss : 115.63355255126953\n",
      "learning rate A :  tf.Tensor(9.897486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 972 is 0.0803 sec\n",
      "train AE loss : 1.0650395154953003, train ANN loss : 3.1960458755493164\n",
      "AE loss : 1.0273654460906982, ANN loss : 3.1302132606506348, Total loss : 105.86675262451172\n",
      "learning rate A :  tf.Tensor(9.897277e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 973 is 0.0789 sec\n",
      "train AE loss : 0.971312940120697, train ANN loss : 3.1820156574249268\n",
      "AE loss : 0.9431764483451843, ANN loss : 3.134956121444702, Total loss : 97.45259857177734\n",
      "learning rate A :  tf.Tensor(9.897068e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 974 is 0.0789 sec\n",
      "train AE loss : 0.8907440900802612, train ANN loss : 3.1877009868621826\n",
      "AE loss : 0.8701444864273071, ANN loss : 3.1423654556274414, Total loss : 90.15681457519531\n",
      "learning rate A :  tf.Tensor(9.89686e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 975 is 0.0780 sec\n",
      "train AE loss : 0.8211489915847778, train ANN loss : 3.1909046173095703\n",
      "AE loss : 0.8065022826194763, ANN loss : 3.1516573429107666, Total loss : 83.80188751220703\n",
      "learning rate A :  tf.Tensor(9.896652e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 976 is 0.0880 sec\n",
      "train AE loss : 0.7606929540634155, train ANN loss : 3.20210337638855\n",
      "AE loss : 0.9236841797828674, ANN loss : 3.1347644329071045, Total loss : 95.503173828125\n",
      "learning rate A :  tf.Tensor(9.896652e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 977 is 0.0834 sec\n",
      "train AE loss : 0.8720616102218628, train ANN loss : 3.1846864223480225\n",
      "AE loss : 1.0966556072235107, ANN loss : 3.125985860824585, Total loss : 112.79154205322266\n",
      "learning rate A :  tf.Tensor(9.896652e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 978 is 0.0810 sec\n",
      "train AE loss : 1.0376485586166382, train ANN loss : 3.1880571842193604\n",
      "AE loss : 1.0029891729354858, ANN loss : 3.127588987350464, Total loss : 103.42650604248047\n",
      "learning rate A :  tf.Tensor(9.896443e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 979 is 0.0799 sec\n",
      "train AE loss : 0.9478690028190613, train ANN loss : 3.1887261867523193\n",
      "AE loss : 0.9221598505973816, ANN loss : 3.132636070251465, Total loss : 95.34862518310547\n",
      "learning rate A :  tf.Tensor(9.8962344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 980 is 0.0785 sec\n",
      "train AE loss : 0.8706471920013428, train ANN loss : 3.1893465518951416\n",
      "AE loss : 1.1186758279800415, ANN loss : 3.1258256435394287, Total loss : 114.993408203125\n",
      "learning rate A :  tf.Tensor(9.8962344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 981 is 0.0798 sec\n",
      "train AE loss : 1.058690071105957, train ANN loss : 3.19665265083313\n",
      "AE loss : 1.0218838453292847, ANN loss : 3.1258020401000977, Total loss : 105.3141860961914\n",
      "learning rate A :  tf.Tensor(9.896026e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 982 is 0.0787 sec\n",
      "train AE loss : 0.9659178853034973, train ANN loss : 3.189446210861206\n",
      "AE loss : 0.9385038614273071, ANN loss : 3.1296677589416504, Total loss : 96.98006439208984\n",
      "learning rate A :  tf.Tensor(9.895818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 983 is 0.0782 sec\n",
      "train AE loss : 0.8862288594245911, train ANN loss : 3.1832547187805176\n",
      "AE loss : 1.1338895559310913, ANN loss : 3.126420259475708, Total loss : 116.515380859375\n",
      "learning rate A :  tf.Tensor(9.895818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 984 is 0.0809 sec\n",
      "train AE loss : 1.0730472803115845, train ANN loss : 3.1971261501312256\n",
      "AE loss : 1.167931079864502, ANN loss : 3.1264429092407227, Total loss : 119.91954803466797\n",
      "learning rate A :  tf.Tensor(9.895818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 985 is 0.0798 sec\n",
      "train AE loss : 1.1054390668869019, train ANN loss : 3.1972665786743164\n",
      "AE loss : 1.037031888961792, ANN loss : 3.1245174407958984, Total loss : 106.82769775390625\n",
      "learning rate A :  tf.Tensor(9.895818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 986 is 0.0805 sec\n",
      "train AE loss : 0.9796448349952698, train ANN loss : 3.182460308074951\n",
      "AE loss : 0.8910449743270874, ANN loss : 3.13873553276062, Total loss : 92.24323272705078\n",
      "learning rate A :  tf.Tensor(9.895818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 987 is 0.0798 sec\n",
      "train AE loss : 0.840033233165741, train ANN loss : 3.1868817806243896\n",
      "AE loss : 0.8497155904769897, ANN loss : 3.1466681957244873, Total loss : 88.11823272705078\n",
      "learning rate A :  tf.Tensor(9.895818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 988 is 0.1063 sec\n",
      "train AE loss : 0.8005824685096741, train ANN loss : 3.1842963695526123\n",
      "AE loss : 0.933657705783844, ANN loss : 3.1338400840759277, Total loss : 96.4996109008789\n",
      "learning rate A :  tf.Tensor(9.895818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 989 is 0.1046 sec\n",
      "train AE loss : 0.8804377317428589, train ANN loss : 3.1783812046051025\n",
      "AE loss : 1.0874440670013428, ANN loss : 3.1216516494750977, Total loss : 111.86605072021484\n",
      "learning rate A :  tf.Tensor(9.895818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 990 is 0.0850 sec\n",
      "train AE loss : 1.0275447368621826, train ANN loss : 3.1824724674224854\n",
      "AE loss : 1.1851109266281128, ANN loss : 3.1212961673736572, Total loss : 121.63239288330078\n",
      "learning rate A :  tf.Tensor(9.895818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 991 is 0.0810 sec\n",
      "train AE loss : 1.1212794780731201, train ANN loss : 3.18062686920166\n",
      "AE loss : 1.1386810541152954, ANN loss : 3.1197457313537598, Total loss : 116.98784637451172\n",
      "learning rate A :  tf.Tensor(9.895818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 992 is 0.0802 sec\n",
      "train AE loss : 1.076830506324768, train ANN loss : 3.187035322189331\n",
      "AE loss : 1.037506341934204, ANN loss : 3.1218669414520264, Total loss : 106.87248992919922\n",
      "learning rate A :  tf.Tensor(9.8956094e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 993 is 0.0895 sec\n",
      "train AE loss : 0.9798598885536194, train ANN loss : 3.17862868309021\n",
      "AE loss : 0.9506183862686157, ANN loss : 3.1277403831481934, Total loss : 98.1895751953125\n",
      "learning rate A :  tf.Tensor(9.8954006e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 994 is 0.0771 sec\n",
      "train AE loss : 0.8968179821968079, train ANN loss : 3.179088830947876\n",
      "AE loss : 0.9707146883010864, ANN loss : 3.1255087852478027, Total loss : 100.19698333740234\n",
      "learning rate A :  tf.Tensor(9.8954006e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 995 is 0.0789 sec\n",
      "train AE loss : 0.9159696102142334, train ANN loss : 3.175971269607544\n",
      "AE loss : 0.8928206562995911, ANN loss : 3.1334879398345947, Total loss : 92.41555786132812\n",
      "learning rate A :  tf.Tensor(9.8951925e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 996 is 0.0768 sec\n",
      "train AE loss : 0.8417026996612549, train ANN loss : 3.187622547149658\n",
      "AE loss : 0.8252159357070923, ANN loss : 3.1435203552246094, Total loss : 85.66510772705078\n",
      "learning rate A :  tf.Tensor(9.894984e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 997 is 0.0772 sec\n",
      "train AE loss : 0.7774394750595093, train ANN loss : 3.187955617904663\n",
      "AE loss : 0.7663745880126953, ANN loss : 3.154860258102417, Total loss : 79.79232025146484\n",
      "learning rate A :  tf.Tensor(9.894775e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 998 is 0.0779 sec\n",
      "train AE loss : 0.721559464931488, train ANN loss : 3.1937458515167236\n",
      "AE loss : 0.9330241680145264, ANN loss : 3.125901699066162, Total loss : 96.42831420898438\n",
      "learning rate A :  tf.Tensor(9.894775e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 999 is 0.0790 sec\n",
      "train AE loss : 0.8799310922622681, train ANN loss : 3.175896644592285\n",
      "AE loss : 1.1777658462524414, ANN loss : 3.1197052001953125, Total loss : 120.89628601074219\n",
      "learning rate A :  tf.Tensor(9.894775e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1000 is 0.0782 sec\n",
      "train AE loss : 1.1140286922454834, train ANN loss : 3.1923975944519043\n",
      "AE loss : 1.2554718255996704, ANN loss : 3.124387741088867, Total loss : 128.6715545654297\n",
      "learning rate A :  tf.Tensor(9.894775e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1001 is 0.0787 sec\n",
      "train AE loss : 1.1884483098983765, train ANN loss : 3.1877615451812744\n",
      "AE loss : 1.135627031326294, ANN loss : 3.1179444789886475, Total loss : 116.68064880371094\n",
      "learning rate A :  tf.Tensor(9.894567e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1002 is 0.0772 sec\n",
      "train AE loss : 1.073425531387329, train ANN loss : 3.1807615756988525\n",
      "AE loss : 1.0698379278182983, ANN loss : 3.1176578998565674, Total loss : 110.10145568847656\n",
      "learning rate A :  tf.Tensor(9.894567e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1003 is 0.0899 sec\n",
      "train AE loss : 1.0101861953735352, train ANN loss : 3.1801598072052\n",
      "AE loss : 0.9771150350570679, ANN loss : 3.1207997798919678, Total loss : 100.83230590820312\n",
      "learning rate A :  tf.Tensor(9.894358e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1004 is 0.0773 sec\n",
      "train AE loss : 0.9214895963668823, train ANN loss : 3.1811108589172363\n",
      "AE loss : 0.9258766174316406, ANN loss : 3.1259267330169678, Total loss : 95.71358489990234\n",
      "learning rate A :  tf.Tensor(9.894358e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1005 is 0.0781 sec\n",
      "train AE loss : 0.8725186586380005, train ANN loss : 3.1773271560668945\n",
      "AE loss : 0.852853000164032, ANN loss : 3.134584426879883, Total loss : 88.4198989868164\n",
      "learning rate A :  tf.Tensor(9.89415e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1006 is 0.0782 sec\n",
      "train AE loss : 0.8030004501342773, train ANN loss : 3.18223237991333\n",
      "AE loss : 0.7895103693008423, ANN loss : 3.144984722137451, Total loss : 82.09601593017578\n",
      "learning rate A :  tf.Tensor(9.893941e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1007 is 0.0764 sec\n",
      "train AE loss : 0.7428566217422485, train ANN loss : 3.1929771900177\n",
      "AE loss : 0.7342066168785095, ANN loss : 3.1564526557922363, Total loss : 76.57711029052734\n",
      "learning rate A :  tf.Tensor(9.893733e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1008 is 0.0944 sec\n",
      "train AE loss : 0.690453052520752, train ANN loss : 3.201849937438965\n",
      "AE loss : 0.847917914390564, ANN loss : 3.133366584777832, Total loss : 87.92516326904297\n",
      "learning rate A :  tf.Tensor(9.893733e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1009 is 0.0810 sec\n",
      "train AE loss : 0.7982696294784546, train ANN loss : 3.175917625427246\n",
      "AE loss : 0.7851192951202393, ANN loss : 3.1435632705688477, Total loss : 81.65550231933594\n",
      "learning rate A :  tf.Tensor(9.893524e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1010 is 0.0759 sec\n",
      "train AE loss : 0.7386545538902283, train ANN loss : 3.1948137283325195\n",
      "AE loss : 0.7302666306495667, ANN loss : 3.154834747314453, Total loss : 76.1814956665039\n",
      "learning rate A :  tf.Tensor(9.893316e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1011 is 0.0758 sec\n",
      "train AE loss : 0.6867135763168335, train ANN loss : 3.1906349658966064\n",
      "AE loss : 0.6820316910743713, ANN loss : 3.166783571243286, Total loss : 71.36994934082031\n",
      "learning rate A :  tf.Tensor(9.893107e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1012 is 0.0769 sec\n",
      "train AE loss : 0.6411981582641602, train ANN loss : 3.2077672481536865\n",
      "AE loss : 0.9665234088897705, ANN loss : 3.1182525157928467, Total loss : 99.77059936523438\n",
      "learning rate A :  tf.Tensor(9.893107e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1013 is 0.0782 sec\n",
      "train AE loss : 0.911128044128418, train ANN loss : 3.174098491668701\n",
      "AE loss : 0.8876523375511169, ANN loss : 3.1241047382354736, Total loss : 91.88933563232422\n",
      "learning rate A :  tf.Tensor(9.892899e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1014 is 0.0764 sec\n",
      "train AE loss : 0.8359909653663635, train ANN loss : 3.177578926086426\n",
      "AE loss : 0.8193936944007874, ANN loss : 3.132340431213379, Total loss : 85.07170867919922\n",
      "learning rate A :  tf.Tensor(9.89269e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1015 is 0.0771 sec\n",
      "train AE loss : 0.7711725234985352, train ANN loss : 3.180426597595215\n",
      "AE loss : 0.7600171566009521, ANN loss : 3.1421172618865967, Total loss : 79.14383697509766\n",
      "learning rate A :  tf.Tensor(9.892482e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1016 is 0.0767 sec\n",
      "train AE loss : 0.7148876786231995, train ANN loss : 3.1875252723693848\n",
      "AE loss : 1.185397982597351, ANN loss : 3.1230885982513428, Total loss : 121.66287994384766\n",
      "learning rate A :  tf.Tensor(9.892482e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1017 is 0.0785 sec\n",
      "train AE loss : 1.120471477508545, train ANN loss : 3.1970577239990234\n",
      "AE loss : 1.4208840131759644, ANN loss : 3.1537117958068848, Total loss : 145.24209594726562\n",
      "learning rate A :  tf.Tensor(9.892482e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1018 is 0.0786 sec\n",
      "train AE loss : 1.3466910123825073, train ANN loss : 3.23343825340271\n",
      "AE loss : 1.1679574251174927, ANN loss : 3.1168103218078613, Total loss : 119.91256713867188\n",
      "learning rate A :  tf.Tensor(9.892482e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1019 is 0.0780 sec\n",
      "train AE loss : 1.1036419868469238, train ANN loss : 3.1854183673858643\n",
      "AE loss : 1.0595512390136719, ANN loss : 3.1140847206115723, Total loss : 109.06920623779297\n",
      "learning rate A :  tf.Tensor(9.8922734e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1020 is 0.0763 sec\n",
      "train AE loss : 0.9997325539588928, train ANN loss : 3.1799774169921875\n",
      "AE loss : 0.9669414758682251, ANN loss : 3.1163506507873535, Total loss : 99.81048583984375\n",
      "learning rate A :  tf.Tensor(9.892065e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1021 is 0.0792 sec\n",
      "train AE loss : 0.9112461805343628, train ANN loss : 3.178715467453003\n",
      "AE loss : 0.8872507214546204, ANN loss : 3.122108221054077, Total loss : 91.84717559814453\n",
      "learning rate A :  tf.Tensor(9.8918565e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1022 is 0.0767 sec\n",
      "train AE loss : 0.8353897929191589, train ANN loss : 3.176973819732666\n",
      "AE loss : 0.8183886408805847, ANN loss : 3.130315065383911, Total loss : 84.96918487548828\n",
      "learning rate A :  tf.Tensor(9.8916484e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1023 is 0.0772 sec\n",
      "train AE loss : 0.770022988319397, train ANN loss : 3.176928758621216\n",
      "AE loss : 0.7428268194198608, ANN loss : 3.1493937969207764, Total loss : 77.43207550048828\n",
      "learning rate A :  tf.Tensor(9.8916484e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1024 is 0.0788 sec\n",
      "train AE loss : 0.6984053254127502, train ANN loss : 3.189753293991089\n",
      "AE loss : 0.8047152757644653, ANN loss : 3.1392579078674316, Total loss : 83.61078643798828\n",
      "learning rate A :  tf.Tensor(9.8916484e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1025 is 0.0795 sec\n",
      "train AE loss : 0.7570180892944336, train ANN loss : 3.1905505657196045\n",
      "AE loss : 0.7462877631187439, ANN loss : 3.1505327224731445, Total loss : 77.77930450439453\n",
      "learning rate A :  tf.Tensor(9.89144e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1026 is 0.0768 sec\n",
      "train AE loss : 0.7016901969909668, train ANN loss : 3.1907260417938232\n",
      "AE loss : 0.6951258182525635, ANN loss : 3.162533760070801, Total loss : 72.67511749267578\n",
      "learning rate A :  tf.Tensor(9.8912315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1027 is 0.0780 sec\n",
      "train AE loss : 0.6533808708190918, train ANN loss : 3.201087713241577\n",
      "AE loss : 0.6500861644744873, ANN loss : 3.1748924255371094, Total loss : 68.18350982666016\n",
      "learning rate A :  tf.Tensor(9.891023e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1028 is 0.0775 sec\n",
      "train AE loss : 0.6109310984611511, train ANN loss : 3.2161967754364014\n",
      "AE loss : 0.6102278828620911, ANN loss : 3.1873393058776855, Total loss : 64.21013641357422\n",
      "learning rate A :  tf.Tensor(9.8908145e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1029 is 0.0770 sec\n",
      "train AE loss : 0.5734656453132629, train ANN loss : 3.225473165512085\n",
      "AE loss : 0.8435880541801453, ANN loss : 3.129455089569092, Total loss : 87.4882583618164\n",
      "learning rate A :  tf.Tensor(9.8908145e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1030 is 0.0801 sec\n",
      "train AE loss : 0.7940227389335632, train ANN loss : 3.176931619644165\n",
      "AE loss : 0.7799165844917297, ANN loss : 3.139392852783203, Total loss : 81.13105773925781\n",
      "learning rate A :  tf.Tensor(9.8906065e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1031 is 0.0769 sec\n",
      "train AE loss : 0.7336537837982178, train ANN loss : 3.1872496604919434\n",
      "AE loss : 1.1743035316467285, ANN loss : 3.1166608333587646, Total loss : 120.54701232910156\n",
      "learning rate A :  tf.Tensor(9.8906065e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1032 is 0.0786 sec\n",
      "train AE loss : 1.1096676588058472, train ANN loss : 3.176952362060547\n",
      "AE loss : 1.0635929107666016, ANN loss : 3.1131508350372314, Total loss : 109.47244262695312\n",
      "learning rate A :  tf.Tensor(9.8903976e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1033 is 0.0777 sec\n",
      "train AE loss : 1.003615140914917, train ANN loss : 3.17266845703125\n",
      "AE loss : 0.9692397117614746, ANN loss : 3.1146929264068604, Total loss : 100.03866577148438\n",
      "learning rate A :  tf.Tensor(9.8901895e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1034 is 0.0768 sec\n",
      "train AE loss : 0.9135410189628601, train ANN loss : 3.1675736904144287\n",
      "AE loss : 0.8882411122322083, ANN loss : 3.119905948638916, Total loss : 91.94402313232422\n",
      "learning rate A :  tf.Tensor(9.889981e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1035 is 0.0777 sec\n",
      "train AE loss : 0.8365257382392883, train ANN loss : 3.1747639179229736\n",
      "AE loss : 1.3228040933609009, ANN loss : 3.135119676589966, Total loss : 135.41554260253906\n",
      "learning rate A :  tf.Tensor(9.889981e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1036 is 0.0815 sec\n",
      "train AE loss : 1.2521132230758667, train ANN loss : 3.2154171466827393\n",
      "AE loss : 1.406599998474121, ANN loss : 3.144226312637329, Total loss : 143.80422973632812\n",
      "learning rate A :  tf.Tensor(9.889981e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1037 is 0.0801 sec\n",
      "train AE loss : 1.332438349723816, train ANN loss : 3.2238335609436035\n",
      "AE loss : 1.2582042217254639, ANN loss : 3.125059127807617, Total loss : 128.9454803466797\n",
      "learning rate A :  tf.Tensor(9.8897726e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1038 is 0.0766 sec\n",
      "train AE loss : 1.1898250579833984, train ANN loss : 3.2047884464263916\n",
      "AE loss : 1.1335636377334595, ANN loss : 3.11552095413208, Total loss : 116.47188568115234\n",
      "learning rate A :  tf.Tensor(9.8895645e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1039 is 0.0777 sec\n",
      "train AE loss : 1.070307970046997, train ANN loss : 3.1758241653442383\n",
      "AE loss : 1.0280640125274658, ANN loss : 3.112746000289917, Total loss : 105.91915893554688\n",
      "learning rate A :  tf.Tensor(9.889355e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1040 is 0.0768 sec\n",
      "train AE loss : 0.9693902134895325, train ANN loss : 3.1727192401885986\n",
      "AE loss : 1.0392367839813232, ANN loss : 3.112372398376465, Total loss : 107.03604888916016\n",
      "learning rate A :  tf.Tensor(9.889355e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1041 is 0.0787 sec\n",
      "train AE loss : 0.9797669053077698, train ANN loss : 3.175126314163208\n",
      "AE loss : 0.9886513352394104, ANN loss : 3.1139564514160156, Total loss : 101.97908782958984\n",
      "learning rate A :  tf.Tensor(9.889355e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1042 is 0.0797 sec\n",
      "train AE loss : 0.9312216639518738, train ANN loss : 3.1651947498321533\n",
      "AE loss : 0.9389030933380127, ANN loss : 3.1188488006591797, Total loss : 97.00916290283203\n",
      "learning rate A :  tf.Tensor(9.889355e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1043 is 0.0786 sec\n",
      "train AE loss : 0.8836256265640259, train ANN loss : 3.178258180618286\n",
      "AE loss : 0.9269949793815613, ANN loss : 3.121346950531006, Total loss : 95.82083892822266\n",
      "learning rate A :  tf.Tensor(9.889355e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1044 is 0.0792 sec\n",
      "train AE loss : 0.8720912337303162, train ANN loss : 3.174802780151367\n",
      "AE loss : 0.8504478931427002, ANN loss : 3.1310906410217285, Total loss : 88.1758804321289\n",
      "learning rate A :  tf.Tensor(9.889146e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1045 is 0.0779 sec\n",
      "train AE loss : 0.7994508743286133, train ANN loss : 3.175041913986206\n",
      "AE loss : 0.9359021186828613, ANN loss : 3.1191818714141846, Total loss : 96.70938873291016\n",
      "learning rate A :  tf.Tensor(9.889146e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1046 is 0.0789 sec\n",
      "train AE loss : 0.8803501725196838, train ANN loss : 3.1801412105560303\n",
      "AE loss : 0.8580155372619629, ANN loss : 3.12878680229187, Total loss : 88.93034362792969\n",
      "learning rate A :  tf.Tensor(9.888938e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1047 is 0.0778 sec\n",
      "train AE loss : 0.8064178228378296, train ANN loss : 3.1675496101379395\n",
      "AE loss : 0.7907679677009583, ANN loss : 3.1401946544647217, Total loss : 82.21699523925781\n",
      "learning rate A :  tf.Tensor(9.88873e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1048 is 0.0784 sec\n",
      "train AE loss : 0.7426712512969971, train ANN loss : 3.188983201980591\n",
      "AE loss : 0.732338011264801, ANN loss : 3.1524546146392822, Total loss : 76.38626098632812\n",
      "learning rate A :  tf.Tensor(9.888522e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1049 is 0.0778 sec\n",
      "train AE loss : 0.6874995827674866, train ANN loss : 3.1914567947387695\n",
      "AE loss : 0.9512423276901245, ANN loss : 3.11434268951416, Total loss : 98.23856353759766\n",
      "learning rate A :  tf.Tensor(9.888522e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1050 is 0.0798 sec\n",
      "train AE loss : 0.8947803974151611, train ANN loss : 3.1599864959716797\n",
      "AE loss : 0.8710159659385681, ANN loss : 3.1227519512176514, Total loss : 90.2243423461914\n",
      "learning rate A :  tf.Tensor(9.888313e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1051 is 0.0796 sec\n",
      "train AE loss : 0.8186174631118774, train ANN loss : 3.1656904220581055\n",
      "AE loss : 1.198347568511963, ANN loss : 3.109426498413086, Total loss : 122.94418334960938\n",
      "learning rate A :  tf.Tensor(9.888313e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1052 is 0.0796 sec\n",
      "train AE loss : 1.1304130554199219, train ANN loss : 3.173522472381592\n",
      "AE loss : 1.3614981174468994, ANN loss : 3.1231753826141357, Total loss : 139.2729949951172\n",
      "learning rate A :  tf.Tensor(9.888313e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1053 is 0.0806 sec\n",
      "train AE loss : 1.2864412069320679, train ANN loss : 3.197549819946289\n",
      "AE loss : 1.1954281330108643, ANN loss : 3.1081185340881348, Total loss : 122.65093231201172\n",
      "learning rate A :  tf.Tensor(9.888313e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1054 is 0.0818 sec\n",
      "train AE loss : 1.1268802881240845, train ANN loss : 3.166130781173706\n",
      "AE loss : 0.938660204410553, ANN loss : 3.118084669113159, Total loss : 96.98410034179688\n",
      "learning rate A :  tf.Tensor(9.888313e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1055 is 0.0795 sec\n",
      "train AE loss : 0.8815094828605652, train ANN loss : 3.1705760955810547\n",
      "AE loss : 0.8586359024047852, ANN loss : 3.128309726715088, Total loss : 88.99189758300781\n",
      "learning rate A :  tf.Tensor(9.888105e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1056 is 0.0774 sec\n",
      "train AE loss : 0.805634617805481, train ANN loss : 3.1737852096557617\n",
      "AE loss : 0.789693295955658, ANN loss : 3.1402037143707275, Total loss : 82.10953521728516\n",
      "learning rate A :  tf.Tensor(9.887897e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1057 is 0.0805 sec\n",
      "train AE loss : 0.740334689617157, train ANN loss : 3.1839964389801025\n",
      "AE loss : 0.7340565323829651, ANN loss : 3.1542348861694336, Total loss : 76.55989074707031\n",
      "learning rate A :  tf.Tensor(9.887897e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1058 is 0.0781 sec\n",
      "train AE loss : 0.687717616558075, train ANN loss : 3.194004535675049\n",
      "AE loss : 0.8309838175773621, ANN loss : 3.1307930946350098, Total loss : 86.22917175292969\n",
      "learning rate A :  tf.Tensor(9.887897e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1059 is 0.0780 sec\n",
      "train AE loss : 0.7794153094291687, train ANN loss : 3.1715292930603027\n",
      "AE loss : 0.766068696975708, ANN loss : 3.143160343170166, Total loss : 79.75003814697266\n",
      "learning rate A :  tf.Tensor(9.887689e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1060 is 0.0766 sec\n",
      "train AE loss : 0.7180129289627075, train ANN loss : 3.189505100250244\n",
      "AE loss : 0.9973190426826477, ANN loss : 3.107099771499634, Total loss : 102.83899688720703\n",
      "learning rate A :  tf.Tensor(9.887689e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1061 is 0.0785 sec\n",
      "train AE loss : 0.9372861981391907, train ANN loss : 3.1675143241882324\n",
      "AE loss : 1.25580632686615, ANN loss : 3.1080374717712402, Total loss : 128.6886749267578\n",
      "learning rate A :  tf.Tensor(9.887689e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1062 is 0.0800 sec\n",
      "train AE loss : 1.1841670274734497, train ANN loss : 3.1859748363494873\n",
      "AE loss : 1.3030298948287964, ANN loss : 3.110180377960205, Total loss : 133.41317749023438\n",
      "learning rate A :  tf.Tensor(9.887689e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1063 is 0.0793 sec\n",
      "train AE loss : 1.229176640510559, train ANN loss : 3.18852162361145\n",
      "AE loss : 1.111289143562317, ANN loss : 3.1013355255126953, Total loss : 114.2302474975586\n",
      "learning rate A :  tf.Tensor(9.887689e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1064 is 0.0784 sec\n",
      "train AE loss : 1.0455129146575928, train ANN loss : 3.161672592163086\n",
      "AE loss : 0.9101026654243469, ANN loss : 3.1190733909606934, Total loss : 94.12934112548828\n",
      "learning rate A :  tf.Tensor(9.887689e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1065 is 0.0791 sec\n",
      "train AE loss : 0.8537603616714478, train ANN loss : 3.1619722843170166\n",
      "AE loss : 0.8299422860145569, ANN loss : 3.135634422302246, Total loss : 86.12986755371094\n",
      "learning rate A :  tf.Tensor(9.887689e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1066 is 0.0879 sec\n",
      "train AE loss : 0.7776381969451904, train ANN loss : 3.18054461479187\n",
      "AE loss : 0.764489471912384, ANN loss : 3.1493306159973145, Total loss : 79.59827423095703\n",
      "learning rate A :  tf.Tensor(9.88748e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1067 is 0.0776 sec\n",
      "train AE loss : 0.715705931186676, train ANN loss : 3.1843833923339844\n",
      "AE loss : 0.8488044142723083, ANN loss : 3.1295547485351562, Total loss : 88.00999450683594\n",
      "learning rate A :  tf.Tensor(9.88748e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1068 is 0.0790 sec\n",
      "train AE loss : 0.7955068945884705, train ANN loss : 3.168025493621826\n",
      "AE loss : 1.0631111860275269, ANN loss : 3.1018054485321045, Total loss : 109.41292572021484\n",
      "learning rate A :  tf.Tensor(9.88748e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1069 is 0.0791 sec\n",
      "train AE loss : 0.9991784691810608, train ANN loss : 3.156851053237915\n",
      "AE loss : 0.9644452333450317, ANN loss : 3.108995199203491, Total loss : 99.55352020263672\n",
      "learning rate A :  tf.Tensor(9.887271e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1070 is 0.0773 sec\n",
      "train AE loss : 0.905207633972168, train ANN loss : 3.166834831237793\n",
      "AE loss : 0.8802263140678406, ANN loss : 3.119295120239258, Total loss : 91.14192199707031\n",
      "learning rate A :  tf.Tensor(9.887063e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1071 is 0.0772 sec\n",
      "train AE loss : 0.8253368735313416, train ANN loss : 3.17102313041687\n",
      "AE loss : 0.807965874671936, ANN loss : 3.1314220428466797, Total loss : 83.92800903320312\n",
      "learning rate A :  tf.Tensor(9.886855e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1072 is 0.0777 sec\n",
      "train AE loss : 0.7568877339363098, train ANN loss : 3.173046112060547\n",
      "AE loss : 1.1367218494415283, ANN loss : 3.098836898803711, Total loss : 116.7710189819336\n",
      "learning rate A :  tf.Tensor(9.886855e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1073 is 0.0783 sec\n",
      "train AE loss : 1.0694637298583984, train ANN loss : 3.1622471809387207\n",
      "AE loss : 1.4097871780395508, ANN loss : 3.1181228160858154, Total loss : 144.09683227539062\n",
      "learning rate A :  tf.Tensor(9.886855e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1074 is 0.0786 sec\n",
      "train AE loss : 1.330739140510559, train ANN loss : 3.1983213424682617\n",
      "AE loss : 1.2549110651016235, ANN loss : 3.1040377616882324, Total loss : 128.5951385498047\n",
      "learning rate A :  tf.Tensor(9.886647e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1075 is 0.0774 sec\n",
      "train AE loss : 1.1823530197143555, train ANN loss : 3.176098108291626\n",
      "AE loss : 1.1259572505950928, ANN loss : 3.0991392135620117, Total loss : 115.69486236572266\n",
      "learning rate A :  tf.Tensor(9.886438e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1076 is 0.0770 sec\n",
      "train AE loss : 1.0590356588363647, train ANN loss : 3.1642308235168457\n",
      "AE loss : 1.0173591375350952, ANN loss : 3.100611686706543, Total loss : 104.83651733398438\n",
      "learning rate A :  tf.Tensor(9.88623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1077 is 0.0770 sec\n",
      "train AE loss : 0.9554792046546936, train ANN loss : 3.1571035385131836\n",
      "AE loss : 1.223852276802063, ANN loss : 3.103135824203491, Total loss : 125.48835754394531\n",
      "learning rate A :  tf.Tensor(9.88623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1078 is 0.0811 sec\n",
      "train AE loss : 1.1523977518081665, train ANN loss : 3.170788526535034\n",
      "AE loss : 1.2405086755752563, ANN loss : 3.1031601428985596, Total loss : 127.15402221679688\n",
      "learning rate A :  tf.Tensor(9.88623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1079 is 0.0790 sec\n",
      "train AE loss : 1.1680498123168945, train ANN loss : 3.1656901836395264\n",
      "AE loss : 1.0864028930664062, ANN loss : 3.1003403663635254, Total loss : 111.74063110351562\n",
      "learning rate A :  tf.Tensor(9.88623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1080 is 0.0795 sec\n",
      "train AE loss : 1.0204885005950928, train ANN loss : 3.1580348014831543\n",
      "AE loss : 0.9294353127479553, ANN loss : 3.1144979000091553, Total loss : 96.05803680419922\n",
      "learning rate A :  tf.Tensor(9.88623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1081 is 0.0798 sec\n",
      "train AE loss : 0.8709961175918579, train ANN loss : 3.154284715652466\n",
      "AE loss : 0.8755063414573669, ANN loss : 3.124250650405884, Total loss : 90.67489624023438\n",
      "learning rate A :  tf.Tensor(9.88623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1082 is 0.0788 sec\n",
      "train AE loss : 0.8197485208511353, train ANN loss : 3.161979913711548\n",
      "AE loss : 0.9333195686340332, ANN loss : 3.1143264770507812, Total loss : 96.44628143310547\n",
      "learning rate A :  tf.Tensor(9.88623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1083 is 0.0792 sec\n",
      "train AE loss : 0.8744544386863708, train ANN loss : 3.1584415435791016\n",
      "AE loss : 1.080802083015442, ANN loss : 3.0987133979797363, Total loss : 111.17892456054688\n",
      "learning rate A :  tf.Tensor(9.88623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1084 is 0.0800 sec\n",
      "train AE loss : 1.0146900415420532, train ANN loss : 3.1530566215515137\n",
      "AE loss : 0.9778444170951843, ANN loss : 3.105686902999878, Total loss : 100.8901138305664\n",
      "learning rate A :  tf.Tensor(9.886021e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1085 is 0.0784 sec\n",
      "train AE loss : 0.9167146682739258, train ANN loss : 3.1581780910491943\n",
      "AE loss : 1.1639530658721924, ANN loss : 3.095172882080078, Total loss : 119.490478515625\n",
      "learning rate A :  tf.Tensor(9.886021e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1086 is 0.0807 sec\n",
      "train AE loss : 1.0939861536026, train ANN loss : 3.1611955165863037\n",
      "AE loss : 1.2751176357269287, ANN loss : 3.097485065460205, Total loss : 130.6092529296875\n",
      "learning rate A :  tf.Tensor(9.886021e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1087 is 0.0810 sec\n",
      "train AE loss : 1.2000986337661743, train ANN loss : 3.168254852294922\n",
      "AE loss : 1.2039889097213745, ANN loss : 3.093794345855713, Total loss : 123.49267578125\n",
      "learning rate A :  tf.Tensor(9.886021e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1088 is 0.0804 sec\n",
      "train AE loss : 1.1320239305496216, train ANN loss : 3.160728931427002\n",
      "AE loss : 1.0362708568572998, ANN loss : 3.0987205505371094, Total loss : 106.7258071899414\n",
      "learning rate A :  tf.Tensor(9.886021e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1089 is 0.0799 sec\n",
      "train AE loss : 0.9718033075332642, train ANN loss : 3.1507208347320557\n",
      "AE loss : 0.9396587610244751, ANN loss : 3.1075053215026855, Total loss : 97.07337951660156\n",
      "learning rate A :  tf.Tensor(9.885813e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1090 is 0.0795 sec\n",
      "train AE loss : 0.8800483345985413, train ANN loss : 3.1547129154205322\n",
      "AE loss : 0.8573026657104492, ANN loss : 3.119051218032837, Total loss : 88.84931182861328\n",
      "learning rate A :  tf.Tensor(9.885606e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1091 is 0.0780 sec\n",
      "train AE loss : 0.8020648956298828, train ANN loss : 3.1627774238586426\n",
      "AE loss : 0.7866007089614868, ANN loss : 3.1320290565490723, Total loss : 81.79209899902344\n",
      "learning rate A :  tf.Tensor(9.885397e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1092 is 0.0792 sec\n",
      "train AE loss : 0.7352315187454224, train ANN loss : 3.1719465255737305\n",
      "AE loss : 0.8060438632965088, ANN loss : 3.1291005611419678, Total loss : 83.73348236083984\n",
      "learning rate A :  tf.Tensor(9.885397e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1093 is 0.0855 sec\n",
      "train AE loss : 0.7534419298171997, train ANN loss : 3.172217845916748\n",
      "AE loss : 0.7422358393669128, ANN loss : 3.1428070068359375, Total loss : 77.36639404296875\n",
      "learning rate A :  tf.Tensor(9.885189e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1094 is 0.0802 sec\n",
      "train AE loss : 0.6932889223098755, train ANN loss : 3.1787924766540527\n",
      "AE loss : 0.9010390043258667, ANN loss : 3.1102702617645264, Total loss : 93.21416473388672\n",
      "learning rate A :  tf.Tensor(9.885189e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1095 is 0.0908 sec\n",
      "train AE loss : 0.8433094024658203, train ANN loss : 3.1510603427886963\n",
      "AE loss : 0.8238814473152161, ANN loss : 3.1218039989471436, Total loss : 85.50995635986328\n",
      "learning rate A :  tf.Tensor(9.88498e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1096 is 0.1199 sec\n",
      "train AE loss : 0.7703149318695068, train ANN loss : 3.160625457763672\n",
      "AE loss : 0.757457971572876, ANN loss : 3.1346895694732666, Total loss : 78.88048553466797\n",
      "learning rate A :  tf.Tensor(9.884772e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1097 is 0.1097 sec\n",
      "train AE loss : 0.707680344581604, train ANN loss : 3.175522565841675\n",
      "AE loss : 1.0583735704421997, ANN loss : 3.0952258110046387, Total loss : 108.93258666992188\n",
      "learning rate A :  tf.Tensor(9.884772e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1098 is 0.1133 sec\n",
      "train AE loss : 0.9926581978797913, train ANN loss : 3.152601718902588\n",
      "AE loss : 0.9576643109321594, ANN loss : 3.1000657081604004, Total loss : 98.86650848388672\n",
      "learning rate A :  tf.Tensor(9.884564e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1099 is 0.1166 sec\n",
      "train AE loss : 0.8970440030097961, train ANN loss : 3.148588180541992\n",
      "AE loss : 1.3115628957748413, ANN loss : 3.107124090194702, Total loss : 134.26341247558594\n",
      "learning rate A :  tf.Tensor(9.884564e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1100 is 0.0888 sec\n",
      "train AE loss : 1.234344244003296, train ANN loss : 3.1759424209594727\n",
      "AE loss : 1.1698696613311768, ANN loss : 3.0970726013183594, Total loss : 120.08404541015625\n",
      "learning rate A :  tf.Tensor(9.884356e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1101 is 0.0838 sec\n",
      "train AE loss : 1.0990126132965088, train ANN loss : 3.1618080139160156\n",
      "AE loss : 1.0514816045761108, ANN loss : 3.0947012901306152, Total loss : 108.24286651611328\n",
      "learning rate A :  tf.Tensor(9.884147e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1102 is 0.0870 sec\n",
      "train AE loss : 0.9862491488456726, train ANN loss : 3.1490485668182373\n",
      "AE loss : 1.3474348783493042, ANN loss : 3.1133179664611816, Total loss : 137.85679626464844\n",
      "learning rate A :  tf.Tensor(9.884147e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1103 is 0.0844 sec\n",
      "train AE loss : 1.2687067985534668, train ANN loss : 3.1975924968719482\n",
      "AE loss : 1.3117841482162476, ANN loss : 3.1033051013946533, Total loss : 134.28172302246094\n",
      "learning rate A :  tf.Tensor(9.884147e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1104 is 0.0871 sec\n",
      "train AE loss : 1.234601616859436, train ANN loss : 3.1798901557922363\n",
      "AE loss : 1.0623892545700073, ANN loss : 3.0926361083984375, Total loss : 109.33155059814453\n",
      "learning rate A :  tf.Tensor(9.884147e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1105 is 0.1079 sec\n",
      "train AE loss : 0.9965599775314331, train ANN loss : 3.154778003692627\n",
      "AE loss : 0.9603379368782043, ANN loss : 3.099146604537964, Total loss : 99.13294219970703\n",
      "learning rate A :  tf.Tensor(9.883939e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1106 is 0.0932 sec\n",
      "train AE loss : 0.8997681140899658, train ANN loss : 3.151489496231079\n",
      "AE loss : 0.8202041387557983, ANN loss : 3.125730037689209, Total loss : 85.14614868164062\n",
      "learning rate A :  tf.Tensor(9.883939e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1107 is 0.0815 sec\n",
      "train AE loss : 0.7670583724975586, train ANN loss : 3.165287971496582\n",
      "AE loss : 0.8229355216026306, ANN loss : 3.1286652088165283, Total loss : 85.42221069335938\n",
      "learning rate A :  tf.Tensor(9.883939e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1108 is 0.0799 sec\n",
      "train AE loss : 0.7696771621704102, train ANN loss : 3.1757779121398926\n",
      "AE loss : 0.958184540271759, ANN loss : 3.104672431945801, Total loss : 98.92312622070312\n",
      "learning rate A :  tf.Tensor(9.883939e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1109 is 0.0817 sec\n",
      "train AE loss : 0.8978376388549805, train ANN loss : 3.152010917663574\n",
      "AE loss : 1.1702712774276733, ANN loss : 3.0883498191833496, Total loss : 120.115478515625\n",
      "learning rate A :  tf.Tensor(9.883939e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1110 is 0.0801 sec\n",
      "train AE loss : 1.0996155738830566, train ANN loss : 3.144268035888672\n",
      "AE loss : 1.0510599613189697, ANN loss : 3.0929906368255615, Total loss : 108.19898986816406\n",
      "learning rate A :  tf.Tensor(9.883731e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1111 is 0.0779 sec\n",
      "train AE loss : 0.98612380027771, train ANN loss : 3.1460604667663574\n",
      "AE loss : 1.2788158655166626, ANN loss : 3.089233875274658, Total loss : 130.97080993652344\n",
      "learning rate A :  tf.Tensor(9.883731e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1112 is 0.0801 sec\n",
      "train AE loss : 1.2032989263534546, train ANN loss : 3.1545569896698\n",
      "AE loss : 1.1416162252426147, ANN loss : 3.0875728130340576, Total loss : 117.24919891357422\n",
      "learning rate A :  tf.Tensor(9.883522e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1113 is 0.0774 sec\n",
      "train AE loss : 1.0723916292190552, train ANN loss : 3.1567344665527344\n",
      "AE loss : 1.2967443466186523, ANN loss : 3.089829206466675, Total loss : 132.76426696777344\n",
      "learning rate A :  tf.Tensor(9.883522e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1114 is 0.0798 sec\n",
      "train AE loss : 1.2204278707504272, train ANN loss : 3.1641039848327637\n",
      "AE loss : 1.2575808763504028, ANN loss : 3.0876705646514893, Total loss : 128.8457489013672\n",
      "learning rate A :  tf.Tensor(9.883522e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1115 is 0.0805 sec\n",
      "train AE loss : 1.1828376054763794, train ANN loss : 3.1525723934173584\n",
      "AE loss : 1.1230566501617432, ANN loss : 3.086961269378662, Total loss : 115.39262390136719\n",
      "learning rate A :  tf.Tensor(9.883314e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1116 is 0.0783 sec\n",
      "train AE loss : 1.0544979572296143, train ANN loss : 3.146874189376831\n",
      "AE loss : 1.062782645225525, ANN loss : 3.091020107269287, Total loss : 109.3692855834961\n",
      "learning rate A :  tf.Tensor(9.883314e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1117 is 0.0813 sec\n",
      "train AE loss : 0.9969861507415771, train ANN loss : 3.1432571411132812\n",
      "AE loss : 1.0159707069396973, ANN loss : 3.0957934856414795, Total loss : 104.69287109375\n",
      "learning rate A :  tf.Tensor(9.883314e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1118 is 0.0808 sec\n",
      "train AE loss : 0.9524869322776794, train ANN loss : 3.1430492401123047\n",
      "AE loss : 0.9198040962219238, ANN loss : 3.105947494506836, Total loss : 95.08635711669922\n",
      "learning rate A :  tf.Tensor(9.8831064e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1119 is 0.0772 sec\n",
      "train AE loss : 0.8613626956939697, train ANN loss : 3.151381254196167\n",
      "AE loss : 0.8380747437477112, ANN loss : 3.1184945106506348, Total loss : 86.92596435546875\n",
      "learning rate A :  tf.Tensor(9.8828976e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1120 is 0.0778 sec\n",
      "train AE loss : 0.7840534448623657, train ANN loss : 3.162916898727417\n",
      "AE loss : 0.7681105732917786, ANN loss : 3.132509708404541, Total loss : 79.94356536865234\n",
      "learning rate A :  tf.Tensor(9.8826895e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1121 is 0.0777 sec\n",
      "train AE loss : 0.7180548310279846, train ANN loss : 3.171625852584839\n",
      "AE loss : 0.8777017593383789, ANN loss : 3.1091339588165283, Total loss : 90.8792953491211\n",
      "learning rate A :  tf.Tensor(9.8826895e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1122 is 0.0788 sec\n",
      "train AE loss : 0.8216071128845215, train ANN loss : 3.148651599884033\n",
      "AE loss : 0.802128791809082, ANN loss : 3.122037887573242, Total loss : 83.33491516113281\n",
      "learning rate A :  tf.Tensor(9.882481e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1123 is 0.0769 sec\n",
      "train AE loss : 0.7502411603927612, train ANN loss : 3.1712424755096436\n",
      "AE loss : 1.0342198610305786, ANN loss : 3.0899064540863037, Total loss : 106.51188659667969\n",
      "learning rate A :  tf.Tensor(9.882481e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1124 is 0.0802 sec\n",
      "train AE loss : 0.9700625538825989, train ANN loss : 3.146609306335449\n",
      "AE loss : 1.3050678968429565, ANN loss : 3.0928945541381836, Total loss : 133.59970092773438\n",
      "learning rate A :  tf.Tensor(9.882481e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1125 is 0.0794 sec\n",
      "train AE loss : 1.2281769514083862, train ANN loss : 3.1641597747802734\n",
      "AE loss : 1.1617556810379028, ANN loss : 3.0863826274871826, Total loss : 119.26194763183594\n",
      "learning rate A :  tf.Tensor(9.8822726e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1126 is 0.0788 sec\n",
      "train AE loss : 1.0914448499679565, train ANN loss : 3.1501457691192627\n",
      "AE loss : 1.0424392223358154, ANN loss : 3.0873706340789795, Total loss : 107.331298828125\n",
      "learning rate A :  tf.Tensor(9.8820645e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1127 is 0.0784 sec\n",
      "train AE loss : 0.977940559387207, train ANN loss : 3.144745111465454\n",
      "AE loss : 0.9421070218086243, ANN loss : 3.0932822227478027, Total loss : 97.30398559570312\n",
      "learning rate A :  tf.Tensor(9.8818564e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1128 is 0.0768 sec\n",
      "train AE loss : 0.882824718952179, train ANN loss : 3.148190975189209\n",
      "AE loss : 1.2123510837554932, ANN loss : 3.0880141258239746, Total loss : 124.3231201171875\n",
      "learning rate A :  tf.Tensor(9.8818564e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1129 is 0.0794 sec\n",
      "train AE loss : 1.1396098136901855, train ANN loss : 3.153364419937134\n",
      "AE loss : 1.329624891281128, ANN loss : 3.093721389770508, Total loss : 136.05621337890625\n",
      "learning rate A :  tf.Tensor(9.8818564e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1130 is 0.0799 sec\n",
      "train AE loss : 1.2514173984527588, train ANN loss : 3.1704821586608887\n",
      "AE loss : 1.1809555292129517, ANN loss : 3.0848777294158936, Total loss : 121.18042755126953\n",
      "learning rate A :  tf.Tensor(9.881648e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1131 is 0.0767 sec\n",
      "train AE loss : 1.1095587015151978, train ANN loss : 3.1465156078338623\n",
      "AE loss : 1.0575368404388428, ANN loss : 3.0842196941375732, Total loss : 108.83790588378906\n",
      "learning rate A :  tf.Tensor(9.88144e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1132 is 0.0771 sec\n",
      "train AE loss : 0.9921403527259827, train ANN loss : 3.1438348293304443\n",
      "AE loss : 1.1060621738433838, ANN loss : 3.083225727081299, Total loss : 113.68944549560547\n",
      "learning rate A :  tf.Tensor(9.88144e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1133 is 0.0795 sec\n",
      "train AE loss : 1.0380487442016602, train ANN loss : 3.144162178039551\n",
      "AE loss : 0.9942240118980408, ANN loss : 3.0873489379882812, Total loss : 102.50975036621094\n",
      "learning rate A :  tf.Tensor(9.8812314e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1134 is 0.0781 sec\n",
      "train AE loss : 0.9318774342536926, train ANN loss : 3.1373469829559326\n",
      "AE loss : 1.0504696369171143, ANN loss : 3.0860631465911865, Total loss : 108.1330337524414\n",
      "learning rate A :  tf.Tensor(9.8812314e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1135 is 0.0793 sec\n",
      "train AE loss : 0.9849908947944641, train ANN loss : 3.1354196071624756\n",
      "AE loss : 0.9469580054283142, ANN loss : 3.0933778285980225, Total loss : 97.78917694091797\n",
      "learning rate A :  tf.Tensor(9.881023e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1136 is 0.0839 sec\n",
      "train AE loss : 0.8868834972381592, train ANN loss : 3.1456491947174072\n",
      "AE loss : 0.8595030307769775, ANN loss : 3.103973150253296, Total loss : 89.05427551269531\n",
      "learning rate A :  tf.Tensor(9.880815e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1137 is 0.0787 sec\n",
      "train AE loss : 0.804180920124054, train ANN loss : 3.155414342880249\n",
      "AE loss : 0.7849552631378174, ANN loss : 3.1165692806243896, Total loss : 81.61209869384766\n",
      "learning rate A :  tf.Tensor(9.880608e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1138 is 0.0774 sec\n",
      "train AE loss : 0.7339185476303101, train ANN loss : 3.165067434310913\n",
      "AE loss : 0.9449485540390015, ANN loss : 3.0946831703186035, Total loss : 97.58954620361328\n",
      "learning rate A :  tf.Tensor(9.880608e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1139 is 0.0804 sec\n",
      "train AE loss : 0.8848417401313782, train ANN loss : 3.140972137451172\n",
      "AE loss : 1.1774088144302368, ANN loss : 3.0853705406188965, Total loss : 120.82625579833984\n",
      "learning rate A :  tf.Tensor(9.880608e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1140 is 0.0793 sec\n",
      "train AE loss : 1.105604887008667, train ANN loss : 3.1481988430023193\n",
      "AE loss : 1.0526344776153564, ANN loss : 3.086700916290283, Total loss : 108.35015869140625\n",
      "learning rate A :  tf.Tensor(9.880399e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1141 is 0.0772 sec\n",
      "train AE loss : 0.9869358539581299, train ANN loss : 3.143148899078369\n",
      "AE loss : 1.2519956827163696, ANN loss : 3.0865488052368164, Total loss : 128.28611755371094\n",
      "learning rate A :  tf.Tensor(9.880399e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1142 is 0.0796 sec\n",
      "train AE loss : 1.176796793937683, train ANN loss : 3.1486294269561768\n",
      "AE loss : 1.2830212116241455, ANN loss : 3.0856990814208984, Total loss : 131.3878173828125\n",
      "learning rate A :  tf.Tensor(9.880399e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1143 is 0.0797 sec\n",
      "train AE loss : 1.2063109874725342, train ANN loss : 3.148879051208496\n",
      "AE loss : 1.1403779983520508, ANN loss : 3.082277297973633, Total loss : 117.12008666992188\n",
      "learning rate A :  tf.Tensor(9.880191e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1144 is 0.0771 sec\n",
      "train AE loss : 1.0704158544540405, train ANN loss : 3.1459927558898926\n",
      "AE loss : 1.0217806100845337, ANN loss : 3.08601713180542, Total loss : 105.26407623291016\n",
      "learning rate A :  tf.Tensor(9.879983e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1145 is 0.0787 sec\n",
      "train AE loss : 0.957813024520874, train ANN loss : 3.141490936279297\n",
      "AE loss : 0.9223057627677917, ANN loss : 3.0942764282226562, Total loss : 95.3248519897461\n",
      "learning rate A :  tf.Tensor(9.879775e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1146 is 0.0772 sec\n",
      "train AE loss : 0.8636094331741333, train ANN loss : 3.145207405090332\n",
      "AE loss : 0.8381667137145996, ANN loss : 3.105302333831787, Total loss : 86.9219741821289\n",
      "learning rate A :  tf.Tensor(9.879566e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1147 is 0.0776 sec\n",
      "train AE loss : 0.7840976119041443, train ANN loss : 3.150052547454834\n",
      "AE loss : 0.7664333581924438, ANN loss : 3.1181907653808594, Total loss : 79.76152801513672\n",
      "learning rate A :  tf.Tensor(9.879358e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1148 is 0.0781 sec\n",
      "train AE loss : 0.7165822386741638, train ANN loss : 3.160763740539551\n",
      "AE loss : 0.9067156910896301, ANN loss : 3.0942859649658203, Total loss : 93.765869140625\n",
      "learning rate A :  tf.Tensor(9.879358e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1149 is 0.0795 sec\n",
      "train AE loss : 0.8489624857902527, train ANN loss : 3.144864797592163\n",
      "AE loss : 1.12559175491333, ANN loss : 3.080648899078369, Total loss : 115.63983154296875\n",
      "learning rate A :  tf.Tensor(9.879358e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1150 is 0.0894 sec\n",
      "train AE loss : 1.0564045906066895, train ANN loss : 3.1395530700683594\n",
      "AE loss : 1.0092343091964722, ANN loss : 3.0840396881103516, Total loss : 104.0074691772461\n",
      "learning rate A :  tf.Tensor(9.8791505e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1151 is 0.0782 sec\n",
      "train AE loss : 0.9460145235061646, train ANN loss : 3.1338977813720703\n",
      "AE loss : 0.9116208553314209, ANN loss : 3.0918655395507812, Total loss : 94.25395202636719\n",
      "learning rate A :  tf.Tensor(9.8789424e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1152 is 0.0777 sec\n",
      "train AE loss : 0.8536196947097778, train ANN loss : 3.1412107944488525\n",
      "AE loss : 0.8289456367492676, ANN loss : 3.1024861335754395, Total loss : 85.9970474243164\n",
      "learning rate A :  tf.Tensor(9.8787335e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1153 is 0.0800 sec\n",
      "train AE loss : 0.7756151556968689, train ANN loss : 3.150395154953003\n",
      "AE loss : 1.1237534284591675, ANN loss : 3.081005573272705, Total loss : 115.45635986328125\n",
      "learning rate A :  tf.Tensor(9.8787335e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1154 is 0.0791 sec\n",
      "train AE loss : 1.054612398147583, train ANN loss : 3.149501085281372\n",
      "AE loss : 1.3487578630447388, ANN loss : 3.0930135250091553, Total loss : 137.9687957763672\n",
      "learning rate A :  tf.Tensor(9.8787335e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1155 is 0.0798 sec\n",
      "train AE loss : 1.2687814235687256, train ANN loss : 3.1617863178253174\n",
      "AE loss : 1.3078343868255615, ANN loss : 3.087209463119507, Total loss : 133.87063598632812\n",
      "learning rate A :  tf.Tensor(9.8787335e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1156 is 0.0804 sec\n",
      "train AE loss : 1.2294491529464722, train ANN loss : 3.1626689434051514\n",
      "AE loss : 1.0782568454742432, ANN loss : 3.0863687992095947, Total loss : 110.91206359863281\n",
      "learning rate A :  tf.Tensor(9.8787335e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1157 is 0.0796 sec\n",
      "train AE loss : 1.0106098651885986, train ANN loss : 3.1410186290740967\n",
      "AE loss : 0.9679526090621948, ANN loss : 3.094907522201538, Total loss : 99.8901596069336\n",
      "learning rate A :  tf.Tensor(9.8785255e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1158 is 0.0771 sec\n",
      "train AE loss : 0.906074047088623, train ANN loss : 3.142009973526001\n",
      "AE loss : 0.8566963076591492, ANN loss : 3.1154370307922363, Total loss : 88.78506469726562\n",
      "learning rate A :  tf.Tensor(9.8785255e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1159 is 0.0916 sec\n",
      "train AE loss : 0.800731897354126, train ANN loss : 3.1602509021759033\n",
      "AE loss : 0.8834437131881714, ANN loss : 3.1095550060272217, Total loss : 91.45391845703125\n",
      "learning rate A :  tf.Tensor(9.8785255e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1160 is 0.0793 sec\n",
      "train AE loss : 0.8259220123291016, train ANN loss : 3.146059274673462\n",
      "AE loss : 1.0250778198242188, ANN loss : 3.086881637573242, Total loss : 105.59466552734375\n",
      "learning rate A :  tf.Tensor(9.8785255e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1161 is 0.0820 sec\n",
      "train AE loss : 0.9599459767341614, train ANN loss : 3.131431818008423\n",
      "AE loss : 0.924070417881012, ANN loss : 3.0984950065612793, Total loss : 95.50553894042969\n",
      "learning rate A :  tf.Tensor(9.8783166e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1162 is 0.0777 sec\n",
      "train AE loss : 0.8643604516983032, train ANN loss : 3.1547060012817383\n",
      "AE loss : 1.1669996976852417, ANN loss : 3.076097011566162, Total loss : 119.77605438232422\n",
      "learning rate A :  tf.Tensor(9.8783166e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1163 is 0.0803 sec\n",
      "train AE loss : 1.0945448875427246, train ANN loss : 3.1311190128326416\n",
      "AE loss : 1.0428390502929688, ANN loss : 3.081566572189331, Total loss : 107.365478515625\n",
      "learning rate A :  tf.Tensor(9.8781085e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1164 is 0.0790 sec\n",
      "train AE loss : 0.9767205119132996, train ANN loss : 3.140467405319214\n",
      "AE loss : 0.9390581846237183, ANN loss : 3.091409683227539, Total loss : 96.99722290039062\n",
      "learning rate A :  tf.Tensor(9.8779004e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1165 is 0.0785 sec\n",
      "train AE loss : 0.8785468339920044, train ANN loss : 3.147359848022461\n",
      "AE loss : 0.8515549302101135, ANN loss : 3.103780508041382, Total loss : 88.25927734375\n",
      "learning rate A :  tf.Tensor(9.877692e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1166 is 0.0780 sec\n",
      "train AE loss : 0.7960408329963684, train ANN loss : 3.1490917205810547\n",
      "AE loss : 1.1739718914031982, ANN loss : 3.075709104537964, Total loss : 120.47290802001953\n",
      "learning rate A :  tf.Tensor(9.877692e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1167 is 0.0798 sec\n",
      "train AE loss : 1.1010355949401855, train ANN loss : 3.145235300064087\n",
      "AE loss : 1.4276142120361328, ANN loss : 3.0908572673797607, Total loss : 145.85227966308594\n",
      "learning rate A :  tf.Tensor(9.877692e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1168 is 0.0797 sec\n",
      "train AE loss : 1.342232346534729, train ANN loss : 3.165290355682373\n",
      "AE loss : 1.3703062534332275, ANN loss : 3.0832760334014893, Total loss : 140.11390686035156\n",
      "learning rate A :  tf.Tensor(9.877692e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1169 is 0.0793 sec\n",
      "train AE loss : 1.2870882749557495, train ANN loss : 3.1574504375457764\n",
      "AE loss : 1.1159727573394775, ANN loss : 3.0804779529571533, Total loss : 114.67774963378906\n",
      "learning rate A :  tf.Tensor(9.877692e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1170 is 0.0809 sec\n",
      "train AE loss : 1.044675350189209, train ANN loss : 3.1296441555023193\n",
      "AE loss : 0.9984760284423828, ANN loss : 3.088449001312256, Total loss : 102.93605041503906\n",
      "learning rate A :  tf.Tensor(9.877484e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1171 is 0.0793 sec\n",
      "train AE loss : 0.933365523815155, train ANN loss : 3.1336076259613037\n",
      "AE loss : 0.9000096917152405, ANN loss : 3.1001980304718018, Total loss : 93.10116577148438\n",
      "learning rate A :  tf.Tensor(9.877276e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1172 is 0.0775 sec\n",
      "train AE loss : 0.8403942584991455, train ANN loss : 3.153459310531616\n",
      "AE loss : 0.8168649673461914, ANN loss : 3.114236831665039, Total loss : 84.80072784423828\n",
      "learning rate A :  tf.Tensor(9.877067e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1173 is 0.0782 sec\n",
      "train AE loss : 0.7622118592262268, train ANN loss : 3.150597333908081\n",
      "AE loss : 0.7461274862289429, ANN loss : 3.1295366287231445, Total loss : 77.74227905273438\n",
      "learning rate A :  tf.Tensor(9.876859e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1174 is 0.0777 sec\n",
      "train AE loss : 0.6958897709846497, train ANN loss : 3.170060157775879\n",
      "AE loss : 0.7379488348960876, ANN loss : 3.1351847648620605, Total loss : 76.93006896972656\n",
      "learning rate A :  tf.Tensor(9.876859e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1175 is 0.0793 sec\n",
      "train AE loss : 0.6880868077278137, train ANN loss : 3.169750452041626\n",
      "AE loss : 0.8840984106063843, ANN loss : 3.1006951332092285, Total loss : 91.51054382324219\n",
      "learning rate A :  tf.Tensor(9.876859e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1176 is 0.0801 sec\n",
      "train AE loss : 0.8255593180656433, train ANN loss : 3.1367075443267822\n",
      "AE loss : 0.8037850260734558, ANN loss : 3.1146812438964844, Total loss : 83.49317932128906\n",
      "learning rate A :  tf.Tensor(9.876651e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1177 is 0.0779 sec\n",
      "train AE loss : 0.7500849962234497, train ANN loss : 3.156301736831665\n",
      "AE loss : 0.7353079319000244, ANN loss : 3.1297037601470947, Total loss : 76.6604995727539\n",
      "learning rate A :  tf.Tensor(9.876444e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1178 is 0.0778 sec\n",
      "train AE loss : 0.6859613656997681, train ANN loss : 3.1652729511260986\n",
      "AE loss : 1.0358233451843262, ANN loss : 3.079063653945923, Total loss : 106.6613998413086\n",
      "learning rate A :  tf.Tensor(9.876444e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1179 is 0.0810 sec\n",
      "train AE loss : 0.9693330526351929, train ANN loss : 3.147700786590576\n",
      "AE loss : 0.9323108792304993, ANN loss : 3.0874671936035156, Total loss : 96.31855010986328\n",
      "learning rate A :  tf.Tensor(9.876236e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1180 is 0.0776 sec\n",
      "train AE loss : 0.8716278076171875, train ANN loss : 3.1363773345947266\n",
      "AE loss : 1.340569019317627, ANN loss : 3.0852222442626953, Total loss : 137.14212036132812\n",
      "learning rate A :  tf.Tensor(9.876236e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1181 is 0.0831 sec\n",
      "train AE loss : 1.258915901184082, train ANN loss : 3.167578935623169\n",
      "AE loss : 1.5212123394012451, ANN loss : 3.1045966148376465, Total loss : 155.225830078125\n",
      "learning rate A :  tf.Tensor(9.876236e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1182 is 0.0791 sec\n",
      "train AE loss : 1.4309868812561035, train ANN loss : 3.1836600303649902\n",
      "AE loss : 1.3317992687225342, ANN loss : 3.0838475227355957, Total loss : 136.26377868652344\n",
      "learning rate A :  tf.Tensor(9.8760276e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1183 is 0.0761 sec\n",
      "train AE loss : 1.2505123615264893, train ANN loss : 3.1610379219055176\n",
      "AE loss : 1.1777498722076416, ANN loss : 3.075770854949951, Total loss : 120.85075378417969\n",
      "learning rate A :  tf.Tensor(9.8758195e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1184 is 0.0759 sec\n",
      "train AE loss : 1.104156494140625, train ANN loss : 3.143925189971924\n",
      "AE loss : 1.0506904125213623, ANN loss : 3.076000928878784, Total loss : 108.14503479003906\n",
      "learning rate A :  tf.Tensor(9.8756114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1185 is 0.0771 sec\n",
      "train AE loss : 0.983873724937439, train ANN loss : 3.1331710815429688\n",
      "AE loss : 1.1580802202224731, ANN loss : 3.0735716819763184, Total loss : 118.88158416748047\n",
      "learning rate A :  tf.Tensor(9.8756114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1186 is 0.0779 sec\n",
      "train AE loss : 1.0851619243621826, train ANN loss : 3.1325953006744385\n",
      "AE loss : 1.0336414575576782, ANN loss : 3.075333833694458, Total loss : 106.4394760131836\n",
      "learning rate A :  tf.Tensor(9.8754026e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1187 is 0.0757 sec\n",
      "train AE loss : 0.967401385307312, train ANN loss : 3.1352040767669678\n",
      "AE loss : 1.1250200271606445, ANN loss : 3.073230266571045, Total loss : 115.57523345947266\n",
      "learning rate A :  tf.Tensor(9.8754026e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1188 is 0.0795 sec\n",
      "train AE loss : 1.0534312725067139, train ANN loss : 3.1319420337677\n",
      "AE loss : 1.1472011804580688, ANN loss : 3.074014902114868, Total loss : 117.79413604736328\n",
      "learning rate A :  tf.Tensor(9.8754026e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1189 is 0.0790 sec\n",
      "train AE loss : 1.0740022659301758, train ANN loss : 3.132230281829834\n",
      "AE loss : 1.1200276613235474, ANN loss : 3.0754878520965576, Total loss : 115.07825469970703\n",
      "learning rate A :  tf.Tensor(9.8754026e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1190 is 0.0787 sec\n",
      "train AE loss : 1.0479004383087158, train ANN loss : 3.133470296859741\n",
      "AE loss : 1.0001945495605469, ANN loss : 3.0831170082092285, Total loss : 103.1025619506836\n",
      "learning rate A :  tf.Tensor(9.8751945e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1191 is 0.0778 sec\n",
      "train AE loss : 0.9346694946289062, train ANN loss : 3.135772705078125\n",
      "AE loss : 0.9001084566116333, ANN loss : 3.094667673110962, Total loss : 93.10551452636719\n",
      "learning rate A :  tf.Tensor(9.8749864e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1192 is 0.0765 sec\n",
      "train AE loss : 0.8402507901191711, train ANN loss : 3.136782646179199\n",
      "AE loss : 0.815778374671936, ANN loss : 3.1085009574890137, Total loss : 84.68634033203125\n",
      "learning rate A :  tf.Tensor(9.874779e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1193 is 0.0769 sec\n",
      "train AE loss : 0.7609719634056091, train ANN loss : 3.1578621864318848\n",
      "AE loss : 0.923084557056427, ANN loss : 3.0908007621765137, Total loss : 95.39925384521484\n",
      "learning rate A :  tf.Tensor(9.874779e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1194 is 0.0828 sec\n",
      "train AE loss : 0.8618580102920532, train ANN loss : 3.1417689323425293\n",
      "AE loss : 0.8351357579231262, ANN loss : 3.104321002960205, Total loss : 86.6178970336914\n",
      "learning rate A :  tf.Tensor(9.874571e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1195 is 0.0791 sec\n",
      "train AE loss : 0.7790992259979248, train ANN loss : 3.1446444988250732\n",
      "AE loss : 0.7605770826339722, ANN loss : 3.1193180084228516, Total loss : 79.17703247070312\n",
      "learning rate A :  tf.Tensor(9.874363e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1196 is 0.0769 sec\n",
      "train AE loss : 0.7091772556304932, train ANN loss : 3.155804395675659\n",
      "AE loss : 0.6968168616294861, ANN loss : 3.1350231170654297, Total loss : 72.81671142578125\n",
      "learning rate A :  tf.Tensor(9.874155e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1197 is 0.0769 sec\n",
      "train AE loss : 0.6496047377586365, train ANN loss : 3.1746485233306885\n",
      "AE loss : 0.9542244672775269, ANN loss : 3.081801176071167, Total loss : 98.5042495727539\n",
      "learning rate A :  tf.Tensor(9.874155e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1198 is 0.0785 sec\n",
      "train AE loss : 0.8915285468101501, train ANN loss : 3.133197069168091\n",
      "AE loss : 0.8614833354949951, ANN loss : 3.0935428142547607, Total loss : 89.24187469482422\n",
      "learning rate A :  tf.Tensor(9.873947e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1199 is 0.0765 sec\n",
      "train AE loss : 0.8041608333587646, train ANN loss : 3.1416916847229004\n",
      "AE loss : 1.2388958930969238, ANN loss : 3.0718326568603516, Total loss : 126.96141815185547\n",
      "learning rate A :  tf.Tensor(9.873947e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1200 is 0.1172 sec\n",
      "train AE loss : 1.1614012718200684, train ANN loss : 3.1389729976654053\n",
      "AE loss : 1.0984996557235718, ANN loss : 3.0708601474761963, Total loss : 112.92082214355469\n",
      "learning rate A :  tf.Tensor(9.8737386e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1201 is 0.0778 sec\n",
      "train AE loss : 1.028188943862915, train ANN loss : 3.134810209274292\n",
      "AE loss : 0.9822226166725159, ANN loss : 3.0760369300842285, Total loss : 101.29828643798828\n",
      "learning rate A :  tf.Tensor(9.8735305e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1202 is 0.0769 sec\n",
      "train AE loss : 0.9182943105697632, train ANN loss : 3.1270081996917725\n",
      "AE loss : 1.3862624168395996, ANN loss : 3.086211919784546, Total loss : 141.7124481201172\n",
      "learning rate A :  tf.Tensor(9.8735305e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1203 is 0.0805 sec\n",
      "train AE loss : 1.3018323183059692, train ANN loss : 3.160052537918091\n",
      "AE loss : 1.5437308549880981, ANN loss : 3.103986978530884, Total loss : 157.4770965576172\n",
      "learning rate A :  tf.Tensor(9.8735305e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1204 is 0.0789 sec\n",
      "train AE loss : 1.4518705606460571, train ANN loss : 3.1789276599884033\n",
      "AE loss : 1.3460975885391235, ANN loss : 3.081559181213379, Total loss : 137.69131469726562\n",
      "learning rate A :  tf.Tensor(9.8733224e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1205 is 0.0770 sec\n",
      "train AE loss : 1.2634341716766357, train ANN loss : 3.150130033493042\n",
      "AE loss : 1.2384567260742188, ANN loss : 3.0730998516082764, Total loss : 126.91877746582031\n",
      "learning rate A :  tf.Tensor(9.8733224e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1206 is 0.0794 sec\n",
      "train AE loss : 1.1606813669204712, train ANN loss : 3.137031316757202\n",
      "AE loss : 1.0389111042022705, ANN loss : 3.081181287765503, Total loss : 106.97229766845703\n",
      "learning rate A :  tf.Tensor(9.8733224e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1207 is 0.0789 sec\n",
      "train AE loss : 0.9710192084312439, train ANN loss : 3.1366236209869385\n",
      "AE loss : 0.9315124154090881, ANN loss : 3.0919806957244873, Total loss : 96.24322509765625\n",
      "learning rate A :  tf.Tensor(9.873114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1208 is 0.0766 sec\n",
      "train AE loss : 0.8696751594543457, train ANN loss : 3.1354732513427734\n",
      "AE loss : 0.841396152973175, ANN loss : 3.1054069995880127, Total loss : 87.24502563476562\n",
      "learning rate A :  tf.Tensor(9.872906e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1209 is 0.0779 sec\n",
      "train AE loss : 0.7848869562149048, train ANN loss : 3.1438708305358887\n",
      "AE loss : 0.8107450604438782, ANN loss : 3.116600751876831, Total loss : 84.19110870361328\n",
      "learning rate A :  tf.Tensor(9.872906e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1210 is 0.0785 sec\n",
      "train AE loss : 0.7558169960975647, train ANN loss : 3.1525042057037354\n",
      "AE loss : 0.7389529347419739, ANN loss : 3.132469892501831, Total loss : 77.02776336669922\n",
      "learning rate A :  tf.Tensor(9.872699e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1211 is 0.0776 sec\n",
      "train AE loss : 0.6885237097740173, train ANN loss : 3.1671249866485596\n",
      "AE loss : 0.8690847158432007, ANN loss : 3.102614641189575, Total loss : 90.0110855102539\n",
      "learning rate A :  tf.Tensor(9.872699e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1212 is 0.0826 sec\n",
      "train AE loss : 0.8105778098106384, train ANN loss : 3.1530895233154297\n",
      "AE loss : 0.7884160280227661, ANN loss : 3.117659330368042, Total loss : 81.95926666259766\n",
      "learning rate A :  tf.Tensor(9.872491e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1213 is 0.0782 sec\n",
      "train AE loss : 0.7348376512527466, train ANN loss : 3.1545333862304688\n",
      "AE loss : 1.066321611404419, ANN loss : 3.0740182399749756, Total loss : 109.70618438720703\n",
      "learning rate A :  tf.Tensor(9.872491e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1214 is 0.0792 sec\n",
      "train AE loss : 0.9967129826545715, train ANN loss : 3.1299333572387695\n",
      "AE loss : 0.9540179967880249, ANN loss : 3.0832958221435547, Total loss : 98.48509979248047\n",
      "learning rate A :  tf.Tensor(9.872283e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1215 is 0.0793 sec\n",
      "train AE loss : 0.8906996846199036, train ANN loss : 3.134387493133545\n",
      "AE loss : 0.8600887060165405, ANN loss : 3.0958175659179688, Total loss : 89.10469055175781\n",
      "learning rate A :  tf.Tensor(9.8720746e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1216 is 0.0788 sec\n",
      "train AE loss : 0.8023041486740112, train ANN loss : 3.140374183654785\n",
      "AE loss : 1.2523863315582275, ANN loss : 3.070866584777832, Total loss : 128.3094940185547\n",
      "learning rate A :  tf.Tensor(9.8720746e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1217 is 0.0795 sec\n",
      "train AE loss : 1.1733373403549194, train ANN loss : 3.1452813148498535\n",
      "AE loss : 1.5326154232025146, ANN loss : 3.0964484214782715, Total loss : 156.3579864501953\n",
      "learning rate A :  tf.Tensor(9.8720746e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1218 is 0.0796 sec\n",
      "train AE loss : 1.4400614500045776, train ANN loss : 3.168973207473755\n",
      "AE loss : 1.4441066980361938, ANN loss : 3.0794501304626465, Total loss : 147.4901123046875\n",
      "learning rate A :  tf.Tensor(9.8720746e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1219 is 0.0787 sec\n",
      "train AE loss : 1.3552645444869995, train ANN loss : 3.154731512069702\n",
      "AE loss : 1.2634860277175903, ANN loss : 3.0684473514556885, Total loss : 129.41705322265625\n",
      "learning rate A :  tf.Tensor(9.8718665e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1220 is 0.0778 sec\n",
      "train AE loss : 1.1835451126098633, train ANN loss : 3.135200023651123\n",
      "AE loss : 1.0772374868392944, ANN loss : 3.071274518966675, Total loss : 110.7950210571289\n",
      "learning rate A :  tf.Tensor(9.8718665e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1221 is 0.0801 sec\n",
      "train AE loss : 1.0066204071044922, train ANN loss : 3.1248083114624023\n",
      "AE loss : 0.9624699950218201, ANN loss : 3.0811119079589844, Total loss : 99.32810974121094\n",
      "learning rate A :  tf.Tensor(9.8716584e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1222 is 0.1810 sec\n",
      "train AE loss : 0.8982648253440857, train ANN loss : 3.1296157836914062\n",
      "AE loss : 0.8666396737098694, ANN loss : 3.0941312313079834, Total loss : 89.75810241699219\n",
      "learning rate A :  tf.Tensor(9.87145e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1223 is 0.0776 sec\n",
      "train AE loss : 0.8081050515174866, train ANN loss : 3.138608694076538\n",
      "AE loss : 0.7859235405921936, ANN loss : 3.1087894439697266, Total loss : 81.7011489868164\n",
      "learning rate A :  tf.Tensor(9.871242e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1224 is 0.0785 sec\n",
      "train AE loss : 0.7323949337005615, train ANN loss : 3.150284767150879\n",
      "AE loss : 0.7863932847976685, ANN loss : 3.1135048866271973, Total loss : 81.7528305053711\n",
      "learning rate A :  tf.Tensor(9.871242e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1225 is 0.0812 sec\n",
      "train AE loss : 0.7326369881629944, train ANN loss : 3.1562039852142334\n",
      "AE loss : 0.7174677848815918, ANN loss : 3.129631280899048, Total loss : 74.87641143798828\n",
      "learning rate A :  tf.Tensor(9.871034e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1226 is 0.0778 sec\n",
      "train AE loss : 0.6682103276252747, train ANN loss : 3.1712875366210938\n",
      "AE loss : 0.8715673089027405, ANN loss : 3.0955886840820312, Total loss : 90.25232696533203\n",
      "learning rate A :  tf.Tensor(9.871034e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1227 is 0.0804 sec\n",
      "train AE loss : 0.8125647306442261, train ANN loss : 3.139458656311035\n",
      "AE loss : 1.15830397605896, ANN loss : 3.067615032196045, Total loss : 118.89801025390625\n",
      "learning rate A :  tf.Tensor(9.871034e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1228 is 0.0810 sec\n",
      "train AE loss : 1.0831611156463623, train ANN loss : 3.125649929046631\n",
      "AE loss : 1.028721570968628, ANN loss : 3.073424816131592, Total loss : 105.9455795288086\n",
      "learning rate A :  tf.Tensor(9.870826e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1229 is 0.0778 sec\n",
      "train AE loss : 0.9606793522834778, train ANN loss : 3.1302273273468018\n",
      "AE loss : 0.9213102459907532, ANN loss : 3.083848237991333, Total loss : 95.21487426757812\n",
      "learning rate A :  tf.Tensor(9.870619e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1230 is 0.0861 sec\n",
      "train AE loss : 0.859452486038208, train ANN loss : 3.134925603866577\n",
      "AE loss : 1.2903976440429688, ANN loss : 3.069826602935791, Total loss : 132.10958862304688\n",
      "learning rate A :  tf.Tensor(9.870619e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1231 is 0.0808 sec\n",
      "train AE loss : 1.208672046661377, train ANN loss : 3.1369457244873047\n",
      "AE loss : 1.136979103088379, ANN loss : 3.0672459602355957, Total loss : 116.7651596069336\n",
      "learning rate A :  tf.Tensor(9.8704106e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1232 is 0.0793 sec\n",
      "train AE loss : 1.0631664991378784, train ANN loss : 3.1369500160217285\n",
      "AE loss : 1.0109601020812988, ANN loss : 3.071707248687744, Total loss : 104.16771697998047\n",
      "learning rate A :  tf.Tensor(9.8702025e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1233 is 0.0988 sec\n",
      "train AE loss : 0.9441654682159424, train ANN loss : 3.1255335807800293\n",
      "AE loss : 1.3904504776000977, ANN loss : 3.0803062915802, Total loss : 142.12535095214844\n",
      "learning rate A :  tf.Tensor(9.8702025e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1234 is 0.0794 sec\n",
      "train AE loss : 1.3038853406906128, train ANN loss : 3.1587252616882324\n",
      "AE loss : 1.5308729410171509, ANN loss : 3.0933969020843506, Total loss : 156.18069458007812\n",
      "learning rate A :  tf.Tensor(9.8702025e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1235 is 0.0859 sec\n",
      "train AE loss : 1.4376013278961182, train ANN loss : 3.181553840637207\n",
      "AE loss : 1.3309139013290405, ANN loss : 3.073155403137207, Total loss : 136.16455078125\n",
      "learning rate A :  tf.Tensor(9.8699944e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1236 is 0.1039 sec\n",
      "train AE loss : 1.247218370437622, train ANN loss : 3.143568754196167\n",
      "AE loss : 1.169382929801941, ANN loss : 3.066288709640503, Total loss : 120.00457763671875\n",
      "learning rate A :  tf.Tensor(9.869786e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1237 is 0.0850 sec\n",
      "train AE loss : 1.0939905643463135, train ANN loss : 3.1231610774993896\n",
      "AE loss : 1.1926366090774536, ANN loss : 3.0650815963745117, Total loss : 122.32875061035156\n",
      "learning rate A :  tf.Tensor(9.869786e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1238 is 0.0920 sec\n",
      "train AE loss : 1.1157671213150024, train ANN loss : 3.134261131286621\n",
      "AE loss : 1.055983304977417, ANN loss : 3.067171096801758, Total loss : 108.66549682617188\n",
      "learning rate A :  tf.Tensor(9.869578e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1239 is 0.0840 sec\n",
      "train AE loss : 0.9865713119506836, train ANN loss : 3.122793436050415\n",
      "AE loss : 0.9432377219200134, ANN loss : 3.0748305320739746, Total loss : 97.39859771728516\n",
      "learning rate A :  tf.Tensor(9.869371e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1240 is 0.0926 sec\n",
      "train AE loss : 0.8802396655082703, train ANN loss : 3.1222715377807617\n",
      "AE loss : 0.8490524888038635, ANN loss : 3.0860307216644287, Total loss : 87.99128723144531\n",
      "learning rate A :  tf.Tensor(9.869163e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1241 is 0.0919 sec\n",
      "train AE loss : 0.7917611002922058, train ANN loss : 3.13369083404541\n",
      "AE loss : 0.7698245644569397, ANN loss : 3.099198579788208, Total loss : 80.08165740966797\n",
      "learning rate A :  tf.Tensor(9.868955e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1242 is 0.0929 sec\n",
      "train AE loss : 0.7174863219261169, train ANN loss : 3.145108938217163\n",
      "AE loss : 0.7024855017662048, ANN loss : 3.113358974456787, Total loss : 73.36190795898438\n",
      "learning rate A :  tf.Tensor(9.8687466e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1243 is 0.0967 sec\n",
      "train AE loss : 0.6546209454536438, train ANN loss : 3.1544172763824463\n",
      "AE loss : 0.8559846878051758, ANN loss : 3.0846729278564453, Total loss : 88.68314361572266\n",
      "learning rate A :  tf.Tensor(9.8687466e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1244 is 0.0851 sec\n",
      "train AE loss : 0.7982938289642334, train ANN loss : 3.1299595832824707\n",
      "AE loss : 1.116921305656433, ANN loss : 3.063976287841797, Total loss : 114.75611114501953\n",
      "learning rate A :  tf.Tensor(9.8687466e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1245 is 0.0830 sec\n",
      "train AE loss : 1.0441594123840332, train ANN loss : 3.130476713180542\n",
      "AE loss : 0.9931469559669495, ANN loss : 3.068779706954956, Total loss : 102.38347625732422\n",
      "learning rate A :  tf.Tensor(9.8685385e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1246 is 0.0805 sec\n",
      "train AE loss : 0.9273396134376526, train ANN loss : 3.1325738430023193\n",
      "AE loss : 1.2822202444076538, ANN loss : 3.066851854324341, Total loss : 131.28887939453125\n",
      "learning rate A :  tf.Tensor(9.8685385e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1247 is 0.0819 sec\n",
      "train AE loss : 1.200717568397522, train ANN loss : 3.1352200508117676\n",
      "AE loss : 1.4121841192245483, ANN loss : 3.073291063308716, Total loss : 144.2917022705078\n",
      "learning rate A :  tf.Tensor(9.8685385e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1248 is 0.0948 sec\n",
      "train AE loss : 1.3240052461624146, train ANN loss : 3.1420862674713135\n",
      "AE loss : 1.2849351167678833, ANN loss : 3.064110040664673, Total loss : 131.55763244628906\n",
      "learning rate A :  tf.Tensor(9.8685385e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1249 is 0.1014 sec\n",
      "train AE loss : 1.2027440071105957, train ANN loss : 3.1320953369140625\n",
      "AE loss : 1.1299514770507812, ANN loss : 3.064972162246704, Total loss : 116.06011199951172\n",
      "learning rate A :  tf.Tensor(9.8683304e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1250 is 0.0838 sec\n",
      "train AE loss : 1.0558524131774902, train ANN loss : 3.1309595108032227\n",
      "AE loss : 1.0030323266983032, ANN loss : 3.0723588466644287, Total loss : 103.3755874633789\n",
      "learning rate A :  tf.Tensor(9.868123e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1251 is 0.0861 sec\n",
      "train AE loss : 0.9360669851303101, train ANN loss : 3.123971700668335\n",
      "AE loss : 0.9580455422401428, ANN loss : 3.0814638137817383, Total loss : 98.88602447509766\n",
      "learning rate A :  tf.Tensor(9.868123e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1252 is 0.0826 sec\n",
      "train AE loss : 0.8933473229408264, train ANN loss : 3.1313207149505615\n",
      "AE loss : 0.9975815415382385, ANN loss : 3.0779225826263428, Total loss : 102.8360824584961\n",
      "learning rate A :  tf.Tensor(9.868123e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1253 is 0.0977 sec\n",
      "train AE loss : 0.9303222894668579, train ANN loss : 3.1258058547973633\n",
      "AE loss : 0.8929715156555176, ANN loss : 3.0914857387542725, Total loss : 92.38863372802734\n",
      "learning rate A :  tf.Tensor(9.867915e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1254 is 0.1049 sec\n",
      "train AE loss : 0.8319606184959412, train ANN loss : 3.1411221027374268\n",
      "AE loss : 0.8054550886154175, ANN loss : 3.1068081855773926, Total loss : 83.65231323242188\n",
      "learning rate A :  tf.Tensor(9.8677076e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1255 is 0.0942 sec\n",
      "train AE loss : 0.7499901652336121, train ANN loss : 3.1524288654327393\n",
      "AE loss : 0.9694727063179016, ANN loss : 3.076808214187622, Total loss : 100.02408599853516\n",
      "learning rate A :  tf.Tensor(9.8677076e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1256 is 0.0912 sec\n",
      "train AE loss : 0.9037886261940002, train ANN loss : 3.122669219970703\n",
      "AE loss : 1.2275365591049194, ANN loss : 3.0585694313049316, Total loss : 125.81222534179688\n",
      "learning rate A :  tf.Tensor(9.8677076e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1257 is 0.0970 sec\n",
      "train AE loss : 1.147320032119751, train ANN loss : 3.126023530960083\n",
      "AE loss : 1.082244634628296, ANN loss : 3.062998056411743, Total loss : 111.28746795654297\n",
      "learning rate A :  tf.Tensor(9.8674995e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1258 is 0.1188 sec\n",
      "train AE loss : 1.0100146532058716, train ANN loss : 3.1204018592834473\n",
      "AE loss : 1.3509641885757446, ANN loss : 3.0617897510528564, Total loss : 138.15821838378906\n",
      "learning rate A :  tf.Tensor(9.8674995e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1259 is 0.0834 sec\n",
      "train AE loss : 1.2642463445663452, train ANN loss : 3.140397787094116\n",
      "AE loss : 1.4278738498687744, ANN loss : 3.065777063369751, Total loss : 145.85316467285156\n",
      "learning rate A :  tf.Tensor(9.8674995e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1260 is 0.0969 sec\n",
      "train AE loss : 1.3369885683059692, train ANN loss : 3.1411306858062744\n",
      "AE loss : 1.2690086364746094, ANN loss : 3.0578203201293945, Total loss : 129.95867919921875\n",
      "learning rate A :  tf.Tensor(9.8674995e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1261 is 0.1018 sec\n",
      "train AE loss : 1.1857107877731323, train ANN loss : 3.128993034362793\n",
      "AE loss : 1.115043044090271, ANN loss : 3.0615808963775635, Total loss : 114.56587982177734\n",
      "learning rate A :  tf.Tensor(9.867291e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1262 is 0.0826 sec\n",
      "train AE loss : 1.040024995803833, train ANN loss : 3.120897054672241\n",
      "AE loss : 1.000387191772461, ANN loss : 3.0749688148498535, Total loss : 103.11368560791016\n",
      "learning rate A :  tf.Tensor(9.867291e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1263 is 0.0837 sec\n",
      "train AE loss : 0.9315832257270813, train ANN loss : 3.1389787197113037\n",
      "AE loss : 0.894081711769104, ANN loss : 3.0893168449401855, Total loss : 92.49748992919922\n",
      "learning rate A :  tf.Tensor(9.8670826e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1264 is 0.1009 sec\n",
      "train AE loss : 0.8317164182662964, train ANN loss : 3.13555908203125\n",
      "AE loss : 0.805367112159729, ANN loss : 3.105419635772705, Total loss : 83.64212799072266\n",
      "learning rate A :  tf.Tensor(9.8668745e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1265 is 0.1069 sec\n",
      "train AE loss : 0.748629093170166, train ANN loss : 3.1464853286743164\n",
      "AE loss : 0.7306485176086426, ANN loss : 3.122406482696533, Total loss : 76.187255859375\n",
      "learning rate A :  tf.Tensor(9.8666664e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1266 is 0.1106 sec\n",
      "train AE loss : 0.6788288354873657, train ANN loss : 3.1648635864257812\n",
      "AE loss : 0.6670952439308167, ANN loss : 3.139626979827881, Total loss : 69.84915161132812\n",
      "learning rate A :  tf.Tensor(9.866459e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1267 is 0.0962 sec\n",
      "train AE loss : 0.6197695732116699, train ANN loss : 3.177419900894165\n",
      "AE loss : 0.756191074848175, ANN loss : 3.114879608154297, Total loss : 78.7339859008789\n",
      "learning rate A :  tf.Tensor(9.866459e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1268 is 0.1007 sec\n",
      "train AE loss : 0.7024891972541809, train ANN loss : 3.153362989425659\n",
      "AE loss : 1.0025479793548584, ANN loss : 3.0700161457061768, Total loss : 103.32482147216797\n",
      "learning rate A :  tf.Tensor(9.866459e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1269 is 0.0977 sec\n",
      "train AE loss : 0.9334558248519897, train ANN loss : 3.127798557281494\n",
      "AE loss : 0.8957396745681763, ANN loss : 3.082674264907837, Total loss : 92.65665435791016\n",
      "learning rate A :  tf.Tensor(9.866251e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1270 is 0.0964 sec\n",
      "train AE loss : 0.833151638507843, train ANN loss : 3.1347873210906982\n",
      "AE loss : 1.2605623006820679, ANN loss : 3.060375213623047, Total loss : 129.11660766601562\n",
      "learning rate A :  tf.Tensor(9.866251e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1271 is 0.1002 sec\n",
      "train AE loss : 1.1768206357955933, train ANN loss : 3.135157585144043\n",
      "AE loss : 1.537261724472046, ANN loss : 3.082008123397827, Total loss : 156.8081817626953\n",
      "learning rate A :  tf.Tensor(9.866251e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1272 is 0.0863 sec\n",
      "train AE loss : 1.4393919706344604, train ANN loss : 3.1579034328460693\n",
      "AE loss : 1.3295059204101562, ANN loss : 3.064523935317993, Total loss : 136.01512145996094\n",
      "learning rate A :  tf.Tensor(9.866043e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1273 is 0.0809 sec\n",
      "train AE loss : 1.2420555353164673, train ANN loss : 3.1283984184265137\n",
      "AE loss : 1.4161471128463745, ANN loss : 3.067488431930542, Total loss : 144.6822052001953\n",
      "learning rate A :  tf.Tensor(9.866043e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1274 is 0.0827 sec\n",
      "train AE loss : 1.3241252899169922, train ANN loss : 3.136237621307373\n",
      "AE loss : 1.2323594093322754, ANN loss : 3.0594935417175293, Total loss : 126.2954330444336\n",
      "learning rate A :  tf.Tensor(9.865835e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1275 is 0.0855 sec\n",
      "train AE loss : 1.1499520540237427, train ANN loss : 3.1264853477478027\n",
      "AE loss : 1.2223211526870728, ANN loss : 3.0582730770111084, Total loss : 125.29039001464844\n",
      "learning rate A :  tf.Tensor(9.865835e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1276 is 0.0810 sec\n",
      "train AE loss : 1.1403028964996338, train ANN loss : 3.121718406677246\n",
      "AE loss : 1.0752174854278564, ANN loss : 3.0627353191375732, Total loss : 110.58448791503906\n",
      "learning rate A :  tf.Tensor(9.865627e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1277 is 0.0833 sec\n",
      "train AE loss : 1.0015429258346558, train ANN loss : 3.1185832023620605\n",
      "AE loss : 1.0718398094177246, ANN loss : 3.064549446105957, Total loss : 110.24852752685547\n",
      "learning rate A :  tf.Tensor(9.865627e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1278 is 0.0832 sec\n",
      "train AE loss : 0.998208224773407, train ANN loss : 3.1115775108337402\n",
      "AE loss : 1.1006618738174438, ANN loss : 3.0626940727233887, Total loss : 113.1288833618164\n",
      "learning rate A :  tf.Tensor(9.865627e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1279 is 0.0822 sec\n",
      "train AE loss : 1.0252618789672852, train ANN loss : 3.1145970821380615\n",
      "AE loss : 1.1486371755599976, ANN loss : 3.0588457584381104, Total loss : 117.92256164550781\n",
      "learning rate A :  tf.Tensor(9.865627e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1280 is 0.0872 sec\n",
      "train AE loss : 1.070388913154602, train ANN loss : 3.1169984340667725\n",
      "AE loss : 1.0146136283874512, ANN loss : 3.069366216659546, Total loss : 104.5307388305664\n",
      "learning rate A :  tf.Tensor(9.865419e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1281 is 0.0831 sec\n",
      "train AE loss : 0.9441744685173035, train ANN loss : 3.115372896194458\n",
      "AE loss : 1.1336990594863892, ANN loss : 3.0579073429107666, Total loss : 116.42781066894531\n",
      "learning rate A :  tf.Tensor(9.865419e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1282 is 0.0999 sec\n",
      "train AE loss : 1.0562149286270142, train ANN loss : 3.1205577850341797\n",
      "AE loss : 1.257364273071289, ANN loss : 3.0533413887023926, Total loss : 128.78977966308594\n",
      "learning rate A :  tf.Tensor(9.865419e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1283 is 0.0844 sec\n",
      "train AE loss : 1.1729366779327393, train ANN loss : 3.1193716526031494\n",
      "AE loss : 1.3095693588256836, ANN loss : 3.053485631942749, Total loss : 134.0104217529297\n",
      "learning rate A :  tf.Tensor(9.865419e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1284 is 0.0868 sec\n",
      "train AE loss : 1.2221876382827759, train ANN loss : 3.1199772357940674\n",
      "AE loss : 1.2614222764968872, ANN loss : 3.053671360015869, Total loss : 129.19590759277344\n",
      "learning rate A :  tf.Tensor(9.865419e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1285 is 0.0824 sec\n",
      "train AE loss : 1.1762332916259766, train ANN loss : 3.108830213546753\n",
      "AE loss : 1.1629722118377686, ANN loss : 3.0577139854431152, Total loss : 119.35492706298828\n",
      "learning rate A :  tf.Tensor(9.865419e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1286 is 0.0839 sec\n",
      "train AE loss : 1.082837462425232, train ANN loss : 3.117413282394409\n",
      "AE loss : 1.0893604755401611, ANN loss : 3.063807487487793, Total loss : 111.9998550415039\n",
      "learning rate A :  tf.Tensor(9.865419e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1287 is 0.0870 sec\n",
      "train AE loss : 1.013144850730896, train ANN loss : 3.113556385040283\n",
      "AE loss : 1.0766953229904175, ANN loss : 3.065216064453125, Total loss : 110.7347412109375\n",
      "learning rate A :  tf.Tensor(9.865419e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1288 is 0.0829 sec\n",
      "train AE loss : 1.0009685754776, train ANN loss : 3.117236852645874\n",
      "AE loss : 1.1310135126113892, ANN loss : 3.0586798191070557, Total loss : 116.16002655029297\n",
      "learning rate A :  tf.Tensor(9.865419e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1289 is 0.0854 sec\n",
      "train AE loss : 1.051997184753418, train ANN loss : 3.120039224624634\n",
      "AE loss : 0.9982080459594727, ANN loss : 3.071702003479004, Total loss : 102.89250183105469\n",
      "learning rate A :  tf.Tensor(9.865212e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1290 is 0.0811 sec\n",
      "train AE loss : 0.9269428849220276, train ANN loss : 3.1116511821746826\n",
      "AE loss : 0.8890498280525208, ANN loss : 3.0878913402557373, Total loss : 91.99286651611328\n",
      "learning rate A :  tf.Tensor(9.865004e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1291 is 0.0798 sec\n",
      "train AE loss : 0.8244996666908264, train ANN loss : 3.128142833709717\n",
      "AE loss : 0.7985610961914062, ANN loss : 3.10548734664917, Total loss : 82.96160125732422\n",
      "learning rate A :  tf.Tensor(9.864796e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1292 is 0.0853 sec\n",
      "train AE loss : 0.7398824095726013, train ANN loss : 3.1411962509155273\n",
      "AE loss : 0.9974174499511719, ANN loss : 3.065629243850708, Total loss : 102.80738067626953\n",
      "learning rate A :  tf.Tensor(9.864796e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1293 is 0.0842 sec\n",
      "train AE loss : 0.9261773824691772, train ANN loss : 3.1215286254882812\n",
      "AE loss : 0.8884101510047913, ANN loss : 3.080456018447876, Total loss : 91.92147827148438\n",
      "learning rate A :  tf.Tensor(9.864588e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1294 is 0.0837 sec\n",
      "train AE loss : 0.8239485621452332, train ANN loss : 3.1254007816314697\n",
      "AE loss : 0.7980217933654785, ANN loss : 3.096982002258301, Total loss : 82.89916229248047\n",
      "learning rate A :  tf.Tensor(9.86438e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1295 is 0.0803 sec\n",
      "train AE loss : 0.7394661903381348, train ANN loss : 3.1427199840545654\n",
      "AE loss : 0.7220652103424072, ANN loss : 3.114274263381958, Total loss : 75.32080078125\n",
      "learning rate A :  tf.Tensor(9.864172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1296 is 0.0798 sec\n",
      "train AE loss : 0.6687191128730774, train ANN loss : 3.164524793624878\n",
      "AE loss : 1.0704574584960938, ANN loss : 3.05557918548584, Total loss : 110.10132598876953\n",
      "learning rate A :  tf.Tensor(9.864172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1297 is 0.0870 sec\n",
      "train AE loss : 0.99500972032547, train ANN loss : 3.109882354736328\n",
      "AE loss : 0.9485344290733337, ANN loss : 3.0651519298553467, Total loss : 97.9186019897461\n",
      "learning rate A :  tf.Tensor(9.863965e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1298 is 0.0899 sec\n",
      "train AE loss : 0.8804658651351929, train ANN loss : 3.1252458095550537\n",
      "AE loss : 0.8478697538375854, ANN loss : 3.078289747238159, Total loss : 87.86526489257812\n",
      "learning rate A :  tf.Tensor(9.863757e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1299 is 0.0833 sec\n",
      "train AE loss : 0.7863276600837708, train ANN loss : 3.124025583267212\n",
      "AE loss : 1.3402459621429443, ANN loss : 3.063628673553467, Total loss : 137.08822631835938\n",
      "learning rate A :  tf.Tensor(9.863757e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1300 is 0.0769 sec\n",
      "train AE loss : 1.2496776580810547, train ANN loss : 3.1427206993103027\n",
      "AE loss : 1.166953206062317, ANN loss : 3.0563101768493652, Total loss : 119.75162506103516\n",
      "learning rate A :  tf.Tensor(9.8635486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1301 is 0.0953 sec\n",
      "train AE loss : 1.0859298706054688, train ANN loss : 3.1260061264038086\n",
      "AE loss : 1.0270473957061768, ANN loss : 3.0582048892974854, Total loss : 105.76295471191406\n",
      "learning rate A :  tf.Tensor(9.863341e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1302 is 0.0833 sec\n",
      "train AE loss : 0.9543731808662415, train ANN loss : 3.123086452484131\n",
      "AE loss : 0.9127170443534851, ANN loss : 3.0657975673675537, Total loss : 94.3375015258789\n",
      "learning rate A :  tf.Tensor(9.863133e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1303 is 0.0826 sec\n",
      "train AE loss : 0.8472124338150024, train ANN loss : 3.127998113632202\n",
      "AE loss : 0.818071186542511, ANN loss : 3.0769951343536377, Total loss : 84.88410949707031\n",
      "learning rate A :  tf.Tensor(9.862925e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1304 is 0.0807 sec\n",
      "train AE loss : 0.7588014006614685, train ANN loss : 3.1261582374572754\n",
      "AE loss : 1.3628871440887451, ANN loss : 3.0758962631225586, Total loss : 139.36460876464844\n",
      "learning rate A :  tf.Tensor(9.862925e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1305 is 0.0892 sec\n",
      "train AE loss : 1.2711082696914673, train ANN loss : 3.156426191329956\n",
      "AE loss : 1.7099612951278687, ANN loss : 3.122972249984741, Total loss : 174.1190948486328\n",
      "learning rate A :  tf.Tensor(9.862925e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1306 is 0.0797 sec\n",
      "train AE loss : 1.6007658243179321, train ANN loss : 3.217512369155884\n",
      "AE loss : 1.456844687461853, ANN loss : 3.0828704833984375, Total loss : 148.767333984375\n",
      "learning rate A :  tf.Tensor(9.862718e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1307 is 0.0809 sec\n",
      "train AE loss : 1.3600397109985352, train ANN loss : 3.162705659866333\n",
      "AE loss : 1.2578901052474976, ANN loss : 3.0636966228485107, Total loss : 128.8527069091797\n",
      "learning rate A :  tf.Tensor(9.8625096e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1308 is 0.0781 sec\n",
      "train AE loss : 1.1719017028808594, train ANN loss : 3.134777307510376\n",
      "AE loss : 1.3507170677185059, ANN loss : 3.0661020278930664, Total loss : 138.13780212402344\n",
      "learning rate A :  tf.Tensor(9.8625096e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1309 is 0.0776 sec\n",
      "train AE loss : 1.2593486309051514, train ANN loss : 3.12956166267395\n",
      "AE loss : 1.2370296716690063, ANN loss : 3.0613601207733154, Total loss : 126.76432800292969\n",
      "learning rate A :  tf.Tensor(9.8625096e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1310 is 0.0813 sec\n",
      "train AE loss : 1.1517518758773804, train ANN loss : 3.119114637374878\n",
      "AE loss : 1.0658072233200073, ANN loss : 3.074293375015259, Total loss : 109.65501403808594\n",
      "learning rate A :  tf.Tensor(9.8625096e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1311 is 0.0782 sec\n",
      "train AE loss : 0.9904419779777527, train ANN loss : 3.127171516418457\n",
      "AE loss : 0.9415556192398071, ANN loss : 3.087416410446167, Total loss : 97.24298858642578\n",
      "learning rate A :  tf.Tensor(9.8623015e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1312 is 0.0793 sec\n",
      "train AE loss : 0.8738714456558228, train ANN loss : 3.1299102306365967\n",
      "AE loss : 0.9144219160079956, ANN loss : 3.094975233078003, Total loss : 94.53716278076172\n",
      "learning rate A :  tf.Tensor(9.8623015e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1313 is 0.0801 sec\n",
      "train AE loss : 0.8486485481262207, train ANN loss : 3.1380155086517334\n",
      "AE loss : 0.8172407150268555, ANN loss : 3.111955165863037, Total loss : 84.83602905273438\n",
      "learning rate A :  tf.Tensor(9.862094e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1314 is 0.0778 sec\n",
      "train AE loss : 0.7578859329223633, train ANN loss : 3.153233528137207\n",
      "AE loss : 0.7361893653869629, ANN loss : 3.129882574081421, Total loss : 76.74881744384766\n",
      "learning rate A :  tf.Tensor(9.861886e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1315 is 0.0761 sec\n",
      "train AE loss : 0.6823397874832153, train ANN loss : 3.16532564163208\n",
      "AE loss : 0.8776901364326477, ANN loss : 3.094071388244629, Total loss : 90.86308288574219\n",
      "learning rate A :  tf.Tensor(9.861886e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1316 is 0.0803 sec\n",
      "train AE loss : 0.8148148655891418, train ANN loss : 3.137587547302246\n",
      "AE loss : 1.1650971174240112, ANN loss : 3.058086633682251, Total loss : 119.56780242919922\n",
      "learning rate A :  tf.Tensor(9.861886e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1317 is 0.0801 sec\n",
      "train AE loss : 1.08481764793396, train ANN loss : 3.1242496967315674\n",
      "AE loss : 1.0239970684051514, ANN loss : 3.067403793334961, Total loss : 105.46711730957031\n",
      "learning rate A :  tf.Tensor(9.861678e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1318 is 0.0780 sec\n",
      "train AE loss : 0.9522368311882019, train ANN loss : 3.128601312637329\n",
      "AE loss : 0.9088050723075867, ANN loss : 3.080587148666382, Total loss : 93.96109008789062\n",
      "learning rate A :  tf.Tensor(9.8614706e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1319 is 0.0820 sec\n",
      "train AE loss : 0.8444024324417114, train ANN loss : 3.1311304569244385\n",
      "AE loss : 1.2899549007415771, ANN loss : 3.0568976402282715, Total loss : 132.05238342285156\n",
      "learning rate A :  tf.Tensor(9.8614706e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1320 is 0.0765 sec\n",
      "train AE loss : 1.20305597782135, train ANN loss : 3.129093647003174\n",
      "AE loss : 1.125149130821228, ANN loss : 3.057976722717285, Total loss : 115.5728988647461\n",
      "learning rate A :  tf.Tensor(9.8612625e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1321 is 0.0795 sec\n",
      "train AE loss : 1.0477205514907837, train ANN loss : 3.118109703063965\n",
      "AE loss : 1.5000321865081787, ANN loss : 3.0708298683166504, Total loss : 153.0740509033203\n",
      "learning rate A :  tf.Tensor(9.8612625e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1322 is 0.0797 sec\n",
      "train AE loss : 1.4019908905029297, train ANN loss : 3.1541733741760254\n",
      "AE loss : 1.5789610147476196, ANN loss : 3.0732946395874023, Total loss : 160.96939086914062\n",
      "learning rate A :  tf.Tensor(9.8612625e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1323 is 0.0822 sec\n",
      "train AE loss : 1.4764049053192139, train ANN loss : 3.155521869659424\n",
      "AE loss : 1.3528831005096436, ANN loss : 3.0545427799224854, Total loss : 138.3428497314453\n",
      "learning rate A :  tf.Tensor(9.861055e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1324 is 0.0770 sec\n",
      "train AE loss : 1.2621417045593262, train ANN loss : 3.1271352767944336\n",
      "AE loss : 1.2529062032699585, ANN loss : 3.051161050796509, Total loss : 128.34178161621094\n",
      "learning rate A :  tf.Tensor(9.861055e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1325 is 0.0811 sec\n",
      "train AE loss : 1.166976809501648, train ANN loss : 3.1181225776672363\n",
      "AE loss : 1.0964293479919434, ANN loss : 3.0602283477783203, Total loss : 112.70317077636719\n",
      "learning rate A :  tf.Tensor(9.861055e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1326 is 0.0781 sec\n",
      "train AE loss : 1.018972635269165, train ANN loss : 3.117906332015991\n",
      "AE loss : 0.9946138262748718, ANN loss : 3.0737218856811523, Total loss : 102.53510284423828\n",
      "learning rate A :  tf.Tensor(9.861055e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1327 is 0.0810 sec\n",
      "train AE loss : 0.9228852391242981, train ANN loss : 3.121680974960327\n",
      "AE loss : 0.8818403482437134, ANN loss : 3.088784694671631, Total loss : 91.2728271484375\n",
      "learning rate A :  tf.Tensor(9.860847e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1328 is 0.0805 sec\n",
      "train AE loss : 0.8173913955688477, train ANN loss : 3.127521514892578\n",
      "AE loss : 0.9209354519844055, ANN loss : 3.080606460571289, Total loss : 95.17415618896484\n",
      "learning rate A :  tf.Tensor(9.860847e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1329 is 0.0797 sec\n",
      "train AE loss : 0.853745698928833, train ANN loss : 3.125100612640381\n",
      "AE loss : 1.0682541131973267, ANN loss : 3.05849552154541, Total loss : 109.88390350341797\n",
      "learning rate A :  tf.Tensor(9.860847e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1330 is 0.0810 sec\n",
      "train AE loss : 0.9917202591896057, train ANN loss : 3.114362955093384\n",
      "AE loss : 1.2582275867462158, ANN loss : 3.048537015914917, Total loss : 128.8712921142578\n",
      "learning rate A :  tf.Tensor(9.860847e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1331 is 0.0799 sec\n",
      "train AE loss : 1.1705373525619507, train ANN loss : 3.113814353942871\n",
      "AE loss : 1.0966521501541138, ANN loss : 3.0522665977478027, Total loss : 112.71747589111328\n",
      "learning rate A :  tf.Tensor(9.860639e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1332 is 0.0789 sec\n",
      "train AE loss : 1.018462896347046, train ANN loss : 3.1083996295928955\n",
      "AE loss : 1.28586745262146, ANN loss : 3.048999071121216, Total loss : 131.6357421875\n",
      "learning rate A :  tf.Tensor(9.860639e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1333 is 0.0788 sec\n",
      "train AE loss : 1.1965800523757935, train ANN loss : 3.1143438816070557\n",
      "AE loss : 1.3581748008728027, ANN loss : 3.0481648445129395, Total loss : 138.8656463623047\n",
      "learning rate A :  tf.Tensor(9.860639e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1334 is 0.0848 sec\n",
      "train AE loss : 1.2646676301956177, train ANN loss : 3.119008779525757\n",
      "AE loss : 1.1763114929199219, ANN loss : 3.0469417572021484, Total loss : 120.67809295654297\n",
      "learning rate A :  tf.Tensor(9.8604316e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1335 is 0.0941 sec\n",
      "train AE loss : 1.0932598114013672, train ANN loss : 3.111220598220825\n",
      "AE loss : 1.030480980873108, ANN loss : 3.053997039794922, Total loss : 106.10208892822266\n",
      "learning rate A :  tf.Tensor(9.860224e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1336 is 0.0794 sec\n",
      "train AE loss : 0.9562707543373108, train ANN loss : 3.112762689590454\n",
      "AE loss : 0.9116913676261902, ANN loss : 3.0658178329467773, Total loss : 94.23495483398438\n",
      "learning rate A :  tf.Tensor(9.860016e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1337 is 0.0818 sec\n",
      "train AE loss : 0.8450754880905151, train ANN loss : 3.122453212738037\n",
      "AE loss : 0.8139070868492126, ANN loss : 3.080148458480835, Total loss : 84.47085571289062\n",
      "learning rate A :  tf.Tensor(9.859809e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1338 is 0.0776 sec\n",
      "train AE loss : 0.7537605166435242, train ANN loss : 3.133270502090454\n",
      "AE loss : 0.7324574589729309, ANN loss : 3.095860004425049, Total loss : 76.34159851074219\n",
      "learning rate A :  tf.Tensor(9.859601e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1339 is 0.0803 sec\n",
      "train AE loss : 0.6779846549034119, train ANN loss : 3.1408519744873047\n",
      "AE loss : 0.9111477732658386, ANN loss : 3.063638925552368, Total loss : 94.17841339111328\n",
      "learning rate A :  tf.Tensor(9.859601e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1340 is 0.0820 sec\n",
      "train AE loss : 0.8444438576698303, train ANN loss : 3.1161813735961914\n",
      "AE loss : 1.1998907327651978, ANN loss : 3.0453267097473145, Total loss : 123.03440856933594\n",
      "learning rate A :  tf.Tensor(9.859601e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1341 is 0.0783 sec\n",
      "train AE loss : 1.1151988506317139, train ANN loss : 3.1129024028778076\n",
      "AE loss : 1.0482101440429688, ANN loss : 3.0500898361206055, Total loss : 107.87110137939453\n",
      "learning rate A :  tf.Tensor(9.859393e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1342 is 0.0799 sec\n",
      "train AE loss : 0.9726075530052185, train ANN loss : 3.1148054599761963\n",
      "AE loss : 1.3588097095489502, ANN loss : 3.0505928993225098, Total loss : 138.93154907226562\n",
      "learning rate A :  tf.Tensor(9.859393e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1343 is 0.0788 sec\n",
      "train AE loss : 1.26490318775177, train ANN loss : 3.1249427795410156\n",
      "AE loss : 1.4991148710250854, ANN loss : 3.060255765914917, Total loss : 152.97174072265625\n",
      "learning rate A :  tf.Tensor(9.859393e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1344 is 0.0953 sec\n",
      "train AE loss : 1.3973596096038818, train ANN loss : 3.1301918029785156\n",
      "AE loss : 1.2847886085510254, ANN loss : 3.0491132736206055, Total loss : 131.52798461914062\n",
      "learning rate A :  tf.Tensor(9.859185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1345 is 0.0821 sec\n",
      "train AE loss : 1.1950461864471436, train ANN loss : 3.109736919403076\n",
      "AE loss : 1.2991197109222412, ANN loss : 3.0506539344787598, Total loss : 132.96263122558594\n",
      "learning rate A :  tf.Tensor(9.859185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1346 is 0.0792 sec\n",
      "train AE loss : 1.208548903465271, train ANN loss : 3.1192586421966553\n",
      "AE loss : 1.2120310068130493, ANN loss : 3.051238536834717, Total loss : 124.25434112548828\n",
      "learning rate A :  tf.Tensor(9.859185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1347 is 0.0860 sec\n",
      "train AE loss : 1.1265599727630615, train ANN loss : 3.113927125930786\n",
      "AE loss : 1.111983060836792, ANN loss : 3.0559208393096924, Total loss : 114.25422668457031\n",
      "learning rate A :  tf.Tensor(9.859185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1348 is 0.0803 sec\n",
      "train AE loss : 1.03262460231781, train ANN loss : 3.1129586696624756\n",
      "AE loss : 1.0552481412887573, ANN loss : 3.0588181018829346, Total loss : 108.5836181640625\n",
      "learning rate A :  tf.Tensor(9.859185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1349 is 0.0848 sec\n",
      "train AE loss : 0.9794723987579346, train ANN loss : 3.1113741397857666\n",
      "AE loss : 1.0809776782989502, ANN loss : 3.0526938438415527, Total loss : 111.15045928955078\n",
      "learning rate A :  tf.Tensor(9.859185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1350 is 0.0810 sec\n",
      "train AE loss : 1.003701090812683, train ANN loss : 3.1142055988311768\n",
      "AE loss : 1.1656036376953125, ANN loss : 3.0439188480377197, Total loss : 119.60427856445312\n",
      "learning rate A :  tf.Tensor(9.859185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1351 is 0.0826 sec\n",
      "train AE loss : 1.0832359790802002, train ANN loss : 3.1040849685668945\n",
      "AE loss : 1.0199940204620361, ANN loss : 3.055474042892456, Total loss : 105.05487823486328\n",
      "learning rate A :  tf.Tensor(9.858978e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1352 is 0.0919 sec\n",
      "train AE loss : 0.9465758800506592, train ANN loss : 3.109720468521118\n",
      "AE loss : 0.9016057252883911, ANN loss : 3.0706892013549805, Total loss : 93.23126220703125\n",
      "learning rate A :  tf.Tensor(9.85877e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1353 is 0.0784 sec\n",
      "train AE loss : 0.8358199596405029, train ANN loss : 3.122298240661621\n",
      "AE loss : 1.0869553089141846, ANN loss : 3.0463151931762695, Total loss : 111.74185943603516\n",
      "learning rate A :  tf.Tensor(9.85877e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1354 is 0.0827 sec\n",
      "train AE loss : 1.00941002368927, train ANN loss : 3.109471559524536\n",
      "AE loss : 0.9561115503311157, ANN loss : 3.0583884716033936, Total loss : 98.66954803466797\n",
      "learning rate A :  tf.Tensor(9.8585624e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1355 is 0.0770 sec\n",
      "train AE loss : 0.8868222832679749, train ANN loss : 3.1141107082366943\n",
      "AE loss : 0.8490597009658813, ANN loss : 3.073624849319458, Total loss : 87.97959899902344\n",
      "learning rate A :  tf.Tensor(9.858354e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1356 is 0.0848 sec\n",
      "train AE loss : 0.7868564128875732, train ANN loss : 3.127711296081543\n",
      "AE loss : 1.138357400894165, ANN loss : 3.0430850982666016, Total loss : 116.87882232666016\n",
      "learning rate A :  tf.Tensor(9.858354e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1357 is 0.0812 sec\n",
      "train AE loss : 1.057576298713684, train ANN loss : 3.107651948928833\n",
      "AE loss : 0.9972967505455017, ANN loss : 3.0508203506469727, Total loss : 102.78050231933594\n",
      "learning rate A :  tf.Tensor(9.858147e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1358 is 0.0795 sec\n",
      "train AE loss : 0.9253188371658325, train ANN loss : 3.1136555671691895\n",
      "AE loss : 0.88242107629776, ANN loss : 3.0631799697875977, Total loss : 91.30530548095703\n",
      "learning rate A :  tf.Tensor(9.857939e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1359 is 0.0785 sec\n",
      "train AE loss : 0.8179619312286377, train ANN loss : 3.1179306507110596\n",
      "AE loss : 0.7877774238586426, ANN loss : 3.0781211853027344, Total loss : 81.85586547851562\n",
      "learning rate A :  tf.Tensor(9.8577315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1360 is 0.0790 sec\n",
      "train AE loss : 0.7297661900520325, train ANN loss : 3.1276655197143555\n",
      "AE loss : 1.1614456176757812, ANN loss : 3.045884370803833, Total loss : 119.19043731689453\n",
      "learning rate A :  tf.Tensor(9.8577315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1361 is 0.0879 sec\n",
      "train AE loss : 1.0791386365890503, train ANN loss : 3.1131045818328857\n",
      "AE loss : 1.5253390073776245, ANN loss : 3.0750808715820312, Total loss : 155.60897827148438\n",
      "learning rate A :  tf.Tensor(9.8577315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1362 is 0.0849 sec\n",
      "train AE loss : 1.4222192764282227, train ANN loss : 3.1594059467315674\n",
      "AE loss : 1.5193427801132202, ANN loss : 3.0669384002685547, Total loss : 155.001220703125\n",
      "learning rate A :  tf.Tensor(9.8577315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1363 is 0.1037 sec\n",
      "train AE loss : 1.4164061546325684, train ANN loss : 3.1468236446380615\n",
      "AE loss : 1.2122379541397095, ANN loss : 3.047269582748413, Total loss : 124.27105712890625\n",
      "learning rate A :  tf.Tensor(9.8577315e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1364 is 0.0926 sec\n",
      "train AE loss : 1.1265662908554077, train ANN loss : 3.099583387374878\n",
      "AE loss : 1.0545250177383423, ANN loss : 3.054527521133423, Total loss : 108.50702667236328\n",
      "learning rate A :  tf.Tensor(9.857524e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1365 is 0.0783 sec\n",
      "train AE loss : 0.9783672094345093, train ANN loss : 3.109994888305664\n",
      "AE loss : 0.9271080493927002, ANN loss : 3.0673208236694336, Total loss : 95.77812194824219\n",
      "learning rate A :  tf.Tensor(9.857316e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1366 is 0.0823 sec\n",
      "train AE loss : 0.8591200709342957, train ANN loss : 3.111459255218506\n",
      "AE loss : 0.8228226900100708, ANN loss : 3.0833022594451904, Total loss : 85.3655776977539\n",
      "learning rate A :  tf.Tensor(9.857109e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1367 is 0.0797 sec\n",
      "train AE loss : 0.7618486881256104, train ANN loss : 3.125732421875\n",
      "AE loss : 0.736615002155304, ANN loss : 3.100878953933716, Total loss : 76.76238250732422\n",
      "learning rate A :  tf.Tensor(9.8569006e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1368 is 0.0816 sec\n",
      "train AE loss : 0.6817198991775513, train ANN loss : 3.1420702934265137\n",
      "AE loss : 0.6645439267158508, ANN loss : 3.1191489696502686, Total loss : 69.57354736328125\n",
      "learning rate A :  tf.Tensor(9.856693e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1369 is 0.0794 sec\n",
      "train AE loss : 0.614992082118988, train ANN loss : 3.1599738597869873\n",
      "AE loss : 0.60371333360672, ANN loss : 3.137596607208252, Total loss : 63.50893020629883\n",
      "learning rate A :  tf.Tensor(9.856485e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1370 is 0.0768 sec\n",
      "train AE loss : 0.5588860511779785, train ANN loss : 3.1763412952423096\n",
      "AE loss : 0.6063519716262817, ANN loss : 3.1440367698669434, Total loss : 63.77923583984375\n",
      "learning rate A :  tf.Tensor(9.856485e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1371 is 0.0853 sec\n",
      "train AE loss : 0.561411440372467, train ANN loss : 3.179215908050537\n",
      "AE loss : 0.5542624592781067, ANN loss : 3.1624538898468018, Total loss : 58.58869934082031\n",
      "learning rate A :  tf.Tensor(9.856276e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1372 is 0.0777 sec\n",
      "train AE loss : 0.5134670734405518, train ANN loss : 3.1872687339782715\n",
      "AE loss : 0.7265466451644897, ANN loss : 3.103015899658203, Total loss : 75.75767517089844\n",
      "learning rate A :  tf.Tensor(9.856276e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1373 is 0.0837 sec\n",
      "train AE loss : 0.6729398965835571, train ANN loss : 3.1429195404052734\n",
      "AE loss : 0.6562719941139221, ANN loss : 3.121058464050293, Total loss : 68.74825286865234\n",
      "learning rate A :  tf.Tensor(9.856069e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1374 is 0.0792 sec\n",
      "train AE loss : 0.6078622937202454, train ANN loss : 3.156782627105713\n",
      "AE loss : 1.0201395750045776, ANN loss : 3.049400806427002, Total loss : 105.0633544921875\n",
      "learning rate A :  tf.Tensor(9.856069e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1375 is 0.0769 sec\n",
      "train AE loss : 0.9472565054893494, train ANN loss : 3.1053178310394287\n",
      "AE loss : 0.8995949625968933, ANN loss : 3.0613062381744385, Total loss : 93.02079010009766\n",
      "learning rate A :  tf.Tensor(9.8558616e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1376 is 0.0763 sec\n",
      "train AE loss : 0.8345790505409241, train ANN loss : 3.12024188041687\n",
      "AE loss : 1.4195138216018677, ANN loss : 3.0536696910858154, Total loss : 145.00506591796875\n",
      "learning rate A :  tf.Tensor(9.8558616e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1377 is 0.0790 sec\n",
      "train AE loss : 1.3233882188796997, train ANN loss : 3.126542091369629\n",
      "AE loss : 1.779388189315796, ANN loss : 3.09906268119812, Total loss : 181.0378875732422\n",
      "learning rate A :  tf.Tensor(9.8558616e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1378 is 0.0788 sec\n",
      "train AE loss : 1.664429783821106, train ANN loss : 3.1964077949523926\n",
      "AE loss : 1.4991259574890137, ANN loss : 3.0617127418518066, Total loss : 152.9743194580078\n",
      "learning rate A :  tf.Tensor(9.8556535e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1379 is 0.0754 sec\n",
      "train AE loss : 1.3987503051757812, train ANN loss : 3.1449356079101562\n",
      "AE loss : 1.281924843788147, ANN loss : 3.0455093383789062, Total loss : 131.23800659179688\n",
      "learning rate A :  tf.Tensor(9.855446e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1380 is 0.0767 sec\n",
      "train AE loss : 1.1935697793960571, train ANN loss : 3.1252048015594482\n",
      "AE loss : 1.1102440357208252, ANN loss : 3.0422861576080322, Total loss : 114.06669616699219\n",
      "learning rate A :  tf.Tensor(9.855238e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1381 is 0.0811 sec\n",
      "train AE loss : 1.0323740243911743, train ANN loss : 3.112269401550293\n",
      "AE loss : 0.9727408289909363, ANN loss : 3.0469348430633545, Total loss : 100.32101440429688\n",
      "learning rate A :  tf.Tensor(9.855031e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1382 is 0.0828 sec\n",
      "train AE loss : 0.9035773277282715, train ANN loss : 3.108729124069214\n",
      "AE loss : 0.8609006404876709, ANN loss : 3.056654691696167, Total loss : 89.14672088623047\n",
      "learning rate A :  tf.Tensor(9.854823e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1383 is 0.0813 sec\n",
      "train AE loss : 0.7991191744804382, train ANN loss : 3.1194236278533936\n",
      "AE loss : 1.1493592262268066, ANN loss : 3.041445255279541, Total loss : 117.97735595703125\n",
      "learning rate A :  tf.Tensor(9.854823e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1384 is 0.0842 sec\n",
      "train AE loss : 1.068983554840088, train ANN loss : 3.1127192974090576\n",
      "AE loss : 1.3742992877960205, ANN loss : 3.0521414279937744, Total loss : 140.48207092285156\n",
      "learning rate A :  tf.Tensor(9.854823e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1385 is 0.1114 sec\n",
      "train AE loss : 1.2803707122802734, train ANN loss : 3.12907075881958\n",
      "AE loss : 1.3694957494735718, ANN loss : 3.0507524013519287, Total loss : 140.00033569335938\n",
      "learning rate A :  tf.Tensor(9.854823e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1386 is 0.1152 sec\n",
      "train AE loss : 1.275614857673645, train ANN loss : 3.1230342388153076\n",
      "AE loss : 1.1963789463043213, ANN loss : 3.046682119369507, Total loss : 122.68457794189453\n",
      "learning rate A :  tf.Tensor(9.854823e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1387 is 0.1163 sec\n",
      "train AE loss : 1.1124321222305298, train ANN loss : 3.1080965995788574\n",
      "AE loss : 1.0166513919830322, ANN loss : 3.0606746673583984, Total loss : 104.72581481933594\n",
      "learning rate A :  tf.Tensor(9.854823e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1388 is 0.1187 sec\n",
      "train AE loss : 0.9436509013175964, train ANN loss : 3.1195056438446045\n",
      "AE loss : 0.894375741481781, ANN loss : 3.076202154159546, Total loss : 92.51377868652344\n",
      "learning rate A :  tf.Tensor(9.854615e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1389 is 0.0919 sec\n",
      "train AE loss : 0.8293595314025879, train ANN loss : 3.1188313961029053\n",
      "AE loss : 0.8609123229980469, ANN loss : 3.082879066467285, Total loss : 89.17411041259766\n",
      "learning rate A :  tf.Tensor(9.854615e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1390 is 0.1181 sec\n",
      "train AE loss : 0.7980384230613708, train ANN loss : 3.1258912086486816\n",
      "AE loss : 0.7669235467910767, ANN loss : 3.1013665199279785, Total loss : 79.79371643066406\n",
      "learning rate A :  tf.Tensor(9.854408e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1391 is 0.0814 sec\n",
      "train AE loss : 0.7105867862701416, train ANN loss : 3.146183967590332\n",
      "AE loss : 0.6887876391410828, ANN loss : 3.120659828186035, Total loss : 71.99942016601562\n",
      "learning rate A :  tf.Tensor(9.8542005e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1392 is 0.0784 sec\n",
      "train AE loss : 0.6380869746208191, train ANN loss : 3.1553215980529785\n",
      "AE loss : 0.6230838298797607, ANN loss : 3.1399664878845215, Total loss : 65.44835662841797\n",
      "learning rate A :  tf.Tensor(9.8539924e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1393 is 0.0773 sec\n",
      "train AE loss : 0.5774341225624084, train ANN loss : 3.18015718460083\n",
      "AE loss : 0.5675851702690125, ANN loss : 3.158757209777832, Total loss : 59.917274475097656\n",
      "learning rate A :  tf.Tensor(9.853785e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1394 is 0.0774 sec\n",
      "train AE loss : 0.5262789726257324, train ANN loss : 3.1907975673675537\n",
      "AE loss : 0.7076140642166138, ANN loss : 3.105842113494873, Total loss : 73.86724853515625\n",
      "learning rate A :  tf.Tensor(9.853785e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1395 is 0.0786 sec\n",
      "train AE loss : 0.6557050943374634, train ANN loss : 3.148439645767212\n",
      "AE loss : 0.6391159892082214, ANN loss : 3.1243438720703125, Total loss : 67.03594207763672\n",
      "learning rate A :  tf.Tensor(9.853578e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1396 is 0.0767 sec\n",
      "train AE loss : 0.592403769493103, train ANN loss : 3.158932685852051\n",
      "AE loss : 0.5814394950866699, ANN loss : 3.142612934112549, Total loss : 61.28656768798828\n",
      "learning rate A :  tf.Tensor(9.8533696e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1397 is 0.0774 sec\n",
      "train AE loss : 0.5391996502876282, train ANN loss : 3.1819136142730713\n",
      "AE loss : 0.5323453545570374, ANN loss : 3.1602275371551514, Total loss : 56.3947639465332\n",
      "learning rate A :  tf.Tensor(9.853162e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1398 is 0.0778 sec\n",
      "train AE loss : 0.4940066337585449, train ANN loss : 3.191234588623047\n",
      "AE loss : 0.8326475024223328, ANN loss : 3.0678040981292725, Total loss : 86.33255004882812\n",
      "learning rate A :  tf.Tensor(9.853162e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1399 is 0.0803 sec\n",
      "train AE loss : 0.7722614407539368, train ANN loss : 3.1115729808807373\n",
      "AE loss : 0.7441223859786987, ANN loss : 3.0836894512176514, Total loss : 77.49592590332031\n",
      "learning rate A :  tf.Tensor(9.852955e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1400 is 0.0786 sec\n",
      "train AE loss : 0.6898842453956604, train ANN loss : 3.1340689659118652\n",
      "AE loss : 1.2438584566116333, ANN loss : 3.0473639965057373, Total loss : 127.43321228027344\n",
      "learning rate A :  tf.Tensor(9.852955e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1401 is 0.0789 sec\n",
      "train AE loss : 1.157228946685791, train ANN loss : 3.1246211528778076\n",
      "AE loss : 1.730926513671875, ANN loss : 3.1012141704559326, Total loss : 176.19384765625\n",
      "learning rate A :  tf.Tensor(9.852955e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1402 is 0.0788 sec\n",
      "train AE loss : 1.6171987056732178, train ANN loss : 3.1986920833587646\n",
      "AE loss : 1.4581935405731201, ANN loss : 3.064969778060913, Total loss : 148.8843231201172\n",
      "learning rate A :  tf.Tensor(9.852747e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1403 is 0.0789 sec\n",
      "train AE loss : 1.3590855598449707, train ANN loss : 3.1588571071624756\n",
      "AE loss : 1.2468873262405396, ANN loss : 3.0487489700317383, Total loss : 127.73748779296875\n",
      "learning rate A :  tf.Tensor(9.8525394e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1404 is 0.0765 sec\n",
      "train AE loss : 1.1600455045700073, train ANN loss : 3.132570266723633\n",
      "AE loss : 1.5160819292068481, ANN loss : 3.0659260749816895, Total loss : 154.67413330078125\n",
      "learning rate A :  tf.Tensor(9.8525394e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1405 is 0.0787 sec\n",
      "train AE loss : 1.4134174585342407, train ANN loss : 3.1500797271728516\n",
      "AE loss : 1.4676629304885864, ANN loss : 3.054077625274658, Total loss : 149.82037353515625\n",
      "learning rate A :  tf.Tensor(9.8525394e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1406 is 0.0796 sec\n",
      "train AE loss : 1.3673839569091797, train ANN loss : 3.126478433609009\n",
      "AE loss : 1.1944459676742554, ANN loss : 3.0474512577056885, Total loss : 122.49205017089844\n",
      "learning rate A :  tf.Tensor(9.8525394e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1407 is 0.0784 sec\n",
      "train AE loss : 1.1100022792816162, train ANN loss : 3.1107068061828613\n",
      "AE loss : 1.0359160900115967, ANN loss : 3.055697202682495, Total loss : 106.64730072021484\n",
      "learning rate A :  tf.Tensor(9.852331e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1408 is 0.0768 sec\n",
      "train AE loss : 0.9613955616950989, train ANN loss : 3.1085562705993652\n",
      "AE loss : 0.9084329605102539, ANN loss : 3.0694236755371094, Total loss : 93.9127197265625\n",
      "learning rate A :  tf.Tensor(9.852124e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1409 is 0.0772 sec\n",
      "train AE loss : 0.8422094583511353, train ANN loss : 3.122481107711792\n",
      "AE loss : 0.8196594715118408, ANN loss : 3.0960280895233154, Total loss : 85.06198120117188\n",
      "learning rate A :  tf.Tensor(9.852124e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1410 is 0.0782 sec\n",
      "train AE loss : 0.7592725157737732, train ANN loss : 3.140610933303833\n",
      "AE loss : 0.7313351035118103, ANN loss : 3.1149728298187256, Total loss : 76.24848175048828\n",
      "learning rate A :  tf.Tensor(9.851916e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1411 is 0.0774 sec\n",
      "train AE loss : 0.677156925201416, train ANN loss : 3.1558215618133545\n",
      "AE loss : 0.8071429133415222, ANN loss : 3.0976386070251465, Total loss : 83.81193542480469\n",
      "learning rate A :  tf.Tensor(9.851916e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1412 is 0.0804 sec\n",
      "train AE loss : 0.7476229667663574, train ANN loss : 3.138378143310547\n",
      "AE loss : 0.7209586501121521, ANN loss : 3.1170034408569336, Total loss : 75.2128677368164\n",
      "learning rate A :  tf.Tensor(9.8517085e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1413 is 0.0779 sec\n",
      "train AE loss : 0.6675463318824768, train ANN loss : 3.1524789333343506\n",
      "AE loss : 0.9526036977767944, ANN loss : 3.063225507736206, Total loss : 98.3235855102539\n",
      "learning rate A :  tf.Tensor(9.8517085e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1414 is 0.0794 sec\n",
      "train AE loss : 0.8835013508796692, train ANN loss : 3.114274263381958\n",
      "AE loss : 1.322715401649475, ANN loss : 3.036499261856079, Total loss : 135.30804443359375\n",
      "learning rate A :  tf.Tensor(9.8517085e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1415 is 0.0793 sec\n",
      "train AE loss : 1.2306220531463623, train ANN loss : 3.1042914390563965\n",
      "AE loss : 1.1381871700286865, ANN loss : 3.039536714553833, Total loss : 116.85826110839844\n",
      "learning rate A :  tf.Tensor(9.851501e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1416 is 0.0769 sec\n",
      "train AE loss : 1.0572844743728638, train ANN loss : 3.0973944664001465\n",
      "AE loss : 1.5224261283874512, ANN loss : 3.048509359359741, Total loss : 155.2911376953125\n",
      "learning rate A :  tf.Tensor(9.851501e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1417 is 0.0782 sec\n",
      "train AE loss : 1.4191945791244507, train ANN loss : 3.1326193809509277\n",
      "AE loss : 1.66921865940094, ANN loss : 3.061540365219116, Total loss : 169.9833984375\n",
      "learning rate A :  tf.Tensor(9.851501e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1418 is 0.0784 sec\n",
      "train AE loss : 1.5577716827392578, train ANN loss : 3.1515328884124756\n",
      "AE loss : 1.408186912536621, ANN loss : 3.0413472652435303, Total loss : 143.86004638671875\n",
      "learning rate A :  tf.Tensor(9.851293e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1419 is 0.0785 sec\n",
      "train AE loss : 1.311071753501892, train ANN loss : 3.1199748516082764\n",
      "AE loss : 1.3635445833206177, ANN loss : 3.0359063148498535, Total loss : 139.39036560058594\n",
      "learning rate A :  tf.Tensor(9.851293e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1420 is 0.0812 sec\n",
      "train AE loss : 1.268596887588501, train ANN loss : 3.104626417160034\n",
      "AE loss : 1.2110049724578857, ANN loss : 3.036597728729248, Total loss : 124.13709259033203\n",
      "learning rate A :  tf.Tensor(9.851293e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1421 is 0.0787 sec\n",
      "train AE loss : 1.1247310638427734, train ANN loss : 3.093380928039551\n",
      "AE loss : 1.0697437524795532, ANN loss : 3.0508015155792236, Total loss : 110.0251693725586\n",
      "learning rate A :  tf.Tensor(9.851293e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1422 is 0.0779 sec\n",
      "train AE loss : 0.9920549392700195, train ANN loss : 3.1077239513397217\n",
      "AE loss : 1.0168657302856445, ANN loss : 3.0608999729156494, Total loss : 104.74748229980469\n",
      "learning rate A :  tf.Tensor(9.851293e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1423 is 0.0771 sec\n",
      "train AE loss : 0.9423002600669861, train ANN loss : 3.107161521911621\n",
      "AE loss : 0.8920003771781921, ANN loss : 3.0788660049438477, Total loss : 92.27889251708984\n",
      "learning rate A :  tf.Tensor(9.8510856e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1424 is 0.0761 sec\n",
      "train AE loss : 0.8257056474685669, train ANN loss : 3.117586374282837\n",
      "AE loss : 0.9757919311523438, ANN loss : 3.0642197132110596, Total loss : 100.64341735839844\n",
      "learning rate A :  tf.Tensor(9.8510856e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1425 is 0.0790 sec\n",
      "train AE loss : 0.9037339091300964, train ANN loss : 3.1069729328155518\n",
      "AE loss : 0.8585496544837952, ANN loss : 3.082547903060913, Total loss : 88.9375228881836\n",
      "learning rate A :  tf.Tensor(9.850878e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1426 is 0.0762 sec\n",
      "train AE loss : 0.7943394184112549, train ANN loss : 3.1204097270965576\n",
      "AE loss : 0.7627332210540771, ANN loss : 3.1020843982696533, Total loss : 79.37541198730469\n",
      "learning rate A :  tf.Tensor(9.850671e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1427 is 0.0765 sec\n",
      "train AE loss : 0.7052724361419678, train ANN loss : 3.1402158737182617\n",
      "AE loss : 0.9939985871315002, ANN loss : 3.0544521808624268, Total loss : 102.45429992675781\n",
      "learning rate A :  tf.Tensor(9.850671e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1428 is 0.0777 sec\n",
      "train AE loss : 0.9206137657165527, train ANN loss : 3.103004217147827\n",
      "AE loss : 1.356250524520874, ANN loss : 3.035327196121216, Total loss : 138.66036987304688\n",
      "learning rate A :  tf.Tensor(9.850671e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1429 is 0.0793 sec\n",
      "train AE loss : 1.260087490081787, train ANN loss : 3.0972280502319336\n",
      "AE loss : 1.6506775617599487, ANN loss : 3.054635763168335, Total loss : 168.12237548828125\n",
      "learning rate A :  tf.Tensor(9.850671e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1430 is 0.0787 sec\n",
      "train AE loss : 1.537846326828003, train ANN loss : 3.1279759407043457\n",
      "AE loss : 1.3905038833618164, ANN loss : 3.0369744300842285, Total loss : 142.08737182617188\n",
      "learning rate A :  tf.Tensor(9.850463e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1431 is 0.0751 sec\n",
      "train AE loss : 1.2922836542129517, train ANN loss : 3.109769821166992\n",
      "AE loss : 1.1889526844024658, ANN loss : 3.0347204208374023, Total loss : 121.92999267578125\n",
      "learning rate A :  tf.Tensor(9.8502554e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1432 is 0.0766 sec\n",
      "train AE loss : 1.1028127670288086, train ANN loss : 3.094562530517578\n",
      "AE loss : 1.029866337776184, ANN loss : 3.041667938232422, Total loss : 106.0282974243164\n",
      "learning rate A :  tf.Tensor(9.850048e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1433 is 0.0774 sec\n",
      "train AE loss : 0.9538930654525757, train ANN loss : 3.1028995513916016\n",
      "AE loss : 0.9022752046585083, ANN loss : 3.0540428161621094, Total loss : 93.28157043457031\n",
      "learning rate A :  tf.Tensor(9.849841e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1434 is 0.0754 sec\n",
      "train AE loss : 0.834775984287262, train ANN loss : 3.1096904277801514\n",
      "AE loss : 1.190514087677002, ANN loss : 3.0357704162597656, Total loss : 122.0871810913086\n",
      "learning rate A :  tf.Tensor(9.849841e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1435 is 0.0772 sec\n",
      "train AE loss : 1.1040700674057007, train ANN loss : 3.106064796447754\n",
      "AE loss : 1.0308674573898315, ANN loss : 3.0407745838165283, Total loss : 106.12753295898438\n",
      "learning rate A :  tf.Tensor(9.849633e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1436 is 0.0753 sec\n",
      "train AE loss : 0.9546797275543213, train ANN loss : 3.0949792861938477\n",
      "AE loss : 0.9030230641365051, ANN loss : 3.0516250133514404, Total loss : 93.35394287109375\n",
      "learning rate A :  tf.Tensor(9.849425e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1437 is 0.0756 sec\n",
      "train AE loss : 0.8353278040885925, train ANN loss : 3.1094703674316406\n",
      "AE loss : 1.2460007667541504, ANN loss : 3.037692070007324, Total loss : 127.63777160644531\n",
      "learning rate A :  tf.Tensor(9.849425e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1438 is 0.0776 sec\n",
      "train AE loss : 1.1559338569641113, train ANN loss : 3.105924367904663\n",
      "AE loss : 1.0744292736053467, ANN loss : 3.038221836090088, Total loss : 110.48114776611328\n",
      "learning rate A :  tf.Tensor(9.849218e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1439 is 0.0776 sec\n",
      "train AE loss : 0.9952452182769775, train ANN loss : 3.0970115661621094\n",
      "AE loss : 0.9376265406608582, ANN loss : 3.0461578369140625, Total loss : 96.8088150024414\n",
      "learning rate A :  tf.Tensor(9.8490105e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1440 is 0.0757 sec\n",
      "train AE loss : 0.8674886226654053, train ANN loss : 3.105841636657715\n",
      "AE loss : 0.8271189332008362, ANN loss : 3.0582640171051025, Total loss : 85.77015686035156\n",
      "learning rate A :  tf.Tensor(9.848803e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1441 is 0.0762 sec\n",
      "train AE loss : 0.7645736336708069, train ANN loss : 3.104093551635742\n",
      "AE loss : 0.7366233468055725, ANN loss : 3.0726358890533447, Total loss : 76.73497772216797\n",
      "learning rate A :  tf.Tensor(9.848596e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1442 is 0.0772 sec\n",
      "train AE loss : 0.6805995106697083, train ANN loss : 3.1294853687286377\n",
      "AE loss : 1.1308659315109253, ANN loss : 3.0377378463745117, Total loss : 116.12432861328125\n",
      "learning rate A :  tf.Tensor(9.848596e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1443 is 0.0773 sec\n",
      "train AE loss : 1.0478816032409668, train ANN loss : 3.107790946960449\n",
      "AE loss : 1.5363839864730835, ANN loss : 3.0620241165161133, Total loss : 156.70042419433594\n",
      "learning rate A :  tf.Tensor(9.848596e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1444 is 0.0778 sec\n",
      "train AE loss : 1.4287444353103638, train ANN loss : 3.139505386352539\n",
      "AE loss : 1.6232848167419434, ANN loss : 3.0639383792877197, Total loss : 165.39242553710938\n",
      "learning rate A :  tf.Tensor(9.848596e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1445 is 0.0787 sec\n",
      "train AE loss : 1.5109429359436035, train ANN loss : 3.1431641578674316\n",
      "AE loss : 1.3597991466522217, ANN loss : 3.0408260822296143, Total loss : 139.020751953125\n",
      "learning rate A :  tf.Tensor(9.848596e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1446 is 0.0780 sec\n",
      "train AE loss : 1.2629677057266235, train ANN loss : 3.1097583770751953\n",
      "AE loss : 1.0651206970214844, ANN loss : 3.0585403442382812, Total loss : 109.57061004638672\n",
      "learning rate A :  tf.Tensor(9.848596e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1447 is 0.0776 sec\n",
      "train AE loss : 0.9871044754981995, train ANN loss : 3.108243703842163\n",
      "AE loss : 0.9078843593597412, ANN loss : 3.086843967437744, Total loss : 93.87528991699219\n",
      "learning rate A :  tf.Tensor(9.848596e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1448 is 0.0784 sec\n",
      "train AE loss : 0.8405433893203735, train ANN loss : 3.1320741176605225\n",
      "AE loss : 0.8016882538795471, ANN loss : 3.1068835258483887, Total loss : 83.27571105957031\n",
      "learning rate A :  tf.Tensor(9.8483884e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1449 is 0.0765 sec\n",
      "train AE loss : 0.7416959404945374, train ANN loss : 3.146855115890503\n",
      "AE loss : 0.7145023941993713, ANN loss : 3.1275835037231445, Total loss : 74.57781982421875\n",
      "learning rate A :  tf.Tensor(9.84818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1450 is 0.0763 sec\n",
      "train AE loss : 0.6608220934867859, train ANN loss : 3.16042160987854\n",
      "AE loss : 0.758566677570343, ANN loss : 3.1135435104370117, Total loss : 78.97020721435547\n",
      "learning rate A :  tf.Tensor(9.84818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1451 is 0.0797 sec\n",
      "train AE loss : 0.7019049525260925, train ANN loss : 3.1468141078948975\n",
      "AE loss : 0.9674462080001831, ANN loss : 3.0637307167053223, Total loss : 99.80834197998047\n",
      "learning rate A :  tf.Tensor(9.84818e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1452 is 0.0791 sec\n",
      "train AE loss : 0.8966143131256104, train ANN loss : 3.1078529357910156\n",
      "AE loss : 0.8508179187774658, ANN loss : 3.0825400352478027, Total loss : 88.1643295288086\n",
      "learning rate A :  tf.Tensor(9.847973e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1453 is 0.0773 sec\n",
      "train AE loss : 0.7879003882408142, train ANN loss : 3.123314380645752\n",
      "AE loss : 1.2081191539764404, ANN loss : 3.03666615486145, Total loss : 123.84857940673828\n",
      "learning rate A :  tf.Tensor(9.847973e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1454 is 0.0793 sec\n",
      "train AE loss : 1.1222379207611084, train ANN loss : 3.0985732078552246\n",
      "AE loss : 1.6093029975891113, ANN loss : 3.046020269393921, Total loss : 163.976318359375\n",
      "learning rate A :  tf.Tensor(9.847973e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1455 is 0.0792 sec\n",
      "train AE loss : 1.5004764795303345, train ANN loss : 3.121253728866577\n",
      "AE loss : 1.7686392068862915, ANN loss : 3.059164524078369, Total loss : 179.92308044433594\n",
      "learning rate A :  tf.Tensor(9.847973e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1456 is 0.0789 sec\n",
      "train AE loss : 1.651100754737854, train ANN loss : 3.15208101272583\n",
      "AE loss : 1.4786477088928223, ANN loss : 3.0361595153808594, Total loss : 150.90093994140625\n",
      "learning rate A :  tf.Tensor(9.8477656e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1457 is 0.0774 sec\n",
      "train AE loss : 1.376596450805664, train ANN loss : 3.1124398708343506\n",
      "AE loss : 1.4394629001617432, ANN loss : 3.0324807167053223, Total loss : 146.978759765625\n",
      "learning rate A :  tf.Tensor(9.8477656e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1458 is 0.0783 sec\n",
      "train AE loss : 1.3392633199691772, train ANN loss : 3.1094062328338623\n",
      "AE loss : 1.2888134717941284, ANN loss : 3.0341272354125977, Total loss : 131.91546630859375\n",
      "learning rate A :  tf.Tensor(9.8477656e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1459 is 0.0782 sec\n",
      "train AE loss : 1.1969999074935913, train ANN loss : 3.0971975326538086\n",
      "AE loss : 1.106502652168274, ANN loss : 3.042106866836548, Total loss : 113.69237518310547\n",
      "learning rate A :  tf.Tensor(9.847558e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1460 is 0.0776 sec\n",
      "train AE loss : 1.026093602180481, train ANN loss : 3.09466290473938\n",
      "AE loss : 1.0443859100341797, ANN loss : 3.054804563522339, Total loss : 107.49340057373047\n",
      "learning rate A :  tf.Tensor(9.847558e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1461 is 0.0776 sec\n",
      "train AE loss : 0.9677368998527527, train ANN loss : 3.104348659515381\n",
      "AE loss : 0.9116076231002808, ANN loss : 3.072023868560791, Total loss : 94.2327880859375\n",
      "learning rate A :  tf.Tensor(9.84735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1462 is 0.0772 sec\n",
      "train AE loss : 0.8437638878822327, train ANN loss : 3.11765193939209\n",
      "AE loss : 0.8042757511138916, ANN loss : 3.09130597114563, Total loss : 83.51888275146484\n",
      "learning rate A :  tf.Tensor(9.847143e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1463 is 0.0769 sec\n",
      "train AE loss : 0.7438932657241821, train ANN loss : 3.13162899017334\n",
      "AE loss : 0.7162353992462158, ANN loss : 3.111414670944214, Total loss : 74.73495483398438\n",
      "learning rate A :  tf.Tensor(9.8469354e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1464 is 0.0771 sec\n",
      "train AE loss : 0.6623172760009766, train ANN loss : 3.1504557132720947\n",
      "AE loss : 0.8302566409111023, ANN loss : 3.08437442779541, Total loss : 86.11003875732422\n",
      "learning rate A :  tf.Tensor(9.8469354e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1465 is 0.0794 sec\n",
      "train AE loss : 0.7678533792495728, train ANN loss : 3.1232423782348633\n",
      "AE loss : 1.0872588157653809, ANN loss : 3.04425048828125, Total loss : 111.77013397216797\n",
      "learning rate A :  tf.Tensor(9.8469354e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1466 is 0.0794 sec\n",
      "train AE loss : 1.0074480772018433, train ANN loss : 3.10146427154541\n",
      "AE loss : 0.9459090828895569, ANN loss : 3.0577287673950195, Total loss : 97.64865112304688\n",
      "learning rate A :  tf.Tensor(9.846728e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1467 is 0.0767 sec\n",
      "train AE loss : 0.8754585981369019, train ANN loss : 3.101597309112549\n",
      "AE loss : 1.3114382028579712, ANN loss : 3.0353641510009766, Total loss : 134.17918395996094\n",
      "learning rate A :  tf.Tensor(9.846728e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1468 is 0.0792 sec\n",
      "train AE loss : 1.217501163482666, train ANN loss : 3.101041078567505\n",
      "AE loss : 1.1238006353378296, ANN loss : 3.0371272563934326, Total loss : 115.41718292236328\n",
      "learning rate A :  tf.Tensor(9.846521e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1469 is 0.0774 sec\n",
      "train AE loss : 1.041559100151062, train ANN loss : 3.0946474075317383\n",
      "AE loss : 0.9752506017684937, ANN loss : 3.0466880798339844, Total loss : 100.5717544555664\n",
      "learning rate A :  tf.Tensor(9.846313e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1470 is 0.0767 sec\n",
      "train AE loss : 0.90279620885849, train ANN loss : 3.0940134525299072\n",
      "AE loss : 0.8560835719108582, ANN loss : 3.0605669021606445, Total loss : 88.66893005371094\n",
      "learning rate A :  tf.Tensor(9.846105e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1471 is 0.0765 sec\n",
      "train AE loss : 0.791818380355835, train ANN loss : 3.11147403717041\n",
      "AE loss : 0.7591317892074585, ANN loss : 3.076815128326416, Total loss : 78.98999786376953\n",
      "learning rate A :  tf.Tensor(9.845898e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1472 is 0.0781 sec\n",
      "train AE loss : 0.7016873955726624, train ANN loss : 3.123933792114258\n",
      "AE loss : 1.1772487163543701, ANN loss : 3.0369460582733154, Total loss : 120.7618179321289\n",
      "learning rate A :  tf.Tensor(9.845898e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1473 is 0.0784 sec\n",
      "train AE loss : 1.0916272401809692, train ANN loss : 3.098477840423584\n",
      "AE loss : 1.6305044889450073, ANN loss : 3.069869041442871, Total loss : 166.1203155517578\n",
      "learning rate A :  tf.Tensor(9.845898e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1474 is 0.0789 sec\n",
      "train AE loss : 1.5177595615386963, train ANN loss : 3.1535720825195312\n",
      "AE loss : 1.74745512008667, ANN loss : 3.070274829864502, Total loss : 177.8157958984375\n",
      "learning rate A :  tf.Tensor(9.845898e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1475 is 0.0801 sec\n",
      "train AE loss : 1.6287493705749512, train ANN loss : 3.151371717453003\n",
      "AE loss : 1.4581971168518066, ANN loss : 3.0419259071350098, Total loss : 148.8616485595703\n",
      "learning rate A :  tf.Tensor(9.8456905e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1476 is 0.0769 sec\n",
      "train AE loss : 1.3555108308792114, train ANN loss : 3.117185354232788\n",
      "AE loss : 1.236868143081665, ANN loss : 3.0325560569763184, Total loss : 126.71936798095703\n",
      "learning rate A :  tf.Tensor(9.845483e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1477 is 0.0782 sec\n",
      "train AE loss : 1.1477292776107788, train ANN loss : 3.0949859619140625\n",
      "AE loss : 1.0643454790115356, ANN loss : 3.0345067977905273, Total loss : 109.46904754638672\n",
      "learning rate A :  tf.Tensor(9.845275e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1478 is 0.0780 sec\n",
      "train AE loss : 0.986168622970581, train ANN loss : 3.0991299152374268\n",
      "AE loss : 0.9274173974990845, ANN loss : 3.043426990509033, Total loss : 95.78517150878906\n",
      "learning rate A :  tf.Tensor(9.845068e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1479 is 0.0766 sec\n",
      "train AE loss : 0.8582167029380798, train ANN loss : 3.0998635292053223\n",
      "AE loss : 0.8171496987342834, ANN loss : 3.0562875270843506, Total loss : 84.7712631225586\n",
      "learning rate A :  tf.Tensor(9.84486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1480 is 0.0782 sec\n",
      "train AE loss : 0.7554870247840881, train ANN loss : 3.107642650604248\n",
      "AE loss : 0.9718852639198303, ANN loss : 3.0405516624450684, Total loss : 100.22908782958984\n",
      "learning rate A :  tf.Tensor(9.84486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1481 is 0.0808 sec\n",
      "train AE loss : 0.899823009967804, train ANN loss : 3.0933971405029297\n",
      "AE loss : 1.187674641609192, ANN loss : 3.032464027404785, Total loss : 121.7999267578125\n",
      "learning rate A :  tf.Tensor(9.84486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1482 is 0.0784 sec\n",
      "train AE loss : 1.1017611026763916, train ANN loss : 3.086341142654419\n",
      "AE loss : 1.3864308595657349, ANN loss : 3.035299777984619, Total loss : 141.67837524414062\n",
      "learning rate A :  tf.Tensor(9.84486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1483 is 0.0784 sec\n",
      "train AE loss : 1.2883328199386597, train ANN loss : 3.103620767593384\n",
      "AE loss : 1.469820261001587, ANN loss : 3.0378992557525635, Total loss : 150.01992797851562\n",
      "learning rate A :  tf.Tensor(9.84486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1484 is 0.0789 sec\n",
      "train AE loss : 1.3668794631958008, train ANN loss : 3.1034350395202637\n",
      "AE loss : 1.4022032022476196, ANN loss : 3.0350687503814697, Total loss : 143.25538635253906\n",
      "learning rate A :  tf.Tensor(9.84486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1485 is 0.0806 sec\n",
      "train AE loss : 1.3032640218734741, train ANN loss : 3.0937106609344482\n",
      "AE loss : 1.1913717985153198, ANN loss : 3.0390865802764893, Total loss : 122.17626953125\n",
      "learning rate A :  tf.Tensor(9.844653e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1486 is 0.0760 sec\n",
      "train AE loss : 1.1053037643432617, train ANN loss : 3.085913896560669\n",
      "AE loss : 1.1621220111846924, ANN loss : 3.041335344314575, Total loss : 119.2535400390625\n",
      "learning rate A :  tf.Tensor(9.844653e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1487 is 0.0779 sec\n",
      "train AE loss : 1.0778323411941528, train ANN loss : 3.0952398777008057\n",
      "AE loss : 1.0034594535827637, ANN loss : 3.0550801753997803, Total loss : 103.40103912353516\n",
      "learning rate A :  tf.Tensor(9.8444456e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1488 is 0.0756 sec\n",
      "train AE loss : 0.929370105266571, train ANN loss : 3.0989990234375\n",
      "AE loss : 0.8767569661140442, ANN loss : 3.0725178718566895, Total loss : 90.74820709228516\n",
      "learning rate A :  tf.Tensor(9.844238e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1489 is 0.0755 sec\n",
      "train AE loss : 0.8111652135848999, train ANN loss : 3.119441032409668\n",
      "AE loss : 0.7741472125053406, ANN loss : 3.0917587280273438, Total loss : 80.50647735595703\n",
      "learning rate A :  tf.Tensor(9.84403e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1490 is 0.0768 sec\n",
      "train AE loss : 0.7157715559005737, train ANN loss : 3.128474712371826\n",
      "AE loss : 0.690005362033844, ANN loss : 3.1116559505462646, Total loss : 72.11219024658203\n",
      "learning rate A :  tf.Tensor(9.843823e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1491 is 0.0759 sec\n",
      "train AE loss : 0.6377588510513306, train ANN loss : 3.148648977279663\n",
      "AE loss : 0.8182327747344971, ANN loss : 3.0781378746032715, Total loss : 84.90142059326172\n",
      "learning rate A :  tf.Tensor(9.843823e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1492 is 0.0782 sec\n",
      "train AE loss : 0.7566244006156921, train ANN loss : 3.1243536472320557\n",
      "AE loss : 0.7265182137489319, ANN loss : 3.097360372543335, Total loss : 75.74919128417969\n",
      "learning rate A :  tf.Tensor(9.8436154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1493 is 0.0766 sec\n",
      "train AE loss : 0.6714192628860474, train ANN loss : 3.147123336791992\n",
      "AE loss : 1.0071765184402466, ANN loss : 3.045100450515747, Total loss : 103.76274871826172\n",
      "learning rate A :  tf.Tensor(9.8436154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1494 is 0.0783 sec\n",
      "train AE loss : 0.9326985478401184, train ANN loss : 3.0979278087615967\n",
      "AE loss : 1.4188629388809204, ANN loss : 3.037590742111206, Total loss : 144.92388916015625\n",
      "learning rate A :  tf.Tensor(9.8436154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1495 is 0.0778 sec\n",
      "train AE loss : 1.3182644844055176, train ANN loss : 3.1034367084503174\n",
      "AE loss : 1.7252906560897827, ANN loss : 3.061856746673584, Total loss : 175.59092712402344\n",
      "learning rate A :  tf.Tensor(9.8436154e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1496 is 0.0783 sec\n",
      "train AE loss : 1.6072055101394653, train ANN loss : 3.141035556793213\n",
      "AE loss : 1.4385250806808472, ANN loss : 3.0389158725738525, Total loss : 146.8914337158203\n",
      "learning rate A :  tf.Tensor(9.843408e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1497 is 0.0755 sec\n",
      "train AE loss : 1.336540699005127, train ANN loss : 3.101024866104126\n",
      "AE loss : 1.5466593503952026, ANN loss : 3.041062593460083, Total loss : 157.70700073242188\n",
      "learning rate A :  tf.Tensor(9.843408e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1498 is 0.0786 sec\n",
      "train AE loss : 1.43800687789917, train ANN loss : 3.1107614040374756\n",
      "AE loss : 1.4390244483947754, ANN loss : 3.0317442417144775, Total loss : 146.9342041015625\n",
      "learning rate A :  tf.Tensor(9.843408e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1499 is 0.0779 sec\n",
      "train AE loss : 1.3364812135696411, train ANN loss : 3.0881333351135254\n",
      "AE loss : 1.2187185287475586, ANN loss : 3.0309407711029053, Total loss : 124.90279388427734\n",
      "learning rate A :  tf.Tensor(9.843201e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1500 is 0.0756 sec\n",
      "train AE loss : 1.1296446323394775, train ANN loss : 3.0852110385894775\n",
      "AE loss : 1.1367263793945312, ANN loss : 3.0393083095550537, Total loss : 116.71194458007812\n",
      "learning rate A :  tf.Tensor(9.843201e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1501 is 0.0784 sec\n",
      "train AE loss : 1.0526716709136963, train ANN loss : 3.0940515995025635\n",
      "AE loss : 0.9817058444023132, ANN loss : 3.0534253120422363, Total loss : 101.22400665283203\n",
      "learning rate A :  tf.Tensor(9.842993e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1502 is 0.0762 sec\n",
      "train AE loss : 0.9078204035758972, train ANN loss : 3.09952712059021\n",
      "AE loss : 0.8580240607261658, ANN loss : 3.070878028869629, Total loss : 88.873291015625\n",
      "learning rate A :  tf.Tensor(9.842786e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1503 is 0.0771 sec\n",
      "train AE loss : 0.7925869226455688, train ANN loss : 3.1162636280059814\n",
      "AE loss : 0.9240974187850952, ANN loss : 3.0646111965179443, Total loss : 95.47435760498047\n",
      "learning rate A :  tf.Tensor(9.842786e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1504 is 0.0782 sec\n",
      "train AE loss : 0.8538710474967957, train ANN loss : 3.111896514892578\n",
      "AE loss : 1.114415168762207, ANN loss : 3.0418426990509033, Total loss : 114.48335266113281\n",
      "learning rate A :  tf.Tensor(9.842786e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1505 is 0.0786 sec\n",
      "train AE loss : 1.0311660766601562, train ANN loss : 3.089386463165283\n",
      "AE loss : 1.3842473030090332, ANN loss : 3.030038595199585, Total loss : 141.4547576904297\n",
      "learning rate A :  tf.Tensor(9.842786e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1506 is 0.0772 sec\n",
      "train AE loss : 1.2837895154953003, train ANN loss : 3.095155715942383\n",
      "AE loss : 1.5882116556167603, ANN loss : 3.0334153175354004, Total loss : 161.85458374023438\n",
      "learning rate A :  tf.Tensor(9.842786e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1507 is 0.0788 sec\n",
      "train AE loss : 1.4755288362503052, train ANN loss : 3.097149610519409\n",
      "AE loss : 1.3311536312103271, ANN loss : 3.0276455879211426, Total loss : 136.1429901123047\n",
      "learning rate A :  tf.Tensor(9.8425786e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1508 is 0.0759 sec\n",
      "train AE loss : 1.2337239980697632, train ANN loss : 3.08717942237854\n",
      "AE loss : 1.133577823638916, ANN loss : 3.033761978149414, Total loss : 116.39154815673828\n",
      "learning rate A :  tf.Tensor(9.842371e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1509 is 0.0765 sec\n",
      "train AE loss : 1.0485886335372925, train ANN loss : 3.0869879722595215\n",
      "AE loss : 1.3494831323623657, ANN loss : 3.0272419452667236, Total loss : 137.97555541992188\n",
      "learning rate A :  tf.Tensor(9.842371e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1510 is 0.0781 sec\n",
      "train AE loss : 1.2506216764450073, train ANN loss : 3.0901312828063965\n",
      "AE loss : 1.5059564113616943, ANN loss : 3.0298426151275635, Total loss : 153.62548828125\n",
      "learning rate A :  tf.Tensor(9.842371e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1511 is 0.0788 sec\n",
      "train AE loss : 1.3975368738174438, train ANN loss : 3.1047346591949463\n",
      "AE loss : 1.5008047819137573, ANN loss : 3.0270094871520996, Total loss : 153.10748291015625\n",
      "learning rate A :  tf.Tensor(9.842371e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1512 is 0.0774 sec\n",
      "train AE loss : 1.3925129175186157, train ANN loss : 3.0791547298431396\n",
      "AE loss : 1.2638741731643677, ANN loss : 3.027921438217163, Total loss : 129.41534423828125\n",
      "learning rate A :  tf.Tensor(9.842164e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1513 is 0.0773 sec\n",
      "train AE loss : 1.1698752641677856, train ANN loss : 3.0842020511627197\n",
      "AE loss : 1.0805344581604004, ANN loss : 3.0381052494049072, Total loss : 111.091552734375\n",
      "learning rate A :  tf.Tensor(9.8419565e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1514 is 0.0781 sec\n",
      "train AE loss : 0.9984632730484009, train ANN loss : 3.0934231281280518\n",
      "AE loss : 0.9362269043922424, ANN loss : 3.0538787841796875, Total loss : 96.67655944824219\n",
      "learning rate A :  tf.Tensor(9.841749e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1515 is 0.0768 sec\n",
      "train AE loss : 0.8640435338020325, train ANN loss : 3.104832887649536\n",
      "AE loss : 0.8209143877029419, ANN loss : 3.0722110271453857, Total loss : 85.16365051269531\n",
      "learning rate A :  tf.Tensor(9.841542e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1516 is 0.0778 sec\n",
      "train AE loss : 0.7568314075469971, train ANN loss : 3.121345043182373\n",
      "AE loss : 0.7271962761878967, ANN loss : 3.0915918350219727, Total loss : 75.81121826171875\n",
      "learning rate A :  tf.Tensor(9.841334e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1517 is 0.0779 sec\n",
      "train AE loss : 0.6699557304382324, train ANN loss : 3.1285560131073\n",
      "AE loss : 0.6500740051269531, ANN loss : 3.111293077468872, Total loss : 68.11869049072266\n",
      "learning rate A :  tf.Tensor(9.841127e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1518 is 0.0776 sec\n",
      "train AE loss : 0.5989741683006287, train ANN loss : 3.152066469192505\n",
      "AE loss : 0.586227297782898, ANN loss : 3.1306865215301514, Total loss : 61.75341796875\n",
      "learning rate A :  tf.Tensor(9.84092e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1519 is 0.0763 sec\n",
      "train AE loss : 0.5403596758842468, train ANN loss : 3.1716055870056152\n",
      "AE loss : 0.5327771902084351, ANN loss : 3.149402379989624, Total loss : 56.4271240234375\n",
      "learning rate A :  tf.Tensor(9.840712e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1520 is 0.0793 sec\n",
      "train AE loss : 0.491327166557312, train ANN loss : 3.1863372325897217\n",
      "AE loss : 0.4875524938106537, ANN loss : 3.167353630065918, Total loss : 51.92259979248047\n",
      "learning rate A :  tf.Tensor(9.840505e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1521 is 0.0767 sec\n",
      "train AE loss : 0.44984251260757446, train ANN loss : 3.2022573947906494\n",
      "AE loss : 0.4490032196044922, ANN loss : 3.1844189167022705, Total loss : 48.08474349975586\n",
      "learning rate A :  tf.Tensor(9.8402976e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1522 is 0.0779 sec\n",
      "train AE loss : 0.41448211669921875, train ANN loss : 3.212297201156616\n",
      "AE loss : 0.41601282358169556, ANN loss : 3.200427532196045, Total loss : 44.80171203613281\n",
      "learning rate A :  tf.Tensor(9.84009e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1523 is 0.0776 sec\n",
      "train AE loss : 0.38417544960975647, train ANN loss : 3.2296836376190186\n",
      "AE loss : 0.3874996602535248, ANN loss : 3.215519905090332, Total loss : 41.965484619140625\n",
      "learning rate A :  tf.Tensor(9.839883e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1524 is 0.0779 sec\n",
      "train AE loss : 0.35804495215415955, train ANN loss : 3.2409653663635254\n",
      "AE loss : 0.36266329884529114, ANN loss : 3.2297935485839844, Total loss : 39.496124267578125\n",
      "learning rate A :  tf.Tensor(9.8396755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1525 is 0.0770 sec\n",
      "train AE loss : 0.3352836072444916, train ANN loss : 3.261531114578247\n",
      "AE loss : 0.4846995174884796, ANN loss : 3.1631689071655273, Total loss : 51.63312530517578\n",
      "learning rate A :  tf.Tensor(9.8396755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1526 is 0.0803 sec\n",
      "train AE loss : 0.44717517495155334, train ANN loss : 3.1986873149871826\n",
      "AE loss : 0.4465968608856201, ANN loss : 3.179915428161621, Total loss : 47.839599609375\n",
      "learning rate A :  tf.Tensor(9.839468e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1527 is 0.0772 sec\n",
      "train AE loss : 0.4122224450111389, train ANN loss : 3.2146315574645996\n",
      "AE loss : 0.4139741361141205, ANN loss : 3.1956472396850586, Total loss : 44.59305953979492\n",
      "learning rate A :  tf.Tensor(9.839261e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1528 is 0.0775 sec\n",
      "train AE loss : 0.3822781443595886, train ANN loss : 3.2348196506500244\n",
      "AE loss : 0.7053881287574768, ANN loss : 3.081803321838379, Total loss : 73.62061309814453\n",
      "learning rate A :  tf.Tensor(9.839261e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1529 is 0.0811 sec\n",
      "train AE loss : 0.6497017741203308, train ANN loss : 3.1353955268859863\n",
      "AE loss : 0.6322945356369019, ANN loss : 3.0994155406951904, Total loss : 66.32887268066406\n",
      "learning rate A :  tf.Tensor(9.8390534e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1530 is 0.0789 sec\n",
      "train AE loss : 0.5825445652008057, train ANN loss : 3.1429381370544434\n",
      "AE loss : 1.1819275617599487, ANN loss : 3.0349299907684326, Total loss : 121.2276840209961\n",
      "learning rate A :  tf.Tensor(9.8390534e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1531 is 0.0792 sec\n",
      "train AE loss : 1.0919967889785767, train ANN loss : 3.094266891479492\n",
      "AE loss : 1.862007975578308, ANN loss : 3.111436128616333, Total loss : 189.31222534179688\n",
      "learning rate A :  tf.Tensor(9.8390534e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1532 is 0.0794 sec\n",
      "train AE loss : 1.7303433418273926, train ANN loss : 3.1884140968322754\n",
      "AE loss : 2.019441604614258, ANN loss : 3.120353937149048, Total loss : 205.0644989013672\n",
      "learning rate A :  tf.Tensor(9.8390534e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1533 is 0.0796 sec\n",
      "train AE loss : 1.8799763917922974, train ANN loss : 3.203397750854492\n",
      "AE loss : 1.5612194538116455, ANN loss : 3.040595531463623, Total loss : 159.16253662109375\n",
      "learning rate A :  tf.Tensor(9.8390534e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1534 is 0.0789 sec\n",
      "train AE loss : 1.4490411281585693, train ANN loss : 3.0992376804351807\n",
      "AE loss : 1.0803136825561523, ANN loss : 3.055304765701294, Total loss : 111.08666229248047\n",
      "learning rate A :  tf.Tensor(9.8390534e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1535 is 0.0796 sec\n",
      "train AE loss : 0.9995883107185364, train ANN loss : 3.106274366378784\n",
      "AE loss : 0.9354355931282043, ANN loss : 3.0731043815612793, Total loss : 96.61666870117188\n",
      "learning rate A :  tf.Tensor(9.838846e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1536 is 0.0770 sec\n",
      "train AE loss : 0.8645489811897278, train ANN loss : 3.1186928749084473\n",
      "AE loss : 0.743025541305542, ANN loss : 3.1288375854492188, Total loss : 77.43138885498047\n",
      "learning rate A :  tf.Tensor(9.838846e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1537 is 0.0790 sec\n",
      "train AE loss : 0.6863515973091125, train ANN loss : 3.1678385734558105\n",
      "AE loss : 0.7309646010398865, ANN loss : 3.1339147090911865, Total loss : 76.2303695678711\n",
      "learning rate A :  tf.Tensor(9.838846e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1538 is 0.0793 sec\n",
      "train AE loss : 0.6756441593170166, train ANN loss : 3.1654486656188965\n",
      "AE loss : 0.8842470049858093, ANN loss : 3.089564085006714, Total loss : 91.5142593383789\n",
      "learning rate A :  tf.Tensor(9.838846e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1539 is 0.0788 sec\n",
      "train AE loss : 0.8184634447097778, train ANN loss : 3.128607749938965\n",
      "AE loss : 0.7793199419975281, ANN loss : 3.111053466796875, Total loss : 81.04304504394531\n",
      "learning rate A :  tf.Tensor(9.838639e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1540 is 0.0773 sec\n",
      "train AE loss : 0.7209261655807495, train ANN loss : 3.1454598903656006\n",
      "AE loss : 0.6934612989425659, ANN loss : 3.1325161457061768, Total loss : 72.47864532470703\n",
      "learning rate A :  tf.Tensor(9.838432e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1541 is 0.0780 sec\n",
      "train AE loss : 0.6414968967437744, train ANN loss : 3.170365571975708\n",
      "AE loss : 0.9919694662094116, ANN loss : 3.0611066818237305, Total loss : 102.25804901123047\n",
      "learning rate A :  tf.Tensor(9.838432e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1542 is 0.0930 sec\n",
      "train AE loss : 0.9193973541259766, train ANN loss : 3.1063036918640137\n",
      "AE loss : 0.8669533729553223, ANN loss : 3.079580068588257, Total loss : 89.77491760253906\n",
      "learning rate A :  tf.Tensor(9.838225e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1543 is 0.0776 sec\n",
      "train AE loss : 0.8029012084007263, train ANN loss : 3.130237579345703\n",
      "AE loss : 0.7656354308128357, ANN loss : 3.099160671234131, Total loss : 79.66270446777344\n",
      "learning rate A :  tf.Tensor(9.838017e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1544 is 0.0779 sec\n",
      "train AE loss : 0.70880126953125, train ANN loss : 3.1512696743011475\n",
      "AE loss : 1.2206584215164185, ANN loss : 3.0379252433776855, Total loss : 125.10377502441406\n",
      "learning rate A :  tf.Tensor(9.838017e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1545 is 0.0799 sec\n",
      "train AE loss : 1.1334940195083618, train ANN loss : 3.109938383102417\n",
      "AE loss : 1.7730168104171753, ANN loss : 3.059346914291382, Total loss : 180.3610076904297\n",
      "learning rate A :  tf.Tensor(9.838017e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1546 is 0.0800 sec\n",
      "train AE loss : 1.6533598899841309, train ANN loss : 3.150222063064575\n",
      "AE loss : 2.033994674682617, ANN loss : 3.0852696895599365, Total loss : 206.4847412109375\n",
      "learning rate A :  tf.Tensor(9.838017e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1547 is 0.0801 sec\n",
      "train AE loss : 1.9001829624176025, train ANN loss : 3.1662373542785645\n",
      "AE loss : 1.7982921600341797, ANN loss : 3.0493011474609375, Total loss : 182.8785400390625\n",
      "learning rate A :  tf.Tensor(9.838017e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1548 is 0.0794 sec\n",
      "train AE loss : 1.6762051582336426, train ANN loss : 3.124143123626709\n",
      "AE loss : 1.4882416725158691, ANN loss : 3.0304362773895264, Total loss : 151.8546142578125\n",
      "learning rate A :  tf.Tensor(9.837809e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1549 is 0.0800 sec\n",
      "train AE loss : 1.3837411403656006, train ANN loss : 3.0932981967926025\n",
      "AE loss : 1.234323501586914, ANN loss : 3.038145065307617, Total loss : 126.47048950195312\n",
      "learning rate A :  tf.Tensor(9.837809e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1550 is 0.0787 sec\n",
      "train AE loss : 1.1448352336883545, train ANN loss : 3.0911829471588135\n",
      "AE loss : 1.056291103363037, ANN loss : 3.050384759902954, Total loss : 108.67949676513672\n",
      "learning rate A :  tf.Tensor(9.837602e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1551 is 0.0766 sec\n",
      "train AE loss : 0.9785789251327515, train ANN loss : 3.1059272289276123\n",
      "AE loss : 0.9158990383148193, ANN loss : 3.0673701763153076, Total loss : 94.65726470947266\n",
      "learning rate A :  tf.Tensor(9.837395e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1552 is 0.0800 sec\n",
      "train AE loss : 0.8479248881340027, train ANN loss : 3.1179988384246826\n",
      "AE loss : 0.803419291973114, ANN loss : 3.0863707065582275, Total loss : 83.42829895019531\n",
      "learning rate A :  tf.Tensor(9.837188e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1553 is 0.0766 sec\n",
      "train AE loss : 0.7435004711151123, train ANN loss : 3.1234817504882812\n",
      "AE loss : 0.7117174863815308, ANN loss : 3.106480360031128, Total loss : 74.27822875976562\n",
      "learning rate A :  tf.Tensor(9.8369805e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1554 is 0.0774 sec\n",
      "train AE loss : 0.6588119864463806, train ANN loss : 3.144561767578125\n",
      "AE loss : 0.6366438865661621, ANN loss : 3.126579761505127, Total loss : 66.79096984863281\n",
      "learning rate A :  tf.Tensor(9.836773e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1555 is 0.0776 sec\n",
      "train AE loss : 0.5895200967788696, train ANN loss : 3.1618807315826416\n",
      "AE loss : 0.5743316411972046, ANN loss : 3.146240234375, Total loss : 60.57940673828125\n",
      "learning rate A :  tf.Tensor(9.8365665e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1556 is 0.0762 sec\n",
      "train AE loss : 0.5319753289222717, train ANN loss : 3.184544324874878\n",
      "AE loss : 0.5827834606170654, ANN loss : 3.1494414806365967, Total loss : 61.42778778076172\n",
      "learning rate A :  tf.Tensor(9.8365665e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1557 is 0.0783 sec\n",
      "train AE loss : 0.5396406650543213, train ANN loss : 3.1887800693511963\n",
      "AE loss : 0.5290762186050415, ANN loss : 3.168550968170166, Total loss : 56.076175689697266\n",
      "learning rate A :  tf.Tensor(9.836359e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1558 is 0.0773 sec\n",
      "train AE loss : 0.49006354808807373, train ANN loss : 3.206908941268921\n",
      "AE loss : 0.6701073050498962, ANN loss : 3.1147329807281494, Total loss : 70.1254653930664\n",
      "learning rate A :  tf.Tensor(9.836359e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1559 is 0.0781 sec\n",
      "train AE loss : 0.6202438473701477, train ANN loss : 3.157486915588379\n",
      "AE loss : 0.6021972894668579, ANN loss : 3.1334500312805176, Total loss : 63.353179931640625\n",
      "learning rate A :  tf.Tensor(9.836151e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1560 is 0.0756 sec\n",
      "train AE loss : 0.5576238632202148, train ANN loss : 3.1660335063934326\n",
      "AE loss : 0.8934042453765869, ANN loss : 3.061786651611328, Total loss : 92.40220642089844\n",
      "learning rate A :  tf.Tensor(9.836151e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1561 is 0.0782 sec\n",
      "train AE loss : 0.8267565965652466, train ANN loss : 3.1068201065063477\n",
      "AE loss : 0.7853339314460754, ANN loss : 3.076616048812866, Total loss : 81.6100082397461\n",
      "learning rate A :  tf.Tensor(9.8359444e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1562 is 0.0757 sec\n",
      "train AE loss : 0.7267941236495972, train ANN loss : 3.121532678604126\n",
      "AE loss : 0.6975505948066711, ANN loss : 3.0927672386169434, Total loss : 72.84782409667969\n",
      "learning rate A :  tf.Tensor(9.835737e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1563 is 0.0761 sec\n",
      "train AE loss : 0.6457853317260742, train ANN loss : 3.1288318634033203\n",
      "AE loss : 1.1261601448059082, ANN loss : 3.0478711128234863, Total loss : 115.66388702392578\n",
      "learning rate A :  tf.Tensor(9.835737e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1564 is 0.0877 sec\n",
      "train AE loss : 1.0432971715927124, train ANN loss : 3.1149091720581055\n",
      "AE loss : 1.6227328777313232, ANN loss : 3.0880308151245117, Total loss : 165.361328125\n",
      "learning rate A :  tf.Tensor(9.835737e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1565 is 0.0789 sec\n",
      "train AE loss : 1.5087507963180542, train ANN loss : 3.1518125534057617\n",
      "AE loss : 1.3556863069534302, ANN loss : 3.0610594749450684, Total loss : 138.6296844482422\n",
      "learning rate A :  tf.Tensor(9.8355296e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1566 is 0.1208 sec\n",
      "train AE loss : 1.2579312324523926, train ANN loss : 3.123843193054199\n",
      "AE loss : 1.1511589288711548, ANN loss : 3.050577163696289, Total loss : 118.16645812988281\n",
      "learning rate A :  tf.Tensor(9.835323e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1567 is 0.0937 sec\n",
      "train AE loss : 1.0669279098510742, train ANN loss : 3.107921600341797\n",
      "AE loss : 1.522883653640747, ANN loss : 3.0679564476013184, Total loss : 155.3563232421875\n",
      "learning rate A :  tf.Tensor(9.835323e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1568 is 0.1063 sec\n",
      "train AE loss : 1.4151884317398071, train ANN loss : 3.134396553039551\n",
      "AE loss : 1.2793034315109253, ANN loss : 3.0480868816375732, Total loss : 130.97842407226562\n",
      "learning rate A :  tf.Tensor(9.8351156e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1569 is 0.0910 sec\n",
      "train AE loss : 1.1868813037872314, train ANN loss : 3.112780809402466\n",
      "AE loss : 1.503571629524231, ANN loss : 3.0473527908325195, Total loss : 153.40451049804688\n",
      "learning rate A :  tf.Tensor(9.8351156e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1570 is 0.0914 sec\n",
      "train AE loss : 1.3973729610443115, train ANN loss : 3.122234582901001\n",
      "AE loss : 1.5058594942092896, ANN loss : 3.0333714485168457, Total loss : 153.61932373046875\n",
      "learning rate A :  tf.Tensor(9.8351156e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1571 is 0.0846 sec\n",
      "train AE loss : 1.399613857269287, train ANN loss : 3.094649076461792\n",
      "AE loss : 1.358486533164978, ANN loss : 3.027866840362549, Total loss : 138.8765106201172\n",
      "learning rate A :  tf.Tensor(9.8351156e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1572 is 0.0795 sec\n",
      "train AE loss : 1.26148521900177, train ANN loss : 3.0873427391052246\n",
      "AE loss : 1.2085269689559937, ANN loss : 3.041221857070923, Total loss : 123.8939208984375\n",
      "learning rate A :  tf.Tensor(9.8351156e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1573 is 0.0782 sec\n",
      "train AE loss : 1.121181607246399, train ANN loss : 3.096205472946167\n",
      "AE loss : 1.0336204767227173, ANN loss : 3.0569541454315186, Total loss : 106.41899871826172\n",
      "learning rate A :  tf.Tensor(9.834908e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1574 is 0.0778 sec\n",
      "train AE loss : 0.9579793214797974, train ANN loss : 3.1055421829223633\n",
      "AE loss : 1.0117117166519165, ANN loss : 3.070303440093994, Total loss : 104.2414779663086\n",
      "learning rate A :  tf.Tensor(9.834908e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1575 is 0.0782 sec\n",
      "train AE loss : 0.9375069737434387, train ANN loss : 3.1191601753234863\n",
      "AE loss : 0.8781858682632446, ANN loss : 3.0913643836975098, Total loss : 90.90996551513672\n",
      "learning rate A :  tf.Tensor(9.834701e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1576 is 0.0764 sec\n",
      "train AE loss : 0.8132675290107727, train ANN loss : 3.131629467010498\n",
      "AE loss : 0.7710701823234558, ANN loss : 3.113605260848999, Total loss : 80.22061920166016\n",
      "learning rate A :  tf.Tensor(9.834493e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1577 is 0.0780 sec\n",
      "train AE loss : 0.7139170169830322, train ANN loss : 3.154022216796875\n",
      "AE loss : 0.6841910481452942, ANN loss : 3.136004686355591, Total loss : 71.55510711669922\n",
      "learning rate A :  tf.Tensor(9.8342854e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1578 is 0.0774 sec\n",
      "train AE loss : 0.6336486339569092, train ANN loss : 3.1761529445648193\n",
      "AE loss : 0.8210879564285278, ANN loss : 3.10343861579895, Total loss : 85.21223449707031\n",
      "learning rate A :  tf.Tensor(9.8342854e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1579 is 0.0790 sec\n",
      "train AE loss : 0.7602608799934387, train ANN loss : 3.149113178253174\n",
      "AE loss : 0.7248424887657166, ANN loss : 3.1256818771362305, Total loss : 75.60992431640625\n",
      "learning rate A :  tf.Tensor(9.834078e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1580 is 0.0772 sec\n",
      "train AE loss : 0.6711927652359009, train ANN loss : 3.168945074081421\n",
      "AE loss : 0.6463412046432495, ANN loss : 3.1476361751556396, Total loss : 67.78174591064453\n",
      "learning rate A :  tf.Tensor(9.8338714e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1581 is 0.0768 sec\n",
      "train AE loss : 0.5986437797546387, train ANN loss : 3.1873555183410645\n",
      "AE loss : 0.9385620355606079, ANN loss : 3.0737743377685547, Total loss : 96.92996978759766\n",
      "learning rate A :  tf.Tensor(9.8338714e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1582 is 0.0830 sec\n",
      "train AE loss : 0.8697565793991089, train ANN loss : 3.1145689487457275\n",
      "AE loss : 1.4365501403808594, ANN loss : 3.034090042114258, Total loss : 146.68910217285156\n",
      "learning rate A :  tf.Tensor(9.8338714e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1583 is 0.0812 sec\n",
      "train AE loss : 1.3351759910583496, train ANN loss : 3.100105047225952\n",
      "AE loss : 1.2091894149780273, ANN loss : 3.0399513244628906, Total loss : 123.95890045166016\n",
      "learning rate A :  tf.Tensor(9.833664e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1584 is 0.0774 sec\n",
      "train AE loss : 1.1223540306091309, train ANN loss : 3.093343734741211\n",
      "AE loss : 1.033841848373413, ANN loss : 3.0535085201263428, Total loss : 106.43768310546875\n",
      "learning rate A :  tf.Tensor(9.833457e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1585 is 0.0766 sec\n",
      "train AE loss : 0.9587100148200989, train ANN loss : 3.0982840061187744\n",
      "AE loss : 0.8962221741676331, ANN loss : 3.0707011222839355, Total loss : 92.69290924072266\n",
      "learning rate A :  tf.Tensor(9.83325e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1586 is 0.0781 sec\n",
      "train AE loss : 0.8306605815887451, train ANN loss : 3.1120893955230713\n",
      "AE loss : 0.7860875129699707, ANN loss : 3.089648485183716, Total loss : 81.69840240478516\n",
      "learning rate A :  tf.Tensor(9.833043e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1587 is 0.0769 sec\n",
      "train AE loss : 0.7284381985664368, train ANN loss : 3.1350975036621094\n",
      "AE loss : 1.3458960056304932, ANN loss : 3.0346548557281494, Total loss : 137.62425231933594\n",
      "learning rate A :  tf.Tensor(9.833043e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1588 is 0.0877 sec\n",
      "train AE loss : 1.2503314018249512, train ANN loss : 3.0963072776794434\n",
      "AE loss : 2.010953903198242, ANN loss : 3.081853151321411, Total loss : 204.17723083496094\n",
      "learning rate A :  tf.Tensor(9.833043e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1589 is 0.0803 sec\n",
      "train AE loss : 1.8769514560699463, train ANN loss : 3.1586880683898926\n",
      "AE loss : 1.6370952129364014, ANN loss : 3.0459845066070557, Total loss : 166.75550842285156\n",
      "learning rate A :  tf.Tensor(9.832835e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1590 is 0.0770 sec\n",
      "train AE loss : 1.5237455368041992, train ANN loss : 3.120299816131592\n",
      "AE loss : 2.0501315593719482, ANN loss : 3.085984945297241, Total loss : 208.09912109375\n",
      "learning rate A :  tf.Tensor(9.832835e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1591 is 0.0796 sec\n",
      "train AE loss : 1.913314700126648, train ANN loss : 3.1721670627593994\n",
      "AE loss : 1.957068681716919, ANN loss : 3.065363883972168, Total loss : 198.772216796875\n",
      "learning rate A :  tf.Tensor(9.832835e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1592 is 0.0795 sec\n",
      "train AE loss : 1.8247257471084595, train ANN loss : 3.1364822387695312\n",
      "AE loss : 1.546884536743164, ANN loss : 3.034533739089966, Total loss : 157.7229766845703\n",
      "learning rate A :  tf.Tensor(9.832835e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1593 is 0.0792 sec\n",
      "train AE loss : 1.4376760721206665, train ANN loss : 3.0829660892486572\n",
      "AE loss : 1.161818504333496, ANN loss : 3.0526981353759766, Total loss : 119.23454284667969\n",
      "learning rate A :  tf.Tensor(9.832835e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1594 is 0.0783 sec\n",
      "train AE loss : 1.0772180557250977, train ANN loss : 3.1027235984802246\n",
      "AE loss : 0.9406442642211914, ANN loss : 3.088430404663086, Total loss : 97.15284729003906\n",
      "learning rate A :  tf.Tensor(9.832835e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1595 is 0.0799 sec\n",
      "train AE loss : 0.8712698817253113, train ANN loss : 3.1354315280914307\n",
      "AE loss : 0.8803794384002686, ANN loss : 3.098311185836792, Total loss : 91.1362533569336\n",
      "learning rate A :  tf.Tensor(9.832835e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1596 is 0.0785 sec\n",
      "train AE loss : 0.815183699131012, train ANN loss : 3.139699697494507\n",
      "AE loss : 0.9558131098747253, ANN loss : 3.076162338256836, Total loss : 98.657470703125\n",
      "learning rate A :  tf.Tensor(9.832835e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1597 is 0.0792 sec\n",
      "train AE loss : 0.8853495121002197, train ANN loss : 3.1170785427093506\n",
      "AE loss : 0.8327895402908325, ANN loss : 3.098057985305786, Total loss : 86.37700653076172\n",
      "learning rate A :  tf.Tensor(9.832628e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1598 is 0.0776 sec\n",
      "train AE loss : 0.7710561156272888, train ANN loss : 3.1328823566436768\n",
      "AE loss : 1.0432839393615723, ANN loss : 3.0523955821990967, Total loss : 107.38079071044922\n",
      "learning rate A :  tf.Tensor(9.832628e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1599 is 0.0785 sec\n",
      "train AE loss : 0.9667999744415283, train ANN loss : 3.0968117713928223\n",
      "AE loss : 1.3784008026123047, ANN loss : 3.03102707862854, Total loss : 140.87110900878906\n",
      "learning rate A :  tf.Tensor(9.832628e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1600 is 0.0785 sec\n",
      "train AE loss : 1.2797088623046875, train ANN loss : 3.088992118835449\n",
      "AE loss : 1.7120360136032104, ANN loss : 3.0449469089508057, Total loss : 174.24856567382812\n",
      "learning rate A :  tf.Tensor(9.832628e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1601 is 0.0806 sec\n",
      "train AE loss : 1.5936691761016846, train ANN loss : 3.113619565963745\n",
      "AE loss : 1.8433830738067627, ANN loss : 3.0480172634124756, Total loss : 187.3863067626953\n",
      "learning rate A :  tf.Tensor(9.832628e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1602 is 0.0781 sec\n",
      "train AE loss : 1.7174314260482788, train ANN loss : 3.1117470264434814\n",
      "AE loss : 1.7222607135772705, ANN loss : 3.028005599975586, Total loss : 175.2540740966797\n",
      "learning rate A :  tf.Tensor(9.832628e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1603 is 0.0782 sec\n",
      "train AE loss : 1.6027824878692627, train ANN loss : 3.092914342880249\n",
      "AE loss : 1.4693228006362915, ANN loss : 3.0223701000213623, Total loss : 149.95465087890625\n",
      "learning rate A :  tf.Tensor(9.832628e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1604 is 0.0792 sec\n",
      "train AE loss : 1.3642927408218384, train ANN loss : 3.083526849746704\n",
      "AE loss : 1.2321288585662842, ANN loss : 3.0330584049224854, Total loss : 126.2459487915039\n",
      "learning rate A :  tf.Tensor(9.8324206e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1605 is 0.0771 sec\n",
      "train AE loss : 1.1421799659729004, train ANN loss : 3.0910165309906006\n",
      "AE loss : 1.1116005182266235, ANN loss : 3.054678440093994, Total loss : 114.21472930908203\n",
      "learning rate A :  tf.Tensor(9.8324206e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1606 is 0.0785 sec\n",
      "train AE loss : 1.02947199344635, train ANN loss : 3.0978755950927734\n",
      "AE loss : 1.1220223903656006, ANN loss : 3.060657501220703, Total loss : 115.26289367675781\n",
      "learning rate A :  tf.Tensor(9.8324206e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1607 is 0.0799 sec\n",
      "train AE loss : 1.0388035774230957, train ANN loss : 3.1038873195648193\n",
      "AE loss : 1.251865267753601, ANN loss : 3.046919107437134, Total loss : 128.2334442138672\n",
      "learning rate A :  tf.Tensor(9.8324206e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1608 is 0.0791 sec\n",
      "train AE loss : 1.1595420837402344, train ANN loss : 3.0939974784851074\n",
      "AE loss : 1.063744306564331, ANN loss : 3.0679333209991455, Total loss : 109.44236755371094\n",
      "learning rate A :  tf.Tensor(9.832214e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1609 is 0.0769 sec\n",
      "train AE loss : 0.9842977523803711, train ANN loss : 3.106679677963257\n",
      "AE loss : 0.9167928695678711, ANN loss : 3.0917303562164307, Total loss : 94.7710189819336\n",
      "learning rate A :  tf.Tensor(9.8320066e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1610 is 0.0779 sec\n",
      "train AE loss : 0.8478074073791504, train ANN loss : 3.1286840438842773\n",
      "AE loss : 0.8000552654266357, ANN loss : 3.1162703037261963, Total loss : 83.12178802490234\n",
      "learning rate A :  tf.Tensor(9.8318e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1611 is 0.0790 sec\n",
      "train AE loss : 0.7396008372306824, train ANN loss : 3.156200885772705\n",
      "AE loss : 0.7060720324516296, ANN loss : 3.140745162963867, Total loss : 73.74794006347656\n",
      "learning rate A :  tf.Tensor(9.831592e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1612 is 0.0773 sec\n",
      "train AE loss : 0.6528269648551941, train ANN loss : 3.1723737716674805\n",
      "AE loss : 0.968816339969635, ANN loss : 3.0713307857513428, Total loss : 99.95297241210938\n",
      "learning rate A :  tf.Tensor(9.831592e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1613 is 0.0787 sec\n",
      "train AE loss : 0.8959664106369019, train ANN loss : 3.1090638637542725\n",
      "AE loss : 0.841694176197052, ANN loss : 3.0942728519439697, Total loss : 87.2636947631836\n",
      "learning rate A :  tf.Tensor(9.831385e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1614 is 0.0757 sec\n",
      "train AE loss : 0.7780381441116333, train ANN loss : 3.1236274242401123\n",
      "AE loss : 1.2949354648590088, ANN loss : 3.0278618335723877, Total loss : 132.5214080810547\n",
      "learning rate A :  tf.Tensor(9.831385e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1615 is 0.0777 sec\n",
      "train AE loss : 1.199389934539795, train ANN loss : 3.0792083740234375\n",
      "AE loss : 1.8733813762664795, ANN loss : 3.0408742427825928, Total loss : 190.37901306152344\n",
      "learning rate A :  tf.Tensor(9.831385e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1616 is 0.0793 sec\n",
      "train AE loss : 1.742633581161499, train ANN loss : 3.1091856956481934\n",
      "AE loss : 1.5323821306228638, ANN loss : 3.0250701904296875, Total loss : 156.26329040527344\n",
      "learning rate A :  tf.Tensor(9.831178e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1617 is 0.0755 sec\n",
      "train AE loss : 1.4216867685317993, train ANN loss : 3.084294319152832\n",
      "AE loss : 1.278249740600586, ANN loss : 3.0258629322052, Total loss : 130.850830078125\n",
      "learning rate A :  tf.Tensor(9.8309705e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1618 is 0.0757 sec\n",
      "train AE loss : 1.1838022470474243, train ANN loss : 3.0792462825775146\n",
      "AE loss : 1.8021173477172852, ANN loss : 3.046886682510376, Total loss : 183.2586212158203\n",
      "learning rate A :  tf.Tensor(9.8309705e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1619 is 0.0783 sec\n",
      "train AE loss : 1.6752618551254272, train ANN loss : 3.1213254928588867\n",
      "AE loss : 2.098395586013794, ANN loss : 3.0788397789001465, Total loss : 212.91839599609375\n",
      "learning rate A :  tf.Tensor(9.8309705e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1620 is 0.0783 sec\n",
      "train AE loss : 1.9547525644302368, train ANN loss : 3.162108898162842\n",
      "AE loss : 1.69478178024292, ANN loss : 3.0407097339630127, Total loss : 172.51889038085938\n",
      "learning rate A :  tf.Tensor(9.830764e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1621 is 0.0758 sec\n",
      "train AE loss : 1.5742131471633911, train ANN loss : 3.104480266571045\n",
      "AE loss : 1.7400354146957397, ANN loss : 3.0380916595458984, Total loss : 177.0416259765625\n",
      "learning rate A :  tf.Tensor(9.830764e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1622 is 0.0779 sec\n",
      "train AE loss : 1.616821050643921, train ANN loss : 3.0974771976470947\n",
      "AE loss : 1.5822700262069702, ANN loss : 3.0256168842315674, Total loss : 161.25262451171875\n",
      "learning rate A :  tf.Tensor(9.830764e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1623 is 0.0774 sec\n",
      "train AE loss : 1.468496322631836, train ANN loss : 3.0679876804351807\n",
      "AE loss : 1.380268931388855, ANN loss : 3.028662919998169, Total loss : 141.0555419921875\n",
      "learning rate A :  tf.Tensor(9.830764e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1624 is 0.0770 sec\n",
      "train AE loss : 1.279237985610962, train ANN loss : 3.071758508682251\n",
      "AE loss : 1.230205774307251, ANN loss : 3.0420219898223877, Total loss : 126.0625991821289\n",
      "learning rate A :  tf.Tensor(9.830764e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1625 is 0.0791 sec\n",
      "train AE loss : 1.1392427682876587, train ANN loss : 3.0868618488311768\n",
      "AE loss : 1.1795666217803955, ANN loss : 3.0472660064697266, Total loss : 121.0039291381836\n",
      "learning rate A :  tf.Tensor(9.830764e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1626 is 0.0777 sec\n",
      "train AE loss : 1.092219591140747, train ANN loss : 3.091780424118042\n",
      "AE loss : 1.0062806606292725, ANN loss : 3.0685336589813232, Total loss : 103.69660186767578\n",
      "learning rate A :  tf.Tensor(9.8305565e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1627 is 0.0760 sec\n",
      "train AE loss : 0.9311708211898804, train ANN loss : 3.1107380390167236\n",
      "AE loss : 1.0917553901672363, ANN loss : 3.051267385482788, Total loss : 112.22679901123047\n",
      "learning rate A :  tf.Tensor(9.8305565e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1628 is 0.0782 sec\n",
      "train AE loss : 1.0107289552688599, train ANN loss : 3.101860284805298\n",
      "AE loss : 0.9377542734146118, ANN loss : 3.0734918117523193, Total loss : 96.84892272949219\n",
      "learning rate A :  tf.Tensor(9.830349e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1629 is 0.0756 sec\n",
      "train AE loss : 0.8677259087562561, train ANN loss : 3.1113016605377197\n",
      "AE loss : 1.1559497117996216, ANN loss : 3.0366852283477783, Total loss : 118.63166046142578\n",
      "learning rate A :  tf.Tensor(9.830349e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1630 is 0.0770 sec\n",
      "train AE loss : 1.0707247257232666, train ANN loss : 3.079906702041626\n",
      "AE loss : 0.9881476759910583, ANN loss : 3.056488037109375, Total loss : 101.87125396728516\n",
      "learning rate A :  tf.Tensor(9.830142e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1631 is 0.0762 sec\n",
      "train AE loss : 0.9147912859916687, train ANN loss : 3.1023929119110107\n",
      "AE loss : 1.3286339044570923, ANN loss : 3.022664785385132, Total loss : 135.88604736328125\n",
      "learning rate A :  tf.Tensor(9.830142e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1632 is 0.0770 sec\n",
      "train AE loss : 1.2320637702941895, train ANN loss : 3.0789268016815186\n",
      "AE loss : 1.7286549806594849, ANN loss : 3.0307538509368896, Total loss : 175.896240234375\n",
      "learning rate A :  tf.Tensor(9.830142e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1633 is 0.0772 sec\n",
      "train AE loss : 1.6073899269104004, train ANN loss : 3.0934534072875977\n",
      "AE loss : 1.4233731031417847, ANN loss : 3.0233154296875, Total loss : 145.36062622070312\n",
      "learning rate A :  tf.Tensor(9.829935e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1634 is 0.0770 sec\n",
      "train AE loss : 1.3209216594696045, train ANN loss : 3.0776708126068115\n",
      "AE loss : 1.750832438468933, ANN loss : 3.0346574783325195, Total loss : 178.11788940429688\n",
      "learning rate A :  tf.Tensor(9.829935e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1635 is 0.0788 sec\n",
      "train AE loss : 1.6284337043762207, train ANN loss : 3.1027941703796387\n",
      "AE loss : 1.4392364025115967, ANN loss : 3.023984432220459, Total loss : 146.9476318359375\n",
      "learning rate A :  tf.Tensor(9.829728e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1636 is 0.0768 sec\n",
      "train AE loss : 1.3359344005584717, train ANN loss : 3.089172601699829\n",
      "AE loss : 1.680397629737854, ANN loss : 3.028550386428833, Total loss : 171.0683135986328\n",
      "learning rate A :  tf.Tensor(9.829728e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1637 is 0.0789 sec\n",
      "train AE loss : 1.5621634721755981, train ANN loss : 3.089648962020874\n",
      "AE loss : 1.7419673204421997, ANN loss : 3.025897741317749, Total loss : 177.22262573242188\n",
      "learning rate A :  tf.Tensor(9.829728e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1638 is 0.0791 sec\n",
      "train AE loss : 1.619971752166748, train ANN loss : 3.092384099960327\n",
      "AE loss : 1.430899739265442, ANN loss : 3.019103765487671, Total loss : 146.1090850830078\n",
      "learning rate A :  tf.Tensor(9.829521e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1639 is 0.0756 sec\n",
      "train AE loss : 1.3279931545257568, train ANN loss : 3.0778286457061768\n",
      "AE loss : 1.4398168325424194, ANN loss : 3.0205273628234863, Total loss : 147.00222778320312\n",
      "learning rate A :  tf.Tensor(9.829521e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1640 is 0.0793 sec\n",
      "train AE loss : 1.3361650705337524, train ANN loss : 3.068983554840088\n",
      "AE loss : 1.4312129020690918, ANN loss : 3.024494171142578, Total loss : 146.1457977294922\n",
      "learning rate A :  tf.Tensor(9.829521e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1641 is 0.0787 sec\n",
      "train AE loss : 1.3279958963394165, train ANN loss : 3.070834159851074\n",
      "AE loss : 1.4401572942733765, ANN loss : 3.0273730754852295, Total loss : 147.04310607910156\n",
      "learning rate A :  tf.Tensor(9.829521e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1642 is 0.0785 sec\n",
      "train AE loss : 1.3362295627593994, train ANN loss : 3.071500301361084\n",
      "AE loss : 1.4770560264587402, ANN loss : 3.026430606842041, Total loss : 150.73202514648438\n",
      "learning rate A :  tf.Tensor(9.829521e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1643 is 0.0804 sec\n",
      "train AE loss : 1.3707084655761719, train ANN loss : 3.0776047706604004\n",
      "AE loss : 1.23129141330719, ANN loss : 3.040076732635498, Total loss : 126.16921997070312\n",
      "learning rate A :  tf.Tensor(9.829314e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1644 is 0.0777 sec\n",
      "train AE loss : 1.1411494016647339, train ANN loss : 3.0789644718170166\n",
      "AE loss : 1.0438814163208008, ANN loss : 3.060593843460083, Total loss : 107.44873046875\n",
      "learning rate A :  tf.Tensor(9.8291064e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1645 is 0.0781 sec\n",
      "train AE loss : 0.9669872522354126, train ANN loss : 3.097529888153076\n",
      "AE loss : 0.8982222080230713, ANN loss : 3.083953619003296, Total loss : 92.90618133544922\n",
      "learning rate A :  tf.Tensor(9.8289e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1646 is 0.0786 sec\n",
      "train AE loss : 0.8317220211029053, train ANN loss : 3.123440980911255\n",
      "AE loss : 1.0707563161849976, ANN loss : 3.051710605621338, Total loss : 110.1273422241211\n",
      "learning rate A :  tf.Tensor(9.8289e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1647 is 0.0790 sec\n",
      "train AE loss : 0.9920215606689453, train ANN loss : 3.0933146476745605\n",
      "AE loss : 1.3861486911773682, ANN loss : 3.0230557918548584, Total loss : 141.637939453125\n",
      "learning rate A :  tf.Tensor(9.8289e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1648 is 0.0788 sec\n",
      "train AE loss : 1.28606116771698, train ANN loss : 3.0703468322753906\n",
      "AE loss : 1.748968482017517, ANN loss : 3.026719093322754, Total loss : 177.923583984375\n",
      "learning rate A :  tf.Tensor(9.8289e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1649 is 0.0788 sec\n",
      "train AE loss : 1.6266592741012573, train ANN loss : 3.083404302597046\n",
      "AE loss : 1.4344682693481445, ANN loss : 3.0218513011932373, Total loss : 146.4686737060547\n",
      "learning rate A :  tf.Tensor(9.8286924e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1650 is 0.0764 sec\n",
      "train AE loss : 1.3316221237182617, train ANN loss : 3.0773680210113525\n",
      "AE loss : 1.1994438171386719, ANN loss : 3.030064582824707, Total loss : 122.97444915771484\n",
      "learning rate A :  tf.Tensor(9.828486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1651 is 0.0766 sec\n",
      "train AE loss : 1.1121660470962524, train ANN loss : 3.0766654014587402\n",
      "AE loss : 1.5583494901657104, ANN loss : 3.0232961177825928, Total loss : 158.85824584960938\n",
      "learning rate A :  tf.Tensor(9.828486e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1652 is 0.0804 sec\n",
      "train AE loss : 1.4480018615722656, train ANN loss : 3.076284408569336\n",
      "AE loss : 1.292707920074463, ANN loss : 3.024144411087036, Total loss : 132.29493713378906\n",
      "learning rate A :  tf.Tensor(9.8282784e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1653 is 0.0771 sec\n",
      "train AE loss : 1.1993528604507446, train ANN loss : 3.0829057693481445\n",
      "AE loss : 1.6475319862365723, ANN loss : 3.0260403156280518, Total loss : 167.7792510986328\n",
      "learning rate A :  tf.Tensor(9.8282784e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1654 is 0.0789 sec\n",
      "train AE loss : 1.5319422483444214, train ANN loss : 3.085785388946533\n",
      "AE loss : 1.852342128753662, ANN loss : 3.0335097312927246, Total loss : 188.2677001953125\n",
      "learning rate A :  tf.Tensor(9.8282784e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1655 is 0.0798 sec\n",
      "train AE loss : 1.724552869796753, train ANN loss : 3.0947937965393066\n",
      "AE loss : 1.7961660623550415, ANN loss : 3.0252106189727783, Total loss : 182.64183044433594\n",
      "learning rate A :  tf.Tensor(9.8282784e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1656 is 0.0789 sec\n",
      "train AE loss : 1.6716673374176025, train ANN loss : 3.084752082824707\n",
      "AE loss : 1.572353720664978, ANN loss : 3.020517110824585, Total loss : 160.25587463378906\n",
      "learning rate A :  tf.Tensor(9.8282784e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1657 is 0.0787 sec\n",
      "train AE loss : 1.4612482786178589, train ANN loss : 3.070967674255371\n",
      "AE loss : 1.360292911529541, ANN loss : 3.0331995487213135, Total loss : 139.0625\n",
      "learning rate A :  tf.Tensor(9.8282784e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1658 is 0.0790 sec\n",
      "train AE loss : 1.2626601457595825, train ANN loss : 3.0801658630371094\n",
      "AE loss : 1.14070725440979, ANN loss : 3.0523204803466797, Total loss : 117.123046875\n",
      "learning rate A :  tf.Tensor(9.828071e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1659 is 0.0774 sec\n",
      "train AE loss : 1.0580005645751953, train ANN loss : 3.088552474975586\n",
      "AE loss : 0.9724619388580322, ANN loss : 3.0756654739379883, Total loss : 100.32186126708984\n",
      "learning rate A :  tf.Tensor(9.827864e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1660 is 0.0774 sec\n",
      "train AE loss : 0.9015002250671387, train ANN loss : 3.107649326324463\n",
      "AE loss : 0.8407369256019592, ANN loss : 3.100508213043213, Total loss : 87.1741943359375\n",
      "learning rate A :  tf.Tensor(9.827657e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1661 is 0.0781 sec\n",
      "train AE loss : 0.779140055179596, train ANN loss : 3.1400861740112305\n",
      "AE loss : 0.8671845197677612, ANN loss : 3.0971498489379883, Total loss : 89.81559753417969\n",
      "learning rate A :  tf.Tensor(9.827657e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1662 is 0.0797 sec\n",
      "train AE loss : 0.8037887811660767, train ANN loss : 3.1274895668029785\n",
      "AE loss : 0.7571955323219299, ANN loss : 3.122499942779541, Total loss : 78.84205627441406\n",
      "learning rate A :  tf.Tensor(9.82745e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1663 is 0.0769 sec\n",
      "train AE loss : 0.7018641233444214, train ANN loss : 3.1517865657806396\n",
      "AE loss : 0.9438552260398865, ANN loss : 3.07299542427063, Total loss : 97.4585189819336\n",
      "learning rate A :  tf.Tensor(9.82745e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1664 is 0.0798 sec\n",
      "train AE loss : 0.8751996159553528, train ANN loss : 3.1092910766601562\n",
      "AE loss : 1.30680513381958, ANN loss : 3.0236968994140625, Total loss : 133.70419311523438\n",
      "learning rate A :  tf.Tensor(9.82745e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1665 is 0.0795 sec\n",
      "train AE loss : 1.2135766744613647, train ANN loss : 3.0652246475219727\n",
      "AE loss : 1.100934624671936, ANN loss : 3.039546012878418, Total loss : 113.13301086425781\n",
      "learning rate A :  tf.Tensor(9.827243e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1666 is 0.0771 sec\n",
      "train AE loss : 1.0216360092163086, train ANN loss : 3.0775880813598633\n",
      "AE loss : 0.9423355460166931, ANN loss : 3.0597755908966064, Total loss : 97.29331970214844\n",
      "learning rate A :  tf.Tensor(9.827036e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1667 is 0.0781 sec\n",
      "train AE loss : 0.8740488290786743, train ANN loss : 3.1000747680664062\n",
      "AE loss : 0.817534863948822, ANN loss : 3.0819647312164307, Total loss : 84.83544921875\n",
      "learning rate A :  tf.Tensor(9.826828e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1668 is 0.0777 sec\n",
      "train AE loss : 0.7582548260688782, train ANN loss : 3.110409736633301\n",
      "AE loss : 1.2774564027786255, ANN loss : 3.0224311351776123, Total loss : 130.76808166503906\n",
      "learning rate A :  tf.Tensor(9.826828e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1669 is 0.0794 sec\n",
      "train AE loss : 1.1866450309753418, train ANN loss : 3.0690548419952393\n",
      "AE loss : 1.0789752006530762, ANN loss : 3.0341596603393555, Total loss : 110.93167877197266\n",
      "learning rate A :  tf.Tensor(9.826622e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1670 is 0.0781 sec\n",
      "train AE loss : 1.0016272068023682, train ANN loss : 3.083573341369629\n",
      "AE loss : 1.6680541038513184, ANN loss : 3.035522222518921, Total loss : 169.84092712402344\n",
      "learning rate A :  tf.Tensor(9.826622e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1671 is 0.0797 sec\n",
      "train AE loss : 1.5534172058105469, train ANN loss : 3.1045327186584473\n",
      "AE loss : 2.1433463096618652, ANN loss : 3.0817155838012695, Total loss : 217.4163360595703\n",
      "learning rate A :  tf.Tensor(9.826622e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1672 is 0.0811 sec\n",
      "train AE loss : 2.001880168914795, train ANN loss : 3.1604385375976562\n",
      "AE loss : 2.1332249641418457, ANN loss : 3.0613393783569336, Total loss : 216.38385009765625\n",
      "learning rate A :  tf.Tensor(9.826622e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1673 is 0.0805 sec\n",
      "train AE loss : 1.9926398992538452, train ANN loss : 3.134366273880005\n",
      "AE loss : 1.7226613759994507, ANN loss : 3.0189297199249268, Total loss : 175.28506469726562\n",
      "learning rate A :  tf.Tensor(9.826622e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1674 is 0.0801 sec\n",
      "train AE loss : 1.6056803464889526, train ANN loss : 3.0794830322265625\n",
      "AE loss : 1.4118741750717163, ANN loss : 3.0176308155059814, Total loss : 144.20504760742188\n",
      "learning rate A :  tf.Tensor(9.826415e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1675 is 0.0801 sec\n",
      "train AE loss : 1.3137576580047607, train ANN loss : 3.0735671520233154\n",
      "AE loss : 1.1796455383300781, ANN loss : 3.028850793838501, Total loss : 120.99340057373047\n",
      "learning rate A :  tf.Tensor(9.826208e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1676 is 0.2577 sec\n",
      "train AE loss : 1.096697449684143, train ANN loss : 3.0793561935424805\n",
      "AE loss : 1.0044671297073364, ANN loss : 3.0633749961853027, Total loss : 103.51010131835938\n",
      "learning rate A :  tf.Tensor(9.826208e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1677 is 0.0940 sec\n",
      "train AE loss : 0.9335858821868896, train ANN loss : 3.104698896408081\n",
      "AE loss : 0.8656830787658691, ANN loss : 3.087148904800415, Total loss : 89.65545654296875\n",
      "learning rate A :  tf.Tensor(9.826001e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1678 is 0.0774 sec\n",
      "train AE loss : 0.8044193983078003, train ANN loss : 3.1272597312927246\n",
      "AE loss : 0.7559011578559875, ANN loss : 3.11153507232666, Total loss : 78.70165252685547\n",
      "learning rate A :  tf.Tensor(9.825793e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1679 is 0.0779 sec\n",
      "train AE loss : 0.7025023698806763, train ANN loss : 3.1450726985931396\n",
      "AE loss : 0.7767965793609619, ANN loss : 3.1164896488189697, Total loss : 80.79615020751953\n",
      "learning rate A :  tf.Tensor(9.825793e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1680 is 0.0781 sec\n",
      "train AE loss : 0.7220558524131775, train ANN loss : 3.156587600708008\n",
      "AE loss : 0.6847200393676758, ANN loss : 3.1412136554718018, Total loss : 71.61321258544922\n",
      "learning rate A :  tf.Tensor(9.825586e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1681 is 0.0764 sec\n",
      "train AE loss : 0.636601448059082, train ANN loss : 3.1748969554901123\n",
      "AE loss : 0.8622261881828308, ANN loss : 3.091639518737793, Total loss : 89.31427001953125\n",
      "learning rate A :  tf.Tensor(9.825586e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1682 is 0.0798 sec\n",
      "train AE loss : 0.8016678094863892, train ANN loss : 3.1280641555786133\n",
      "AE loss : 0.7531891465187073, ANN loss : 3.1161811351776123, Total loss : 78.43509674072266\n",
      "learning rate A :  tf.Tensor(9.82538e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1683 is 0.0769 sec\n",
      "train AE loss : 0.7003970742225647, train ANN loss : 3.1486237049102783\n",
      "AE loss : 0.6658765077590942, ANN loss : 3.1402230262756348, Total loss : 69.72787475585938\n",
      "learning rate A :  tf.Tensor(9.825172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1684 is 0.0765 sec\n",
      "train AE loss : 0.6193833351135254, train ANN loss : 3.179678440093994\n",
      "AE loss : 0.5946972966194153, ANN loss : 3.1633124351501465, Total loss : 62.63304138183594\n",
      "learning rate A :  tf.Tensor(9.824966e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1685 is 0.0768 sec\n",
      "train AE loss : 0.5534015893936157, train ANN loss : 3.18705677986145\n",
      "AE loss : 0.5360186100006104, ANN loss : 3.1851882934570312, Total loss : 56.78704833984375\n",
      "learning rate A :  tf.Tensor(9.824758e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1686 is 0.0756 sec\n",
      "train AE loss : 0.4990014135837555, train ANN loss : 3.21747088432312\n",
      "AE loss : 0.8358038067817688, ANN loss : 3.0790762901306152, Total loss : 86.65946197509766\n",
      "learning rate A :  tf.Tensor(9.824758e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1687 is 0.0778 sec\n",
      "train AE loss : 0.7774269580841064, train ANN loss : 3.1117935180664062\n",
      "AE loss : 0.7325261235237122, ANN loss : 3.101639986038208, Total loss : 76.354248046875\n",
      "learning rate A :  tf.Tensor(9.824551e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1688 is 0.0770 sec\n",
      "train AE loss : 0.6815727353096008, train ANN loss : 3.1393325328826904\n",
      "AE loss : 1.2599197626113892, ANN loss : 3.0211658477783203, Total loss : 129.0131378173828\n",
      "learning rate A :  tf.Tensor(9.824551e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1689 is 0.0773 sec\n",
      "train AE loss : 1.173080325126648, train ANN loss : 3.0678646564483643\n",
      "AE loss : 1.9643545150756836, ANN loss : 3.0608811378479004, Total loss : 199.496337890625\n",
      "learning rate A :  tf.Tensor(9.824551e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1690 is 0.0779 sec\n",
      "train AE loss : 1.8359630107879639, train ANN loss : 3.141300678253174\n",
      "AE loss : 2.3518826961517334, ANN loss : 3.1119349002838135, Total loss : 238.30018615722656\n",
      "learning rate A :  tf.Tensor(9.824551e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1691 is 0.0786 sec\n",
      "train AE loss : 2.2028355598449707, train ANN loss : 3.192225456237793\n",
      "AE loss : 2.128037214279175, ANN loss : 3.0599851608276367, Total loss : 215.86370849609375\n",
      "learning rate A :  tf.Tensor(9.824551e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1692 is 0.0777 sec\n",
      "train AE loss : 1.9915567636489868, train ANN loss : 3.140005588531494\n",
      "AE loss : 1.7065999507904053, ANN loss : 3.026571273803711, Total loss : 173.68655395507812\n",
      "learning rate A :  tf.Tensor(9.824344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1693 is 0.0765 sec\n",
      "train AE loss : 1.5936700105667114, train ANN loss : 3.090522527694702\n",
      "AE loss : 1.4027513265609741, ANN loss : 3.0176963806152344, Total loss : 143.29283142089844\n",
      "learning rate A :  tf.Tensor(9.824344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1694 is 0.0787 sec\n",
      "train AE loss : 1.3083336353302002, train ANN loss : 3.0734829902648926\n",
      "AE loss : 1.1651283502578735, ANN loss : 3.040550947189331, Total loss : 119.55337524414062\n",
      "learning rate A :  tf.Tensor(9.824344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1695 is 0.0780 sec\n",
      "train AE loss : 1.08588445186615, train ANN loss : 3.0799927711486816\n",
      "AE loss : 1.0561788082122803, ANN loss : 3.0637576580047607, Total loss : 108.681640625\n",
      "learning rate A :  tf.Tensor(9.824344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1696 is 0.0783 sec\n",
      "train AE loss : 0.9841052293777466, train ANN loss : 3.1026127338409424\n",
      "AE loss : 0.9060615301132202, ANN loss : 3.0883352756500244, Total loss : 93.69449615478516\n",
      "learning rate A :  tf.Tensor(9.824138e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1697 is 0.0776 sec\n",
      "train AE loss : 0.8441451787948608, train ANN loss : 3.1278727054595947\n",
      "AE loss : 0.7878788113594055, ANN loss : 3.11352801322937, Total loss : 81.90141296386719\n",
      "learning rate A :  tf.Tensor(9.82393e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1698 is 0.0765 sec\n",
      "train AE loss : 0.7342814207077026, train ANN loss : 3.1447010040283203\n",
      "AE loss : 0.6938058733940125, ANN loss : 3.1381359100341797, Total loss : 72.51872253417969\n",
      "learning rate A :  tf.Tensor(9.823723e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1699 is 0.0758 sec\n",
      "train AE loss : 0.6467075943946838, train ANN loss : 3.164452314376831\n",
      "AE loss : 0.6174891591072083, ANN loss : 3.16164493560791, Total loss : 64.91056060791016\n",
      "learning rate A :  tf.Tensor(9.8235156e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1700 is 0.0764 sec\n",
      "train AE loss : 0.5758032202720642, train ANN loss : 3.1909050941467285\n",
      "AE loss : 0.5547766089439392, ANN loss : 3.1840343475341797, Total loss : 58.66169357299805\n",
      "learning rate A :  tf.Tensor(9.823308e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1701 is 0.0763 sec\n",
      "train AE loss : 0.5175726413726807, train ANN loss : 3.214625358581543\n",
      "AE loss : 0.5028029084205627, ANN loss : 3.204885959625244, Total loss : 53.48518371582031\n",
      "learning rate A :  tf.Tensor(9.8231016e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1702 is 0.0761 sec\n",
      "train AE loss : 0.46924859285354614, train ANN loss : 3.2358906269073486\n",
      "AE loss : 0.4592369794845581, ANN loss : 3.224358081817627, Total loss : 49.14805603027344\n",
      "learning rate A :  tf.Tensor(9.822895e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1703 is 0.0769 sec\n",
      "train AE loss : 0.42873746156692505, train ANN loss : 3.2514870166778564\n",
      "AE loss : 0.5293099880218506, ANN loss : 3.191737651824951, Total loss : 56.122737884521484\n",
      "learning rate A :  tf.Tensor(9.822895e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1704 is 0.0774 sec\n",
      "train AE loss : 0.4939993619918823, train ANN loss : 3.2176742553710938\n",
      "AE loss : 0.4815044105052948, ANN loss : 3.2118337154388428, Total loss : 51.362274169921875\n",
      "learning rate A :  tf.Tensor(9.8226876e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1705 is 0.0769 sec\n",
      "train AE loss : 0.44951966404914856, train ANN loss : 3.2371363639831543\n",
      "AE loss : 0.4413954019546509, ANN loss : 3.230579376220703, Total loss : 47.37012481689453\n",
      "learning rate A :  tf.Tensor(9.822481e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1706 is 0.0790 sec\n",
      "train AE loss : 0.41219186782836914, train ANN loss : 3.263944387435913\n",
      "AE loss : 0.639573335647583, ANN loss : 3.1343202590942383, Total loss : 67.0916519165039\n",
      "learning rate A :  tf.Tensor(9.822481e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1707 is 0.0805 sec\n",
      "train AE loss : 0.5967386364936829, train ANN loss : 3.168071746826172\n",
      "AE loss : 1.0540748834609985, ANN loss : 3.0375120639801025, Total loss : 108.44499969482422\n",
      "learning rate A :  tf.Tensor(9.822481e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1708 is 0.0796 sec\n",
      "train AE loss : 0.9827857613563538, train ANN loss : 3.0825743675231934\n",
      "AE loss : 0.9050627946853638, ANN loss : 3.055661201477051, Total loss : 93.56194305419922\n",
      "learning rate A :  tf.Tensor(9.8222736e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1709 is 0.0778 sec\n",
      "train AE loss : 0.8440439701080322, train ANN loss : 3.101759910583496\n",
      "AE loss : 1.5180537700653076, ANN loss : 3.025404214859009, Total loss : 154.83078002929688\n",
      "learning rate A :  tf.Tensor(9.8222736e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1710 is 0.0794 sec\n",
      "train AE loss : 1.4176702499389648, train ANN loss : 3.0792906284332275\n",
      "AE loss : 1.2602756023406982, ANN loss : 3.0230743885040283, Total loss : 129.05062866210938\n",
      "learning rate A :  tf.Tensor(9.822067e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1711 is 0.0777 sec\n",
      "train AE loss : 1.1758060455322266, train ANN loss : 3.0783910751342773\n",
      "AE loss : 1.065077304840088, ANN loss : 3.0308141708374023, Total loss : 109.53854370117188\n",
      "learning rate A :  tf.Tensor(9.82186e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1712 is 0.0783 sec\n",
      "train AE loss : 0.993425726890564, train ANN loss : 3.0821774005889893\n",
      "AE loss : 1.733233094215393, ANN loss : 3.0518057346343994, Total loss : 176.37510681152344\n",
      "learning rate A :  tf.Tensor(9.82186e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1713 is 0.0797 sec\n",
      "train AE loss : 1.6200575828552246, train ANN loss : 3.123656749725342\n",
      "AE loss : 1.4199845790863037, ANN loss : 3.0318644046783447, Total loss : 145.0303192138672\n",
      "learning rate A :  tf.Tensor(9.821653e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1714 is 0.0778 sec\n",
      "train AE loss : 1.32545006275177, train ANN loss : 3.1033124923706055\n",
      "AE loss : 1.186984896659851, ANN loss : 3.0279464721679688, Total loss : 121.72643280029297\n",
      "learning rate A :  tf.Tensor(9.8214456e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1715 is 0.0778 sec\n",
      "train AE loss : 1.1073037385940552, train ANN loss : 3.0907089710235596\n",
      "AE loss : 1.0091466903686523, ANN loss : 3.0335936546325684, Total loss : 103.94825744628906\n",
      "learning rate A :  tf.Tensor(9.821239e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1716 is 0.0814 sec\n",
      "train AE loss : 0.9413377642631531, train ANN loss : 3.0876412391662598\n",
      "AE loss : 0.8710495829582214, ANN loss : 3.0442373752593994, Total loss : 90.14918518066406\n",
      "learning rate A :  tf.Tensor(9.821032e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1717 is 0.0799 sec\n",
      "train AE loss : 0.8129438757896423, train ANN loss : 3.0905630588531494\n",
      "AE loss : 1.4651631116867065, ANN loss : 3.0430848598480225, Total loss : 149.5594024658203\n",
      "learning rate A :  tf.Tensor(9.821032e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1718 is 0.0798 sec\n",
      "train AE loss : 1.3676975965499878, train ANN loss : 3.103308916091919\n",
      "AE loss : 1.2207990884780884, ANN loss : 3.0317413806915283, Total loss : 125.11164093017578\n",
      "learning rate A :  tf.Tensor(9.820825e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1719 is 0.0783 sec\n",
      "train AE loss : 1.138667345046997, train ANN loss : 3.0935850143432617\n",
      "AE loss : 1.034952163696289, ANN loss : 3.0322320461273193, Total loss : 106.5274429321289\n",
      "learning rate A :  tf.Tensor(9.820618e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1720 is 0.0773 sec\n",
      "train AE loss : 0.9653791189193726, train ANN loss : 3.098341464996338\n",
      "AE loss : 1.6379649639129639, ANN loss : 3.062317371368408, Total loss : 166.8588104248047\n",
      "learning rate A :  tf.Tensor(9.820618e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1721 is 0.0807 sec\n",
      "train AE loss : 1.5300830602645874, train ANN loss : 3.1418275833129883\n",
      "AE loss : 1.3489885330200195, ANN loss : 3.037951707839966, Total loss : 137.93679809570312\n",
      "learning rate A :  tf.Tensor(9.820412e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1722 is 0.0775 sec\n",
      "train AE loss : 1.2585285902023315, train ANN loss : 3.1040477752685547\n",
      "AE loss : 1.832042932510376, ANN loss : 3.0796539783477783, Total loss : 186.28395080566406\n",
      "learning rate A :  tf.Tensor(9.820412e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1723 is 0.0793 sec\n",
      "train AE loss : 1.713046669960022, train ANN loss : 3.154433012008667\n",
      "AE loss : 1.9567064046859741, ANN loss : 3.0764918327331543, Total loss : 198.7471160888672\n",
      "learning rate A :  tf.Tensor(9.820412e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1724 is 0.0793 sec\n",
      "train AE loss : 1.8313841819763184, train ANN loss : 3.1513004302978516\n",
      "AE loss : 1.5793451070785522, ANN loss : 3.040435314178467, Total loss : 160.97494506835938\n",
      "learning rate A :  tf.Tensor(9.820204e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1725 is 0.0771 sec\n",
      "train AE loss : 1.4757035970687866, train ANN loss : 3.10517954826355\n",
      "AE loss : 1.3032292127609253, ANN loss : 3.0273215770721436, Total loss : 133.35023498535156\n",
      "learning rate A :  tf.Tensor(9.819998e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1726 is 0.0776 sec\n",
      "train AE loss : 1.2164703607559204, train ANN loss : 3.0892603397369385\n",
      "AE loss : 1.0957568883895874, ANN loss : 3.027615547180176, Total loss : 112.60330200195312\n",
      "learning rate A :  tf.Tensor(9.81979e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1727 is 0.0778 sec\n",
      "train AE loss : 1.0223702192306519, train ANN loss : 3.0797786712646484\n",
      "AE loss : 0.9370269775390625, ANN loss : 3.0358433723449707, Total loss : 96.7385482788086\n",
      "learning rate A :  tf.Tensor(9.819584e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1728 is 0.0770 sec\n",
      "train AE loss : 0.8742498755455017, train ANN loss : 3.0920157432556152\n",
      "AE loss : 1.0521389245986938, ANN loss : 3.0349550247192383, Total loss : 108.24884796142578\n",
      "learning rate A :  tf.Tensor(9.819584e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1729 is 0.0796 sec\n",
      "train AE loss : 0.9819756150245667, train ANN loss : 3.0882606506347656\n",
      "AE loss : 0.9023811221122742, ANN loss : 3.047398090362549, Total loss : 93.28551483154297\n",
      "learning rate A :  tf.Tensor(9.819377e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1730 is 0.0776 sec\n",
      "train AE loss : 0.8422011733055115, train ANN loss : 3.09660005569458\n",
      "AE loss : 1.0795751810073853, ANN loss : 3.03725266456604, Total loss : 110.9947738647461\n",
      "learning rate A :  tf.Tensor(9.819377e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1731 is 0.0785 sec\n",
      "train AE loss : 1.0078548192977905, train ANN loss : 3.0876877307891846\n",
      "AE loss : 0.9232863187789917, ANN loss : 3.0506341457366943, Total loss : 95.37926483154297\n",
      "learning rate A :  tf.Tensor(9.81917e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1732 is 0.0775 sec\n",
      "train AE loss : 0.8618422746658325, train ANN loss : 3.1012356281280518\n",
      "AE loss : 0.8011487722396851, ANN loss : 3.067196846008301, Total loss : 83.18206787109375\n",
      "learning rate A :  tf.Tensor(9.818963e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1733 is 0.0866 sec\n",
      "train AE loss : 0.747967541217804, train ANN loss : 3.114381790161133\n",
      "AE loss : 1.0533883571624756, ANN loss : 3.0380215644836426, Total loss : 108.3768539428711\n",
      "learning rate A :  tf.Tensor(9.818963e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1734 is 0.0803 sec\n",
      "train AE loss : 0.9834475517272949, train ANN loss : 3.090968132019043\n",
      "AE loss : 0.902784526348114, ANN loss : 3.0520665645599365, Total loss : 93.33051300048828\n",
      "learning rate A :  tf.Tensor(9.818756e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1735 is 0.0943 sec\n",
      "train AE loss : 0.842778742313385, train ANN loss : 3.102217197418213\n",
      "AE loss : 1.2419145107269287, ANN loss : 3.0258829593658447, Total loss : 127.21733093261719\n",
      "learning rate A :  tf.Tensor(9.818756e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1736 is 0.0864 sec\n",
      "train AE loss : 1.1600018739700317, train ANN loss : 3.079812526702881\n",
      "AE loss : 1.6155891418457031, ANN loss : 3.0310585498809814, Total loss : 164.58998107910156\n",
      "learning rate A :  tf.Tensor(9.818756e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1737 is 0.0831 sec\n",
      "train AE loss : 1.511560320854187, train ANN loss : 3.0818185806274414\n",
      "AE loss : 1.328574538230896, ANN loss : 3.0228490829467773, Total loss : 135.88031005859375\n",
      "learning rate A :  tf.Tensor(9.818549e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1738 is 0.0799 sec\n",
      "train AE loss : 1.241335391998291, train ANN loss : 3.0818841457366943\n",
      "AE loss : 1.1139373779296875, ANN loss : 3.0269696712493896, Total loss : 114.42070007324219\n",
      "learning rate A :  tf.Tensor(9.8183424e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1739 is 0.0784 sec\n",
      "train AE loss : 1.040004014968872, train ANN loss : 3.076582431793213\n",
      "AE loss : 0.9501272439956665, ANN loss : 3.038118362426758, Total loss : 98.05084228515625\n",
      "learning rate A :  tf.Tensor(9.818135e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1740 is 0.0772 sec\n",
      "train AE loss : 0.8868156671524048, train ANN loss : 3.091054916381836\n",
      "AE loss : 1.2982676029205322, ANN loss : 3.022090435028076, Total loss : 132.84884643554688\n",
      "learning rate A :  tf.Tensor(9.818135e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1741 is 0.0796 sec\n",
      "train AE loss : 1.2126343250274658, train ANN loss : 3.074354648590088\n",
      "AE loss : 1.6497650146484375, ANN loss : 3.035281181335449, Total loss : 168.01177978515625\n",
      "learning rate A :  tf.Tensor(9.818135e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1742 is 0.0793 sec\n",
      "train AE loss : 1.5435642004013062, train ANN loss : 3.0939788818359375\n",
      "AE loss : 1.8254191875457764, ANN loss : 3.043997049331665, Total loss : 185.58592224121094\n",
      "learning rate A :  tf.Tensor(9.818135e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1743 is 0.0799 sec\n",
      "train AE loss : 1.7094473838806152, train ANN loss : 3.1264772415161133\n",
      "AE loss : 1.481600046157837, ANN loss : 3.0238966941833496, Total loss : 151.18389892578125\n",
      "learning rate A :  tf.Tensor(9.8179284e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1744 is 0.0807 sec\n",
      "train AE loss : 1.3850879669189453, train ANN loss : 3.0855515003204346\n",
      "AE loss : 1.2285711765289307, ANN loss : 3.0211822986602783, Total loss : 125.87828826904297\n",
      "learning rate A :  tf.Tensor(9.817722e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1745 is 0.0787 sec\n",
      "train AE loss : 1.147108793258667, train ANN loss : 3.075759172439575\n",
      "AE loss : 1.3393580913543701, ANN loss : 3.0186758041381836, Total loss : 136.95448303222656\n",
      "learning rate A :  tf.Tensor(9.817722e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1746 is 0.0802 sec\n",
      "train AE loss : 1.2511496543884277, train ANN loss : 3.079474687576294\n",
      "AE loss : 1.4066312313079834, ANN loss : 3.017268419265747, Total loss : 143.68038940429688\n",
      "learning rate A :  tf.Tensor(9.817722e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1747 is 0.0897 sec\n",
      "train AE loss : 1.3143457174301147, train ANN loss : 3.0706710815429688\n",
      "AE loss : 1.4297802448272705, ANN loss : 3.0165185928344727, Total loss : 145.9945526123047\n",
      "learning rate A :  tf.Tensor(9.817722e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1748 is 0.0798 sec\n",
      "train AE loss : 1.336012840270996, train ANN loss : 3.0660667419433594\n",
      "AE loss : 1.1878893375396729, ANN loss : 3.022947072982788, Total loss : 121.81188201904297\n",
      "learning rate A :  tf.Tensor(9.817515e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1749 is 0.0780 sec\n",
      "train AE loss : 1.1086865663528442, train ANN loss : 3.080596923828125\n",
      "AE loss : 1.236963152885437, ANN loss : 3.0225346088409424, Total loss : 126.71884155273438\n",
      "learning rate A :  tf.Tensor(9.817515e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1750 is 0.0804 sec\n",
      "train AE loss : 1.1545101404190063, train ANN loss : 3.069767713546753\n",
      "AE loss : 1.0423669815063477, ANN loss : 3.036642074584961, Total loss : 107.27334594726562\n",
      "learning rate A :  tf.Tensor(9.817308e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1751 is 0.0791 sec\n",
      "train AE loss : 0.9719943404197693, train ANN loss : 3.0890514850616455\n",
      "AE loss : 1.1616896390914917, ANN loss : 3.0275256633758545, Total loss : 119.19648742675781\n",
      "learning rate A :  tf.Tensor(9.817308e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1752 is 0.0806 sec\n",
      "train AE loss : 1.0835130214691162, train ANN loss : 3.080160140991211\n",
      "AE loss : 0.984544038772583, ANN loss : 3.0439369678497314, Total loss : 101.49834442138672\n",
      "learning rate A :  tf.Tensor(9.817101e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1753 is 0.0796 sec\n",
      "train AE loss : 0.9175704717636108, train ANN loss : 3.091586112976074\n",
      "AE loss : 0.8476522564888, ANN loss : 3.0634756088256836, Total loss : 87.82870483398438\n",
      "learning rate A :  tf.Tensor(9.8168945e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1754 is 0.0783 sec\n",
      "train AE loss : 0.789803683757782, train ANN loss : 3.1046321392059326\n",
      "AE loss : 1.0608408451080322, ANN loss : 3.03330659866333, Total loss : 109.11739349365234\n",
      "learning rate A :  tf.Tensor(9.8168945e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1755 is 0.0804 sec\n",
      "train AE loss : 0.9884934425354004, train ANN loss : 3.0783281326293945\n",
      "AE loss : 0.9067923426628113, ANN loss : 3.0509486198425293, Total loss : 93.73018646240234\n",
      "learning rate A :  tf.Tensor(9.816688e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1756 is 0.0782 sec\n",
      "train AE loss : 0.8446798324584961, train ANN loss : 3.0967040061950684\n",
      "AE loss : 0.7868825197219849, ANN loss : 3.070732593536377, Total loss : 81.75898742675781\n",
      "learning rate A :  tf.Tensor(9.8164805e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1757 is 0.0780 sec\n",
      "train AE loss : 0.7326845526695251, train ANN loss : 3.112579822540283\n",
      "AE loss : 0.6917224526405334, ANN loss : 3.0911548137664795, Total loss : 72.2634048461914\n",
      "learning rate A :  tf.Tensor(9.816274e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1758 is 0.0793 sec\n",
      "train AE loss : 0.6438971161842346, train ANN loss : 3.124878168106079\n",
      "AE loss : 0.9957510232925415, ANN loss : 3.035130500793457, Total loss : 102.61023712158203\n",
      "learning rate A :  tf.Tensor(9.816274e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1759 is 0.0803 sec\n",
      "train AE loss : 0.9272559881210327, train ANN loss : 3.0737664699554443\n",
      "AE loss : 0.8566086888313293, ANN loss : 3.051727294921875, Total loss : 88.71259307861328\n",
      "learning rate A :  tf.Tensor(9.816067e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1760 is 0.0780 sec\n",
      "train AE loss : 0.7973282933235168, train ANN loss : 3.097177028656006\n",
      "AE loss : 0.747469425201416, ANN loss : 3.0702669620513916, Total loss : 77.81720733642578\n",
      "learning rate A :  tf.Tensor(9.81586e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1761 is 0.1093 sec\n",
      "train AE loss : 0.695431113243103, train ANN loss : 3.112297534942627\n",
      "AE loss : 0.6601805090904236, ANN loss : 3.0893778800964355, Total loss : 69.10742950439453\n",
      "learning rate A :  tf.Tensor(9.815653e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1762 is 0.1024 sec\n",
      "train AE loss : 0.6140388250350952, train ANN loss : 3.1264989376068115\n",
      "AE loss : 1.0678520202636719, ANN loss : 3.028521776199341, Total loss : 109.813720703125\n",
      "learning rate A :  tf.Tensor(9.815653e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1763 is 0.0844 sec\n",
      "train AE loss : 0.9941177368164062, train ANN loss : 3.079345703125\n",
      "AE loss : 1.6266231536865234, ANN loss : 3.0481066703796387, Total loss : 165.71041870117188\n",
      "learning rate A :  tf.Tensor(9.815653e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1764 is 0.0810 sec\n",
      "train AE loss : 1.5182756185531616, train ANN loss : 3.1145009994506836\n",
      "AE loss : 2.045032262802124, ANN loss : 3.0868160724639893, Total loss : 207.5900421142578\n",
      "learning rate A :  tf.Tensor(9.815653e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1765 is 0.0809 sec\n",
      "train AE loss : 1.9139333963394165, train ANN loss : 3.154175043106079\n",
      "AE loss : 1.632692813873291, ANN loss : 3.044495105743408, Total loss : 166.31378173828125\n",
      "learning rate A :  tf.Tensor(9.8154465e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1766 is 0.0795 sec\n",
      "train AE loss : 1.5240938663482666, train ANN loss : 3.110159397125244\n",
      "AE loss : 1.772700309753418, ANN loss : 3.038591146469116, Total loss : 180.30860900878906\n",
      "learning rate A :  tf.Tensor(9.8154465e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1767 is 0.0821 sec\n",
      "train AE loss : 1.6565920114517212, train ANN loss : 3.0988290309906006\n",
      "AE loss : 1.4375941753387451, ANN loss : 3.021164894104004, Total loss : 146.78057861328125\n",
      "learning rate A :  tf.Tensor(9.81524e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1768 is 0.0798 sec\n",
      "train AE loss : 1.3405840396881104, train ANN loss : 3.0801143646240234\n",
      "AE loss : 1.1918426752090454, ANN loss : 3.021089553833008, Total loss : 122.20536041259766\n",
      "learning rate A :  tf.Tensor(9.8150325e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1769 is 0.0786 sec\n",
      "train AE loss : 1.1097861528396606, train ANN loss : 3.069732427597046\n",
      "AE loss : 1.0068182945251465, ANN loss : 3.0303382873535156, Total loss : 103.7121810913086\n",
      "learning rate A :  tf.Tensor(9.814826e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1770 is 0.0804 sec\n",
      "train AE loss : 0.9366515278816223, train ANN loss : 3.0763981342315674\n",
      "AE loss : 1.1537227630615234, ANN loss : 3.023003339767456, Total loss : 118.39527893066406\n",
      "learning rate A :  tf.Tensor(9.814826e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1771 is 0.0808 sec\n",
      "train AE loss : 1.074320673942566, train ANN loss : 3.068326234817505\n",
      "AE loss : 0.9770327806472778, ANN loss : 3.036581039428711, Total loss : 100.73986053466797\n",
      "learning rate A :  tf.Tensor(9.814619e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1772 is 0.0789 sec\n",
      "train AE loss : 0.9090385437011719, train ANN loss : 3.083919048309326\n",
      "AE loss : 0.8410850167274475, ANN loss : 3.053687572479248, Total loss : 87.16218566894531\n",
      "learning rate A :  tf.Tensor(9.8144126e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1773 is 0.0797 sec\n",
      "train AE loss : 0.7820808291435242, train ANN loss : 3.093968391418457\n",
      "AE loss : 0.7342580556869507, ANN loss : 3.0726473331451416, Total loss : 76.49845886230469\n",
      "learning rate A :  tf.Tensor(9.814205e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1774 is 0.0790 sec\n",
      "train AE loss : 0.6824012994766235, train ANN loss : 3.1143407821655273\n",
      "AE loss : 0.9527145028114319, ANN loss : 3.043238401412964, Total loss : 98.314697265625\n",
      "learning rate A :  tf.Tensor(9.814205e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1775 is 0.0809 sec\n",
      "train AE loss : 0.8864468932151794, train ANN loss : 3.090545654296875\n",
      "AE loss : 0.8216789364814758, ANN loss : 3.0620007514953613, Total loss : 85.22989654541016\n",
      "learning rate A :  tf.Tensor(9.8139986e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1776 is 0.0803 sec\n",
      "train AE loss : 0.7640635371208191, train ANN loss : 3.105430841445923\n",
      "AE loss : 1.1708705425262451, ANN loss : 3.024270534515381, Total loss : 120.11132049560547\n",
      "learning rate A :  tf.Tensor(9.8139986e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1777 is 0.0811 sec\n",
      "train AE loss : 1.0905097723007202, train ANN loss : 3.0675549507141113\n",
      "AE loss : 0.9893192648887634, ANN loss : 3.038973093032837, Total loss : 101.97090148925781\n",
      "learning rate A :  tf.Tensor(9.813792e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1778 is 0.0796 sec\n",
      "train AE loss : 0.920634925365448, train ANN loss : 3.0763251781463623\n",
      "AE loss : 1.4595346450805664, ANN loss : 3.016461133956909, Total loss : 148.96990966796875\n",
      "learning rate A :  tf.Tensor(9.813792e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1779 is 0.0816 sec\n",
      "train AE loss : 1.3618072271347046, train ANN loss : 3.0763230323791504\n",
      "AE loss : 1.9426904916763306, ANN loss : 3.03963041305542, Total loss : 197.3086700439453\n",
      "learning rate A :  tf.Tensor(9.813792e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1780 is 0.0805 sec\n",
      "train AE loss : 1.8187544345855713, train ANN loss : 3.101276397705078\n",
      "AE loss : 1.5572013854980469, ANN loss : 3.0181853771209717, Total loss : 158.73831176757812\n",
      "learning rate A :  tf.Tensor(9.813585e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1781 is 0.0790 sec\n",
      "train AE loss : 1.453801155090332, train ANN loss : 3.073129177093506\n",
      "AE loss : 1.9042025804519653, ANN loss : 3.036590099334717, Total loss : 193.45684814453125\n",
      "learning rate A :  tf.Tensor(9.813585e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1782 is 0.0818 sec\n",
      "train AE loss : 1.7821837663650513, train ANN loss : 3.09940767288208\n",
      "AE loss : 1.9949842691421509, ANN loss : 3.0388267040252686, Total loss : 202.53726196289062\n",
      "learning rate A :  tf.Tensor(9.813585e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1783 is 0.0820 sec\n",
      "train AE loss : 1.8682520389556885, train ANN loss : 3.10082745552063\n",
      "AE loss : 1.5943324565887451, ANN loss : 3.017608404159546, Total loss : 162.4508514404297\n",
      "learning rate A :  tf.Tensor(9.813378e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1784 is 0.0798 sec\n",
      "train AE loss : 1.488865852355957, train ANN loss : 3.070192337036133\n",
      "AE loss : 1.3051352500915527, ANN loss : 3.017322063446045, Total loss : 133.5308380126953\n",
      "learning rate A :  tf.Tensor(9.813172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1785 is 0.0803 sec\n",
      "train AE loss : 1.2165206670761108, train ANN loss : 3.0729222297668457\n",
      "AE loss : 1.3727781772613525, ANN loss : 3.016456365585327, Total loss : 140.2942657470703\n",
      "learning rate A :  tf.Tensor(9.813172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1786 is 0.0811 sec\n",
      "train AE loss : 1.2803065776824951, train ANN loss : 3.0671138763427734\n",
      "AE loss : 1.4374399185180664, ANN loss : 3.0154335498809814, Total loss : 146.75941467285156\n",
      "learning rate A :  tf.Tensor(9.813172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1787 is 0.0826 sec\n",
      "train AE loss : 1.341351866722107, train ANN loss : 3.0725700855255127\n",
      "AE loss : 1.4827914237976074, ANN loss : 3.0139756202697754, Total loss : 151.29312133789062\n",
      "learning rate A :  tf.Tensor(9.813172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1788 is 0.0813 sec\n",
      "train AE loss : 1.3842976093292236, train ANN loss : 3.067551851272583\n",
      "AE loss : 1.5072457790374756, ANN loss : 3.0123109817504883, Total loss : 153.7368927001953\n",
      "learning rate A :  tf.Tensor(9.813172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1789 is 0.0822 sec\n",
      "train AE loss : 1.407537817955017, train ANN loss : 3.0695550441741943\n",
      "AE loss : 1.5075305700302124, ANN loss : 3.0113723278045654, Total loss : 153.76441955566406\n",
      "learning rate A :  tf.Tensor(9.813172e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1790 is 0.0835 sec\n",
      "train AE loss : 1.4079020023345947, train ANN loss : 3.065279483795166\n",
      "AE loss : 1.2414253950119019, ANN loss : 3.022824287414551, Total loss : 127.16537475585938\n",
      "learning rate A :  tf.Tensor(9.8129654e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1791 is 0.0790 sec\n",
      "train AE loss : 1.157302737236023, train ANN loss : 3.0676026344299316\n",
      "AE loss : 1.0423818826675415, ANN loss : 3.0411994457244873, Total loss : 107.27938842773438\n",
      "learning rate A :  tf.Tensor(9.812758e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1792 is 0.0792 sec\n",
      "train AE loss : 0.9706723093986511, train ANN loss : 3.0832133293151855\n",
      "AE loss : 0.8904834389686584, ANN loss : 3.06280517578125, Total loss : 92.11116027832031\n",
      "learning rate A :  tf.Tensor(9.812551e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1793 is 0.0803 sec\n",
      "train AE loss : 0.8287574052810669, train ANN loss : 3.0984277725219727\n",
      "AE loss : 0.7720223665237427, ANN loss : 3.0855348110198975, Total loss : 80.28777313232422\n",
      "learning rate A :  tf.Tensor(9.812344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1794 is 0.0802 sec\n",
      "train AE loss : 0.7182531356811523, train ANN loss : 3.124476432800293\n",
      "AE loss : 0.9023389220237732, ANN loss : 3.057565212249756, Total loss : 93.29145812988281\n",
      "learning rate A :  tf.Tensor(9.812344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1795 is 0.0823 sec\n",
      "train AE loss : 0.8397775292396545, train ANN loss : 3.1016595363616943\n",
      "AE loss : 0.7813848853111267, ANN loss : 3.079718828201294, Total loss : 81.21820831298828\n",
      "learning rate A :  tf.Tensor(9.8121374e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1796 is 0.0807 sec\n",
      "train AE loss : 0.7269771695137024, train ANN loss : 3.1228957176208496\n",
      "AE loss : 0.6856464147567749, ANN loss : 3.1018588542938232, Total loss : 71.66650390625\n",
      "learning rate A :  tf.Tensor(9.81193e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1797 is 0.0791 sec\n",
      "train AE loss : 0.6377440094947815, train ANN loss : 3.1376564502716064\n",
      "AE loss : 0.9415217041969299, ANN loss : 3.0446643829345703, Total loss : 97.19683837890625\n",
      "learning rate A :  tf.Tensor(9.81193e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1798 is 0.0803 sec\n",
      "train AE loss : 0.8761738538742065, train ANN loss : 3.0919997692108154\n",
      "AE loss : 0.8122669458389282, ANN loss : 3.0647406578063965, Total loss : 84.29143524169922\n",
      "learning rate A :  tf.Tensor(9.8117234e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1799 is 0.0797 sec\n",
      "train AE loss : 0.7556565403938293, train ANN loss : 3.1091809272766113\n",
      "AE loss : 1.2146965265274048, ANN loss : 3.019127607345581, Total loss : 124.48876953125\n",
      "learning rate A :  tf.Tensor(9.8117234e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1800 is 0.1168 sec\n",
      "train AE loss : 1.13114333152771, train ANN loss : 3.0785040855407715\n",
      "AE loss : 1.7202798128128052, ANN loss : 3.0343496799468994, Total loss : 175.0623321533203\n",
      "learning rate A :  tf.Tensor(9.8117234e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1801 is 0.1546 sec\n",
      "train AE loss : 1.6065655946731567, train ANN loss : 3.0987658500671387\n",
      "AE loss : 1.3967082500457764, ANN loss : 3.0206987857818604, Total loss : 142.69151306152344\n",
      "learning rate A :  tf.Tensor(9.811517e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1802 is 0.0952 sec\n",
      "train AE loss : 1.3016358613967896, train ANN loss : 3.0722856521606445\n",
      "AE loss : 1.824783205986023, ANN loss : 3.0438344478607178, Total loss : 185.52215576171875\n",
      "learning rate A :  tf.Tensor(9.811517e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1803 is 0.0793 sec\n",
      "train AE loss : 1.7049680948257446, train ANN loss : 3.1076033115386963\n",
      "AE loss : 1.9991401433944702, ANN loss : 3.0458760261535645, Total loss : 202.95989990234375\n",
      "learning rate A :  tf.Tensor(9.811517e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1804 is 0.0788 sec\n",
      "train AE loss : 1.8701226711273193, train ANN loss : 3.1176559925079346\n",
      "AE loss : 1.824285864830017, ANN loss : 3.0187489986419678, Total loss : 185.44735717773438\n",
      "learning rate A :  tf.Tensor(9.811517e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1805 is 0.0788 sec\n",
      "train AE loss : 1.7052757740020752, train ANN loss : 3.077423572540283\n",
      "AE loss : 1.4697461128234863, ANN loss : 3.012387990951538, Total loss : 149.98699951171875\n",
      "learning rate A :  tf.Tensor(9.81131e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1806 is 0.0791 sec\n",
      "train AE loss : 1.3705168962478638, train ANN loss : 3.0651705265045166\n",
      "AE loss : 1.2118104696273804, ANN loss : 3.0215203762054443, Total loss : 124.20256805419922\n",
      "learning rate A :  tf.Tensor(9.8111035e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1807 is 0.0771 sec\n",
      "train AE loss : 1.1282713413238525, train ANN loss : 3.0752158164978027\n",
      "AE loss : 1.0182679891586304, ANN loss : 3.0381290912628174, Total loss : 104.86492156982422\n",
      "learning rate A :  tf.Tensor(9.810897e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1808 is 0.0773 sec\n",
      "train AE loss : 0.9474194049835205, train ANN loss : 3.076582670211792\n",
      "AE loss : 0.8704893589019775, ANN loss : 3.0582115650177, Total loss : 90.10714721679688\n",
      "learning rate A :  tf.Tensor(9.81069e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1809 is 0.0788 sec\n",
      "train AE loss : 0.8096694946289062, train ANN loss : 3.0959346294403076\n",
      "AE loss : 0.9009641408920288, ANN loss : 3.063196897506714, Total loss : 93.15961456298828\n",
      "learning rate A :  tf.Tensor(9.81069e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1810 is 0.0790 sec\n",
      "train AE loss : 0.8381453156471252, train ANN loss : 3.1002190113067627\n",
      "AE loss : 1.0629291534423828, ANN loss : 3.0431525707244873, Total loss : 109.33606719970703\n",
      "learning rate A :  tf.Tensor(9.81069e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1811 is 0.0788 sec\n",
      "train AE loss : 0.9892261624336243, train ANN loss : 3.088365077972412\n",
      "AE loss : 0.9043073058128357, ANN loss : 3.065401792526245, Total loss : 93.49613189697266\n",
      "learning rate A :  tf.Tensor(9.810483e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1812 is 0.0777 sec\n",
      "train AE loss : 0.8412618041038513, train ANN loss : 3.1156177520751953\n",
      "AE loss : 1.1942954063415527, ANN loss : 3.0268585681915283, Total loss : 122.4563980102539\n",
      "learning rate A :  tf.Tensor(9.810483e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1813 is 0.0783 sec\n",
      "train AE loss : 1.1119552850723267, train ANN loss : 3.076693534851074\n",
      "AE loss : 1.0044978857040405, ANN loss : 3.046170949935913, Total loss : 103.4959716796875\n",
      "learning rate A :  tf.Tensor(9.810276e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1814 is 0.0775 sec\n",
      "train AE loss : 0.9346654415130615, train ANN loss : 3.0904898643493652\n",
      "AE loss : 1.4199559688568115, ANN loss : 3.011837959289551, Total loss : 145.00743103027344\n",
      "learning rate A :  tf.Tensor(9.810276e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1815 is 0.0794 sec\n",
      "train AE loss : 1.3233896493911743, train ANN loss : 3.068014144897461\n",
      "AE loss : 1.8943228721618652, ANN loss : 3.02494740486145, Total loss : 192.45724487304688\n",
      "learning rate A :  tf.Tensor(9.810276e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1816 is 0.0788 sec\n",
      "train AE loss : 1.7705538272857666, train ANN loss : 3.0994479656219482\n",
      "AE loss : 1.5193510055541992, ANN loss : 3.01196551322937, Total loss : 154.9470672607422\n",
      "learning rate A :  tf.Tensor(9.8100696e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1817 is 0.0773 sec\n",
      "train AE loss : 1.4164361953735352, train ANN loss : 3.06427001953125\n",
      "AE loss : 1.9021272659301758, ANN loss : 3.030013084411621, Total loss : 193.24273681640625\n",
      "learning rate A :  tf.Tensor(9.8100696e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1818 is 0.0796 sec\n",
      "train AE loss : 1.7775588035583496, train ANN loss : 3.093963861465454\n",
      "AE loss : 2.031682252883911, ANN loss : 3.0345406532287598, Total loss : 206.2027587890625\n",
      "learning rate A :  tf.Tensor(9.8100696e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1819 is 0.0791 sec\n",
      "train AE loss : 1.900292992591858, train ANN loss : 3.0967631340026855\n",
      "AE loss : 1.8827829360961914, ANN loss : 3.016791820526123, Total loss : 191.29507446289062\n",
      "learning rate A :  tf.Tensor(9.8100696e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1820 is 0.0785 sec\n",
      "train AE loss : 1.759759783744812, train ANN loss : 3.0834527015686035\n",
      "AE loss : 1.5105985403060913, ANN loss : 3.00862455368042, Total loss : 154.0684814453125\n",
      "learning rate A :  tf.Tensor(9.809863e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1821 is 0.0784 sec\n",
      "train AE loss : 1.4082255363464355, train ANN loss : 3.062833547592163\n",
      "AE loss : 1.2404589653015137, ANN loss : 3.0163936614990234, Total loss : 127.06229400634766\n",
      "learning rate A :  tf.Tensor(9.809656e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1822 is 0.0771 sec\n",
      "train AE loss : 1.154772400856018, train ANN loss : 3.0649051666259766\n",
      "AE loss : 1.2068265676498413, ANN loss : 3.0218591690063477, Total loss : 123.70452117919922\n",
      "learning rate A :  tf.Tensor(9.809656e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1823 is 0.0792 sec\n",
      "train AE loss : 1.1233278512954712, train ANN loss : 3.07401442527771\n",
      "AE loss : 1.0133121013641357, ANN loss : 3.0407097339630127, Total loss : 104.37191772460938\n",
      "learning rate A :  tf.Tensor(9.80945e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1824 is 0.0784 sec\n",
      "train AE loss : 0.9426017999649048, train ANN loss : 3.0835390090942383\n",
      "AE loss : 0.8656377196311951, ANN loss : 3.062502145767212, Total loss : 89.62627410888672\n",
      "learning rate A :  tf.Tensor(9.809243e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1825 is 0.0776 sec\n",
      "train AE loss : 0.8049166798591614, train ANN loss : 3.092461585998535\n",
      "AE loss : 0.7506120800971985, ANN loss : 3.0850589275360107, Total loss : 78.1462631225586\n",
      "learning rate A :  tf.Tensor(9.8090364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1826 is 0.0773 sec\n",
      "train AE loss : 0.6978305578231812, train ANN loss : 3.1291098594665527\n",
      "AE loss : 0.8579694628715515, ANN loss : 3.0635569095611572, Total loss : 88.86050415039062\n",
      "learning rate A :  tf.Tensor(9.8090364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1827 is 0.0804 sec\n",
      "train AE loss : 0.7977939248085022, train ANN loss : 3.101994514465332\n",
      "AE loss : 0.744460940361023, ANN loss : 3.0860788822174072, Total loss : 77.53216552734375\n",
      "learning rate A :  tf.Tensor(9.80883e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1828 is 0.0787 sec\n",
      "train AE loss : 0.6921151280403137, train ANN loss : 3.127091646194458\n",
      "AE loss : 0.9877714514732361, ANN loss : 3.0376009941101074, Total loss : 101.81475067138672\n",
      "learning rate A :  tf.Tensor(9.80883e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1829 is 0.0797 sec\n",
      "train AE loss : 0.918776273727417, train ANN loss : 3.087066650390625\n",
      "AE loss : 0.8456765413284302, ANN loss : 3.058130979537964, Total loss : 87.62577819824219\n",
      "learning rate A :  tf.Tensor(9.808623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1830 is 0.0785 sec\n",
      "train AE loss : 0.7863673567771912, train ANN loss : 3.1017184257507324\n",
      "AE loss : 0.7348231673240662, ANN loss : 3.0795977115631104, Total loss : 76.5619125366211\n",
      "learning rate A :  tf.Tensor(9.808416e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1831 is 0.0782 sec\n",
      "train AE loss : 0.6832110285758972, train ANN loss : 3.118346929550171\n",
      "AE loss : 1.1024153232574463, ANN loss : 3.0201539993286133, Total loss : 113.26168823242188\n",
      "learning rate A :  tf.Tensor(9.808416e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1832 is 0.0806 sec\n",
      "train AE loss : 1.0255050659179688, train ANN loss : 3.067873239517212\n",
      "AE loss : 0.9336148500442505, ANN loss : 3.0360164642333984, Total loss : 96.39749908447266\n",
      "learning rate A :  tf.Tensor(9.80821e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1833 is 0.0800 sec\n",
      "train AE loss : 0.8682817220687866, train ANN loss : 3.073942184448242\n",
      "AE loss : 0.8037517666816711, ANN loss : 3.054597854614258, Total loss : 83.42977142333984\n",
      "learning rate A :  tf.Tensor(9.808003e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1834 is 0.0786 sec\n",
      "train AE loss : 0.7475301623344421, train ANN loss : 3.0964088439941406\n",
      "AE loss : 1.286117672920227, ANN loss : 3.0163280963897705, Total loss : 131.62811279296875\n",
      "learning rate A :  tf.Tensor(9.808003e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1835 is 0.0796 sec\n",
      "train AE loss : 1.1970112323760986, train ANN loss : 3.0751988887786865\n",
      "AE loss : 1.0722674131393433, ANN loss : 3.0218749046325684, Total loss : 110.24861145019531\n",
      "learning rate A :  tf.Tensor(9.807796e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1836 is 0.0792 sec\n",
      "train AE loss : 0.9974841475486755, train ANN loss : 3.0725808143615723\n",
      "AE loss : 1.6522102355957031, ANN loss : 3.042236804962158, Total loss : 168.2632598876953\n",
      "learning rate A :  tf.Tensor(9.807796e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1837 is 0.0798 sec\n",
      "train AE loss : 1.5404243469238281, train ANN loss : 3.1184310913085938\n",
      "AE loss : 2.0741353034973145, ANN loss : 3.0766429901123047, Total loss : 210.49017333984375\n",
      "learning rate A :  tf.Tensor(9.807796e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1838 is 0.0796 sec\n",
      "train AE loss : 1.9390878677368164, train ANN loss : 3.1449155807495117\n",
      "AE loss : 1.6421014070510864, ANN loss : 3.0345206260681152, Total loss : 167.2446746826172\n",
      "learning rate A :  tf.Tensor(9.80759e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1839 is 0.0784 sec\n",
      "train AE loss : 1.5312050580978394, train ANN loss : 3.1045522689819336\n",
      "AE loss : 1.795196771621704, ANN loss : 3.029186248779297, Total loss : 182.54885864257812\n",
      "learning rate A :  tf.Tensor(9.80759e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1840 is 0.0885 sec\n",
      "train AE loss : 1.6761300563812256, train ANN loss : 3.082719326019287\n",
      "AE loss : 1.7195080518722534, ANN loss : 3.0140092372894287, Total loss : 174.96481323242188\n",
      "learning rate A :  tf.Tensor(9.80759e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1841 is 0.0820 sec\n",
      "train AE loss : 1.6054586172103882, train ANN loss : 3.067370891571045\n",
      "AE loss : 1.5366990566253662, ANN loss : 3.0150575637817383, Total loss : 156.68496704101562\n",
      "learning rate A :  tf.Tensor(9.80759e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1842 is 0.0801 sec\n",
      "train AE loss : 1.4341484308242798, train ANN loss : 3.0655746459960938\n",
      "AE loss : 1.3753167390823364, ANN loss : 3.030048131942749, Total loss : 140.5617218017578\n",
      "learning rate A :  tf.Tensor(9.80759e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1843 is 0.0799 sec\n",
      "train AE loss : 1.282922387123108, train ANN loss : 3.0801730155944824\n",
      "AE loss : 1.3094983100891113, ANN loss : 3.0413882732391357, Total loss : 133.9912109375\n",
      "learning rate A :  tf.Tensor(9.80759e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1844 is 0.0825 sec\n",
      "train AE loss : 1.2213404178619385, train ANN loss : 3.0856919288635254\n",
      "AE loss : 1.0874651670455933, ANN loss : 3.065413236618042, Total loss : 111.81192779541016\n",
      "learning rate A :  tf.Tensor(9.807383e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1845 is 0.0790 sec\n",
      "train AE loss : 1.0130951404571533, train ANN loss : 3.111009120941162\n",
      "AE loss : 1.1743279695510864, ANN loss : 3.052277088165283, Total loss : 120.48506927490234\n",
      "learning rate A :  tf.Tensor(9.807383e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1846 is 0.0811 sec\n",
      "train AE loss : 1.094441294670105, train ANN loss : 3.0983998775482178\n",
      "AE loss : 1.4006917476654053, ANN loss : 3.025984525680542, Total loss : 143.09515380859375\n",
      "learning rate A :  tf.Tensor(9.807383e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1847 is 0.0809 sec\n",
      "train AE loss : 1.3069638013839722, train ANN loss : 3.071833848953247\n",
      "AE loss : 1.155725121498108, ANN loss : 3.046281099319458, Total loss : 118.61878967285156\n",
      "learning rate A :  tf.Tensor(9.807177e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1848 is 0.0791 sec\n",
      "train AE loss : 1.07687509059906, train ANN loss : 3.0876355171203613\n",
      "AE loss : 1.493493914604187, ANN loss : 3.0169146060943604, Total loss : 152.36631774902344\n",
      "learning rate A :  tf.Tensor(9.807177e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1849 is 0.0808 sec\n",
      "train AE loss : 1.393880844116211, train ANN loss : 3.0730903148651123\n",
      "AE loss : 1.2243461608886719, ANN loss : 3.0318360328674316, Total loss : 125.4664535522461\n",
      "learning rate A :  tf.Tensor(9.806969e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1850 is 0.0801 sec\n",
      "train AE loss : 1.1410104036331177, train ANN loss : 3.0818393230438232\n",
      "AE loss : 1.0240594148635864, ANN loss : 3.0527853965759277, Total loss : 105.45872497558594\n",
      "learning rate A :  tf.Tensor(9.8067634e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1851 is 0.0788 sec\n",
      "train AE loss : 0.9536569118499756, train ANN loss : 3.1096653938293457\n",
      "AE loss : 1.451673150062561, ANN loss : 3.015976905822754, Total loss : 148.18328857421875\n",
      "learning rate A :  tf.Tensor(9.8067634e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1852 is 0.1210 sec\n",
      "train AE loss : 1.3540828227996826, train ANN loss : 3.07066011428833\n",
      "AE loss : 1.9559354782104492, ANN loss : 3.0272603034973145, Total loss : 198.6208038330078\n",
      "learning rate A :  tf.Tensor(9.8067634e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1853 is 0.1133 sec\n",
      "train AE loss : 1.8287466764450073, train ANN loss : 3.0944199562072754\n",
      "AE loss : 2.2627909183502197, ANN loss : 3.046778917312622, Total loss : 229.3258819580078\n",
      "learning rate A :  tf.Tensor(9.8067634e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1854 is 0.1153 sec\n",
      "train AE loss : 2.118992567062378, train ANN loss : 3.1173205375671387\n",
      "AE loss : 1.7683097124099731, ANN loss : 3.014019012451172, Total loss : 179.8450164794922\n",
      "learning rate A :  tf.Tensor(9.806557e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1855 is 0.1195 sec\n",
      "train AE loss : 1.6507893800735474, train ANN loss : 3.0733978748321533\n",
      "AE loss : 1.8852510452270508, ANN loss : 3.0151188373565674, Total loss : 191.54022216796875\n",
      "learning rate A :  tf.Tensor(9.806557e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1856 is 0.0864 sec\n",
      "train AE loss : 1.7607684135437012, train ANN loss : 3.0792975425720215\n",
      "AE loss : 1.8095711469650269, ANN loss : 3.0112335681915283, Total loss : 183.96835327148438\n",
      "learning rate A :  tf.Tensor(9.806557e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1857 is 0.0791 sec\n",
      "train AE loss : 1.689013123512268, train ANN loss : 3.067070960998535\n",
      "AE loss : 1.4490737915039062, ANN loss : 3.010221242904663, Total loss : 147.91758728027344\n",
      "learning rate A :  tf.Tensor(9.8063494e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1858 is 0.0832 sec\n",
      "train AE loss : 1.349751353263855, train ANN loss : 3.0595500469207764\n",
      "AE loss : 1.1885420083999634, ANN loss : 3.023454427719116, Total loss : 121.87765502929688\n",
      "learning rate A :  tf.Tensor(9.8061435e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1859 is 0.0871 sec\n",
      "train AE loss : 1.1059377193450928, train ANN loss : 3.0762593746185303\n",
      "AE loss : 1.210919737815857, ANN loss : 3.027580499649048, Total loss : 124.11955261230469\n",
      "learning rate A :  tf.Tensor(9.8061435e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1860 is 0.0892 sec\n",
      "train AE loss : 1.1266604661941528, train ANN loss : 3.079038619995117\n",
      "AE loss : 1.3191683292388916, ANN loss : 3.0222761631011963, Total loss : 134.93911743164062\n",
      "learning rate A :  tf.Tensor(9.8061435e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1861 is 0.0872 sec\n",
      "train AE loss : 1.2277586460113525, train ANN loss : 3.070021867752075\n",
      "AE loss : 1.4948179721832275, ANN loss : 3.0142734050750732, Total loss : 152.49607849121094\n",
      "learning rate A :  tf.Tensor(9.8061435e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1862 is 0.0771 sec\n",
      "train AE loss : 1.3922735452651978, train ANN loss : 3.0585289001464844\n",
      "AE loss : 1.6890373229980469, ANN loss : 3.0097358226776123, Total loss : 171.91346740722656\n",
      "learning rate A :  tf.Tensor(9.8061435e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1863 is 0.0809 sec\n",
      "train AE loss : 1.5748366117477417, train ANN loss : 3.059553861618042\n",
      "AE loss : 1.3611669540405273, ANN loss : 3.0155208110809326, Total loss : 139.13221740722656\n",
      "learning rate A :  tf.Tensor(9.805937e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1864 is 0.0788 sec\n",
      "train AE loss : 1.2668930292129517, train ANN loss : 3.062495708465576\n",
      "AE loss : 1.5769318342208862, ANN loss : 3.0066816806793213, Total loss : 160.69985961914062\n",
      "learning rate A :  tf.Tensor(9.805937e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1865 is 0.0798 sec\n",
      "train AE loss : 1.4694368839263916, train ANN loss : 3.0669779777526855\n",
      "AE loss : 1.7676154375076294, ANN loss : 3.007742404937744, Total loss : 179.769287109375\n",
      "learning rate A :  tf.Tensor(9.805937e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1866 is 0.0796 sec\n",
      "train AE loss : 1.6492587327957153, train ANN loss : 3.0666446685791016\n",
      "AE loss : 1.8509494066238403, ANN loss : 3.007633924484253, Total loss : 188.1025848388672\n",
      "learning rate A :  tf.Tensor(9.805937e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1867 is 0.0796 sec\n",
      "train AE loss : 1.728419303894043, train ANN loss : 3.0693700313568115\n",
      "AE loss : 1.4763312339782715, ANN loss : 3.0060527324676514, Total loss : 150.63917541503906\n",
      "learning rate A :  tf.Tensor(9.80573e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1868 is 0.0770 sec\n",
      "train AE loss : 1.3756420612335205, train ANN loss : 3.0623581409454346\n",
      "AE loss : 1.5633814334869385, ANN loss : 3.0037319660186768, Total loss : 159.34188842773438\n",
      "learning rate A :  tf.Tensor(9.80573e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1869 is 0.0788 sec\n",
      "train AE loss : 1.4578524827957153, train ANN loss : 3.055631637573242\n",
      "AE loss : 1.2706797122955322, ANN loss : 3.0156404972076416, Total loss : 130.0836181640625\n",
      "learning rate A :  tf.Tensor(9.8055236e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1870 is 0.0769 sec\n",
      "train AE loss : 1.183071255683899, train ANN loss : 3.0570499897003174\n",
      "AE loss : 1.4249757528305054, ANN loss : 3.0071427822113037, Total loss : 145.5047149658203\n",
      "learning rate A :  tf.Tensor(9.8055236e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1871 is 0.0778 sec\n",
      "train AE loss : 1.32793390750885, train ANN loss : 3.0624866485595703\n",
      "AE loss : 1.629021167755127, ANN loss : 3.001469135284424, Total loss : 165.90357971191406\n",
      "learning rate A :  tf.Tensor(9.8055236e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1872 is 0.0785 sec\n",
      "train AE loss : 1.5196763277053833, train ANN loss : 3.05511736869812\n",
      "AE loss : 1.3166611194610596, ANN loss : 3.012866973876953, Total loss : 134.67897033691406\n",
      "learning rate A :  tf.Tensor(9.805317e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1873 is 0.0776 sec\n",
      "train AE loss : 1.2263001203536987, train ANN loss : 3.054450750350952\n",
      "AE loss : 1.567859411239624, ANN loss : 3.00213885307312, Total loss : 159.78807067871094\n",
      "learning rate A :  tf.Tensor(9.805317e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1874 is 0.0779 sec\n",
      "train AE loss : 1.462024211883545, train ANN loss : 3.058640241622925\n",
      "AE loss : 1.2717565298080444, ANN loss : 3.015815258026123, Total loss : 130.19146728515625\n",
      "learning rate A :  tf.Tensor(9.80511e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1875 is 0.0764 sec\n",
      "train AE loss : 1.1841583251953125, train ANN loss : 3.0655298233032227\n",
      "AE loss : 1.588433861732483, ANN loss : 3.001465082168579, Total loss : 161.8448486328125\n",
      "learning rate A :  tf.Tensor(9.80511e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1876 is 0.0799 sec\n",
      "train AE loss : 1.4811367988586426, train ANN loss : 3.0566627979278564\n",
      "AE loss : 1.2857946157455444, ANN loss : 3.0137391090393066, Total loss : 131.59320068359375\n",
      "learning rate A :  tf.Tensor(9.804904e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1877 is 0.0761 sec\n",
      "train AE loss : 1.197190761566162, train ANN loss : 3.0578224658966064\n",
      "AE loss : 1.671229600906372, ANN loss : 3.001289129257202, Total loss : 170.12425231933594\n",
      "learning rate A :  tf.Tensor(9.804904e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1878 is 0.0783 sec\n",
      "train AE loss : 1.5587207078933716, train ANN loss : 3.0528929233551025\n",
      "AE loss : 1.3446497917175293, ANN loss : 3.0084352493286133, Total loss : 137.47341918945312\n",
      "learning rate A :  tf.Tensor(9.804697e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1879 is 0.0850 sec\n",
      "train AE loss : 1.2521369457244873, train ANN loss : 3.06083607673645\n",
      "AE loss : 1.772965431213379, ANN loss : 3.00394606590271, Total loss : 180.30050659179688\n",
      "learning rate A :  tf.Tensor(9.804697e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1880 is 0.0793 sec\n",
      "train AE loss : 1.6542388200759888, train ANN loss : 3.060666799545288\n",
      "AE loss : 1.4161840677261353, ANN loss : 3.0042343139648438, Total loss : 144.62265014648438\n",
      "learning rate A :  tf.Tensor(9.8044904e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1881 is 0.0769 sec\n",
      "train AE loss : 1.3188941478729248, train ANN loss : 3.0586979389190674\n",
      "AE loss : 1.8357408046722412, ANN loss : 3.0077013969421387, Total loss : 186.58177185058594\n",
      "learning rate A :  tf.Tensor(9.8044904e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1882 is 0.0787 sec\n",
      "train AE loss : 1.7131911516189575, train ANN loss : 3.0641026496887207\n",
      "AE loss : 1.4597041606903076, ANN loss : 3.002865791320801, Total loss : 148.97328186035156\n",
      "learning rate A :  tf.Tensor(9.804284e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1883 is 0.0766 sec\n",
      "train AE loss : 1.3595490455627441, train ANN loss : 3.067044258117676\n",
      "AE loss : 1.8367754220962524, ANN loss : 3.0091114044189453, Total loss : 186.68663024902344\n",
      "learning rate A :  tf.Tensor(9.804284e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1884 is 0.0791 sec\n",
      "train AE loss : 1.7139779329299927, train ANN loss : 3.060861349105835\n",
      "AE loss : 2.055495023727417, ANN loss : 3.0179951190948486, Total loss : 208.5675048828125\n",
      "learning rate A :  tf.Tensor(9.804284e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1885 is 0.0792 sec\n",
      "train AE loss : 1.9207050800323486, train ANN loss : 3.076009511947632\n",
      "AE loss : 1.6117206811904907, ANN loss : 3.0021159648895264, Total loss : 164.1741943359375\n",
      "learning rate A :  tf.Tensor(9.804078e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1886 is 0.0771 sec\n",
      "train AE loss : 1.5022499561309814, train ANN loss : 3.0678489208221436\n",
      "AE loss : 1.7424519062042236, ANN loss : 3.0032687187194824, Total loss : 177.2484588623047\n",
      "learning rate A :  tf.Tensor(9.804078e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1887 is 0.0781 sec\n",
      "train AE loss : 1.6254169940948486, train ANN loss : 3.0607643127441406\n",
      "AE loss : 1.3929080963134766, ANN loss : 3.0052237510681152, Total loss : 142.2960205078125\n",
      "learning rate A :  tf.Tensor(9.8038705e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1888 is 0.0783 sec\n",
      "train AE loss : 1.2972644567489624, train ANN loss : 3.05922794342041\n",
      "AE loss : 1.141400694847107, ANN loss : 3.0202112197875977, Total loss : 117.1602783203125\n",
      "learning rate A :  tf.Tensor(9.8036646e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1889 is 0.0772 sec\n",
      "train AE loss : 1.0625090599060059, train ANN loss : 3.0633678436279297\n",
      "AE loss : 0.9550776481628418, ANN loss : 3.0412662029266357, Total loss : 98.54902648925781\n",
      "learning rate A :  tf.Tensor(9.803458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1890 is 0.0787 sec\n",
      "train AE loss : 0.8892104625701904, train ANN loss : 3.089179277420044\n",
      "AE loss : 1.1673214435577393, ANN loss : 3.01759672164917, Total loss : 119.749755859375\n",
      "learning rate A :  tf.Tensor(9.803458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1891 is 0.0827 sec\n",
      "train AE loss : 1.0868563652038574, train ANN loss : 3.0661110877990723\n",
      "AE loss : 1.4788897037506104, ANN loss : 3.0023787021636963, Total loss : 150.891357421875\n",
      "learning rate A :  tf.Tensor(9.803458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1892 is 0.0860 sec\n",
      "train AE loss : 1.3783588409423828, train ANN loss : 3.052628993988037\n",
      "AE loss : 1.8058034181594849, ANN loss : 3.005930185317993, Total loss : 183.58627319335938\n",
      "learning rate A :  tf.Tensor(9.803458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1893 is 0.0828 sec\n",
      "train AE loss : 1.6859785318374634, train ANN loss : 3.072953939437866\n",
      "AE loss : 1.9982047080993652, ANN loss : 3.010568380355835, Total loss : 202.83102416992188\n",
      "learning rate A :  tf.Tensor(9.803458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1894 is 0.0850 sec\n",
      "train AE loss : 1.8679919242858887, train ANN loss : 3.056644916534424\n",
      "AE loss : 1.982537031173706, ANN loss : 3.0048446655273438, Total loss : 201.25856018066406\n",
      "learning rate A :  tf.Tensor(9.803458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1895 is 0.0827 sec\n",
      "train AE loss : 1.8536903858184814, train ANN loss : 3.063509464263916\n",
      "AE loss : 1.822826862335205, ANN loss : 2.9994187355041504, Total loss : 185.28211975097656\n",
      "learning rate A :  tf.Tensor(9.803458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1896 is 0.0838 sec\n",
      "train AE loss : 1.7031892538070679, train ANN loss : 3.049534559249878\n",
      "AE loss : 1.6446948051452637, ANN loss : 3.0039570331573486, Total loss : 167.4734344482422\n",
      "learning rate A :  tf.Tensor(9.803458e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1897 is 0.0835 sec\n",
      "train AE loss : 1.5356725454330444, train ANN loss : 3.0575335025787354\n",
      "AE loss : 1.3220329284667969, ANN loss : 3.021862268447876, Total loss : 135.22515869140625\n",
      "learning rate A :  tf.Tensor(9.8032506e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1898 is 0.0785 sec\n",
      "train AE loss : 1.2328200340270996, train ANN loss : 3.066948890686035\n",
      "AE loss : 1.308478832244873, ANN loss : 3.025087356567383, Total loss : 133.8729705810547\n",
      "learning rate A :  tf.Tensor(9.8032506e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1899 is 0.0811 sec\n",
      "train AE loss : 1.22023344039917, train ANN loss : 3.0709173679351807\n",
      "AE loss : 1.4151906967163086, ANN loss : 3.0145516395568848, Total loss : 144.53363037109375\n",
      "learning rate A :  tf.Tensor(9.8032506e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1900 is 0.0797 sec\n",
      "train AE loss : 1.3202736377716064, train ANN loss : 3.069854259490967\n",
      "AE loss : 1.6144764423370361, ANN loss : 3.001413345336914, Total loss : 164.4490509033203\n",
      "learning rate A :  tf.Tensor(9.8032506e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1901 is 0.0796 sec\n",
      "train AE loss : 1.5073343515396118, train ANN loss : 3.0533463954925537\n",
      "AE loss : 1.8665425777435303, ANN loss : 2.9976613521575928, Total loss : 189.65191650390625\n",
      "learning rate A :  tf.Tensor(9.8032506e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1902 is 0.0821 sec\n",
      "train AE loss : 1.7447152137756348, train ANN loss : 3.0553841590881348\n",
      "AE loss : 2.049793004989624, ANN loss : 3.0013110637664795, Total loss : 207.9805908203125\n",
      "learning rate A :  tf.Tensor(9.8032506e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1903 is 0.0800 sec\n",
      "train AE loss : 1.917599081993103, train ANN loss : 3.057053565979004\n",
      "AE loss : 1.60621976852417, ANN loss : 2.9986014366149902, Total loss : 163.62057495117188\n",
      "learning rate A :  tf.Tensor(9.803045e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1904 is 0.0773 sec\n",
      "train AE loss : 1.499184489250183, train ANN loss : 3.0506086349487305\n",
      "AE loss : 1.769708275794983, ANN loss : 2.9963037967681885, Total loss : 179.96713256835938\n",
      "learning rate A :  tf.Tensor(9.803045e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1905 is 0.0810 sec\n",
      "train AE loss : 1.6526821851730347, train ANN loss : 3.05629301071167\n",
      "AE loss : 1.8842393159866333, ANN loss : 2.996248722076416, Total loss : 191.42018127441406\n",
      "learning rate A :  tf.Tensor(9.803045e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1906 is 0.0793 sec\n",
      "train AE loss : 1.7602460384368896, train ANN loss : 3.049596071243286\n",
      "AE loss : 1.4898130893707275, ANN loss : 3.0008437633514404, Total loss : 151.98214721679688\n",
      "learning rate A :  tf.Tensor(9.802838e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1907 is 0.0774 sec\n",
      "train AE loss : 1.389279842376709, train ANN loss : 3.0559680461883545\n",
      "AE loss : 1.2098307609558105, ANN loss : 3.01920485496521, Total loss : 124.00228118896484\n",
      "learning rate A :  tf.Tensor(9.8026314e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1908 is 0.0791 sec\n",
      "train AE loss : 1.1274385452270508, train ANN loss : 3.0671865940093994\n",
      "AE loss : 1.004630446434021, ANN loss : 3.0435357093811035, Total loss : 103.50658416748047\n",
      "learning rate A :  tf.Tensor(9.802425e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1909 is 0.0773 sec\n",
      "train AE loss : 0.9361112713813782, train ANN loss : 3.0910720825195312\n",
      "AE loss : 0.8505401611328125, ANN loss : 3.0694494247436523, Total loss : 88.12346649169922\n",
      "learning rate A :  tf.Tensor(9.802218e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1910 is 0.0778 sec\n",
      "train AE loss : 0.7926843762397766, train ANN loss : 3.1044082641601562\n",
      "AE loss : 1.067029356956482, ANN loss : 3.031613826751709, Total loss : 109.73454284667969\n",
      "learning rate A :  tf.Tensor(9.802218e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1911 is 0.0802 sec\n",
      "train AE loss : 0.9939257502555847, train ANN loss : 3.078479766845703\n",
      "AE loss : 1.4408740997314453, ANN loss : 3.0018107891082764, Total loss : 147.08921813964844\n",
      "learning rate A :  tf.Tensor(9.802218e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1912 is 0.0793 sec\n",
      "train AE loss : 1.3425893783569336, train ANN loss : 3.0497772693634033\n",
      "AE loss : 1.1731892824172974, ANN loss : 3.016324520111084, Total loss : 120.33525848388672\n",
      "learning rate A :  tf.Tensor(9.8020115e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1913 is 0.0781 sec\n",
      "train AE loss : 1.0925490856170654, train ANN loss : 3.064199686050415\n",
      "AE loss : 1.6397665739059448, ANN loss : 3.0030226707458496, Total loss : 166.97967529296875\n",
      "learning rate A :  tf.Tensor(9.8020115e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1914 is 0.0804 sec\n",
      "train AE loss : 1.528601884841919, train ANN loss : 3.0615415573120117\n",
      "AE loss : 1.3154133558273315, ANN loss : 3.0060434341430664, Total loss : 134.54737854003906\n",
      "learning rate A :  tf.Tensor(9.8018056e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1915 is 0.0778 sec\n",
      "train AE loss : 1.2248820066452026, train ANN loss : 3.0634853839874268\n",
      "AE loss : 1.8179763555526733, ANN loss : 3.011953592300415, Total loss : 184.80960083007812\n",
      "learning rate A :  tf.Tensor(9.8018056e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1916 is 0.0793 sec\n",
      "train AE loss : 1.695631742477417, train ANN loss : 3.0695178508758545\n",
      "AE loss : 2.1816177368164062, ANN loss : 3.030822515487671, Total loss : 221.19259643554688\n",
      "learning rate A :  tf.Tensor(9.8018056e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1917 is 0.0807 sec\n",
      "train AE loss : 2.0384700298309326, train ANN loss : 3.089674949645996\n",
      "AE loss : 1.688550591468811, ANN loss : 3.0040283203125, Total loss : 171.8590850830078\n",
      "learning rate A :  tf.Tensor(9.801598e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1918 is 0.0784 sec\n",
      "train AE loss : 1.5743519067764282, train ANN loss : 3.0574467182159424\n",
      "AE loss : 1.9108234643936157, ANN loss : 3.0072693824768066, Total loss : 194.08963012695312\n",
      "learning rate A :  tf.Tensor(9.801598e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1919 is 0.1930 sec\n",
      "train AE loss : 1.7838777303695679, train ANN loss : 3.0598957538604736\n",
      "AE loss : 1.9575334787368774, ANN loss : 3.004119634628296, Total loss : 198.7574462890625\n",
      "learning rate A :  tf.Tensor(9.801598e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1920 is 0.0921 sec\n",
      "train AE loss : 1.8287853002548218, train ANN loss : 3.0623092651367188\n",
      "AE loss : 1.8602474927902222, ANN loss : 3.000753164291382, Total loss : 189.02549743652344\n",
      "learning rate A :  tf.Tensor(9.801598e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1921 is 0.0792 sec\n",
      "train AE loss : 1.7377618551254272, train ANN loss : 3.058074712753296\n",
      "AE loss : 1.7151663303375244, ANN loss : 3.004591464996338, Total loss : 174.52122497558594\n",
      "learning rate A :  tf.Tensor(9.801598e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1922 is 0.0802 sec\n",
      "train AE loss : 1.601715087890625, train ANN loss : 3.051931619644165\n",
      "AE loss : 1.3657457828521729, ANN loss : 3.0210697650909424, Total loss : 139.5956573486328\n",
      "learning rate A :  tf.Tensor(9.801391e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1923 is 0.0765 sec\n",
      "train AE loss : 1.2736725807189941, train ANN loss : 3.07306170463562\n",
      "AE loss : 1.3722573518753052, ANN loss : 3.022076368331909, Total loss : 140.24781799316406\n",
      "learning rate A :  tf.Tensor(9.801391e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1924 is 0.0789 sec\n",
      "train AE loss : 1.2800418138504028, train ANN loss : 3.07099986076355\n",
      "AE loss : 1.502151370048523, ANN loss : 3.0104541778564453, Total loss : 153.2255859375\n",
      "learning rate A :  tf.Tensor(9.801391e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1925 is 0.0793 sec\n",
      "train AE loss : 1.4019944667816162, train ANN loss : 3.0629169940948486\n",
      "AE loss : 1.21478271484375, ANN loss : 3.033076524734497, Total loss : 124.5113525390625\n",
      "learning rate A :  tf.Tensor(9.801185e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1926 is 0.0778 sec\n",
      "train AE loss : 1.1326754093170166, train ANN loss : 3.079969882965088\n",
      "AE loss : 1.0053857564926147, ANN loss : 3.0607261657714844, Total loss : 103.59930419921875\n",
      "learning rate A :  tf.Tensor(9.800978e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1927 is 0.0773 sec\n",
      "train AE loss : 0.9372502565383911, train ANN loss : 3.1027610301971436\n",
      "AE loss : 0.8489382266998291, ANN loss : 3.0895018577575684, Total loss : 87.98332214355469\n",
      "learning rate A :  tf.Tensor(9.800772e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1928 is 0.0770 sec\n",
      "train AE loss : 0.7915980219841003, train ANN loss : 3.1245951652526855\n",
      "AE loss : 0.729889988899231, ANN loss : 3.1178231239318848, Total loss : 76.10681915283203\n",
      "learning rate A :  tf.Tensor(9.800566e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1929 is 0.0772 sec\n",
      "train AE loss : 0.6804088950157166, train ANN loss : 3.1543498039245605\n",
      "AE loss : 0.9826763868331909, ANN loss : 3.0518364906311035, Total loss : 101.31947326660156\n",
      "learning rate A :  tf.Tensor(9.800566e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1930 is 0.0788 sec\n",
      "train AE loss : 0.9163552522659302, train ANN loss : 3.0901167392730713\n",
      "AE loss : 1.450750708580017, ANN loss : 3.0050952434539795, Total loss : 148.08016967773438\n",
      "learning rate A :  tf.Tensor(9.800566e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1931 is 0.0793 sec\n",
      "train AE loss : 1.3536769151687622, train ANN loss : 3.0528135299682617\n",
      "AE loss : 1.1775097846984863, ANN loss : 3.0199098587036133, Total loss : 120.7708969116211\n",
      "learning rate A :  tf.Tensor(9.800359e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1932 is 0.0772 sec\n",
      "train AE loss : 1.0981559753417969, train ANN loss : 3.0664236545562744\n",
      "AE loss : 1.7736084461212158, ANN loss : 3.011887788772583, Total loss : 180.37271118164062\n",
      "learning rate A :  tf.Tensor(9.800359e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1933 is 0.0786 sec\n",
      "train AE loss : 1.6564733982086182, train ANN loss : 3.0741114616394043\n",
      "AE loss : 1.405935525894165, ANN loss : 3.007148265838623, Total loss : 143.6007080078125\n",
      "learning rate A :  tf.Tensor(9.8001525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1934 is 0.0778 sec\n",
      "train AE loss : 1.311532974243164, train ANN loss : 3.0659642219543457\n",
      "AE loss : 2.0151541233062744, ANN loss : 3.030269145965576, Total loss : 204.54566955566406\n",
      "learning rate A :  tf.Tensor(9.8001525e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1935 is 0.0790 sec\n",
      "train AE loss : 1.8831433057785034, train ANN loss : 3.0909066200256348\n",
      "AE loss : 1.5708969831466675, ANN loss : 3.0073163509368896, Total loss : 160.09701538085938\n",
      "learning rate A :  tf.Tensor(9.799946e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1936 is 0.0765 sec\n",
      "train AE loss : 1.4656530618667603, train ANN loss : 3.063476085662842\n",
      "AE loss : 1.2619667053222656, ANN loss : 3.0071845054626465, Total loss : 129.203857421875\n",
      "learning rate A :  tf.Tensor(9.799739e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1937 is 0.0767 sec\n",
      "train AE loss : 1.1770473718643188, train ANN loss : 3.0595240592956543\n",
      "AE loss : 1.0395503044128418, ANN loss : 3.0192387104034424, Total loss : 106.97427368164062\n",
      "learning rate A :  tf.Tensor(9.7995326e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1938 is 0.0762 sec\n",
      "train AE loss : 0.9695956110954285, train ANN loss : 3.068265438079834\n",
      "AE loss : 0.8750066161155701, ANN loss : 3.036785364151001, Total loss : 90.5374526977539\n",
      "learning rate A :  tf.Tensor(9.799327e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1939 is 0.0769 sec\n",
      "train AE loss : 0.816245973110199, train ANN loss : 3.0852315425872803\n",
      "AE loss : 0.7504534125328064, ANN loss : 3.0564005374908447, Total loss : 78.10173797607422\n",
      "learning rate A :  tf.Tensor(9.79912e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1940 is 0.0778 sec\n",
      "train AE loss : 0.7001417279243469, train ANN loss : 3.101547956466675\n",
      "AE loss : 1.2399829626083374, ANN loss : 3.0080859661102295, Total loss : 127.00638580322266\n",
      "learning rate A :  tf.Tensor(9.79912e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1941 is 0.0793 sec\n",
      "train AE loss : 1.1559925079345703, train ANN loss : 3.0664310455322266\n",
      "AE loss : 1.8788297176361084, ANN loss : 3.0398201942443848, Total loss : 190.92279052734375\n",
      "learning rate A :  tf.Tensor(9.79912e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1942 is 0.0807 sec\n",
      "train AE loss : 1.7535490989685059, train ANN loss : 3.1013834476470947\n",
      "AE loss : 1.4737136363983154, ANN loss : 3.0133495330810547, Total loss : 150.38470458984375\n",
      "learning rate A :  tf.Tensor(9.7989134e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1943 is 0.0792 sec\n",
      "train AE loss : 1.3737872838974, train ANN loss : 3.071303606033325\n",
      "AE loss : 1.1902191638946533, ANN loss : 3.010211706161499, Total loss : 122.03213500976562\n",
      "learning rate A :  tf.Tensor(9.7987075e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1944 is 0.0778 sec\n",
      "train AE loss : 1.109259009361267, train ANN loss : 3.065462827682495\n",
      "AE loss : 0.9853383898735046, ANN loss : 3.0193190574645996, Total loss : 101.55316162109375\n",
      "learning rate A :  tf.Tensor(9.7985e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1945 is 0.0774 sec\n",
      "train AE loss : 0.9183030724525452, train ANN loss : 3.07749080657959\n",
      "AE loss : 1.545814871788025, ANN loss : 3.0200655460357666, Total loss : 157.60154724121094\n",
      "learning rate A :  tf.Tensor(9.7985e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1946 is 0.0791 sec\n",
      "train AE loss : 1.441088318824768, train ANN loss : 3.0730538368225098\n",
      "AE loss : 1.2401312589645386, ANN loss : 3.011854648590088, Total loss : 127.02497863769531\n",
      "learning rate A :  tf.Tensor(9.798294e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1947 is 0.0775 sec\n",
      "train AE loss : 1.1556440591812134, train ANN loss : 3.070678472518921\n",
      "AE loss : 1.7982451915740967, ANN loss : 3.036810874938965, Total loss : 182.86134338378906\n",
      "learning rate A :  tf.Tensor(9.798294e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1948 is 0.0805 sec\n",
      "train AE loss : 1.6779924631118774, train ANN loss : 3.1038644313812256\n",
      "AE loss : 1.4154441356658936, ANN loss : 3.014467477798462, Total loss : 144.5588836669922\n",
      "learning rate A :  tf.Tensor(9.7980876e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1949 is 0.0789 sec\n",
      "train AE loss : 1.3194392919540405, train ANN loss : 3.066636800765991\n",
      "AE loss : 1.146878957748413, ANN loss : 3.0137484073638916, Total loss : 117.70165252685547\n",
      "learning rate A :  tf.Tensor(9.797881e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1950 is 0.0776 sec\n",
      "train AE loss : 1.0687298774719238, train ANN loss : 3.0724689960479736\n",
      "AE loss : 0.9522260427474976, ANN loss : 3.0241384506225586, Total loss : 98.24674224853516\n",
      "learning rate A :  tf.Tensor(9.797675e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1951 is 0.0777 sec\n",
      "train AE loss : 0.8873500823974609, train ANN loss : 3.0825486183166504\n",
      "AE loss : 0.807375967502594, ANN loss : 3.0402586460113525, Total loss : 83.7778549194336\n",
      "learning rate A :  tf.Tensor(9.7974684e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1952 is 0.0779 sec\n",
      "train AE loss : 0.7526559829711914, train ANN loss : 3.0891478061676025\n",
      "AE loss : 1.29336678981781, ANN loss : 3.0119481086730957, Total loss : 132.3486328125\n",
      "learning rate A :  tf.Tensor(9.7974684e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1953 is 0.0788 sec\n",
      "train AE loss : 1.2056264877319336, train ANN loss : 3.071481466293335\n",
      "AE loss : 1.0585781335830688, ANN loss : 3.016146421432495, Total loss : 108.87396240234375\n",
      "learning rate A :  tf.Tensor(9.797262e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1954 is 0.0775 sec\n",
      "train AE loss : 0.9865608215332031, train ANN loss : 3.0697433948516846\n",
      "AE loss : 0.8866000771522522, ANN loss : 3.02912974357605, Total loss : 91.68914794921875\n",
      "learning rate A :  tf.Tensor(9.797056e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1955 is 0.0795 sec\n",
      "train AE loss : 0.8265138864517212, train ANN loss : 3.076184034347534\n",
      "AE loss : 0.7571830153465271, ANN loss : 3.046140193939209, Total loss : 78.76443481445312\n",
      "learning rate A :  tf.Tensor(9.796849e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1956 is 0.0779 sec\n",
      "train AE loss : 0.7062669992446899, train ANN loss : 3.0933518409729004\n",
      "AE loss : 0.6577491760253906, ANN loss : 3.064981460571289, Total loss : 68.83989715576172\n",
      "learning rate A :  tf.Tensor(9.7966426e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1957 is 0.0774 sec\n",
      "train AE loss : 0.6139389276504517, train ANN loss : 3.1118650436401367\n",
      "AE loss : 0.5796855092048645, ANN loss : 3.0843281745910645, Total loss : 61.052879333496094\n",
      "learning rate A :  tf.Tensor(9.796436e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1958 is 0.0782 sec\n",
      "train AE loss : 0.5413447022438049, train ANN loss : 3.121094226837158\n",
      "AE loss : 1.0716345310211182, ANN loss : 3.0124495029449463, Total loss : 110.1759033203125\n",
      "learning rate A :  tf.Tensor(9.796436e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1959 is 0.0797 sec\n",
      "train AE loss : 0.998998761177063, train ANN loss : 3.062411308288574\n",
      "AE loss : 0.8963940739631653, ANN loss : 3.0221052169799805, Total loss : 92.66151428222656\n",
      "learning rate A :  tf.Tensor(9.79623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1960 is 0.0780 sec\n",
      "train AE loss : 0.8357276916503906, train ANN loss : 3.070561408996582\n",
      "AE loss : 1.6046552658081055, ANN loss : 3.034679412841797, Total loss : 163.5001983642578\n",
      "learning rate A :  tf.Tensor(9.79623e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1961 is 0.0804 sec\n",
      "train AE loss : 1.4970941543579102, train ANN loss : 3.0987062454223633\n",
      "AE loss : 1.2790950536727905, ANN loss : 3.013521194458008, Total loss : 130.92303466796875\n",
      "learning rate A :  tf.Tensor(9.7960234e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1962 is 0.0782 sec\n",
      "train AE loss : 1.1929912567138672, train ANN loss : 3.077908754348755\n",
      "AE loss : 1.048513412475586, ANN loss : 3.011082887649536, Total loss : 107.86243438720703\n",
      "learning rate A :  tf.Tensor(9.795817e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1963 is 0.0777 sec\n",
      "train AE loss : 0.9777301549911499, train ANN loss : 3.058847427368164\n",
      "AE loss : 0.8796077966690063, ANN loss : 3.0185787677764893, Total loss : 90.97935485839844\n",
      "learning rate A :  tf.Tensor(9.795611e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1964 is 0.0781 sec\n",
      "train AE loss : 0.8204060792922974, train ANN loss : 3.071237802505493\n",
      "AE loss : 0.7524715065956116, ANN loss : 3.0314040184020996, Total loss : 78.27855682373047\n",
      "learning rate A :  tf.Tensor(9.795404e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1965 is 0.0776 sec\n",
      "train AE loss : 0.7023718953132629, train ANN loss : 3.0846376419067383\n",
      "AE loss : 0.6545450091362, ANN loss : 3.0467588901519775, Total loss : 68.50125122070312\n",
      "learning rate A :  tf.Tensor(9.7951975e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1966 is 0.0778 sec\n",
      "train AE loss : 0.6116259694099426, train ANN loss : 3.0982019901275635\n",
      "AE loss : 0.5776888132095337, ANN loss : 3.0630416870117188, Total loss : 60.83192443847656\n",
      "learning rate A :  tf.Tensor(9.794992e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1967 is 0.0780 sec\n",
      "train AE loss : 0.540209949016571, train ANN loss : 3.1068665981292725\n",
      "AE loss : 0.5162617564201355, ANN loss : 3.0793049335479736, Total loss : 54.70547866821289\n",
      "learning rate A :  tf.Tensor(9.794785e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1968 is 0.0791 sec\n",
      "train AE loss : 0.48292291164398193, train ANN loss : 3.1211020946502686\n",
      "AE loss : 1.077250599861145, ANN loss : 3.0168869495391846, Total loss : 110.74195861816406\n",
      "learning rate A :  tf.Tensor(9.794785e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1969 is 0.0796 sec\n",
      "train AE loss : 1.0050127506256104, train ANN loss : 3.086082935333252\n",
      "AE loss : 1.851017951965332, ANN loss : 3.0963187217712402, Total loss : 188.19808959960938\n",
      "learning rate A :  tf.Tensor(9.794785e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1970 is 0.0800 sec\n",
      "train AE loss : 1.7291940450668335, train ANN loss : 3.154129981994629\n",
      "AE loss : 1.4481476545333862, ANN loss : 3.0446417331695557, Total loss : 147.85940551757812\n",
      "learning rate A :  tf.Tensor(9.794579e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1971 is 0.0777 sec\n",
      "train AE loss : 1.3521591424942017, train ANN loss : 3.1177423000335693\n",
      "AE loss : 1.979720950126648, ANN loss : 3.0883636474609375, Total loss : 201.0604705810547\n",
      "learning rate A :  tf.Tensor(9.794579e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1972 is 0.0798 sec\n",
      "train AE loss : 1.8521902561187744, train ANN loss : 3.150733709335327\n",
      "AE loss : 2.000727891921997, ANN loss : 3.051318645477295, Total loss : 203.12411499023438\n",
      "learning rate A :  tf.Tensor(9.794579e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1973 is 0.0798 sec\n",
      "train AE loss : 1.875093698501587, train ANN loss : 3.1124138832092285\n",
      "AE loss : 1.5478676557540894, ANN loss : 3.0150809288024902, Total loss : 157.8018341064453\n",
      "learning rate A :  tf.Tensor(9.794372e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1974 is 0.0773 sec\n",
      "train AE loss : 1.4488495588302612, train ANN loss : 3.0787296295166016\n",
      "AE loss : 1.2368643283843994, ANN loss : 3.0073299407958984, Total loss : 126.6937484741211\n",
      "learning rate A :  tf.Tensor(9.794166e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1975 is 0.0777 sec\n",
      "train AE loss : 1.1571264266967773, train ANN loss : 3.063359022140503\n",
      "AE loss : 1.2426631450653076, ANN loss : 3.017512559890747, Total loss : 127.28382110595703\n",
      "learning rate A :  tf.Tensor(9.794166e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1976 is 0.0808 sec\n",
      "train AE loss : 1.1641671657562256, train ANN loss : 3.0781238079071045\n",
      "AE loss : 1.2890843152999878, ANN loss : 3.0277163982391357, Total loss : 131.93614196777344\n",
      "learning rate A :  tf.Tensor(9.794166e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1977 is 0.0841 sec\n",
      "train AE loss : 1.2092392444610596, train ANN loss : 3.081543207168579\n",
      "AE loss : 1.398392677307129, ANN loss : 3.028223991394043, Total loss : 142.8674774169922\n",
      "learning rate A :  tf.Tensor(9.794166e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1978 is 0.0803 sec\n",
      "train AE loss : 1.3132907152175903, train ANN loss : 3.089742422103882\n",
      "AE loss : 1.1302766799926758, ANN loss : 3.044520139694214, Total loss : 116.07217407226562\n",
      "learning rate A :  tf.Tensor(9.79396e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1979 is 0.0781 sec\n",
      "train AE loss : 1.060860276222229, train ANN loss : 3.095625162124634\n",
      "AE loss : 1.3340595960617065, ANN loss : 3.025322198867798, Total loss : 136.43128967285156\n",
      "learning rate A :  tf.Tensor(9.79396e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1980 is 0.0805 sec\n",
      "train AE loss : 1.2532974481582642, train ANN loss : 3.083934783935547\n",
      "AE loss : 1.617714285850525, ANN loss : 3.010187864303589, Total loss : 164.7816162109375\n",
      "learning rate A :  tf.Tensor(9.79396e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1981 is 0.0801 sec\n",
      "train AE loss : 1.5214203596115112, train ANN loss : 3.0786075592041016\n",
      "AE loss : 1.2867411375045776, ANN loss : 3.0184268951416016, Total loss : 131.6925506591797\n",
      "learning rate A :  tf.Tensor(9.793753e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1982 is 0.0794 sec\n",
      "train AE loss : 1.2090741395950317, train ANN loss : 3.0825135707855225\n",
      "AE loss : 1.0512551069259644, ANN loss : 3.0367398262023926, Total loss : 108.16224670410156\n",
      "learning rate A :  tf.Tensor(9.7935466e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1983 is 0.0790 sec\n",
      "train AE loss : 0.9871552586555481, train ANN loss : 3.0882668495178223\n",
      "AE loss : 0.8787596225738525, ANN loss : 3.0594325065612793, Total loss : 90.9354019165039\n",
      "learning rate A :  tf.Tensor(9.79334e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1984 is 0.0820 sec\n",
      "train AE loss : 0.8248714804649353, train ANN loss : 3.1088504791259766\n",
      "AE loss : 1.2226386070251465, ANN loss : 3.0127649307250977, Total loss : 125.27661895751953\n",
      "learning rate A :  tf.Tensor(9.79334e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1985 is 0.2413 sec\n",
      "train AE loss : 1.1486752033233643, train ANN loss : 3.0630977153778076\n",
      "AE loss : 1.6949808597564697, ANN loss : 3.0100674629211426, Total loss : 172.50816345214844\n",
      "learning rate A :  tf.Tensor(9.79334e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1986 is 0.0971 sec\n",
      "train AE loss : 1.5940577983856201, train ANN loss : 3.072124481201172\n",
      "AE loss : 1.3413461446762085, ANN loss : 3.005535125732422, Total loss : 137.14015197753906\n",
      "learning rate A :  tf.Tensor(9.793134e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1987 is 0.0780 sec\n",
      "train AE loss : 1.2604821920394897, train ANN loss : 3.074226140975952\n",
      "AE loss : 1.78276526927948, ANN loss : 3.017780303955078, Total loss : 181.29429626464844\n",
      "learning rate A :  tf.Tensor(9.793134e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1988 is 0.0809 sec\n",
      "train AE loss : 1.676675796508789, train ANN loss : 3.0884652137756348\n",
      "AE loss : 1.401755690574646, ANN loss : 3.0040595531463623, Total loss : 143.17962646484375\n",
      "learning rate A :  tf.Tensor(9.792928e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1989 is 0.0799 sec\n",
      "train AE loss : 1.3171981573104858, train ANN loss : 3.0601744651794434\n",
      "AE loss : 1.1349823474884033, ANN loss : 3.0082662105560303, Total loss : 116.50650787353516\n",
      "learning rate A :  tf.Tensor(9.7927215e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1990 is 0.0788 sec\n",
      "train AE loss : 1.0658198595046997, train ANN loss : 3.073817491531372\n",
      "AE loss : 0.9421074986457825, ANN loss : 3.0211572647094727, Total loss : 97.23190307617188\n",
      "learning rate A :  tf.Tensor(9.7925156e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1991 is 0.0806 sec\n",
      "train AE loss : 0.8842320442199707, train ANN loss : 3.0765082836151123\n",
      "AE loss : 1.3414747714996338, ANN loss : 3.005192518234253, Total loss : 137.1526641845703\n",
      "learning rate A :  tf.Tensor(9.7925156e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1992 is 0.0840 sec\n",
      "train AE loss : 1.2599729299545288, train ANN loss : 3.0670833587646484\n",
      "AE loss : 1.7463254928588867, ANN loss : 3.0286834239959717, Total loss : 177.66123962402344\n",
      "learning rate A :  tf.Tensor(9.7925156e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1993 is 0.0837 sec\n",
      "train AE loss : 1.641188144683838, train ANN loss : 3.098522424697876\n",
      "AE loss : 1.3749407529830933, ANN loss : 3.0073554515838623, Total loss : 140.50143432617188\n",
      "learning rate A :  tf.Tensor(9.792308e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1994 is 0.0830 sec\n",
      "train AE loss : 1.2911615371704102, train ANN loss : 3.0736446380615234\n",
      "AE loss : 1.1145930290222168, ANN loss : 3.006960391998291, Total loss : 114.46627044677734\n",
      "learning rate A :  tf.Tensor(9.792102e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1995 is 0.0787 sec\n",
      "train AE loss : 1.0463347434997559, train ANN loss : 3.0573065280914307\n",
      "AE loss : 0.9261010885238647, ANN loss : 3.0165441036224365, Total loss : 95.62665557861328\n",
      "learning rate A :  tf.Tensor(9.7918964e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1996 is 0.0761 sec\n",
      "train AE loss : 0.869144082069397, train ANN loss : 3.0679118633270264\n",
      "AE loss : 1.2684892416000366, ANN loss : 3.008988380432129, Total loss : 129.85791015625\n",
      "learning rate A :  tf.Tensor(9.7918964e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1997 is 0.0803 sec\n",
      "train AE loss : 1.1909421682357788, train ANN loss : 3.0655224323272705\n",
      "AE loss : 1.0373097658157349, ANN loss : 3.0119388103485107, Total loss : 106.74292755126953\n",
      "learning rate A :  tf.Tensor(9.79169e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1998 is 0.0760 sec\n",
      "train AE loss : 0.9735968708992004, train ANN loss : 3.065988302230835\n",
      "AE loss : 1.4083422422409058, ANN loss : 3.0142366886138916, Total loss : 143.8484649658203\n",
      "learning rate A :  tf.Tensor(9.79169e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1999 is 0.0778 sec\n",
      "train AE loss : 1.3222583532333374, train ANN loss : 3.074366331100464\n",
      "AE loss : 1.1361340284347534, ANN loss : 3.010883331298828, Total loss : 116.62428283691406\n",
      "learning rate A :  tf.Tensor(9.791484e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 2000 is 0.0766 sec\n"
     ]
    }
   ],
   "source": [
    "train(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHoAAARYCAYAAAB+q7RIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOz9e1TUB57n/z/rRoFQ3FFAQEBB0YCKoOKFhAS700nMjO2mu81kv2e6J+P5/b47Oz39292Z72zP7sxuz2/nzO58p2f2fGdnfnYm3dntnnTSE+M2XtJGiQpeETpCFAQVARGD3EQuRUFV/f4gVASrFLCgAF+Pc/qon/rU5/P+mD5avut9MbjdbkREREREREREZP4zBjoAERERERERERHxDyV6REREREREREQWCCV6REREREREREQWCCV6REREREREREQWCCV6REREREREREQWCCV6REREREREREQWCHOgAwikffv2WYCTQC5gBdK+eKlxwqmNe/fuTZ/N2EREREREREREpuqpTvQAbuAgcAt47YtjLYwmfP4ZyAaCgLP79u1LRQkgEREREREREZnDnurWrb17947s3bv3vwD1DxxzMpr42Q9c++Lw/2I0AZT8xf82MZok+nhWAxYREREREREReYSnvaLHq717947s27fvvwF//MWhugcSQOzbt+9NwAD83WSvuW/fPivwI2AnYAI+Af7l3r17ex8456FWsr17997ct2/fCWDdF+87Afwfe/fu7X6CRxQRERERERGRBeipruh5jF1A2MSDXyRj9gJle/furZ7C9V4E/iXwP4B/C7wKfHvCOWOtZL+ccPwzoAj418ArwPemcF8REREREREReUo89Ymeffv2rQJivvjl8n379iV88fP/E+jz8pZdQAJTqOb5wg3AwWhVUNMXx+4/eIK3VrIvjv/e3r17fw2UfHEoeor3FhEREREREZGnwFOf6AFqgf/XFz8/BvzFvn371gDPAnVfHJ+YALrD6AyfqWgAPmK0oucwUAa8M9k379u3zwD8FTAE7JvivUVERERERETkKWBwu92TPjk2Ntadmpo6c9HMIXv37h3366tXr1JdXc1rr71GZWUllZWVU7peVlYW27dv58KFC9y/f58XXniBc+fOUV39cPdXXl4eubm5/NM//RN9faNFRdu2bWPlypUcO3aMpqamh94jIiIiIiIiIvNTZWVlh9vtjvPHtaaU6MnLy3NfvHjRH/d96tTW1lJWVkZBQQE2m42jR4/yzDPPsGXLlnHn9fT0UFNTQ21tLS+//DJRUVFcvHiRuro6tm3bRkpKCiaTiZCQkAA9iYiIiIiIiIj4k8FgqHS73Xn+uJa2bs2SjIwMWltbqaysxOVykZiYSE5OzkPnvf/++56fHzp0iMzMTOrrR0f2lJeXA5CQkMDOnTtnJ3ARERERERERmTdU0SMiIiIiIiIiEkD+rOjRMGYRERERERERkQVCrVszpPtmLRff/nO6b9YStMjG1j/4a+JWbQh0WCIiIiIiIiKygM3pRI/L5aKkpISOjg6cTid79uzBZrNRUlJCW1ub57yCggKys7MDGOl4w4P9fPJffhdb/DJ2/OBd+u+2YgoKnta1Ojo6+PDDD3G73bz55pt0dnZSVlbGvXv3WLx4Mc8++yxhYWF+fgIRERERERERmY/mdKIHICUlhdDQUG7cuDHueHp6Ops3bwbAarUGIjSfbledYOheJ9u+9zdELVtJ1LKV077W2bNnMRqNOJ1OAI4fP47VauU3fuM3+NWvfsWpU6d46aWX/BW6iIiIiIiIiMxjc3pGj9FoZP369URERDz0WnNzM/v376esrIyhoaEAROdbf8dtAC7901+z/3e3Uvrn36GvvXXK12lsbKSvr4/U1FQAHA4Hvb29JCYmEh0dzeLFi2ltbcXlcvkzfBERERERERGZp+Z0oseXrKwsXnnlFQoLC7l9+zZnz54NdEjjWMMiAYjJWMu27/0N7ZcvUP3zH07pGi6XiwsXLrBx40ZMJhMAQUFBmM1murq6cDqd9PT04Ha751yiS0REREREREQCY14melasWMHixYtJTU0lOjqarq6uQIc0TnzOFowmC0azBVOQFQwGjJaptZfV1tZitVpJS0vD7XYD4Ha72bJlC7dv3+btt9+mr68Pk8lEcPD05v+IiIiIiIiIyMIy52f09PT0YLfbAejt7cVkMlFZWUlWVhZ2u52uri5SUlICHOV4oXFL2fSv/oKa9/6Who9+Rnx2AWu/9d0pXePevXu0t7fz1ltveY698847/MZv/Aa7du3i/v37VFRUsGTJEgwGg78fQURERERERETmIcNYtchk5OXluS9evDiD4Txs3759436dmZlJf38/7e3tAMTHx1NYWEhoaOisxjUdvraI3blzh7KyMvr7+0lKSqKwsBCHw8Hg4CAAlZWVNDc3s2vXLhoaGqitrcVisZCamkpBQQEWiyXATyYiIiIiIiIi02UwGCrdbneeX6411xM9C4nL5eLSpUt0dnZy48YN9uzZQ0hICO+++y7x8fGsXbuWw4cPk5GRwdatWwMdroiIiIiIiIjMAn8memaldctbJQvAu+++O+48m83meW0hGtsiVlFR4TnW3t7O4OAgy5cvxzLQieleG1cqO7n7v/49W//gr4lbtSGAEYuIiIiIiIjIfDJrM3pSUlIIDQ3lxo0bAISGhvL6668DMDAwwIEDB1i6dOmkr9d9s5aLb/853TdrCVpkm7dJkYGBAQCMbief/Jffxbj6RQyh4eS9+aeYgrwNWZ6Y4PsZsHKmwxQRERERERGReWBWEj3eKlmMRiNhYWEA1NXVAbBmzZpJXW94sJ9P/svvYotfxo4fvEv/3VYfSZG5b9GiRQB8fvUSQ/c6CVu6AsOwi6S85x/xru8BxV/8PGamQxQRERERERGReSLg69VdLhd1dXXEx8cTEzO5pMXtqhMM3etk7Z7vEbVsJUl5zxOdPrkkUaBN3CIWERFBcHAwt+524Qxfwt3OLkauX6T0z79DX3urj6v8CPht4O1ZilpERERERERE5oOAJ3oaGxsZGBjwWs3T95OfcCd/E23rcun9b3/F2ODo/o7bAFz6p79m/+9ufUxSZG55//33qa2tBeDQoUNUVFRQXFyM3WVkYP1uIswunv+N12i/fIHqn//QyxX+CNgH/AvgA2D/7AUvIiIiIiIiInParM3omVjJYjKZWLRoEVeuXCEkJIS0tLRx5zuqq7n3/f9A+Pf/PaYlS+j+/e9iWbOGkJe+hjUsEoCYjLWs3fM9Sn/wbap//kO2/P5fzdbjTNvevXu9Hv/NV77Gwe9+jYxXfpuQRaFgMGC0WL2c+doXP6YAfw80zFCkIiIiIiIiIjLfzFpFz8RKlgsXLtDV1UVbWxsbEg0Yf2CBPzOAcwQA+9GPAVj0rW8S8vVdGEJCGPzVUQDic7ZgNFkwmi2YgqyPSIrMH6FxS9n0r/6ClnMfUfqfv018dgFrv/XdCWfVAv8LaAR+8cWxjFmNU0RERERERETmrlmr6PFVybJ371748XNgtIBzyHPcebcDAGNoKAaDAUNYGK6Ou8CXSZGa9/6Who9+5iMpMv+kbn2Z1K0vP+KMEOAj4B+++PlrwNdnIzQRERERERERmQdmLdHjU+2HcK8JsnbBZz/3HDbFxQLg6uvDGBSEu68PY2yc53VvSZGFsnLdt1RG16mLiIiIiIiIiDwssMOYncPw8R9B8V+CaXzrVXDxCwAMvPc+g/s/xD04SMiOYm9XAb5cuQ6w4wfvkvfmn87blesiIiIiIiIiItMR2Iqeyh/BohjI+jo0HBo95nYCZoLWrSPiB/+Zvv/x97iHhwn7179H8Msv+bzU2Mr1bd/7G6KWrSRq2crZeYYZ5nK5KCkpoaOjA6fTyZ49e7DZbJSUlNDZ2Ynb7SYhIYGioiKs1vk9p0hEREREREREnkxgEz2d9XDrHPzA8uWxv4yB7/cBEPadbxP2nW9P6lIPrly/f6eJyGUr2bj3B4QtXur3sGdbSkoKoaGh3Lhxw3MsKiqKgoICOjs7OXnyJDU1NeTl5QUwShEREREREREJtBlL9DyqEqWtre2Ls1ZTUHyI7LTFcPI/Qf1B+O0T07rffF65/ihGo5H169dTUVEx7vi2bdsACAsLA/CsrhcRERERERGRp9eMVvR4q0QBSE9PZ/PmzQCj7UYWC7xe8kT3mq8r10ecLv7fP67galsvjhEX+/+gkMSokEm91+12c+7cOUwmE1lZWTMcqYiIiIiIiIjMdTM2jHmsEiUiIuKh15qbm9m/fz9lZWUMDQ15effUja1cbzn3EaX/+dvzauX61sw4tq+Me/yJD3C73ZSXl3Pt2jVeeOEFYmJiZig6EREREREREZkvZn1GT1ZWFps2bWJgYIDjx49z9uxZduzY4Zdre1u5PteZTUZ+uzCdfzje4POcnp4eT2tWb28vJpOJixcvUldXx7Zt24iNjWVwcJCQkMlVAomIiIiIiIjIwjTriZ4VK1Z4fh4dHU1XVxcAQ31DHPjDX3H/bj8mi4mUvESe/b0CzEGm2Q5xznn//fc9Pz906BCZmZnU19cDUF5eDkBCQgI7d+4MSHwiIiIiIiIiMjfMaKLHWyVKZWUlWVlZ2O12urq6SElJAcBoMpL/xjpi0qK48lE91QdqSducQvqWFL/GNDQ0xIEDB+jr68NkMpGSkkJhYSGlpaW0trbidruJjY3lueeeIzw83K/3nq69e/eO+7XT6cTtdtPU1ITb7SYxMZGioqIARSciIiIiIiIic8WMJnq8VaL09/dTUjI6eDkxMZEtW7YAYAmxeJI6trhQTBYjEYk2z/udTienTp16KLkRFBQEQEdHBx9++CFut5s333wTl8vF2bNnuXnzJiMjI2RlZbF582aMRiP5+fnExMRQW1tLdXU1qampZGRksHHjRrq6ujh27Bg1NTVs3bp1Jn97PG7e7ePewDAArd0DBJmNxNp8D5JuaWmhoaGBdevWYbPZKCsr4+rVq2RnZ48779KlS9TU1OB2u1m5ciX5+fkYDIYZfRYRERERERERCZwZTfRMrER5nLbL7Rz6s+M4HU6S1iUQviTM89rjkhtnz57FaDTidDo9v25oaOD5558nLCyM7u5uACwWC+np6cDoanKTyURkZCTR0dHA6Fp4wPPr2fCt/+e05+f/+p2LvLQukf+4K9vn+eHh4RiNRsLCwjzr1S0Wy7hz7ty5w/nz58nPzycsLIxPPvmE6Ojoca1zIiIiIiIiIrKwzPqMnkeJWxHN7h++ROOZZip+dom6Y9fJ3rkKeHRyo7Gxkb6+PlJTU7l+/ToA9fX1rFixgtTUVABiY2M992lra+Pw4cM4nU6SkpKw2UYrh/7xH95m5JQZ7Is4+0k1t/M7Z2VO0Ln/9NUpnR8eHk5ycjLl5eUYDAbi4+NZtmwZ7733nqclbeyZEhISqKmpAeDEiRMMDAyQk5Pj92cQERERERERkcCbM4mejhtd2HuHCI8Pw2wdDcts/TLB4i25kZmZicvl4sKFC2zcuJGWlhYA7HY7TqeT9vZ2fvrTn2KxWNiwYYOnmiUuLo7du3dz48YNLl68yNWrV3nmmWf4jV2/wbX4RmqaLrGoaxHXTt6ckTlBT6q+vp6mpiby8/Ox2WyUlpZSW1v7UEsajA5r7u/vB0aTXXNl7pCIiIiIiIiI+J9fEj0ul4uSkhI6OjpwOp3s2bMHm81GeXk5165dw+Fw8Oyzz7Jy5Uqf1xi8Z+fU351noHsQa1gQa17KJPP55Z7XvSU3ampqMJvNWK1W0tLSaG5uBsBq/XK+zVe/+lXOnTvHiRMnWLZsGffu3cNutxMeHo7ZPPr4TqeT69evExsbS3J+Apc7LxHkND80J2iuGJuzYzabPc9gt9sfakmLiYmhvb0do9Ho+fVYhZOIiIiIiIiILDx+q+hJSUkhNDSUGzdueI7FxMQQHBxMVVXVY9+fvD6R33prl8/XvSU3xipV2tvbeeuttzznvvPOOyxdupTBwUFMJhNGoxGDwYDRaMRut3Pq1CkGBgawWq2sXr2ajIwMDh06RG9vL4ZuE84zVu46e0lal0BY3CL+9//+3w8lsUpKSujs7MTtdpOQkEBRUdG4BNNMysjIoLW1lcrKSlwuF4mJieTk5IxrSVu6dCmpqam0t7cTEhJCf38/ra2t3Lp1i6SkpFmJU0RERERERERml8Htdk/65M0vbnJv+sN8WvtvYzVZKU4p5tvP/I7n9YqKCn796197kiEAt27d4vDhw4+t6HmckZERTpw4wa1bt3C5XCxevJjnnnsOgMHBQQAqKytpbm5m165dBAcHc/LkSdrb2wkNDWXTpk2TqmYZGRrh/t1+z5yggt/ZgDPZTmdnJzdu3BhXrbRq1So6Ozs5efIkubm55OXlTfv5/GFkZIS+vj5PS5rVamVoaAiz2cz69etpaGjA4XDwxhtvBDROEREREREREfmSwWCodLvdfkkqTKmix2A2UJTyArmLN3CosYQPr+0nd0kea+PW+iOWRzKbzRQXF3t9bWw484svvjju+CuvvDKle3ibE2QJNpOzfj0VFRXjzt22bdu4e9vt9indy986OjoeaknbuHEjNTU1hIaGkpqayrVr1zCZZnawtIiIiIiIiIgEzpQSPcN3RvjNFaPtVWtj13Gk8TB9jvtTvunIp22M/PLquGOmdfFYXl015Wv50+PmBE3kdrs5d+4cJpOJrKysWYz0Yd5a0lauXElcXBxlZWXs37+fyMhIioqKAhqniIiIiIiIiMycac3o6R/u5+dX3yUhNJG8JaOVRT09PZ6qlt7eXkwmEy6XyzNHZ2BggN7eXsLDwzGtWYwpPQoAZ10HIx9dw/jFrwPpcXOCHuR2uz3DpouLi4mJiZnh6B4tKSmJ119//aHjsbGx7No1uWcSERERERERkfltyome/uF+/vTMn9Dr6OUvtv8lVnMwAO+//77nnEOHDpGZmcn9+/dpa2sDRuf31NbW8vrrr2OwmMAy2kLkrL0LoRaMWXH+eB4ALl26RE1NDW63m5UrV5Kfn+8Z5jwd3pJYFy9epK6ujm3bthEbG8vg4CAhISH+egQRERERERERkSmb2oyeIAP/8fSf0NZ/mz/e+H0sRgsDwwMssixi7969U765q70fd9M9TNuXYTAZp/x+b+7cucP58+fJz88nLCyMTz75hOjoaFasWDHta3pLYtXX1wNQXl4OQEJCAjt37nyy4EVEREREREREnsCUEj2WBDMNPaMJju+f/mMAvrXydV7P+q1p3dx5sRWMBswbEqf1fm8+//xzAFJTUwkPD+eTTz6hubn5iRI93pJYYxu/RERERERERETmiiklehxNw/zyNw/55cbuoRGc1Z9jXBmDIdzql2sChIaGAtDZ2cnw8DAAQ0NDfru+iIiIiIiIiMhcNa1hzP7grP4cHE5MeUv9et309HQaGhooLS3FbDZjMpk8K9Cf3MSV9j8DVvrp2iIiIiIiIiIiT2ZaiZ7m3mb+74v/ldb+21hNVopTivn2M78ztRvnL8Wc798kz5jc3Fw2bNhAR0cHp0+fJjMz049X/x5Q/MXPA7tpS0RERERERETkQdNK9Ay7HBSlvEDu4g0caizhw2v7yV2Sx9q4tf6Ob8qcTielpaX09/cTHh5OUVERS5Ys8eMdfgT8FCgE/p0frzt93d3dlJaW0tPTg9lsZuXKlWzevJmSkhLP1jOAgoICsrOzAxipiIiIiIiIiMykaSV6lkeuYHnk6HDjtbHrONJ4mD7Hfb8GNl0Wi4U9e/bM0NX/CFgLlAF/DywHvjFD95o8p9NJRkYGycnJXL58merqapKTk4HRVrbNmzcDYLX6bxaSiIiIiIiIiMw9TzSjp3+4n59ffZeE0ETylozOr+no6ODDDz/E7Xbz5ptv8tZbbz30vsmuYh/qG+LAH/6K+3f7MVlMpOQl8uzvFWAOMnnOcblclJSU0NHRgdPpZM+ePdhsNo4ePUpraytut5vY2Fiee+45wsPDn+Rxgde++DGF0URPwxNezz9iY2OJjY0FIDExkStXrngGUDc3N3P79m3i4uLYvn07FoslkKGKiIiIiIiIyAyadqKnf7ifPz3zJ/Q6evmL7X+J1RwMwNmzZzEajTidTgBef/11YLTqZP/+/VNqozKajOS/sY6YtCiufFRP9YFa0jankL4lZdx5KSkphIaGcuPGDc+xjIwMNm7cSFdXF8eOHaOmpoatW7dO93GBWuAisA0oH7vLE1zvYROTZEajEbfbzf79++ns7KSwsJBVq1b5fL/D4aCqqorw8HBSUlJwuVxs2rSJgYEBjh8/ztmzZ9mxY4dfYxYRERERERGRucM4nTcNDA/wH0//Cbf7bvNvNvxbLEYLA8MDNDY20tfXR2pqqufcsLAwwsLCuHPnDsPDw6xevXrS97GEWEjfkkJEgg1bXCgmi5GIRNv4BzAaWb9+PREREeOOp6WlETlwg8h3nwUgOjJyOo/6gBDgI+AN4B1Gq3u+/oTXHG8sSfag+vp6enp6Hvteh8PBoUOHsNvtvPTSS5jNZlasWMHixYtJTU0lOjqarq4uv8YrIiIiIiIiInPLtCp6rvdco6GnHoDvn/5jAL6ZuQdzjYWNGzfS0tLy0HuuXLmCzWZj2bJlU7pX2+V2Dv3ZcZwOJ0nrEghfMvlV6W8fPMtI3H/ANnKXhPh4z3Ff7V4lJSV0dnbidrtJSEigqKjogbk2qYyuU58ZDybJrl+/DsDw8DAVFRVkZ2fz6aef+nzvWJKnt7eXHTt2YDQacTgcnD9/nqysLOx2O11dXaSkpPi8hoiIiIiIiIjMf9NK9GTH5fDL3zw07tjly5dpsDaQlpZGc3MzAG63G4D29nbu3r3Lxo0bMRgMU7pX3Ipodv/wJRrPNFPxs0vUHbtO9k7f7UsetR+ye/Af6Y7byrH7q7hw8SJf+epXPS97a/eKioqioKCAzs5OTp48yT//8z9jt9unkAyaHpfLxYULFx5Kkl26dImYmBiSkpIemejp6Ojg7t27ABw8eBAYXTHf29tLSUkJMDq7Z8uWLU8Up4iIiIiIiIjMbU80jPlB9+7do729fdzw5XfeeYfvfOc7XLlyBZPJ9Mj5Mt503OjC3jtEeHwYZutoqGar6aHzenp6sNvtAPT29mLCRdvxt4nd8u+w3DiLATdm85ePOtbuVVFRMe4627ZtA0bbzQAWLVrEkiVLHpkMqqmpIS8vb0rPNVFtbS1Wq3Vcksxut1NTU8Orr77qeTa3243b7X4oWZaYmDjpAdciIiIiIiIisnD5LdGTk5NDRsbocOLKykqam5vZuXMndrud69evk56eTnBw8JSuOXjPzqm/O89A9yDWsCDWvJRJ5vPLHzrv/fff9/z80KFDJNtc9AVto7d6AJNrFUuHrrAx7/VJ3dPtdnPu3DlMJhOFhYXjkjzwcDJoLAnzJLwlyX7+858zMjLCBx984DlWVlaG1WolPT39ie8pIiIiIiIiIguP3xI9Y0OXAV588cVxr/3O7/zOtK6ZvD6R33pr12PPe6ia5cgfwNW/HX/s//kJfL/vkddxu92Ul5dz7do1iouLiYmJeSjRM3beWDIoKyvrsfE9jrck2de+9jVPFdLdu3cpLy9n/fr1LF269InvJyIiIiIiIiILk98SPdPhayjy0aNHaW1txe12Exsby3PPPUd4ePjkL7zl30DOG6M/P/mfoP4g/PaJcac81O5lMnHx4kXq6urYtm0bsbGxDA4OPnRpb8mgJ/WoJBlAXFzclLaViYiIiIiIiMjTKaCJHvA+FDkjI4ONGzfS1dXFsWPHqKmpYevWrZO/aETy6P8AXi/xesrEdq/MzEzq60c3iZWXlwOjCZbY2Fjg0cmgkJCQqTzytHV3d1NaWkpPTw9ms5mVK1eyefNmSkpKaGtr85xXUFBAdnb2rMQkIiIiIiIiInNHQBM9voYip6WlAaMVPwDR0dF+v7e34cXPPffcuF/v27fPs83KVzIoISGBnTt3+j0+b5xOJxkZGSQnJ3P58mWqq6tJTh5NaKWnp7N582aAJ94CJiIiIiIiIiLzU8Arenx5++23GRkZwWazkZCQ4PfrO51OTp06RVNTE263m8TERIqKiggKCvKcM5lk0GyKjY31VBglJiZy5coVhoaGAGhubub27dvExcWxfft2LBZLwOIUERERERERkcAwBjoAX3bv3s1XvvIV+vv7uXDhgt+v39LSQkNDA6tXr2bz5s00NTVx9epVv99nJjgcDqqqqggPDyclJYWsrCxeeeUVCgsLuX37NmfPng10iCIiIiIiIiISAAGv6PE2FLmtrY3Y2FgsFgsGg8GzfcqfwsPDMRqN4wYhz4cqGIfDwaFDh7Db7bz66quYzWZWrFjheT06Opqurq4ARigiIiIiIiIigRLwRM/EocjJycn09fV5kj5Lly5l48aNfr9veHg4ycnJlJeXYzAYiI+PJzMz0+/38aexJE9vby87duzAaDTicDg4f/48WVlZ2O12urq6SElJCXSoIiIiIiIiIhIAAU/0eJuDM10jn7Yx8svx7VemdfFYXl310Ln19fU0NTWRn5+PzWajtLSUmpoa1q5d67d4/K2jo8MzHPrgwYMA5Obm0tvbS0nJ6HaxxMREtmzZErAYRURERERERCRwAp7o8SfTmsWY0qMAcNZ1MPLRNYxf/Hoig8EAgNls9rSG9ff3+yWOyQx6no7ExES/JsZEREREREREZGFZUIkeg8UEFhMAztq7EGrBmBXn9dyMjAxaW1uprKzE5XKRmJhITk7OtO/d0dHBhx9+iNvtpri4mIaGBkJCQnA4HDQ1NVFdXU1eXt60ry8iIiIiIiIi8jgLKtEzxtXej7vpHqbtyzCYvC8WM5vNFBcX++2eZ8+exWg04nQ6CQ8PB8BkMlFQUEB5eTmNjY1K9IiIiIiIiIjIjJqz69WfhPNiKxgNmDckzsr9Ghsb6evrIzU1FYDg4GAA+vr6OH36NMHBwfT09OByuWYlHhERERERERF5Oi24RI97aARn9ecYV8ZgCLfO+P1cLhcXLlxg48aNmEyjbWNNTU0ARERE8Oyzz2K323G73QwNDc14PCIiIiIiIiLy9Jp3rVuPG3TsrP4cHE5MeUtnJZ7a2lqsVitpaWk0NzePe623t5cTJ04Ao8Ofxyp9RERERERERERmwrxL9LS0tNDQ0MC6deuw2WyUlZVx9epVsrOzATDnL8WcPztJHoB79+7R3t7OW2+95Tl2/vx5li5dyueff47L5cJoNJKamurZ9CUiIiIiIiIiMhPmXaInPDwco9FIWFgYYWFhAFgsloDFk5OTQ0ZGBgCVlZU0Nzezc+dOGhoauHPnDkFBQaSmplJQUBCwGEVERERERETk6TAvEz3JycmUl5djMBiIj48nMzMzYPE8mHB68cUXPcfj4uLYsmVLoMISERERERERkafQvBvGXF9fT1NTE/n5+RQVFXHnzh1qamoCHZaIiIiIiIiISMDNu4qesTk3ZrMZs3k0/P7+/kCGNCUul4uSkhI6OjpwOp3s2bMHm61o3DmnT/8r8vJex2qd+a1hIiIiIiIiIrJwzLtET0ZGBq2trVRWVuJyuUhMTCQnJyfQYU1JSkoKoaGh3Lhxw3Psxo3fJDJyN93d3Vy5Uo3VWkNeXl4AoxQRERERERGR+SbgiR7vFS427HY75eXltLa24na7yc3NJScnB7PZTHFxcaDDnjaj0cj69eupqKgYdzw9/RhwmvDwrUAsdrs9IPGJiIiIiIiIyPwV8EQPeK9w+eSTT+jo6GDHjh2YTCYGBwef8C4Tq2N+Bqx8wmv6yx8Ba3G7yzCb/541a7azcuVrgQ5KREREREREROaZgCd6vFW4DAwM0NLSQl5eHomJiX682/eAsWqgGD9e90m9htvt5syZFrZuhezsYGy2uRSfiIiIiIiIiMwHAU/0eNPX1wfAzZs3uXz5MosWLWLTpk0kJSU94ZV/BPwUKAT+nc+znE4np06doqmpCbfbTWJiIkVFRQQFBT3h/Uf19PR4WrMGB6uwWq9x6VI4JtMJAIKCVjM4OEhISIhf7iciIiIiIiIiT4c5uV59bNtUUFAQX/va13A6nZw4ceIJr/pHwD7gXwAfAPt9ntnS0kJDQwOrV69m8+bNNDU1cfXq1Se8/5fef/99amtrAfjkk/MMDf2S9ev/irVrL3H58mr+5//s59ixY367n4iIiIiIiIg8HeZERc+DFS69vb1ERkYSGRmJwWDAZDJ5fnwyYzNvUoC/Bxp8nhkeHo7RaCQsLIywsDAALBbLE97/S3v37vV63GyGNWtG/yciIiIiIiIiMlVzItHz/vvve35+6NAhMjMzef755ykrK2P//v1ERkZSVFT0BHeoBS4C24DyL45l+Dw7PDyc5ORkysvLMRgMxMfHk5mZ+QT3FxERERERERGZeXMi0eOrwmXXrl1+ukMI8BHwD1/8/DXg6z7Prq+vp6mpifz8fGw2G6WlpdTU1LB27drH3qm7u5vS0lJ6enowm82sXLmSzZs3c+vWLc6cOUN/fz9JSUkUFhZ6WtRERERERERERPxhTs7oeVJ9P/kJd/I30bYul97/9le43csYXad+GjjG6Lwe3zkug8EAgNlsxmwePa+/v39S93Y6nWRkZPD1r3+d5cuXU11dzc2bNzl27BgRERG88sor3Lp1iwsXLjzRM4qIiIiIiIiITDQnKnr8yVFdzb3v/wfCv//vMS1ZQvfvfxfLmjWEvPS1SV8jIyOD1tZWKisrcblcJCYmkpOTM6n3xsbGEhsbC0BiYiJXrlzh3r17OBwOkpOTiYuLIzo6mubm5mk9n4iIiIiIiIiILwsu0WM/+jEAi771TYxRUfT80f/F4K+OTinRYzabKS4ufqI4HA4HVVVVhIeHs3z5cs6fP09nZycOh4Pe3l4cDscTXV9EREREREREZKIFl+hx3u0AwBgaisFgwBAWhqvj7qzG4HA4OHToEHa7nVdffZWwsDA2bNhAVVUVdXV1WCwWzzYvERERERERERF/WXCJHlPcaNuUq68PY1AQ7r4+jLFxs3b/sSRPb28vO3bswGg04nA4SElJITU1ld7eXs6cOcPKlStnLSYREREREREReTosuERPcPEL3P/h3zDw3vuYlizBPThIyI4na8Oaio6ODu7eHa0gOnjwIAC5ubl0dXXR0tJCcHAwGRkZk575IyIiIiIiIiIyWQa32z3pk/Py8twXL16cwXD8o+/tH9P3P/4e9/Awi/Z8i/A/+kPPJi0RERERERERkbnEYDBUut3uPH9ca15W9Fy6dImamhrcbjcrV64kPz9/XCIn7DvfJuw73w5ghCIiIiIiIiIis88Y6ACm6s6dO5w/f541a9awefNmPv30U65fvx7osEREREREREREAm7eJXo+//xzAFJTU0lPTwegubk5kCGJiIiIiIiIiMwJ8651KzQ0FIDOzk6Gh4cBGBoaCmRIIiIiIiIiIiJzwrxL9KSnp9PQ0EBpaSlmsxmTyURYWFigwxIRERERERERCbh5l+iB0XXlGzZsoKOjg9OnT5OZmRnokEREREREREREAm5OJnqcTienTp2iqakJt9tNYmIiRUVFBAUF4XQ6KS0tpb+/n/DwcIqKiliyZEmgQxYRERERERERCbg5mehpaWmhoaGBdevWYbPZKCsr4+rVq2RnZ2OxWNizZ8+484eGhjhw4AB9fX2YTCZSUlIoLCzkyJEjtLW1ec4rKCggOzt7th9HRERERERERGRWzMlET3h4OEajkbCwMM/8HYvF4vN8o9FIfn4+MTEx1NbWUl1dTWpqKjA602fz5s0AWK3WGY9dRERERERERCRQ5myiJzk5mfLycgwGA/Hx8Y+cw2OxWDyr1sPCwjCZTERGRgKjq9dv375NXFwc27dvf2TCSERERERERERkPpuTiZ76+nqamprIz8/HZrNRWlpKTU0Na9eu9fmetrY2Dh8+jNPpJCkpCZvNRlZWFps2bWJgYIDjx49z9uxZduzYMYtPIiIiIiIiIiIye+ZkosdgMABgNpsxm0dD7O/vf+R74uLi2L17Nzdu3ODixYvU1tbS2dnpGehsMpno7Oyc8dhFRERERERERAJlTiZ6MjIyaG1tpbKyEpfLRWJiIjk5OT7P7+jowG63Ex4e7kkM9fb20tDQwMqVK7FYLHz22WeeeT8iIiIiIiIiIgvRHEv05AFgNkNxMcDPgJUADPUN8d7/+Uvu3+3HZDGRkpfIs79XgDnIhN1u59SpUwwMDGC1Wlm9ejWrVq3iypUrNDQ0eCqEHjXnR0RERERERERkvptjiR6A7wHFX/w8xnPUaDKS/8Y6YtKiuPJRPdUHaknbnEL6lhSSkpJ4/fXXx11lZGSEZcuW0dTU5Bno/Mwzz8zeY4iIiIiIiIiIzDJjoAN42I+A3wbeHnfUEmIhfUsKEQk2bHGhmCxGIhJtPq/y4EDnoqIi7ty5Q01NzYxGLiIiIiIiIiISSHOsouePgLVAGfD3wHLgG55X2y63c+jPjuN0OElal0D4Et8zd6Yz0FlEREREREREZD6bY4me1774MYXRRE/DuFfjVkSz+4cv0XimmYqfXaLu2HWyd67yeqWpDnQWEREREREREZnv5lCipxa4CGwDyr84luF5teNGF/beIcLjwzBbR8M2W00+r2Y2mykuLvb5uoiIiIiIiIjIQjOHEj0hwEfAP3zx89eAr3teHbxn59TfnWegexBrWBCrv5ZBm7GF8z85jdvtJjExkaKiIoKCggITvoiIiIiIiIhIgBncbvekT87Ly3NfvHhxBsOZvJs3b3L06FHWrVuHzWajrKyMgoICsrOzAx2aiIiIiIiIiMikGQyGSrfbneePa83BrVuTEx4ejtFoJCwsjLCw0aHMFoslwFGJiIiIiIiIiATOHGrdmprw8HCSk5MpLy/HYDAQHx9PZmZmoMMSEREREREREQmYeVvRU19fT1NTE/n5+RQVFXHnzh1qamoCHZaIiIiIiIiISMDM20SPwWAARrdrmc2jhUn9/f2BDElEREREREREJKDmbetWRkYGra2tVFZW4nK5SExMJCcnJ9BhiYiIiIiIiIgEzLxN9JjNZoqLiwMdhoiIiIiIiIjInDFvW7dERERERERERGQ8JXpERERERERERBaIedu6NVVOp5NTp07R1NSE2+0mMTGRoqIigoKCAh2aiIiIiIiIiIhfPDWJnpaWFhoaGli3bh02m42ysjKuXr1KdnZ2oEObV1wuFyUlJXR0dOB0OtmzZw82m41bt25x5swZ+vv7SUpKorCwEKvVGuhwRURERERERJ4qT03rVnh4OEajkbCwMMLCwgCwWCwBjmp+SklJYdmyZZ5fDw8Pc+zYMSIiInjllVe4desWFy5cCGCEIiIiIiIiIk+np6aiJzw8nOTkZMrLyzEYDMTHx5OZmRnosOYdo9HI+vXrqaio8Bzr6enB4XCQnJxMXFwc0dHRNDc3BzBKERERERERkafTU1PRU19fT1NTE/n5+RQVFXHnzh1qamoCHdaCEBISAkBnZycOh4Pe3l7sdnuAoxIRERERERF5+jw1FT0GgwEAs9mM2Tz62P39/YEMacEICwtjw4YNVFVVUVdXh8Vi8bTHiYiIiIiIiMjseWoSPRkZGbS2tlJZWYnL5SIxMZGcnJxAhzUv9fT0eCp2ent7MZlMpKSkkJqaSm9vL2fOnGHlypUBjlJERERERETk6fPUJHrMZjPFxcWBDmNBeP/99z0/P3ToEJmZmTgcDlpaWggODiYjI0NJNBEREREREZEAeGoSPeI/e/fufejYg2vXP/30U7KysrR2XURERERERGSWPTXDmGXmae26iIiIiIiISGCpokfGebAyx+l0smfPnklV5mjtuoiIiIiIiEjgqaJHHuKvyhytXRcRERERERGZXarokXH8WZmjtesiIiIiIiIis0uJHnksb5U5DofjofO0dl1EREREREQksJTomeemO1NnKiZbmaO16yIiIiIiIiKB5ddET3NvM//3xf9Ka/9trCYrxSnFfPuZ3/HnLcSLlJQUQkNDuXHjBvDlTJ2EhASKioo4ePAgFy5cYPv27ZO63nQrc7ytXZ9oaGiIAwcO0NfX57luYWEhZrOZ4eFh3nvvPQYGBnjppZdISkqawu+CiIiIiIiIiPg10TPsclCU8gK5izdwqLGED6/tJ3dJHmvj1vrzNvKAmdh2NZOVOUajkfz8fGJiYqitraW6uprU1FTS09O5dOkSQ0ND07quiIiIiIiIiPg50bM8cgXLI1cAsDZ2HUcaD9PnuO/PW8gkPDhTx263c/fuXVwuF/v27ZtUa9dkKnOmy2KxkJ6eDoy2hJlMJiIjI+nv76empoY1a9ZQXV09Y/cXERERERERWchmZL16/3A/P7/6LgmhieQtyZuJW8gjjM3Uqaur43/+z/+JwWDAYrF4Xp/uunR/aWtr4x//8R85c+YMCQkJ2Gw2Lly4QEZGBlFRUbMWh4iIiIiIiMhC4/dhzP3D/fzpmT+h19HLX2z/S6zmYH/fQiaYzEydiIgIbt++7Tn/SVq7nlRcXBy7d+/mxo0bXLx4katXr9LU1MQ3vvGNWY1DRERERLzztvAD4N133x13ns1m87wmIiJzg18TPQPDA/zH039CW/9t/njj97EYLQwMD7DIssift5EJJjNTB/Akeia7Ln0mdHR0YLfbCQ8Px2we/b+f0+nE4XDw05/+1HPe4cOH2bVrF3FxcbMSl4iIiIiMN3HhR2hoKK+//joAAwMDHDhwgKVLlwYyRBER8cKviZ7rPddo6KkH4Pun/xiAb618ndezfsuft5EJJjNT58FhzZNdlz4T7HY7p06dYmBgAKvVyurVq8nMzCQxMRGApqYmqqqqCA0N5Ze//CVms5mVK1eyefNm7ty5Q1lZ2bi5QkFBQbMSt4iIiMhC4a1ax+Vy8d577407LzQ0lP7+fuDLSp6xCp66ujoA1qxZM7vBi4jIY/k10ZMdl8Mvf/OQPy8pfjDddekzISkpyfNN0IPGqozi4uJITU3l9u3bJCcnc/nyZaqrq0lMTOTkyZPEx8fz7LPPcvjwYSoqKti6deusxC0iIiKykHir1lm7di3d3d2eVvqkpCTMZjOXL19mx44dfPzxxyxduhSXy0VdXR3x8fHExMQE8jFERMSLGRnGLHPL+++/T21tLTDa2nXhwgV+/etfc+DAAc6cOfNE69JnQmxsLDk5OURFRXkqfUZGRhgcHGT58uUsXryYJUuW0NTUFOBIRUREROYfo9HI+vXriYiI8Bwzm81s2rRpXOLmmWee8VRPj40AWLNmDY2NjQwMDKiaR0RkjvL7MGaZe2ZyXfpMcjgcVFVVER4ejsvlAvBsD7NYLAwODgYyPBEREZEFx+12A6NfvJlMJk9V+LVr14iLiyMmJoYzZ84QEhJCWlpaIEMVEREflOiROcnhcHDo0CHsdjuvvvoqfX19wOhq+LEfx9q9RERERMQ/enp6AMjMzBy38GNoaAiz2UxXVxdtbW3k5uZiNKo5QERkLlKiR2ZFR0cHH374IW63mzfffJN79+5x8uRJuru7Wbx4Mc899xyhoaHAl0me3t5eduzYgdFoJDY2luDgYK5fv05YWBiff/45K1asCPBTiYiIiMxP3mY4OhwO2tvbAYiIiOCNN95g0aJFlJSU0NPTw8svv4zRaJy31eIiIk8LJXoWiKGhIQ4cOEBfX59n2HJhYSFms5nh4WHee+89BgYGeOmll0hKSpr1+M6ePYvRaMTpdAJw/PhxLBYLr776KkePHqWsrIwXX3wRGE0K3b17F4CDBw8CkJubS3FxMadPn+bQoUMkJSWRn58/q8/gbUOFzWbDbrdTXl5Oa2srbreb3NzcOTXzSERERGSiB6t1Dh06RGZmJvX19Z5jR44cITMzk5ycHFXwiIjMM0r0LBBGo5H8/HxiYmKora2lurqa1NRU0tPTuXTpEkNDQ369X3d3N6WlpfT09IxbgQ48lFgaHh6mr6+P1NRUrl+/zv379+nq6vIM/EtOTqa2thaXy4XRaCQxMdHnN0WvvfaaX59jqiZuqAD45JNP6OjoYMeOHZhMJs0OEhERkTnP22et5557btLniojI3KW0/AJhsVhIT08nIiKCsLAwTCYTkZGR9Pf3U1NT4/etCE6nk4yMDL7+9a+zfPlyqquraW1tBRiXWHK73Vy4cIGNGzdiMpkAPImQBwcru91uT/nwXOVtQ8XAwAAtLS2sWbOGxMRElixZQmpqauCCFBERERERkaeaKnoWkLa2Ng4fPozT6SQpKQmbzUZ5eTlJSUlUV1cDo+1H3d3dPufjTFZsbCyxsbEAJCYmcuXKFYaGhsYllqqrq2lubsZqtZKWlkZzczMAwcHBwPjBygaDwXN8PhkbEn3z5k0uX77MokWL2LRpU0Da40RERERERESU6JmjfM3cGRkZ8TkPJi4ujt27d3Pjxg0uXrzI1atXaWpqIjIyEoPB4FmX+aj5OFP14Ar0lJQUysrKyMjIICoqCoD+/n7a29t56623PO/Zv38/UVFR3Lx5k6SkJFpaWkhOTp6Xfd9WqxWAoKAgCgsLKS0t5cSJE7zxxhsBjkxERETEO29zBwHefffdcefZbDbPa9N5j4iIBIYSPXOUr5k7V69e9ToPpqOjA7vdTnh4OGbz6H9Wp9M5bnsCwEcffQTgcz7OVExcgd7b20tTUxPf+MY3PNU7aWlprF+/HoDKykqam5vZuXMnJpOJkydP8stf/pLFixezbdu2J/49mw0TN1RERkZ6Emkmk8nzo4iIiMhsmkoi5pvf/OZDcwdDQ0N5/fXXgdHW9AMHDrB06dJx753Oe0REZPYp0TNHjc3cATwzd0JCQmhpaSEvL4/ExMRx59vtdk6dOsXAwABWq5XVq1ezYsUKPvvsM1avXs3169fp6uoiOzubmpoar/NxFi1a5LOSqKOjg7KyMvr7+0lKSmLz5s18/PHH41agd3R04HA4+OlPf+qJ65NPPmHXrl3ExcU9VDW0a9cuz8+HhoZ477335uzWsDHeNlQ8//zzlJWVsX//fiIjIykqKgpYfCIiIvL0mmwiZmzuYEVFhee9RqORsLAwAOrq6gDGzXiczntERCQwlOiZwybO3BlrvfI2DyYpKcnzF/mYy5cvExoaytq1a+np6aGrq4usrCxqamp8zsfxVkmUnJzMuXPniI+P59lnn+Xw4cOcOXPmoRXozzzzjCd509TURFVVFVu3bvW0cT3KbG8Nmy5fWyceTFqJiIiIzDZ/JWJcLhd1dXXEx8cTExMzqXtP5z0iIjJz5kyip7m3mf/74n+ltf82VpOV4pRivv3M7wQ6rICaOHMnJSUFmPw8mHv37k15Po63SqKhoSEGBwdZvnw5ixcvZsmSJXR0dDxy1WZcXBx5eXmTflZv9524NWxsoLSIiIiITN5kEzEul4tf/OIXDAwMMDAwwP3797HZbBw9etQzH3JsPuGDGhsbGRgYoKCgYCYfQ0REJmnOJHqGXQ6KUl4gd/EGDjWW8OG1/eQuyWNt3NpAhxYQ3mbujCU/JjsPJicnh4yMDGBq83EmVhKN/YX+YLvX2Gwgf/K1NezB4c4iIiIiMjXeEjET5w6aTCaCg4NxuVyYTCacTqfn3IyMDDZu3EhLSwtnz54d955FixZx5coVQkJCSEtLm90HExERr+ZMomd55AqWR64AYG3sOo40HqbPcT/AUQWOt5k7K1euJC4ubtLzYMLCwjzluo+ajzPRxEqi5ORkYPw69JCQkCd9xMfed2xr2IPDnUVERETEO2/JG1+JGG9zB3Nycrh//z7x8fHcuXPH8/rY+3y9p62tjdzc3Hm5QVVEZCHya6LHH+1X/cP9/PzquySEJpK3ZPKtPwuNt5k7ALGxsTM6D8ZbJdHYDJ/r168TFhbG559/zooVK2b8vmNbwx4c7nz48GHPcGcRERER+dJUEjG+WvD37t1LRUXFuEQPwNtvvw2Mbu362te+RmRk5GOvJSIigeHXRM+Ttl/1D/fzp2f+hF5HL3+x/S+xmoP9GZ5MgrdKotWrVxMdHc3p06c5dOgQSUlJ5Ofnz/h9MzMzPdvFpjrcWURERORp86jkzZPavXs33d3dHDt2jAsXLvCVr3zlia8pIiIzw6+JnidpvxoYHuA/nv4T2vpv88cbv4/FaGFgeIBFlkX+DFEew1slUXd3N2fPnqW3txez2YzNZsNqtVJSUkJbW5vnvIKCArKzs/12X8DTIjbV4c4iIiIiMnXe2r/a2tqIjY3FYrFgMBg81dciIjI3zcif0tNpv7rec42GnnoAvn/6jwH41srXeT3rt2YiRJkCp9NJRkYGycnJXL582bNyHSA9PZ3NmzcDPLSFobu7m9LSUnp6ejCbzaxcuZLNmzdjt9spLS31bG8wmUysWbOGzZs3c+nSJWpqahgaGiI6OprCwsJ5tabT5XJRUlJCR0cHTqeTPXv2YLPZKCkpobOzE7fbTUJCAkVFRV63VoiIiIgE0sT2r+TkZPr6+jxJn6VLl7Jx48YARigiIo/j90TPdNuvsuNy+OVvHvJ3OAteR0cHH374IW63mzfffNOTKLHb7eTm5vqlCiY2NpbY2FgAEhMTuXLlCkNDQwA0Nzdz+/Zt4uLi2L59u2czF/hOEFVXV3P37l1WrVpFQkIC9fX1VFdXs2TJEs6fP+/5AHHgwAEqKyvnXWlwSkoKoaGh3Lhxw3MsKiqKgoICOjs7OXnyJDU1NapQEhERkTlH83ZEROY/v47GH2u/ut13m3+z4d962q9k5pw9e3bcYL3w8HDWr18/I/dyOBxUVVURHh5OSkoKWVlZvPLKKxQWFnL79m3Pus0xsbGx5OTkEBUV5Zm1MzAwQEtLC9nZ2Wzfvp0VK1awatUqANxuN2FhYVitViIiIjAajeMSR/OB0Whk/fr1REREjDu+bds2YmNjWbZsGYCnJFpERERERETEn/xa0aP2q9nV2NhIX18fqampXL9+HYDly5dz7969h5IuT8rhcHDo0CHsdjuvvvoqZrN53Oat6Ohourq6fL53LEEUGhoKwM2bN7l8+TLBwcGMjIx4kkf37t2joqKCxsZGQkJCFlTVi9vt5ty5c5hMJrKysgIdjoiIiIiIiCxAfk30eGu/6ujo4Ec/+pGntcjlcnH27Flu3rzJyMgIWVlZnhkvMnkul4sLFy6wceNGWlpaZvReY0me3t5eduzYgdFopL29nSNHjjA8PIzJZGJkZITU1NSHBjTn5+dz8+ZNT4LI7XYDEBQUREFBAYcPH8blcvHNb36Tvr4+KioqSEtLIzs7m6NHj3L69GlefPHFGX2+2eB2uykvL+fatWsUFxfPq7lDIiIiIiIiMn/M+Mj8sdYip9Pp+XVDQwPPP/88YWFhdHd3z3QIC1JtbS1Wq5W0tDSam5sBPEkUf+vo6ODu3bsAHDx4EIBVq1ZhtVpxOp04nU7cbjfp6elcuXLFM6B5eHiYTz75hPv373sSREFBQURGRgJQXl6Oy+UiJCQEo9HI8PAwACaTCbPZjMFgoL+/f0aeaSZ521Zx8eJF6urqPC1cg4ODno1iIiIiIhN136zl4tt/TvfNWoIW2dj6B39N3KoNgQ5LRETmAcNUkgN5eXnuixcvTvr8xsZGzp07x+LFi7l+/TpvvvkmP/7xj1mxYgXPPvvsdOKVL5w5c4bPPvts3DGz2cy3vvUturq6OHz4MKtXr+aZZ57xJFZmyo0bNzh27BjFxcVcvnyZu3fvetawjyWIxuTm5pKamsrx48e5d+/eQ68ZjUbPsOeIiAi2bNnime8zX+zbt2/crzMzM6mvrx93LCEhgZ07d85mWCIiIjJPDA/2U/Ldr2KLX0be7/xH+u+2sih6CdHpawIdmoiIzBCDwVDpdrv9MrtkxhI9LpeLX/ziF+Tl5dHS0kJ9fT1vvPEGP/3pT4mKimJoaAiLxcKGDRvGzXpZCHytFZ/Y1lRQUEB2dva07tHX18fg4CAAlZWVNDc3s2vXLqqqqmhqahp37t69e2dsbbnD4eCXv/wlIyMj/It/8S+4efMm4eHhDAwMcPz4cVJSUtixY8cT30dERETkadF0+hBn/vu/5YU/+18szlo48wpFRMQ3fyZ6Zqx1y1trkdVq9bz+1a9+lXPnznHixAmWLVs277YrPYqvteKAp60Jxv9+TFVYWBhhYWEA42bYbNiwgYSEBM+9r1y5QmNj44ysLX+SAc3zha+k3dGjR2ltbcXtdhMbG8tzzz1HeHh4oMMVERGRBaC/4zYAl/7pr7l/p4nIZSvZuPcHhC1eGuDIRERkPpixRM+9e/dob2/nrbfe8hx75513WLp0KYODg5hMJoxGIwaDYdx68IUgNjaW2NhYABITEz2tSADNzc3cvn2buLg4tm/f/kQJrqGhIQ4cOEBfXx8mk4mUlBQKCwuJjY1leHiYa9euAaMzdvy9ttzbgGaHw8H58+fJysrCbrfT1dVFSkrKE90n0Hwl7TIyMti4cSNdXV0cO3aMmpoatm7dGuhwRUREZAGwhkUCEJOxlrV7vkfpD75N9c9/yJbf/6vABiYiIvOC3xI9LpeLkpISOjo6cDqdvPrqq2RkZDA0NMSJEycYGBjAYDCwePFiPv/8cw4cOEBoaCgvvPACJpPJX2HMKQ+uFU9JScHlcrFp0yZPW9PZs2efqK3JaDSSn59PTEwMtbW1VFdXk5qaSnp6OlVVVTgcDgAWL16M2Wz269pybwOac3Nz6e3tpaSkBBhNcm3ZsuWJ7hNovpJ26enpwOj/72G0eklERETEH+JztmA0WTCaLZiCrGAwYLRMvxJcRESeLn6t6ElJSSE0NJQbN24QGhqKzWbjyJEjuN1uXnnlFUwmE4ODg+Tn5/vztnPSbLQ1WSwWT8IhLCwMk8lEZGQk3d3dVFdXYzabGRkZYXBw0O9ryxMTE9m7d+8TxT+fTEzaAbz99tuMjIxgs9lISEgIcIQiIiKyUITGLWXTv/oLat77Wxo++hnx2QWs/dZ3Ax2WiIjME35L9BiNRtavX09FRYXn2MDAAC0tLeTl5T12c9JQ3xAH/vBX3L/bj8liIiUvkWd/rwBz0Pyr9pnNtqa2tjYOHz6M0+kkKSkJq9XKP//zP2MwGFizZg2XLl3yVJ2MrS13u900Nzezb98+3nzzTe7du8fJkyfp7u5m8eLFPPfcc4SGhj5xbA/yNevGbrdTWlrqmXdjMplYs2YNmzdv5s6dO5SVldHf309SUhKFhYUEBQX5Na7J8Ja0A9i9ezfd3d0cO3aMCxcuPPHMIxEREZExqVtfJnXry4EOQ0RE5qEZm9EDo5uhAG7evMnly5dZtGgRmzZtIikp6aFzjSYj+W+sIyYtiisf1VN9oJa0zSmkb5l/M15ms60pLi6O3bt3c+PGDS5evMinn37qmQd06dIlAO7cuUNeXp5nMPPYXKSxBNDx48exWCy8+uqrHD16lLKysieq9vHG16yb6upq7t69y6pVq0hISKC+vp7q6moSExM5efIk8fHxPPvssxw+fJiKiopZn4PjLWl3//592tvbiY2NxWKxYDAYPMkfERERERERkUCa0X+djm2VCgoKorCwkNLSUk6cOMEbb7zx0LmWEIsnqWOLC8VkMRKRaJvJ8GbMbLU1dXR0YLfbCQ8P9yQaxjZxPejatWvs2rWL3NxcGhsbOXfuHIsXL+b69evcv3+frq4uNm3aRExMDMnJydTW1uJyufw6JNvbrJsHK75yc3OB0cqwW7dueVrOli9fzuLFi4mOjqa2tpa6urpxFUEzXfXjLWm3evVq2tra6O3txWQyebaZiYiIiIiIiASaXxM9PT092O12AHp7e4mMjCQyMhKDwYDJZPL86Evb5XYO/dlxnA4nSesSCF/ycNJCvmS32zl16hQDAwNYrVZWr15NZmamp02uqamJqqoqtm7dSlRUFC6XiwsXLrBx40ZaWloAGBwcBPBs4bJYLLjdbux2O4sWLfJ7zA/OuhlrDxur+AoODmZkZITw8HBPtdFYXCaTCbfbze7duz0VQbNR9fO0zSISERERERGR+c2viZ7333/f8/NDhw6RmZnJ888/T1lZGfv37ycyMpKioiKf749bEc3uH75E45lmKn52ibpj18neucqfIc64idvH9uzZg81m49atW5w5c2Zc5clYxdN0JSUl8frrrz90PCQkBBht63pwu9bly5exWq2kpaXR3NwMQHBwMADDw8OeHw0Gg+e4P02cdeN2u4HRiq+CggIOHz6My+Xim9/8pqftbywuo9FIaGgoUVFRnoqgiVU/S5YsoampaVbau3yttr9z547f/zuLiIiIzBXePusCvPvuu+POs9lsntdERGR2+TXR46vyYdeuXY99b8eNLuy9Q4THh2G2joZlts6/QcwwfvsYjCYrPv74Y4xGI06nk8bGRoxGIy+88MKMJIB8uXfvHu3t7bz11lueY/v37ycqKoqbN2+SlJRES0sLycnJfm3bAu+zboKCgoiMjASgvLwcl8tFSEgIRqOR2NhYgoODuX79OmFhYXz++eesWLFiXEXQxKofi8XiqVDyh0clc06fPk1fXx8JCQlERERw+fJlkpKSOHPmDAkJCRQVFXHw4EEuXLjA9u3b/RaTiIiISKBN/KwbGhrq+fJxYGCAAwcOsHTp0kCGKCLyVJv1CbK+tmsN3rNz6u/OM9A9iDUsiDUvZZL5/PLZDu+Jeds+1tPTw/DwMMuWLcNkMnHjxg1u377N8PAwx44dm7XEQE5ODhkZGQBUVlbS3NzMzp07MZlMnDx5kl/+8pcsXryYbdu2+f3evgZUP//88xw/fpx79+4Box8O/umf/onc3FyKi4s5ffo0hw4dIikpibVr146rCJpY9TM8POypZvIHo9FIfn4+MTEx1NbWUl1dPS6Z8/zzz3Pw4EGGh4cxGo1UVFTgcDi4desWFouFqKgorl69yvXr13G73Z7/zqrwERERkfnK22ddo9HomRNZV1cHwJo1awISn4iIBCDR86jtWr/11uMrf+ajseTDokWLPNUnQ0ND9PT04HA4SE5OJi4ujujoaE9L1UwICwvz/CU8cavWZKqunsSjZt1885vf9Pm+1157DfBeEeSr6sdfLBYL6enpwOjvnclkwmazef6bjYyMMDw8zOeff05iYiLp6emUl5cTERHBtWvXCAoKwu1288orr9DZ2cnJkyepqakZ104nIiIislC4XC7q6uqIj48nJiYm0OGIiDy1Zj3RM3G7ltFspOLKeT6p/XjcNqXZbGmaaWFhYWzYsIGqqirPXJrQ0FBPAqizsxOHw0Fvby8OhyOQoc5ZviqCJlb95Ofn+/W+bW1tHD58GKfTSVJSEjbb6Ca4zs5OUlNTsVqtOBwObt++TWpqKhs2bKCyshIY/bATHh5ObGysJ8E2NqxcREREZKFpbGxkYGCAgoKCQIciIvJUm/VED4zfrrV4TQypa+NJXZHq2aYUHx/PiRMn5u2sk4nbx8bmu6SmplJVVUVjYyPp6enjEkB1dXVYLBav69Hl0RVBY1U/MyEuLo7du3dz48YNLl68yM2bNz3JnNraWiwWC1arFbvdTn9/PzU1NQDExMQwODjIypUrcbvdnDt3DpPJRFZW1ozFKiIiIjIbvH3WXbRoEVeuXCEkJIS0tLQARygi8nQLSKJn4natFQVpROV/uU3p3r17s9rS5G/eto85HA6am5s9Q44TExMZGBjwJIB6e3s5c+YMK1euDFTYMkFHRwd2u53w8HDM5i8GhJvNLFmyhJCQEC5evOj5kLN69WrWr1/P3bt3aWtro7Ozk6VLl5KdnU15eTnXrl2juLhYZcwiIiIy73n7rJuTk0NbWxu5ubl+X+ohIiJTM+uJHl/btR7cprR8+XLOnz8/b1uafFWe7Nu3z7Mp6siRI54EUEtLC8HBwWRkZJCTkzOboT4xX5upzp07x7Vr13A4HDz77LPzMoFlt9s5deoUAwMDWK1WVq9eTWZmJseOHfP8N1u3bh15eXl0dXXR3t5OYWEhN2/e5Ny5cyxfvpzTp09TV1fHtm3biI2NZXBw0K8Do0VERERmm6/Pur6Oi4jI7JqVRE9HRwcffvghbrebF/Ne5tTfnae/awAsboxpLjqDP6fu0GeebUoLtaVpIf7l520zVWpqKjExMQQHB1NVVRXoEKctKSnJsyr0QV/5ylceOuYrKXTq1ClgdH08QEJCAjt37pzZwEVEREREROSpNSuJnrNnz2I0GkcH2q5LYNlvx9HQ0MDzzz+P1WrlxIkTOBwOzzYlh8OhlqZ5wttmqsjISKKjo7l161aAo5s9vpJCCzG5JyIiIiIiInPXjCd6Ghsb6evrIzU1levXrwNQX1/PihUrSE1N5fbt29y/fx8Yv02pq6trXrc0PU18baZ6FG8tX7m5ueN6vgFsNht79uyZqdBFREREREREFpQZTfS4XC4uXLjAxo0baWlpAUZbXJxOJ+3t7fz0pz/FYrHw/PPPs2LFipkMJaC6u7spLS2lp6dn3Ap5gOHhYd577z0GBgZ46aWXSEpK8vt9ZnpV/cTNVFevXuWZZ5555Ht8tXyNVcUMDAxw4MABli5d6rc4RURERERERBa6GR2JX1tbi9VqJS0tDbfbDTAuwfDVr36VRYsWceLECYaHh2cylIByOp1kZGTw9a9/neXLl1NdXU1raysAly5dYmhoaMbuc/PmTY4dO0ZERASvvPIKt27d4sKFC365H4zOX7pz5w5Go3HcZqq+vj76+/uB0aRNb2/vuPeNtXxFRESMa/kKCwsjLCzMs2VtzZo1fotVREREREREZKGb0Yqee/fu0d7ezltvveU59s4777B06VIGBwcxmUwYjUYMBsOCXsMYGxtLbGwsgGeF/NDQEP39/dTU1LBmzRqqq6tn5D4zvare1xDiQ4cO0dbWBkBFRQW1tbUPzbDx1fLlcrmoq6sjPj5e68hFREREREREpmBGEz05OTlkZGQAUFlZSXNzMzt37iQ4OJiTJ09y4MABQkNDeeGFFzCZTDMZypzw4Ar5lJQUysrKyMjIICoqCoCTJ09it9vHrSk3m81Tbu+azVX1voYQT2azlK+Wr8bGRgYGBigoKPBbnNPlqx3ObrdTXl5Oa2srbreb3NxczZESERERERGRgJvRRM9YGw7Aiy++OO61V155ZSZvPec4HA4OHTrkWSHf29tLU1MT3/jGNzwVNpmZmWRmZo6bWZOens6lS5ew2+0AfPTRR5jNZk8i6PLly9TU1DA0NER0dDQFBQWcPXvWL6vqvQ1Mnm7yaaKOjg7sdjvh4eHjWr4Arly5QkhICGlpaVO65kwYa4dLTk7m8uXLVFdXk5yczKeffsrt27cxGAyYTCaamppYvXo1586d49q1azgcDp599lltixMRERGvXC4XJSUldHR04HQ62bNnDzabjZKSEjo7O3G73SQkJFBUVOTX2YoiIrLwzcp69dkQqEHEkzGW5Ont7fWskO/o6MDhcPDTn/7Uc96vf/1rUlNTx82sGWvvWr16NZ999hnbt2+nu7vbkwg6f/48S5cuZePGjXz44Yd89NFHGAwGv6yq9zUweSz59CSzhXy1fHV1ddHW1ubZvPbhhx/idrt58803OXXqFPX19Z5rPPPMM2zZsmXaMUyGt3a4gYEBWltbWb58OXl5eZ7fm+bmZmJiYggODqaqqmpG4xIREZH5LyUlhdDQUG7cuOE5FhUVRUFBAZ2dnZw8eZKamhry8vICGKWIiMw3CybR463yIj4+nhMnTni+DTl48CAXLlxg+/btsxpbR0cHd+/eBb5cIf/MM8+wa9cuAJqamqiqqiI7O5v//b//Ny6XyzOzpry8nIyMDM+smtDQUIaHh8cNL7ZarURERHgSOw/e50lW1Y8NTAa8Jp+eZLaQr5av6Oho9u7dC0BJSQlGoxGn0+l5ffHixRQXFwMQFBQ0rXtPx4PtcKGhocDoDKpf/vKXnhlTkZGRREdHc+vWrVmLS0REROYno9HI+vXrqaioGHd827ZtAJ4K7LGqbhERkcmaE4meob4hDvzhr7h/tx+TxURKXiLP/l4B5qDJz+0JxCDiyUpMTPQkL7yJi4sjLy+PkZERsrKyxs2smdje9dFHH41LBGVlZVFRUUFjYyMhISH8xm/8hmeosT94G5g8lnwamy00ExobG+nr6yM1NZXr1697jnd2dvLBBx8QHR3N1q1biY6OnrEYxkxsuxvbIOd2uxkaGsLlcmE0Gv36+y4iIiJPL7fbzblz5zCZTGRlZQU6nCnz1ZZ2584dysrK6O/vZ3FUOKaqA9xr/IygRTa2/sFfE7dqQ6BDFxFZEGZ11ZXT6eSTTz7hJz/5CT/+8Y/51a9+hcPhwGgykv/GOl7776+wasdyrp28SfPF1mndY+IgYmDcIOK5+q2ItzXlTqfT09516tQpYPQvzqysLG7dusXVq1epqKggLS2NnTt34nK5OH36tF/jGhuYnJeX57lnU1MTubm5noSHv7lcLi5cuMDGjRvHDelOTU3l5ZdfZseOHXR3d3Py5MkZuf+DHmy7e/755zEajYSEhBAZGYnVamXHjh0EBwfjcrm4evXqjMcjIiIiC5vb7aa8vJxr167xwgsvzNsNpCkpKSxbtszz65GRET7++GMiIyP5ygtFtLbepjs8lR0/eJe8N/8UU1BwAKMVEVlYZrWip6WlhYaGBtatW4fNZqOsrIyrV6+SnZ1N+pYUAGxxoZgsRiISp14dMbHy4kkGEc82XzNrEhMTAaitraWuro4NGzZ4VtGPJUFMJhNmsxmDwUB/f/+k7jeZbVIul4sVK1awdu1ar8mnMYcPH2bXrl3ExcX55feitrYWq9VKWlqap5LJ7XaTmprqOScxMZGmpia/3O9RvLXd5ebmkpubS2VlJR9//DHBwaMfTMxmM319fZ7/BgMDA/T29hIeHj7jcYqIiMj809PT4/kSsre3F5PJxMWLF6mrq2Pbtm3ExsYyODhISEhIgCOdGm9tae3t7QwODrJ8+XKGmz/D2NPK8NKVRC0b/Z+IiPjPrCZ6wsPDMRqN47ZxWSwWANout3Poz47jdDhJWpdA+JKpJWS8DTx+kkHEs83XzJqxv9iHhoa4desWv/71rz2JoJUrVzI4OMiVK1dobGwkIiJi0ivJfW2Tqq6upqOjgx07dtDZ2cmvf/1rrl696jX5NDZbaOvWrV7buHxt7Tpy5Mgjt0ncu3eP9vZ23nrrLc+xd955h9WrV3s2cbW1tc1K25avtrtbt255Zge53W7P782hQ4doa2sDoKKigtraWq//XUVERETef/99z88PHTpEZmamZ/FEeXk5AAkJCezcuTMg8fnTwMAAMPrZ/37HbQxOB3a7nf2/u5XIZSvZuPcHhC1eGuAoRUQWBsNU2m/y8vLcFy9enPbNRkZGOH78OE1NTRgMBpYsWcIrr7yC0WhkZGiE+3f7aTzTTMXPLrHlzRNk7/z0gXf/DPCepHE6nXz00Ue0to5v9/I2iDgvL89TESOjbty4wbFjxygqKuKTTz4hLy+P3NzcJ77u8PAwLS0t47Z2FRcXc/v2bVatWuXZJpGbmztum0RfXx+Dg4MAVFZW0tzczK5du7h8+TJNTU2MjIwQFxfH9u3bZ3ROkIiIiIhMX0VFBb/+9a/Zs2cP9+/f5+DBgxQXF+NurOTE+SpMsSm8tC2P0h98m5SCF9ny+38V6JBFRALGYDBUut1uv6xZnNWKnvr6epqamsjPz8dms1FaWkpNTQ1LbcnYe4cIjw/DbB0NyWwdBr4HFH/xbt/9yS0tLbS2to5rCSsoKCA7O3vGn2m+87ZN6ubNm1y+fJlFixaxadMmkpKSpnVtX1u7HjwGD2+TeLDi68UXX/Qcf+6556YVh4iIiIjMroltaZGRkQQHB3P9+nUyU1bjvHqHGIsTU5AVDAaMFutjrigiIpM1q4keg8EwelOz2TPzpb+/n0GXnVN/d56B7kGsYUGseSmTzOf/O9AE/BQoBP6dz+s+qiVMfPO1TSooKIjCwkJKS0s5ceIEb7zxxrTv4W1rF8z/bRIiIiIi4pu3trTi4mJOnz5N6a1bLI6OwHDuPUo/+THx2QWs/dZ3AxitiMjCMquJnoyMDFpbW6msrMTlcpGYmEhOTg5hYWH81lu7Jpz974C1QBnw98By4BterxseHk5ycjLl5eUYDAbi4+PJzMyc0WeZ77zNNAoKCiIyMhKDwYDJZPL8+CTGtnY9uDJ+zZo1nm0SxcXF83abhIiIiIh4523GIcBrr7325S++9X/MUjQiIk+XWU30mM1miouLH38iAGN/CaQwmuhp8Hmmr5awtWvXPmHEC5evbVLPP/88ZWVl7N+/n8jISIqKip7oHna7nfDwcE8Fl9lspqysbNw2iba2Ns6cOfPQBrCxZJDD4eDZZ5+ds4O0RUREROaTRy3MGFsqAaObXdPS0ry+pjEJIiJz16wmeiavFrgIbAPKvziW4fNsXy1h4puvbVIAu3ZNrK6aHl8r40+dOgV8uU0iNjbW6wawmJgYgoODqaqq8ks8IiIiIgvVZJM3BQUFrFq1ivz8/HELM1JTUwFITU0lKSmJqKgobty4weXLlz2vpaens3nzZoBxG1NFRGRumaOJnhDgI+Afvvj5a8DXfZ7tqyXsaefrL/zLly9TU1PD0NAQ0dHRFBYWTrl9qru7m9LS0nFVOGvWrOHdd98dd57NZuPNN98cd8xXgglGE1BXrlxhaGiIrKwsbt26NaW4RERERAJv4tIU39tj/cVoNPpM3kxM0PhamAFw69Yt7ty5Q1xcHIsXLx73WnNzM7dv3/ZsP9VMTBGRuWlOJno6OsL48MPncLvdvPnmmxiNRtxuN/v3f0BnZyeFhYWsWrXKc/6jWsJcLhclJSV0dHTgdDrZs2cPNpuNkpISOjs7cbvdJCQkUFRUtOC+mfD1F/758+dZunQpGzdu5MCBA1RWVvKVr3xlStd2Op1eq3Bef/11AAYGBjhw4ABLly6d9DUf3ACWkpIypXhERERE5pbJbY990FSqcia2TT0qeeMtQeNtYUZWVhabNm2itbWViooKWlpaHnptYGCA48ePc/bsWXbs2PHo5+kb4sAf/or7d/sxWUwkrY+nc+lt+gf7p/x8IiIyeXMy0XP27FmMRiNOp9NzrL6+np6enmldLyUlhdDQUG7cuOE5FhUVRUFBAZ2dnZw8eZKamhry8vyysn7O8PUXflhYGFarlYiICIxG47S+jYmNjSU2NhYYX4UztvWsrq4OgDVr1kzqehM3gI214ImIiIjMTz9iMttjHzSVqhxvvCVvnqmrI+jn7+F2DHM9+xnOmkzs+MpXvC7MeOaZZwCIjo7m+vXr9Pf3c+vWrXGvjb3e1dX1+OcxGcl/Yx0xaVFc+aie6gO15Pz2Sla/nDmt5xMRkcmZc/+abmxspK+vj9TUVK5fvw7A8PAwFRUVZGdn8+mnn07pekajkfXr11NRUTHu+LZt2wA8iQm73f7kwc9Bvr6tqaiooLGxkZCQkCdKcHmrwnG5XNTV1REfHz+pljBvG8AcDgcOh8Mza2lgYIDe3l7Cw8OnHauIiIjI7PgjJrs99kFTrcqZaGLy5trBg4T/zX8n/Pv/HtOSJRh+/7vUpqXRkZvrc2FGfHw8fX193Lt3j4iICIaGhjyvZWVlYbfb6erqmlT1tSXEQvqW0fNscaGYLEYy168gIiJiWs8nIiKTM6cSPS6XiwsXLrBx40ZaWlo8xy9dukRMTAxJSUlTTvQ8itvt5ty5c5hMJrKysvx23bnE27c1FRUVpKWlkZ2dzdGjRzl9+jQvvvjilK/tqwqnsbGRgYEBCgoKJnUdXxvA2traPGW8FRUV1NbWelrDREREROauyW+PnehRLVWPapvytu3Uev4CAPYdxQwFB+O0WEhoaPC5MOP69es0NDQwMjICjH7R9uBrJSUlwGg195YtWyb3PJfbOfRnx3E6nCStS2DA3ceBf9w/5ecTEZHJm1OJntraWqxWK2lpaTQ3NwOjlTY1NTW8+uqrnqobt9uN2+32bNuaDrfb7VnfXVxcPOVhxPOBt7/wTSaT50ez2YzBYJjWhjJfVThBQUFcuXKFkJAQ0tLSJnWtR20AExEREZlfprY9dqJHtVSB77Ypb8mbqOs3GAAOnTiB22ymKCSEWKORJUlJXr88e/nll33G9ajXHvk8K6LZ/cOXaDzTTMXPLpFUnTCt5xMRkcmbU4mee/fu0d7ezltvveU59vOf/5yRkRE++OADz7GysjKsVquntPVxenp6PEmi3t5eTCYTFy9epK6ujm3bthEbG8vg4CAhISH+faAA8/YX/sqVKxkcHOTKlSs0NjYSEREx6cqbB/mqwklPT6etrY3c3FyMRuO493jb1LV582bsdjvl5eW0trbidrvJzc3V1jQRERGZp7xvj3U6nZw6dYqmpibcbjeJiYkUFRURFBTkeae3L+km2zaV5CV503vuPAD/8utfxxgdTdv/9y+wLImfmcf2ouNGF/beIcLjwzBbR5+n396H0Wic8vOJiMjkGdxu96RPzsvLc1+8eHHGgunr62NwcBCAyspKmpub2blzp+cvgrt371JeXs769evJycmZ9KC2ffv2jft1ZmYm9fX1444lJCSwc+dOPzzF/OFts0Nubi7vv//+uPNsNht79ux54vt1dHRw+/Ztz6auK1eu8PLLL1NdXU1HRwcvvPACJpOJwcFBz2A+ERERkYXg5s2bHD16lHXr1mGz2SgrK3tou9StW7fGfUmXlpbGli1bOHLkCO3t7QDEx8dTWFhIaGjoY+/p+PRT7r680zOjp/v3v0v0/+8fCHlletU5U9Xy69uc+rvzDHQPYg0LYnFONJ1L2hi0D/rl+UREFhKDwVDpdrv9siFqTiV6ZHYNDw/T0tIybrNDcXExixcvBr5ckb5q1SoKCwv9eu8bN25w7NgxioqK+OSTT8jLyyM3N9ev9xARERGZK7q6uti/fz9btmzBZrNx5MgRCgsLWbVq1Yzet+/tH9P3P/4e9/Awi/Z8i/A/+sMnGn8gIiIzw5+JnjnVuiWz61Hr12HqK9In68FNXWPf1ty8eZPLly+zaNEiNm3aRFJSkl/vOZs6Ojr48MMPcbvdvPnmm5w6dWpcBdkzzzwz6QGGIiIisjCEh4eTnJxMeXk5BoOB+Ph4MjMzZ/y+Yd/5NmHf+faM30dEROYOJXqect42O8DUV6RP1sRNXWMVZUFBQRQWFlJaWsqJEyd44403/HbP2Xb27FmMRiNOp9NzbPHixRQXFwOM68UXERGRp0N9fT1NTU3k5+djs9koLS2lpqaGtWvXBjo0ERFZYJToWSCmO2/H12aHqa5Inwxvm7qCgoKIjIzEYDBgMpk8P85XjY2N9PX1kZqayvXr1z3HOzs7+eCDD4iOjmbr1q1ER0cHMEoRERGZbWPtUmaz2TN/cjqbT2XucblclJSU0NHRgdPp9HzWfvfdd8ed56+5lyIijzNvEj3e/gC12WzcunWLM2fO0N/fT1JSEoWFhZMe0ryQGI1G8vPzx83bSU1N9WxfGJu3s3TpUs97fG12AKa8In0yfG3qev755ykrK2P//v1ERkZSVFTkt3vOJpfLxYULF9i4cSMtLS2e46mpqaxatQqXy8WxY8c4efIku3btCmCkIiIiMtsyMjJobW2lsrISl8tFYmLivNgy2n2zlotv/zndN2sJWmRj6x/8NXGrNgQ6rDknJSWF0NBQbty4AUBoaOgjP4eLiMykeZPogYf/AB0eHubYsWMkJCRQVFTEwYMHuXDhAtu3bw9wpP7lay15SUkJbW1tnvMKCgpIT0+f9Lwdb+vXMzMz6erq8rki/UkkJiayd+9er8/ncrmA0c1rN2/eJD4+nvLycq5du4bD4eDZZ59l5cqVfovlQRNn6pw9e5aGhgacTidRUVFs27bNM6D6UWpraz0bJJqbmwFwu93jNoglJibS1NQ0I88hIiIic5fZbPa0cc8Xw4P9fPJffhdb/DJ2/OBd+u+2YgoKDnRYc47RaGT9+vVUVFSMOzbTcy9FRHyZN4keb3+A9vT04HA4SE5OJi4ujujoaM8/sBcSp9NJRkaGZy15dXU1ycnJAKSnp7N582ZgNGHyj//4j5Oet5OUlOT5pmHs/R9++CE9PT1YrVZGRkaA0YRQeXk5ra2tuN1ucnJyaGhoGNcmVlhYiNlsZnh4mPfee4+BgQFeeumlSQ1V9vV8MTExBAcHU1VVxcmTJzl58uSMDDeeOFMnOTmZrKws7HY7R44coaqqihdffPGx17l37x7t7e289dZbnmPvvPMOq1ev9lRGtbW1qW1LRERE5oXbVScYutfJtu/9DVHLVhK1bGa+dFvIZmrupYjIo8ybRI83ISEhwOj8E4fDQW9vLw6HI8BR+V9sbCyxsbHAaEXIlStXGBoaAqC5uZnbt28TFxdHQUHBE83b8ZVwqa6upqOjgx07dmAymbh//77XNrH09HQuXbrkie1Jny8rK4tbt24Bo33tY4ObwX/Djb3N1ElJSQFGy2wNBsOkEzM5OTlkZGQAUFlZSXNzMzt37uTy5ct89NFHjIyMEBcXt+AqzkRERGTu6/vJT+j7u9E166G/9Tq2f/tvHrtmvb/jNgCX/umvuX+nichlK9m49weELVYL0mTNxNxLEZHHmdeJnrCwMDZs2EBVVRV1dXVYLBZPieRC9OBa8pSUFFwuF5s2bWJgYIBjx45x6tQpnnvuuWnP2/GWcBkYGKClpYW8vDwSExMBWLJkiec9D7aJ9ff3U1NTw5o1a6iurn7i5wO4c+eOJ7ax+T7gn+HGvmbqAPziF7+gu7ub4OBgli1bNqnrhYWFef7/92AF0HPPPTfl2EREREQexdsijsLCQo4cOfJQa/9Kt5t73/8PhH//32NasoTu3/8uljVrCHnpa57zvCWCrGGRAMRkrGXtnu9R+oNvU/3zH7Ll9//K8z7N8BnV09OD3W4HoLe3F5PJxKJFi2Zk7qWIyOPMq0SPtz9AU1JSSE1Npbe3lzNnzszYHJdAm7iW3Gw2s2LFCs/rNpuNzz//nPfff/+J5+08mHAJDQ0F4ObNm1y+fJlFixaxadMmTCbTQ2vZy8vLycjIICoqyi/PN1bqCoyL3V/DjX3N1IHRRE1vby/Hjx+nvLyc3bt3T/n6IiIyeU6nk1OnTtHU1ITb7SYxMZGioqInqtoUmU0TZ/4ZjUbcbjf79++ns7OTwsJCVq1a5bf7+VrEAeNb+61WK4N/+98BWPStb2KMiqLnj/4vBn911JPocVRXe00ExedvwWiyYDRbMAVZwWDAaPly6Ylm+HzpwU23hw4dIjMzk5ycnBmZeyki8jjzKtHj7Q9Qh8NBS0sLwcHBZGRkzIvtBVPlbS25w+Hg/PnznlkyY+1HO3bsGPfe6OhorwOQH3evsYTLWOIjKCiIwsJCSktLOXHiBN/61rceahNramriG9/4xpTnJPl6vs8++8yzan1sXpA/hxv7mqmzZcsW4uPjsVgsGAwGT2WUiIjMnJaWFhoaGli3bh02m42ysjKuXr1KdnZ2oEMTmZSJM/8A6uvr6enpmZH7WSwW0tPTgfEV1jC+tX/79u0473YAYAwNxWAwYAgLw9XxZaW0/ejHwMOJoOiXvsamf/UX1Lz3tzR89DPiswtY+63vet6nGT5f8vV5eyqfw0VE/GVe/Qv2af2D0tda8t7eXkpKSoDRhMeTDCQG7wmXoKAgIiMjMRgMmEwmTy/3nTt3xq1ldzqdOBwOfvrTn3qud/jwYXbt2kVcXNy0nq+uro6BgQFgtFUL/Dvc2NdMnVOnTnH69GmMRiOxsbFs3bp1WtcXkYXH5XJRUlJCR0cHTqeTPXv2YLPZuHXrFmfOnKG/v5+kpCQKCwuxWq2Pv6B4hIeHe7bUjLXBWiyWAEclMjneZv4NDw9TUVFBdnY2n3766Yzct62t7aEK66ysLE9r//Hjxzl79iyb4kZb8119fRiDgnD39WGM/fLz2aMSQalbXyZ168uec7tv1lL+w+/RfbMWwxdVKprhIyIyt8yrRM/Tytdacn/zlXB5/vnnKSsrY//+/URGRrJ27Vqva9nHZvg0NTVRVVXF1q1bJ9XG5ev5Vq1axeDgIDAzw419zdRRm5aIPEpKSgqhoaHcuHEDGP3H3LFjx0hISKCoqIiDBw9y4cIFDV6fovDwcJKTkykvL8dgMBAfH09mZmagwxJ5LF8z/z799FNGRkY8cwvHxg+UlJTQ2dmJ2+32/Lkx3cRwXFyc10UcY6Kjo+nq6iK4+AXu//BvGHjvfUxLluAeHCRkx5er3k2PSQSNmdiqdf3Y+zQc/adHzvAREZHZp0SPeDwqoTRxBo63UvqxLWhxcXHk5eU9cTwabiwic43RaGT9+vVUVFR4jvX09OBwOEhOTiYuLo7o6Ogpt7DKaItLU1MT+fn52Gw2SktLqampYe3atYEOTeSRvM38s9vtfPbZZ+PmJbrdbtxuN1FRURQUFNDZ2cnJkyepqamZ1uemjo4O7Hb7uAprs9lMWVmZp7W/q6uLlJQUgtatI+IH/5m+/zE6bDnsX/8ewS+/5LnW4xJBYya2amW9+h2uH/+Fzxk+IiISGEr0LADd3d2UlpbS09OD2Wxm5cqVbN68GbvdTnl5Oa2trbjdbnJzcxfkDCMRkUD5+OOP6erqAkZbWlesWEFPTw9DQ0P8+Mc/VhvXFIy1BpvNZs8/Wvv7+wMZksikeJv59/Of/5yRkREuX77sOVZRUUFERATbtm0D8HyZNVbpM1V2u91rhfX169e9tvaHfefbhH3n216v9bhE0Bhv69bX/st/R8OR/+V1ho+IiASGEj0LgNPpJCMjg+TkZC5fvkx1dTXJyclUV1fT0dHBjh07MJlMnjYomT5fSbVLly5RU1PD0NAQ0dHRFBYWEhMTE+hwRWSGJSYmEh4ezo0bN7h+/TrXr1/H7XZjNpt55ZVX1MY1BRkZGbS2tlJZWYnL5SIxMVFfTsi84G3m39e+9jVPwvLixYu0tLSwZs0ali4dnV3jdrs5d+4cJpOJrKysad03KSmJ119//aHjL7/8spezH+9RiaAx3tatB4dHs/O/H53WPUVEZGYo0bMAxMbGEhs72ludmJjIlStXGBgYoKWlhby8PM/sHHly3pJqS5Ys4fz58yxdupSNGzdy4MABKisr+cpXvhLocEVkBvT09Hi+gU9OTvZs/vvqV79KV1cXFy5cICkpSW1cU2Q2mykufrhVRGSu89VqPiY2NpaWlhZycnKwWq243W7Ky8u5du0axcXF8+qLoficR69bFxGRuUGJngXE4XBQVVVFeHg4oaGhANy8eZPLly+zaNEiNm3aRFJSUoCjnN+8JdXcbjdhYWFYrVYiIiIwGo3aFCOygL3//vuenx86dMiz+e/o0aOeFq3g4GAcDge9vb04HI6AxCkigfdgYri3txeTycTFixepq6tj27ZtxMbGMjg46JlzONeFxi195Lp1ERGZG5ToWSDGVqPb7XZeffVV3G43AEFBQRQWFlJaWsqJEyd44403AhzpwvBgUi0lJYV79+5RUVFBY2MjISEhfhlGLSJz08Sh9RUVFXR1dfHNb34Tm81GZWUlVVVVXL16FYvF4vmmX6bP11p7tc3KXDcxMZyZmUl9fT0A5eXlACQkJLBz586AxDcdE9eti4jI3KNEzwIwluTp7e1lx44dGI1GgoKCiIyMxGAwYDKZPD/OZb7m39y5c4eysjL6+/s9g02DgoICFufEpFpfXx8VFRWkpaWRnZ3N0aNHOX36tNfybRFZWLx9W5+SkkJqaiq9vb2cOXOGlStXBjjKhWHiWvuhoSG1zcqc522bqbaHiojITFOiZwHo6Ojg7t27ABw8eBCA3Nxcnn/+ecrKyti/fz+RkZEUFRUFMszH8jb/JjExkZMnTxIfH8+zzz7L4cOHqaioYOvWrQGJ0VtSbXh4GACTyYTZbMZgMGhTjMhTwtu39Q6Hg5aWFoKDg8nIyNBAYT/wttZ+rFpKbbOyEHV0dPDhhx/idrt58803MRqNuN1u9u/fT2dnJ4WFhaxatSrQYYqIyBylRM8CkJiY6PUbI4Bdu3ZN6hpzYUW7t/k3IyMjDA4Osnz5chYvXsySJUtoamoKWKLHV1ItLy+PK1eu0NjYSEREBKtXr+ZHP/qR5wPaqVOnPKXaAM8884xn3amIzF++/uyVmWc0GsnKylLbrCxIZ8+exWg04nQ6Pcfq6+vp6ekJXFDT5Kv1cq5VbIuILCRK9AjgnxXtE799Onv2LA0NDTidTqKioti2bRuLFy9+bCwPzr9xuVwAnm9pLRaLzxhmo/XrUUm13Nxcz89LSkoe+oC2ePFiz0YZfZAREXkyPT09apuVBamxsZG+vj5SU1O5fv06AMPDw1RUVJCdnc2nn34a2ACnYWLr5cjICB9//PGcqdgWEVlolOgRwD8r2id++5ScnExWVhZ2u50jR45QVVX12A/g3ubfAJ72qP7+fpxOJ/v27fNaKRMfH8/Xv/71gLZ+efuABtDZ2ckHH3xAdHQ0W7du9WzqEZH5Q0OBA2fiPCSDwQCobVYWFpfLxYULF9i4cSMtLS2e45cuXSImJoakpKRZT/QM9Q1x4A9/xf27/ZgsJlLyEnn29wowB01u9qO31sv29vY5VbEtIrLQKNEj40x3Rbu35EZKSgoAAwMDGAyGxyY2vM2/iY2NJTg4mOvXrxMWFkZ7ezsGg8GzVQwerpQJCgoKWOuXrw9oqamprFq1CpfLxbFjxzh58uSk2+pEZG7RUODA8DYPaWLbbEFBQQAjFHlytbW1WK1W0tLSaG5uBsBut1NTU8Orr77qSXa63W7cbrcn4TmTjCYj+W+sIyYtiisf1VN9oJa0zSmkbxn9nDexovvatWtUVlYyMDBAeHg4W7dufegLw4GBAWByFdsiIjJ1SvSIx3RXtPtKbgD84he/oLu7m+DgYJYtW/bI+/uaf1NcXMzp06cpKSnBZDKRlJTEzZs3Pe+bWCkTFhY2rdavMUNDQxw4cIC+vj7PBp3CwkKOHDlCZ2cnbrebhIQEcnJyOHTokNc5PMePHycyMhIY/TCWmprquX5iYiJNTU2PjEFE5iYNBQ6cybTNisx39+7do729nbfeestz7Oc//zkjIyN88MEHnmNlZWVYrVbS09OndZ/um7VcfPvP6b5ZS9AiG1v/4K+JW7XB67mWEIsnqWOLC8VkMRKRaPO8PrGi+8yZM4SGhrJr1y4OHjzI2bNn2b1797hrLlq0CPiyYnt4eJiQkJBpPYuIiDxMiR4BnmxFu7dvn8aSRC+++CK9vb0cP36c8vLyh/6if9Cj5t/s3r2bX/ziF+Tl5T2yUuaTTz7BaDT6bP2azAcJo9FIfn4+MTEx1NbWUl1dTWpqKlFRURQUFNDZ2cnJkyfp6up6aA5PSEiIJ5E0NjDxnXfeYfXq1aSlpQHQ1tamti2RBURDgUXEX3JycsjIyACgsrKS5uZmvva1r2E2j35kv3v3LuXl5axfv56lS5dO6x7Dg/188l9+F1v8Mnb84F3677ZiCgp+5HvaLrdz6M+O43Q4SVqXQPiSMMB7RXdUVBRut5uIiAjMZjMWi+Wh1svIyMhxFduff/45K1asmNbziIjIw5ToEeDJVrR7+/bpnXfeYcuWLcTHx2OxWDAYDJ4PKdPhK5n0YKVMfHw8N2/exGq1+mz9mswHCYvF4vmGLCwsDJPJRGRk5LhjMFr5M3EOj8PhICgoCJvNhsFgoKOjA5fLxWeffcbly5eB0cHXg4OD7Nu3DwCbzcaePXum/XsjIoGlocDzh68ZSyUlJeMqNouKirBarYEOV55CYWFhns8Z3v4MiYuLY/Xq1U90j9tVJxi618m27/0NUctWErVs5WPfE7cimt0/fInGM81U/OwSdceus+blTK8V3VlZWZw8eZK3334bs9nMjh07vLZejlVsHzp0iKSkJPLz85/ouURE5EtK9AjwZCvavX37tHPnTk6dOsXp06c9CZcnmYvjK5k0sVIGRhMw3lq/pvJBoq2tjcOHD+N0OklKSsJmGy1RdrvdnD171nPt7u5uz3smVhcFBwezefNmzyazK1eu8PLLLxMREQGM9qcfOHBg2t/IiUhgaCjwzHI6nZw6dYqmpibcbjeJiYkUFRX5bVvhxBlLwEMVmzU1NarKkgWrv+M2AJf+6a+5f6eJyGUr2bj3B4Qt9v55pONGF/beIcLjwzBbR//pYLaavH4JNzIywqlTp1iyZAmbNm3i1KlTnDhxwudnzNdee20GnlBERJTokSfm69unR7VpTZWvZNLly5f56KOPGBkZIS4uju3btxMVFfXQ+6f6QSIuLo7du3dz48YNLl68yNWrV1mzZg3l5eVcu3aNiIgIsrOzOXnyJOB7Dk9OTo7n11euXGFoaMjze1VXVwfAmjVrpvz7ISKBo6HAM6ulpYWGhgbWrVuHzWajrKzM82fwk2488zZjCWDbtm3AlxWbY4k8kYXIGhYJQEzGWtbu+R6lP/g21T//IVt+/6+8nj94z86pvzvPQPcg1rAg1ryUSebzyzl/4dxDX8L95Cc/wWAwYDQaPW3/Y230IiIye5TokXnBVzLpueee8/u9Ojo6sNvthIeHe9rNzGYzZWVl1NXVkZiYyO3btx9bXTQ2h+fBTWZjm8hcLhd1dXXEx8drBbPIPKOhwDMrPDwco9E47s/9seHWM7nxzO12c+7cOUwmE1lZWf57IJE5Jj5nC0aTBaPZginICgYDRovvVsXk9Yn81lsPV3d7+xJu165ddHZ2UlVVxYEDBwgLC2P79u0z9iyB8tiV87er4Ef54HbBfxgGk/7JJSKzS3/qiExgt9s5deoUAwMDWK1WVq9eTVxcHKdOnQLg9u3RkueYmBiGhobo6+tjZGSEzz//nKtXr46rLpq4yWwscdTY2MjAwIC+9RcRmSA8PJzk5GTKy8sxGAzEx8eTmZk5oxvP3G63p2KzuLhYCXiZlPk68yk0bimb/tVfUPPe39Lw0c+Izy5g7be+O+Xr+PoSLi4ujlWrVvkt3rnocSvn+dX/B4wWcA4FNlAReWop0SMzzte68jt37nDmzBn6+/tJSkqisLBwTnwQSkpK4vXXXx93rKOj46F5O5s3b6a3t5f+/n6qqqpYtWoVK1d+OdDQ2yazsWHNV65cISQkxFMBJCIio+rr62lqaiI/Px+bzUZpaSk1NTWsXbv2oXMf3Hg2VuXT2NiIw+HwOddn4owlk8nExYsXqaurY9u2bcTGxjI4OKhVzzIpvmY+DQ0N0d3dTXNzMxcvXmTr1q1zKgGUuvVlUre+/MhzfH1+O3LkiGcuIkBBQQHZ2dkzHfKc8siV87Ufwr0myNoFn/08gFGKyNNMiR55Ih0dHXz44Ye43W7efPNNTp06RX19vef1Z555hvz8/IfWlSclJXHmzBnPB52DBw9y4cKFOVveGxsbS2xsLDB+3k5WVha3bt3y+h5fm8zS09Npa2sjNzcXo9E4Ow8gIjJPjA23NpvNnipIX8OtxzaeLV68mPb2dkwmE+Hh4TQ1NXH16lWv//j0NmNp7O+t8vJyABISEti5c6dfn0sWHl8zn7Zs2cKlS5dob2+nqakJh8MBzL+h30aj8aHPb2PzCNPT09m8eTPAnPiSLhC8rpx3DsPHfwTFfwkNhwMdoog8xZTokSdy9uxZjEYjTqfTc2zx4sUUFxcDEBQU5HVduc1mw+FwkJycTFxcHNHR0Z6NDXOZt3k7vjxqk5mv4yIiTwNfLS+XLl2iuroag8HAuXPnMBqNJCYmegbb+9p4ZrVaMRqNGI1GhoeHAXy2cHn783cm5r3J08toNLJu3To++OADAJYvXw7Mv6Hf3j6/RUZGAtDc3Mzt27c9rerTaZmc77ytnM9eUgqLYiDr69BwaPREtxP9k0tEZpv+1JFpa2xspK+vj9TUVK5fv+453tnZyQcffEB0dDRbt24lOjra57ryzs5OHA4Hvb29nm+85ipf83ZERGTqJjNYOTk5edxgZV8bzy5fvozL5cLlcjE8POyZ6yMSCGMzn7q7uwHGbQOdb0O/vX1+y8rKYtOmTQwMDHD8+HHOnj3Ljh07Ah3qrPK1cp7Oerh1Dn7wQOLrL2Pg+9o8JiKzS/9SlWlxuVxcuHCBjRs30tLS4jmemprKqlWrcLlcHDt2jJMnT7Jr166H1pXfvHmTDRs2UFVVRV1dnWeg5lzla96Ow+HwtBQMDAzQ29tLeHj4lK8/sQXu3r17nDx5ku7ubhYvXsxzzz1HaGiovx9LRCQgpjtY2Vc1ZHBwMOXl5ZOa6yPiT4+a+ZSUlMStW7ew2+3YbLZ5OfR74ue3q1ev8swzz3hej46OpqurK4ARBoavlfP0/xvIeWP0pJP/CeoPwm+fCGisIvJ0UqJHfJqYfDhx4gQtLS2MjIwQGhqKxWIhLS3N03Lldrs9vdsw2rrU1NTkc135kiVLSE1N9fSqDw8P85Of/GTcsL9HDS3s7u6mtLSUnp4ezGYzK1euZPPmzZ4PUQ6Hg2effXbcgOQn+b3wNm+nra3NM5CwoqKC2trahwY5T8bEFrjjx49jsVh49dVXOXr0KGVlZeM2WoiILDQPDlZubGwkJCRk0vNLpjLXR8SfHjXzaWyG38mTJ3nppZfm3dBvX5/fysrKyMrKwm6309XV9dhW9oXI18p5IpJH/wfwesnsBiUi8gAlesSnicmH0NBQXnzxRRwOB0eOHAHgrbfe8pz/zjvvsHr1as8mqba2NqKjo72uK8/MzOTYsWO0tLRgtVpZtmwZ+fn5XL161TPs73FDC51OJxkZGZ5NWNXV1SQnJxMTE0NwcDBVVVV++7141LwdXyYmys6ePUtDQwNOp5OoqCi2bdvG4sWLH2qBu3//Pl1dXWzatImYmBiSk5Opra3F5XJpeLOILFhjg5XT0tLIzs7m6NGjnD59elJJ7oyMDFpbW6msrMTlco2b6yMyk3zNfNq3b5/n111dXVy4cGHODf0e6hviwB/+ivt3+zFZTKTkJfLs7xVgDjIB+Pz8dv36dUpKRpMYiYmJbNmyJWDPICIi3inRI155m7+zadMmYLRyx2KxEB0dzZYtW6isrKS5uZmdO3dy+fJlPvroI0ZGRjwD+qKiorxWuTw4d2HMg8P+HhwACA8PLZzOJqzZNDFRlpyc7PkG7MiRI1RVVfGVr3zloRa4wcFB4MtBohaLBbfbjd1uZ9GiRYF5GHnqTUxcGo1G3G43+/fvp7Ozk8LCQlatWhXoMGUe8TVY2WQyYTabMRgMk67KMZvNniUAInPBfBj6bTQZyX9jHTFpUVz5qJ7qA7WkbU7xrA1PSkry+vnt5Zcfs5b9MQmk2aY18SLyNFKiRx7ia/7OmMrKSoaHh8nNzSUuLm7ct63T/RDja1jzZIYWTmUT1mzxligbi21gYACDwUB0dDS1tbVYrdZxLXDBwcEAns0xw8PDGAwGz3GRQPC2Ya++vp6enp7ABTXH+GonLSkp0T8mvJjY8jKW6GltbaWxsZGgoCD6+vr4x3/8R6KjoyksLJwXM01kYZqY7L527RqVlZUMDAwQHh7O1q1bSUxMDHSYU2IJsXiSOra4UEwWIxGJtie+7uMSSLNNa+JF5GmkRI88xFvywe12A/DrX/+aqqoqtm3bRnJyst/u6W3Y35o1aygvL6ehoQG3280HH3zAm2++yalTpzzlzwAhISEYDIY5swnrUYmyX/ziF3R3dxMcHMyyZcu4/v9n79/DorrzRN//XTcuUtwpQG4CBhEVjKgQRVHTmk5Maw9xkmkzefYz3b/MPL/z7Jme6enZu+fs3j1nz8789v71eeZM9z7nzMz+ddLueHZ6OklPDB3FJF5QLoKKkAAR8AqIihfuFgUUVNXvD7pWuKyFBRQUl8/refJIiqq1vgulaq3P+lxu3eLRo0fjSuCOHTtGeHg4LS0tJCQk0NbWRmJiopRtCZ9RC1wODw9TVVVFZmYmX375pW8XuEBolZOCXEyocWc8OJ1Oamtr6ezs5Pbt2/ze7/0efn5+HD16lPj4eLZs2cJvf/tbZVS1exT78ePHp+zjJoQ3TQx2V1RUEBQUREFBASdOnKCyspJDhw75eJXT1371EUX/6SwOu4OEZ1cSEuPZYAzru+9i/cd/xjU8TNAfvk7wX/1QCdbOVQBppmRMvBBiOfL9VbFYcHp7eycFH44ePapMSNm0aRNJSUnYbDavlBJN1eyvqamJkJAQrFYrTqdTeU10dDS7du3i7NmzWK1WXnjhBa9PwvJ07RMbVre0tDAyMsLVq1eVLBx3oOzFF1+kr6+Ps2fPUl5ezje/+U3S0tIAxpXAGQwGSkpK+OSTT4iOjmbHjh1zsn4hnkYrcFlbW0tkZCQJCQkS6PkdrXJSkIuJqXgygUun0xEUFITV+vWI4qf1cRPCW9SC3eHh4bhcLkJDQzEajYv2d9ryTASHfraf5oo7VP2qlqYzt8g8MHUZrr2ujt4f/4SQH/8HDDExdH//zzGtX0/g/peU58w0gDRXZEy8EGK5kUCPmCQrK0s1+HD69GlgNKvniy++wGw2z2jC1ERazf5KS0uB0d4NE3V2dlJYWKiUN83FJCxPqDWsTkpK4vbt2zx48EB53tGjR9m+fTuxsbGYTCZ0Oh1GoxGz2az0IJrYcLSgQGWagxDzTC3Db3BwkPr6eg4ePKj0WHG5XLhcLuWO7nI2sZzU6XROupjIz89X7Rlx8eJFr08NXIzUJnCtWrWKq1evKs9xB8C1+rgJ4Q1awe6MjAxKSko4cuQIRqNxUQYIOm53Mdg3REisGaP/7260+T+9j87gqdHzwRXf+QP04eH0/OivGfj81LhAz0wCSHNJxsQLIZYbCfSISbSCD3MVLNFq9vfmm2/ym9/8hi1bttDW1qaUayUnJ7N27VqcTidnzpwhODh4zoIiU41wv379utJ02j16PTc3F6vVSlZWFidOnMBoNDI4OMiBAwcoLS3lwoUL6PV6oqKiyMvLm5M1C+FNahl+77//PiMjI0opDUBZWRn+/v5KevxCpdWU02g0Mjw8zAcffIDNZmP//v0kJCRMe/t2u52ioiIGBweVctJnnnlG+b77YkKrZ8RcTA1cjNQmcKn1jPOkj5sQs6EW7B4ZGaG0tJSYmBhyc3MpLS3l/PnzvPrqqzidTo4fP05HRwcOh2NBlxoO9A5S+o+XsHUP4G/2Y/3+Nax5fvVTX+d43AGAPigInU6HzmzG2fFY+f5MA0hzRcbECyGWIwn0iAVLq1eQu4EejJZHtLa2ztkatHpuhIeHYzAYGBkZmdQ7x2w209TUxMjICPv27VN6dCzG2n0h1DL8XnrpJeVk+fHjx5SXl7Np0ybi4+N9uVSPaAVYUlNTqa2tVUqtZsId5Onr62Pfvn1KOemlS5cmXUxo9YyIiIhYEFMD55snE7jcGZxuLpdL6dlmNBr55JNPiIuLY8+ePfj5+c37MYilSS3Y/e6776LT6dDr9RgMBnQ63biywqSkJIKCgrh9+7by2EIsNUzcFMcfvjP9G2UGy2iJqtNqRe/nh8tqRR9lUb4/0wDSXJmTMfH3a+DtreBywk+GwTD6mdjd0siVI39Hd0sjfiuCyfuLf8CydvNcHJYQQkxJAj1iwdLqFbRu3TpSUlKA0ZrriIiIOVvDVD03AgMDx10UznXDaiF8YaryQhhNh1+3bt18L2vGtAIs/f391NfXs379eurq6ma07Y6ODiW7b2w5aV9fn+rFhNa0weVo4gSuNWvWsGXLFhoaGmhubsZsNhMaGsqdO3fo6+vDYDAo5RcA6enpmM1mLl26xLVr12SqmfAatWB3QUEBnZ2d1NTUUFhYiNlsZufOnYB6zylYWqWGAXu/wZOf/RzbBx9iiInBNTBA4L69yvdnGkCaKzMdEz+lz/8S9CZwfH0eODzQz7n/8scEx65i31u/pv/xPQx+MjFVCOEbEugRC5ZWr6CrV6/y2WefKWVT7pOruTSx58bly5eVsdIPHz4E5rZhtRDCe9QCLOXl5aSlpREeHj7j7cbFxSmTpDzxtJ4Ry4nWzy07OxuAX/ziF/T29gJfB4LGTl+8evWqEvRfrE1xhXdolVzfvXuXiooK+vv7SUhIID8/36PSKa1gt8ViYe3a6fWcWSqlhn7PPkvoW/8Z6z+NTt0y/9mfEvDyfl8va/40fgy9rZBRAF+9rzx8v+Y8Q72d7PjBzwlflU74quXZY00IsTBIoEcsWFonV7t3757Xdaj13MjKyiIkJISKigql58ZcNqwWQniPWoCltbWV1157TSkTnWtaPSOsVuu8TQ1cTNQCQbt372ZkZISzZ8/S2tpKd3c3sbGxrFmzxgcrFAuFWsl1bGws58+fV3rjnDhxgsuXL0+4UTSxjOpXgPcu1F0uF+Xl5dy8eZO9e/cSGRnptW0/jVZvsk8//VQZXgGwbds2j7PhzN/7LubvfXeulrxwOYbh9I9g70/hxslx3+rvuA9A7b/8A08etBK2Kp2cP3kLc/TCL2sWQiw9EugRy8LEMejuvg5uGzZsUK3N1uq5AV/fNV69ejUvvPACISEhEtQRYoFTC7A4HA7sdjvvvfee8ryTJ09SUFCAxWLR2tSsaPWMKCoqmrepgUvB9evXaW1tZevWrQQHB1NcXEx9fT0bN2709dKEj6iVXPf29mK320lMTMRisRAREaER1P0B4C5BenogRit76OOPP6azsxOAs2fPsmvXLurr62lqamLHjh1ERUUxMDBAYGCgdw76KbR6kwGkpqby3HPPAWhmOE08h7p58ybV1dXYbDZCQkLIy8sjLi7Oa+udi8CU11S/DSsiIeMVuFE0+pjLARjxN4cBEJm2kY2Hf0DxW9+l7v2fsf37fz+/axRCCCTQI5aJiWPQAaKjo9m7d/SETqtxp1bPjfkc4S7EQqd1Uj4yMkJ5eTn37t3D5XKRnZ1NVlaWT9eqFWBxX6S0trZSU1NDXl7erMq4nkarZ8SBAwfmbJ9Lkbtps9FoVAJ37owosbyNLblevXo1ly5dorOzE7vdTl9fn3LTZry3gfeAfODfPXUfWgMb3OcNAI8ePeLs2bPK6O7y8nIAVq5cOW+/71q9yQDu3LnD/fv3lVJ4tdLHiedQFRUVBAUFUVBQwIkTJ6isrPRo4MSIw8n/8j+quNbeh33EybG/yCcufHKwa7aBqTnVeR3uXoS3xvycfhoJP7YSm7UdvcGE3mjC4OcPOh16k28nqwkhli8J9Iglr7m5GavVSnJyMrdu3VIe7+zs5KOPPiIiIoK8vDzVps7T7bkhxHKkdVJ+7do1Ojo62LdvHwaDgYGBAV8vVTPA4r6zbrFYfD4JZ7maSW+VtLQ07t27R3V1NU6nk7i4OJ8HE4XvTSy5NpvNbN68mZqaGpqamjCZTEpp+Nd+BGwEyoB/BlYDr025H62BDe7zhsHBQf6f/+f/ITY2lt///d/38lFOj1pvsoyMDHJzc7HZbJw9exazefeEV/2K5ma/SedQ4eHhuFwuQkNDMRqN0+qLlbfGQnSIP2evPtR8zmwDU3Nq+w8h643Rr0v+Fq6fgD86D0CQJZ7cf/tfqf/gv3Hjs18Rm7mNjd/58/ldnxBC/I4EesSS5nQ6uXz5Mjk5ObS1tSmPJycns3btWpxOJ2fOnKGkpISCgoUzIUKIxUTtpDwwMJC2tja2bNni1ZR+sXTNpLeK0WhUMjOFAO2S66SkJJKTk+nr66OiooL09In9d1793Z9JjAZ6bkxrn2MHNoB242Wn08nx48fp6OjA4XBw+PBhgoODOX78OJ2dnbhcLuXfuzczVp7W/P3rm11fl685neFcvvzxpHOojIwMSkpKOHLkCEajkX379nm0BqNBzx/lp/Lfzz79Z+tJYKqystLjfXtNaOLofwCvH5/07eS8l0nOm8U0LyGE8BIJ9IglrbGxEX9/f1JSUpR6fJfLpaQAw+hduNbWVh+tUIilYeJJucvlAqClpYWrV6+yYsUKcnNzSUhI8PFKhS9pXeTW1tZSX1/P0NAQt27dYvXq1QDT6K0ixCitkuuuri7a2toICAggLS1tQuZXI3AF2AGU/+6xNI/2pzaw4WmNl5OSkggKCuL27dvKY+Hh4Wzbto3Ozk5KSkqor6/3WnahVvP3srIyMjIyGBwcVErLxpavNTbun3QONTIyQmlpKTExMeTm5lJaWsr58+d59dVXVfc9U54Epr5e8+Lkq6CfEGJ5kECPWNJ6e3t59OgR77zzjvLY0aNHWbduHSkpKcDoBapa2ZYQwnMTT8rdd7X9/PzIz8+nuLiY8+fP88Ybb/h4pTOjVVY0NkARERFBfn7+vE7TWYwmXuQODQ1x6dIl4uPjycnJ4eOPP6anp2eavVWEGDWzkutA4DPgv//u61eBV576Kq3soYsXL2o2Xtbr9WzatImqqqpx29qxYweAUlI2ODg4zWPQptWb7NatWxw/PpqVEhcXx9DQX+Dvn4O7fC0wUMejRxHjzqHeffdddDoder0eg8GATqfDarV6ba3geWDK/TkzXVoBlgcPHlBWVjauTFSrh6O3zHfQTwixfEigR8zaTCdagfbFk/tOmN1uZ9euXSop1p7JysoiLW30rlx1dTV37tzhwIEDXL16lc8++4yRkRGlzlsIMTNqJ+Xungo6nU65GDAYDD5e6dem20BarawoJiZmXICisLCQ6upqXnjhBV8f3oKldpHr7pfi7++vXAw7nU7279/vYW8VIWYrmdFx6tOjlT3U1NQETL/xsla512xp9SZ7+WWtEqPR8rXExCGlrN19DlVQUEBnZyc1NTUUFhZiNpundQ7V8thKr20YgHvdNvyMeqKCx2ereBqY0jq39MTEAMvIyAinT58mNjaWXbt2cfLkSaqqqsjLy5vxPp7GF0E/IcTyIYEeMWsznWgF2hMrIiMjCQgIoKamZlZrM5vNygfliy++qDy+e/fuWW1XCPE1tZPy9PR0LBYLZWVlHDt2jLCwMPbs2ePrpSqm20Baremqy+VSAhShoaHo9fr5bwy6BOj1ejIyMqiqqlIuunbt2jWN3ipC+IZW9tBMsi+eVu7lTeoZLXfp6ztHRYWOyMhGtm4FnW4NFosFGH8OZbFYWLt27Yz2/Z3/+4Ly9Z8dvcL+Z+P4m4LxI9KnH5iaHrUAy6NHjxgYGGD16tVER0cTExNDa2vrnAZ6pjJXQT8hxPIhgR4xK7OZaAXaEysyMjK4e/eu6msmZhBVVlZy48YNHA4H4eHh7Nixg+joaO8frBBCldZJeVRU1IJtcj7TBtITm6729vZSVVVFc3MzgYGBkl4/Az09PVRVVREbG8uDBw8AKC4uBjzprSLE/Jt4HqLX63G5XBw7dozOzk7y8/OnDIT09PQoWRp9fX0YDAauXLmiWe41FyZntPgxPHycffu60elW0NiYSVdXPN6Oc1z8229O+zVaGZiffvop7e3tyvO2bdtGZmbmFFvSZrPZAJRgvclk+npS5P0aeHsruJzwk2EwzO3l03wG/YQQS5cEesSMeXOildrECi0TM4gSExOVmu1PP/2UmpqacXeehBBCzXQbSE9sumq1WqmqqiIlJYXMzExOnTrFhQsX5P3nKSZe5Op0OmA04PbKK6/w6aefsmLFCg4dOuTLZQqhSS2T+fr16/T09Hj0+g8//FD5uqioiDVr1igl79Mt95oJ9YyWQE6cKGDv3r2kpqbS0vIp3d13vR7omQmtDEyA1NRUnnvuOex2O6dPn+by5cszCgatWLECgOHhYeVPJdD2+V+C3gSOIa8f20II+gkhliYJ9IgZ89ZEK7WJFVrUMojcgSGbzYZOp5PGykIIj0yngbRa01X3BYHBYMBoNKLT6ejv7/flIS0Kahe5W7ZsoaGhgebmZkJDQ9m2bZsPVyiENrXzkOHhYaqqqsjMzOTLL7986jbUyr18XVI+ZUaLj6llYIaFhQFw584d7t+/T2RkJJmZmcTHx2sGgwBletXEAEtYWBgBAQHcunULs9nMw4cPeeaZZ6DxY+hthYwC+Op9rx+br4N+QoilSwI9Ysa8MdFKa2KF3W5XLphsNht9fX2YzWbVDCKA3/zmN3R3dxMQEMCqVavm4GiFEEvJdBtIazVddQcobt++jU6nY2hoiKNHjyqN5WH0IvCDDz7AZrOxf//+ZT9iXmsiUnZ29rj/12rWD/IzFVPrbmnkypG/o7ulEb8VweT9xT9gWbt51tvVymSura0lMjKShIQEDwI9E8s7fwX4vu/UlBktC8DEDMzg4GAyMjLIzc3FZrNx9uxZ/Pz8WLdunWYwyD18w2QyqQZY9u7dy4ULFygqKiIhIYGt2c/CO9mw96dw4+ScHNdCDPoJIZYGCfQsM948cfbGRCuti6f29nYl1baqqorGxkY2btyomkEEo00C+/r6OHv2LOXl5ZLyL4SY0tMaSH/00UfKc999912SkpL43ve+N24q11dffUV2djZvvPEGHR0d3L9/f1Jj+fj4eGpraxka8n7K/1Kn1axffqZiKsMD/Zz7L39McOwq9r31a/of38PgF+CVbatlMg8ODlJfX8/BgweVDBGXy4XL5VLKEif7AbD3d1/7pv+KxxktC8TEDMxr166xYcMG5fsRERE8evSIX/7yl1MGgyorK9m3b59mwPnVV1/9+n8u/xOsiISMV+BG0ehjLgdy+SSEWAzknWqZ8eaJszcmWmlNrFBTUVGhmkG0fft2YmNjMZlM6HS6KUu/5opWo8CLFy96ZUy8EMK7ntZAenh4mLa2tllN5RoaGqK/v5/6+nrWr19PXV3dvB7jYic/UzET92vOM9TbyY4f/JzwVemEr/Le565aJvP777/PyMjIuOBwWVkZ/v7+SrnRZG8D7wH5wL/z2vqmw6OMlq1bfbK2idQyMI1GI2VlZUqPxq6uLhISEsjNzZ0yGNTV1eX5jjuvw92L8NaYaYo/jYQfW711aEIIMWck0LPMzPTEWSuQ0dHRQVlZGf39/SQkJJCfnz/lOPXZ0MogKi0t5cKFC+j1eqKionwyClOrUaC3xsQLIeaXt6ZylZWVkZaWRnh4+Hwuf0mRn6mYjv6O+wDU/ss/8ORBK2Gr0sn5k7cwR8fPettq5yEvvfSSEnx4/Pgx5eXlbNq0ifh4rf39CNgIlAH/DKwGXpv12qbLo4yWBUItA3PNmjXcunWL48ePA6Pnt6tXr0av108ZDHrawI9xtv8Qst4Y/brkb+H6Cfij86pPnTiJ7ebNm1RXV2Oz2QgJCSEvL0/zc0MIIeaCBHqWqemeOKsFMhITE7l48SKxsbHs2rWLkydPUlVVNWeBFq0MooVQpqXVKDAiIkJzTLwQYmbmq3fLbKdy9fX10draymuvvaaUeYjpkZ+pmC5/cxgAkWkb2Xj4BxS/9V3q3v8Z27//97PettZ5iJvFYmHdunVP2Yo7kJLEaKDnxqzXtdRpZWC+/PLLytd37959ajAoLi6O7du3e77j0MTR/wBePz7lUydOYquoqCAoKIiCggJOnDhBZWXlgjhfFUIsHxLoWYZmcuKsFsgYGhpiYGCA1atXEx0dTUxMDK2trT7JqFkI1BoFCrFcaGX9GY1GrzfOna/eLbOdytXR0YHdbue9995Ttnny5EkKCgqwWCxeWeNSJj9TMROxWdvRG0zojSYMfv6g06E3+T/1dVrvYWP7crlcLrKzs8nKyppyW06nk+PHj9PR0YHD4eDw4cMEBwfT0VHOvXuF3L4dx/r1PaxZA5DmnQNf5jwJBinu18DbW8HlhJ8Mg2F2l0Nqk9jCw8NxuVyEhoZiNBqVaWZCCDFfJNCzzMzmxHliIMM9onIhjuL0hac1ChRiKdMqX0xNTfV641ytEtT79+9TU1OjNEBtbGwkISGB48ePK83dAbZt20ZmZuaU+/DGVK4NGzZQUFAAQGtrKzU1NeTl5UnJkYfkZypmIsgST+6//a/Uf/DfuPHZr4jN3MbG7/z5U1+n9R6m1ZfraZKSkggKCuL27dsAjIyMcOFCDXv2XCcrq4KhIT3t7TtZufKVWR2vmIHP/xL0JnDM/nNJaxJbRkYGJSUlHDlyBKPRyL59+2a9L/f+1IKIp06dUoKRUVFR7N69m5CQEK/sUwixOEmgZ5nx9MQ5NzeXs2fP0t/fr9zZ2r59O+vXr6exsZG7d+/S3d0NLNxRnPNJq1Gg1WqdNCZePnjFUqRVvjiXjXMnlqCePn2amJgYEhMTuXLlCs3Nzdy7dw+A1NRUpbzLHaSeytOmch07doywsDD27NkDPL2xvMViYcuWiWOVxVTkZypmKjnvZZLzVDI5pjDTvlxq9Ho9mzZtoqqqSnns0aNHPHwYQEfHzwkJSeXcuU/p7u7m9dfVT8W1LugfPHgwb70Rl6TGj6G3FTIK4Kv3Z785lUlsIyMjlJaWEhMTQ25uLqWlpZw/f96j/kdaf+/Hjx+ns7MTp9NJUFAQCQkJtLa2Kq9LS0sjJyeHrq4uzpw5Q319/bLNsBdCjJJAzzLj6Ynz8PAwwcHB4+5sBQcHU1dXR2hoKL29vfT396PX6xfsKM75pNUosKioaNKYeLXUYiGWArXyxfLy8nH9v8rKyrDZbON668wk40atBPXhw4eTSlDdmUR37tzh/v37WCwWdu7c+dQ0+qdN5ZqKN0tAlhKtC5ja2lrq6+sZGhoiIiKC/Px8IiN9M3JaLG/T7cs1HTabDZheFrRaVtDp06fnrTfikuMYhtM/gr0/hRsnvbJJtUls7777LjqdDr1er2R/Wq2eT+qa+PcOo6Vg27Zto7Ozk5KSkkk3VlNSUoDR91kYnTAmhFjeJNAjVE28s6XX67l27RoAVquVtWvXcv36dVauXElPT8+MR3HOV1PVuaZ1UXjgwAEfrEYI31ArX5zY/ys+Pp7MzMxxvXVgehk3npagAgQGBpKRkUFubi42m42zZ89SWVnptTR6Nd4uAVlKJl7ADA0NcenSJeLj48nJyaGwsJDq6mpeeOEFH69ULEfT6cs1XStWrAA8z4LWygpayr0Rre++i/Uf/xnX8DBBf/g6wX/1Q6UU1yuq34YVkZDxCtwoGn3M5WA2l0Nqk9gKCgro7OykpqaGwsJCzGYzO3fu9Gh7an/vADt27ABQmoGPjIxMeu2RI0cYGRkhODiYlStXzviYhBBLgwR6hKaJd7b27dvHV199RVVVFdeuXSMwMJD8/PxZNR2er6aqQoi5pVa+6HA4JgVfmpqayMjIGNdbB6aXcTNVCerw8DDFxcXYbDays7OxWCzjTngjIiLo6ury+vGP5c0SkKVE7QLGZDJhNpvx9/cnNDQUvV4vTUuFT0y3L9fT9PT0MDg4CEBfXx9hYWEEBATMKgt6JllBi4W9ro7eH/+EkB//BwwxMXR//88xrV9P4P6XvLeTzutw9yK8NeY95qeR8GPPs20m0prEZrFYWLt27Yy3q8blcnHx4kUMBgNRUVF0dHSM+/6hQ4fo7u7mzJkzXL58WQLmQixzEugRmtTuzldVVZGSkkJmZianTp3iwoULqiNGPaXVVHUu+3oIIbxPq3zRHdQY2zg3KCiIkpISpbeO0+mcVsaNVgmqO9PH4XDwrW99i5CQEJxOJ2VlZWRkZDA4OEhXV5dyl34uzWUJyFKi1+vJyMigqqqK5uZmAgMDpe+O8Imn9uX66CMMg734NZ6hsOS/k/cX/4Bl7WbN7X344YfK10VFRaxZs4a9e/dy4cKFGWdBTzcraDEZPHUagBXf+QP04eH0/OivGfj8lHcDPdt/CFm/y8Yq+Vu4fgL+6Lz3tj+HXC4X5eXl3Lx5k23btik3LPr6+jAYDLS3txMVFYXJZEKn0ynBSiHE8iXvAkKV1p0t959GoxGdTqc0Gp6tiU1Vy8rKxvX1EEIsPFqll+5moR988IHSLHTLli1s2bJlUm8do9E47q72bDJu1DJ9srOz6evr4/jx48BokGj79u2zPPKnm8sSkOlayL1xenp6vH4DQYiZmKov17defIHjf/5NgmNXseXf/5z+x/cw+AWMe97E98OsrCylB1lnZyfNzc3K+54nDeFhbrKCFirH49HsFH1QEDqdDp3ZjLPjsXd3Epo4+h/A68ef/nwvj2H31MS/d4PBwJUrV2hqamLHjh2Ul5crzy0qKiIxMRGr1ao8110KK4RY3iTQI1Rp3dkaGBjgq6++4ubNm8BojfDFixd57rnnGBwcnFGjUbWmqhP7ekyH1sWnjJ4UwrvUSi/j4uIoKSlRbRaq1lvHbrdz6dIlr2TcPK3Z/HzxdgmINyyU3jgTL2Dc/Tfm4gaCEN5yv+Y8Q72d7PjBzwlflU74qvRJz9EqRQ8PDyc3N5fi4mLu3LnD0aNHJ01ScrlcrFy5kj179owLAs1FVtBCZbCMZnc7rVb0fn64rFb0URbfLsqLY9inQ+3v/fr16wBKkGflypXSB1IIMSUJ9CxxWkGPp93J1bqzlZ2dTVJSEvfv3590MlNXV0dHRwc5OTmUlZVx8eJFNmzYQGlpqfIBBaO9NNx31D1tqnry5EkKCgqwWCZ/6Hd0dPDxxx/jcrl48803uXjxIr29vcDoxYN7fTJ6UgjvUiu9HBkZ0WwWupAybubSdEezz7WF1BtH7QJmy5YtNDQ00NzcTGhoKNu2bZvzdQgxHf0d9wGo/Zd/4MmDVsJWpZPzJ29hjo5XnqNVir5jxw6cTierV6/miy++GLfdiZOU6uvrx5UuagWuPRnTvdgE7P0GT372c2wffIghJgbXwACB+/Z6bwfTzc7x8hj26VD7e9+9e7dX9zHnja+FED4ngZ4lTu0OU0xMzKzu5KqdzNhsNqXR6M2bNzEYDDgcDuU10dHR7N07+oHt5+enPD5VU1UY39dDq4yrsrISvV6v7C8iIoLs7GzsdjufffYZMHr32t0cVUZPCuFdY0sv3b9fas1CF0rGzVybzWj2+eKr3jhaf//Z2dlzvm+xdGnd1Hpaxoyn/M1hAESmbWTj4R9Q/NZ3qXv/Z2z//t9Peu7EUnRAyVTT6XRKvy6YPEnJne22HPk9+yyhb/1nrP80Gnww/9mfEvDyfu/tYDrZOXMwhn0hmZfG10IIn5NAzxKnFpRxuVxeuZM79mQmKCgIgGvXrmG1WjGZTOMCPZ2dnXz00UdERESQl5enBFmeduFnsVimvPhobm7GarWSnJzMrVu3AMjNzQVQpvkYjUblZEtGTwrhXRNLL63W0eklS6VZ6FQXkO3t7crztm3bRmZmpg9XOtnEtYeGhirfe/z4sZLhk5uby5dffjmvvXG0fq4w+m/mgw8+wGazsX///mXfsFo83VRlU1NlzHgqNms7eoMJvdGEwc8fdDr0pskBI7UeZGOb6KakpCjlk25jJyllZGTM+GewFJi/913M3/uu9zc83eycGYxhHxoaorCwEKvVisFgICkpifz8fD799NMF91kxL42vhRA+J4GeZWLiHabe3l7VO7mennzv27ePL7/8UjmZcd+hstlsbN26dVx6cnJyMmvXrsXpdHLmzBlKSkq8clfb6XRy+fJlcnJyaGtrm3S8v/nNb3C5XOTl5Sl9MmT0pBDeo1Z6GRUVtaSahWpdQAKkpqYq748zyRKYa2PXXlNTowTD+/r6xpXTunsGzWdvHK2fa3x8PLW1tUqgXghPTFU2BbPPmAmyxJP7b/8r9R/8N2589itiM7ex8Tt/Pu45Wj3ILl68qDTR7e7uHveasUGgvXv3znsz9KVoUvP5136f4NM/4kHO/4eyxof0x+aSUFxM/q7d4zLMx5nBGHa9Xs/WrVuJjIyksbGRuro6kpOTgYX3WTEvja+FED4ngZ5lQO2Ou9aUE09Pvi9fvszg4KByMuPn50dgYCAjIyMkJiby5ZdfAqMnMe4POhg9AWttbfXKcTU2NuLv709KSorStNnlcmG32/nwww+x2Wxs2LCB+Ph4njx5wqNHj546elIt0LV+/Xp+/etfj3tecHAwhw8f9spxCLFYafXcWUrNQrUuIAHu3LnD/fv3sVgs7Ny5c1563EzH2LW7gzww2htHp9MRExPDw4cPqaysJCwsbF5742j9XPv7+6mvr2f9+vXU1dXN23rE0qBWNuWtjJnkvJdJzntZ8/ta74dNTU0A4yYlqU1SioqKYmBgYFFnQC4U45rP17/PSGA0p2+OEGsYYNeDtzkZ8DfKkABVU4xh1+ptYzKZlBYBZrNZacAPC++zYkE2vhZCeJ0EepY4tTtM7pIKtSknnp58u5sdjz2ZiYuL49atW3z00UfK/o8ePcq6detISUkBoL293Wu9cXp7e3n06BHvvPPOuP2tXr0am80GwFdffcVXX33FunXraG9vf+roSa1Al7vfhs1mo7CwkPj4+EmvFWK5mar08mnNQhdCSdR01pCenj7uAtLpdJKbm4vNZuPs2bNUVlayb9++OVnnbNntdiIiIhgZGeH3f//3KSsrw2QyYbFYePjwIS+++KLPyqMmXpiXlZWRlpam9GTr6+vj7bffVprt6/V6XC4Xx44do7Ozk/z8fNauXeuTtYuF5WllU3OdMaP1fujOmP7FL36hPCaTlObOpObzPc08evyYgahBVnd9RPRwCzFP6mht9dcO9GiMYX9ab5v29nZOnjyJw+EgISGB4OBgMjIyFtxnxZw3vhZCLAgS6FnitO4wPW3KydNOvtX6JlitVmWcenV1NXfu3OHAgQNcvXqVzz77jJGREeVuhjdkZWWRlpY2aX+nT58e9zyz2aykbz+NVqDLnfbtvjO3fv16rxyDEMvVQiiJ8nQNOp1u0gXk2HK0sLAwWltb+eUvfzmuN4PRaPR5v5mJF799fX20trby2muvKZmQvuLJ2hoaGsY12we4fv06PT09Plq1WIg8KZvydcbMfExSEio2v4ktdh980YIpbiPcrsGUvJ2BR33T3tTTettYLBYOHTrE7du3uXLlCteuXWPDhg3K6yMiIujq6vJ4f5PK0A4fJjg4mFOnTnHv3j1cLhdRUVHs3r2bkJAQj7c7542vhRALggR6lrip7rhrTTmZ6YWB2WxWAiJjG3rO1YmM1v7Upt1Ml1r6t9PppKmpidjYWKmjF2KWFkJJlCdriIyMZGBggP7+/nEXkJcuXSIjI4PBwUF6enqIiopiz54943ozpKam+rTfjNrFb0dHB3a7nffee0953smTJykoKMBimb/UfU/X1tXVRXx8PPfu3QNGe8VVVVWRmZmplAgL4WnZlGTMLEPBcayID4YvWhje8R/h3xxh+NNPCQwcnvampupt09HRweDgICEhIUprAKPRSFlZmfJZ0dXVpZxTempcGdrvpKWlkZOTQ1dXF2fOnKG+vl47O0mDu/H1kHWIwn//OU9eex+DyUDSljh2/ek2jH6GaW1PCLHwSKBHjLOQLwzmi1r6N4xO+LLZbPPax0KIxU7rjuTdu3epqKhQykaDg4N9VhI1MbA7dg1nzpxRRsaPvYDs6+vj+PHRlP64uDjy8/MJCgoa15vB1/1m1C5+N2zYoDTDb21tpaamhry8PCVbc6GsraWlhS+++IL09PRx2Ty1tbVERkaSkJAggR6heFrZlFheenp6lMbbfX19hIWFeWVIwFS9bQYHByktLcVms+Hv78+6detYs2YNt27dGvdZsX37do/3N6kM7Xfc7RDcn02zaYmgN+jZ+sazRKaE0/DZdeoKG0l5LonU7dMLSAkhFh4J9IhxFvKFwXzQSv/28/OjoaGBwMBA5QNWCOGZiXckh4eHOXPmDDExMcDoSXlSUtKkkqjpprnPhFpgd+waIiMjsdvt/MEf/MGU22lvb+f9998f15uhvLx8XMnrfJsqoxNGywx8dSH8tLU9evSI6Ohodu7cSUlJCTB6IVVfX8/BgweViziXy4XL5UKn083LuoUQC8+Iw8n/8j+quNbeh33Eyasxdwk2jgaI3f2QvDEkYKreNgkJCaoZ5S+/rN3AezaOHDnCyMgIwcHBrFy5csbbMQWalKBOsCUIg0lPaFywt5YphPAhCfSIcRbyhcF80Er/Tk1Npb29nezsbPR6vS+XKMSionZHsqenB7vdTnd3N8PDw4SHh9Pe3j6pJGomae7ToRXYncka1HozLJReOIuRWrP9999/n5GRkXEN/8vKyvD391em3Qghlqe8NRaiQ/w5e/Uhhw+/Tlz45F5MTxsS8DQLqbfNoUOH6O7u5syZM1y+fJkXXnhhxttqv/qIov90FofdQcKzKwmJMXtxpUIIX5FAjxBjTBXomioAJoTwnLsZqtVqBVB62NTV1U0qiZpOmvt0aQV2p7sGtd4MDodjWZW8eptas/2XXnpJ+fk+fvyY8vJyNm3aJFMQhVjmjAY9f5Sfyn8/e2Par1V61Dzu96hHjbu3zXyZWIZmMBhob28nKioKk8mETqdT3hdnyvJMBId+tp/mijtU/aqWpjO3yDwg0wyFWOwk0COEEGJGpjOefCyz2czmzZupqakBwGQyERgYOO/Zgk/LYPSUVm+GuLg4YOmXvM4FrWb7bhaLhXXr1s33soQQS8xC71Hz4YcfKl8XFRWRmJiI1WpVgj7x8fHk5OTMePsdt7sY7BsiJNaM0f93TaT9pRGzEEuBBHqEoqOjg48//hiXy8Wbb76JXq/H5XJx7NgxOjs7yc/PZ+1aifALIUZ5Op58YGBAmX7jPjlNSkoiOTmZvr4+KioqSE9Pf+r+hoaGKCwsxGq1jhtjPjIyQnl5uTJuNjs7m6ysrLk78Am0ejO4M5eWesmrEEIsVvPao+Z+Dby9FVxO+MkwGJ5+GTbX2eQDvYOU/uMlbN0D+Jv9WL9/DWueXz2n+xRCzA8J9AhFZWUler1+3IST69ev09PT47tFCSEWLE9HpLe1tSmvcTfGtNvttLW1ERAQQFpamkeBGb1ez9atW4mMjBw3xvzatWt0dHSwb98+DAYDAwMDc3PAQgghFqSWx1Z6baMj0+912/Az6okK9vfotfPWo+bzvwS9CRxDc7P9GUjcFMcfvlPg62UIIeaABHoEMDo63Gq1kpyczK1bt4DRyThVVVVkZmbKGFshhKapxpOfPXuWlJQUr4xIN5lMStNd9xjzwMBA2tra2LJli1IqJRYXrRJAGP0c+uCDD7DZbOzfv5+EhAQfr1YI4S3Wd9/F+o+jjY2D/vB1gv/qhzOeoPed//uC8vWfHb3C/mfj+JuCTI9eOy89aho/ht5WyCiAr9737raFEEKFBHoETqeTy5cvk5OTM+7Oe21tLZGRkSQkJEigRwih6mnjyb09Ir29vZ2TJ08qY8xdLhcALS0tXL16lRUrVpCbmysBgUVEqwQwPj6e2tpaJUtMiMVAq8T0008/ndS7LDPTs0DEUmSvq6P3xz8h5Mf/AUNMDN3f/3NM69cTuP+lGW3v4t9+c0avm5ceNY5hOP0j2PtTuHHSu9teIJxOJ8ePH6ejowOHw8Hhw4cJDg5mcHDQp6XVQixnEugRNDY24u/vT0pKijIGeHBwkPr6eg4ePKh0+3e5XLhcrhnfbXHTunsrHwZCLC7eHE/uqYljzN3b9vPzIz8/n+LiYs6fP88bb7zhtX2KuaVVAtjf3099fT3r16+nrq7Ox6sUwjNaJaYwvneZv79nZUVL1eCp0wCs+M4foA8Pp+dHf83A56dmHOiZqZn0qJl2MK/6bVgRCRmvwI2i0W+6HCy1y7CkpCSCgoK4ffu28ti5c+ektFoIH1la7zBiRnp7e3n06BHvvPOO8tj777/PyMgIH330kfJYWVkZ/v7+SunETGndva2rq5MPAyEWEU/Gk1ssFnp6evjlL3+pBHbXr1/Pr3/963HbCg4O5vDhw0/d38Qx5gaDgbCwMHQ6HQaDQflTLD4TSwDLyspIS0uTSWViXjkcDkpLS2ltbcXlchEXF8eePXvw8/Pz6PVqJaZhYWHA+N5lO3fuxGQyzdVhLHiOxx0A6IOC0Ol06MxmnB2P530dM+lRM+1gXud1uHsR3hrz9/3TSPixddx2u1sauXLk7+huacRvRTB5f/EPWNZunvGxzSe9Xs+mTZuoqqpSHrPZbFJaLYQPSaBHkJWVRVpaGgDV1dXcuXOHl156SbmQevz4MeXl5WzatIn4+PhZ70/t7q18GAix+Hgynryjo4P79+9PCuy6p1TZbDYKCws9em9RG2Oenp6OxWKhrKyMY8eOERYWxp49e7xyfGL+TCwB7Ovro7W1lddee03JNBViPrS1tXHjxg2effZZgoODKSsr49q1a9Mqs5pYYhocHExGRsa43mWVlZVe6V22WBkso+eBTqsVvZ8fLqsVfZTFx6vyzLSDedt/CFm/yzIt+Vu4fgL+6Py4bQ4P9HPuv/wxwbGr2PfWr+l/fA+DX8A8HpX3Wa2jgSwprRbCNyTQIzCbzZjNoxMGXnzxxUnft1gsrFu3zuv7HXv3NigoCJAPAyGWGq2yHPd7jnvs+vr168e9Tis1/vXXX1ca9DY0NJCcnExCQgIFBTI1ZLFSKwHs6OjAbrfz3nvvKc87efIkBQUFWCyL42JQLE4hISHo9fpx50bTzbyZWGJ67do1NmzYoHzf273LFqOAvd/gyc9+ju2DDzHExOAaGCBw315fL8tj0wrmhSaO/gfw+nHV7d2vOc9Qbyc7fvBzwlelE74qfR6PZm64M5qktFoI35BAj/CJiXdv3Q1V5cNAiKVpYlkOjDZvbGpqIjY2lsjIyHHP10qNT01N9WqDXq2eYQ8ePKCsrIz+/n4SEhLIz8/3uHRDTI9aCeCGDRuU4F1rays1NTXk5eVJGZeYcyEhISQmJlJeXo5OpyM2NpY1a9Z4/Hq1ElOj0UhZWdmc9S5byDo6Ovj4449xuVy8+eabOJ1OKisraWlpYfh/+wmrautY9//7BeY/+1MCXt7v6+V6zNvBvP6O+wDU/ss/8ORBK2Gr0sn5k7cwR88+k36+9PT0KH09+/r6CAsLk9JqIXxIAj1i3qndvfXz85MPAyGWKLXJXADNzc3YbDa2bds26TVaqfHebtCr1jMsLi6OkpISYmNj2bVrFydPnqSqqoq8vLxZ709M9rQSQIvFwpYtW+ZxRWI5u379Oq2trWzdupXg4GCKi4upr69n48aNHr1ercR0zZo13Lp1S+ldFu3vwnn+l3z4P3+06HqxTFdlZSV6vR6Hw6H8/40bN3j++ecxm81079nDyn/8v+d1TRODTzdv3qS6uhqbzUZISAh5eXlTthGYi2CevzkMgMi0jWw8/AOK3/oude//jO3f//tZHet8+vDDD5Wvi4qKWLNmDc8//7yUVgvhIxLoEfNOq4GrfBgIMTe0xp7W1tZSX1/P0NAQERER5OfnT8qsmS2tyVx+fn40NDQQGBhISkqK6mvVUuPLy8uf2qBXK0vn+PHjqhNRJpaWjYyMMDAwwOrVq4mOjiYmJobW1lYJ9AixDLgnixqNRuUivr+/3+PXJyQkKD3Ixnr55ZeB0V4sx//8mwQsoV4sWpqbm7FarSQnJ3Pr1i1gNJD2zDPPKM2L3e+/82li8KmiooKgoCAKCgo4ceIElZWVHDp0SPP1ngTz4uLi2L59u8dris3ajt5gQm80YfDzB50OvWlxTWbTCthLabUQviGBHqFJq0eG0WhUemTYbDb2798/rV46andvJ95dKS0t5ZNPPlG+v2HDhml9YAohxps49nRoaIhLly4RHx9PTk4OhYWFVFdX88ILL3h1v1qB3dTUVNrb28nOzkav16u+Vi013pMGvVqT/UB7vPHY0jKn0wl83ZfDZDLJFEAhlom0tDTu3btHdXU1TqeTuLg4srKyvLb9pdiLRY3T6eTy5cvk5OTQ1tYGjAZIHA4Hjx494r333sNkMrF582aeeeaZeVuXWvApPDwcl8tFcHAwdrudwcFBfvGLXyg3RY4fP05nZycul4uVK1eyZ8+eKYN5MxFkiSf33/5X6j/4b9z47FfEZm5j43f+fMbbE0IICfQITfPVIwMm310BiI6OZu/e0cZ80htDiJlTG3tqMpkwm834+/sTGhqKXq+fk1G/U5XlTFWuo5Ya73A4PGrQq9UAGtQnokwsLXNPChkeHlb+DAwMnMVPQQix0Ey8waTX63G5XPz2t7+ls7OT/Px81q5d6/X9LoVeLJ5obGzE39+flJQUJTA/Nrj+zW9+k4sXL3L+/HlWrVo1L6Pm1YJPABkZGZSUlPDuu++i0+mIj4/n3r17yvfDw8PZtm0bnZ2dlJSUUF9fPyflpMl5L5OcN/NgkRBCjCWBHqFptj0yPM0Iys7OnnR3BaCzs5OPPvqIiIgI8vLyiIiImPNjFmK50Ov1ZGRkUFVVRXNzM4GBgQuqD4pWary7b4InDXonNoB2Op2TJqLs2rVrUmlZVFQUAQEB3Lp1C7PZzMOHD+f1jrMQYu6p3WC6fv06PT09c7rfpdCLxRO9vb08evSId955R3ns6NGjxMfHMzAwgMFgQK/Xo9PpNLM6vU0t+DQyMkJpaSkxMTHk5uZSWlpKZ2fnuNft2LEDQJnC5m44LIQQC5kEesSUZtojAzzPCGpqamLbtm3j7q4kJyezdu1anE4nZ86coaSkRGp8hfCinp4eqqqqSElJITMzk1OnTnHhwgVefPHFGW3P26WeWn0u3Jk1FouFzMxMCgsLuXjx4rh9uk/cW1tbcblcbNq0CaPROC5Y456IolVatnfvXi5cuEBRUREJCQls3bp1Rj8XIcTCo1a+Mzw8TFVVFZmZmXz55Zdztu+l0IvFE1lZWaSlpQFQXV3NnTt3OHDgAAEBAZSUlFBYWEhQUBDf+MY35m34hlrwyZ3Fo9frlWEg7mzOsVwul/JZk5GRMS/rXQi0evxp9bwTQiwcEugRU5ppjwzwPCPIz89v3N0Vl8ulNOmD0dKL1tbWOTtGIZaDiWNP3Q1HDQYDRqMRnU43rYajE81nqefT9tnY2Mj9+/cxGo3k5OQoDaAvXbo0aSLKVKVlr776qtfXLLzHl03GxeKlVb5TW1tLZGQkCQkJcxroWS69WMxms5IBM/EGwre+9S1fLEk1+FRQUEBnZyc1NTUUFhZiNptJTEykpaVFeZ3L5aK8vJybN2+yd+/eZfd+MrHHn5tWz7uJ1N6rAX7961+Pe15wcLDyPSHE7EmgR2iaTY8MN08ygnp6eial9q5bt06ZxNPe3i5lW0LMktrY0y1bttDQ0EBzczOhoaGqY849NV/j0J+2z8DAQKW3wvDwMBcuXABGp+b09fXNeCKKWJh81WRcLF5q5TuDg4PU19dz8OBBJSDucrlwuVxKUHwirUDjgwcPKCsro7+/n4SEBPLz8yf1GZReLL6hFXyyWCxKP6aenh7q6+uB0ZsiBoOBK1eu0NTUxI4dO4iKimJgYGDZ9G1T6/HnptbzTsvE9+qgoCAla9dms1FYWEh8/NLqUyWEr0mgR2jyRo8MTzKCtm/fTkxMzLjU3qtXr/LZZ58xMjKCTqejs7OTo0ePKmOS3XdW7HY7u3btIj19aU6tEMJbtLJWsrOzvbaP2ZR6emufLpcLGG3I3N/fz4oVK8jNzZ3WZECxOPiyybhYvNTKd95//31GRkb46KOPlMfKysrw9/dXgslqJl68joyMcPr0aWJjY9m1axcnT56kqqqKvLy8uTsg4VVqN0WuX78OQHl5OQArV67kwIEDPlnfQpGRkTGp592+fftUn6v2Xq3X65WgW1NTEwDr16+f+4ULsYxIoEdo8qRHxlTNWz3NCKqoqKCgoGDc3ZXdu3cr27h///6kMcmRkZEEBARQU1PjjUMVQnjBbEo9vbXPpKQkYHRSX35+PsXFxZw/f5433nhjTvYvFpaF3mRc+J5a+c5LL72knKc8fvyY8vJyNm3aNGWGgdrF66NHjxgYGGD16tVER0cTExNDa2vrogz0TJxKdvPmTaqrq7HZbISEhJCXl6fc+FtK1G6KuM9JxdfUet7NhNPppKmpidjY2GVXEifEXJNAj5gz3sgI0hqTnJGRwd27d+ftWIQQU/NGqac39ukuGdPpdEpjzflq9Cl8z9tNxsXSM1XvGBgNHq9bt27CoxODhb8CJmcS22w2ACWLzGQyMTAwMNsl+8TEqWQVFRUEBQVRUFDAiRMnqKys5NChQz5epZgPE3v8GQwGqqurJ/W8m4nm5mZsNtusSseFEOok0CPmzGwzgsaaOCZZCLGweCOw6419pqenY7FYKCsr49ixY4SFhbFnzx6v7E8sPHPdZFyIr/0A2Pu7r9UzD1asWAGgTG0aHh5elL1c1KaShYeH43K5CA0NxWg0SknkMqJWztbf3z+tnndqwaIVK1bQ0NBAYGCg0pdTCOE9EugRC57dbqeoqIjBwUEOHjyo3LkXQiwc3gzsuk0sHaisrOTGjRs4HA7Cw8PZsWOH6j6joqIoKCiY2YGIRWWum4wL8bW3gfeAfODfAZMvXsPCwggICODWrVuYzWYePnw4rsRlMdCaSpaRkUFJSQlHjhzBaDRq9mMRS49Wj7/pUHuvzsrKor29nezsbPR6/az3IYQYT66YxYLmDvL09fWxb98+ZUyy3W5X7tLabDb6+voICQnx8WqFEN40sXQgMTFRSRX/9NNPqampkZKcZW4+mowLAT8CNgJlwD8Dq4HXVC9e9+7dy4ULFygqKiIhIYGtW7f6ZskzpDaVbGRkhNLSUmJiYsjNzaW0tJTz58/z6quv+ni1i4f13Xex/uM/4xoeJugPXyf4r36oOdFtPg0NDVFYWIjVasVgMJCUlER+fj6ffvop7e3tyvO2bdtGZmbmjPej9V7tjSCSEEKdBHrEgqE2qvTJkyc8fvwYgBMnTgCjjRQfP36sfABVVVXR2NioemdfCLE4qZUOuMs2bTYbOp2OiIgIXy5RLCBao65ra2upr69naGiIiIgI8vPzpeGnmAF3QCOJ0UDPDUD7InUxB0DUppK9++676HQ69Hq90vvMarX6cJWLi72ujt4f/4SQH/8HDDExdH//zzGtX0/g/pd8vTT0ej1bt24lMjKSxsZG6urqSE5OBiA1NZXnnnsOAH9/fx+uUggxExLoEQvKxFGlFosFPz8/Vq5cSXZ2NidOnGB4eHjZj7UUYinTKh0A+M1vfkN3dzcBAQGsWrXKRysUC9HEz4+hoSEuXbpEfHw8OTk5FBYWUl1dzQsvvODjlYrFpRG4AuwAyn/3WJrvljPH1KaSFRQU0NnZSU1NDYWFhZjNZnbu3OnjlS4eg6dOA7DiO3+APjycnh/9NQOfn5pdoOd+Dby9FVxO+MkwGGZ2SWcymUhNTQVGG5W7BxoA3Llzh/v372OxWNi5c6f0ZRJikZFAj1gw1EaV9vT0YLfbSUxMxGKxEBERMWdjmoUQC4Na6YDL5QJGp+T09fVx9uxZysvLZerLIqZVMjAyMkJ5eTn37t3D5XKRnZ1NVlbWlNtS+/wwmUyYzWb8/f0JDQ1Fr9fLhYqYgUDgM+C//+7rV4FXfLqiuaQ1lcxisbB27VpfLWtRczzuAEAfFIROp0NnNuPseDy7jX7+l6A3gWNo1utrb2/n5MmTOBwOEhISCA4OJiMjg9zcXGw2G2fPnqWysnJO+zJpZWXCaFPzDz74AJvNxv79+0lISJizdQixlEigRyxo7kaunZ2d2O12+vr6sNvtPl6VEGIuqZUOHD16lO3btxMbG4vJZEKn00lj9kVOq2Tg2rVrdHR0sG/fPgwGw4zHU+v1ejIyMqiqqqK5uZnAwMBpNwQXApIZHacuxMwYLFEAOK1W9H5+uKxW9FGWmW+w8WPobYWMAvjq/Vmvz2KxcOjQIW7fvs2VK1e4du0aGzZsUL4fERFBV1fXrPfzNBOzMt1qa2sZGpp9QEuI5UbOksWCZjab2bx5MzU1NTQ1NSl3aIUQS5da6cCBAwcoLS3lwoUL6PV6oqKiyMvLm7c1eZJ94nQ6MRgMDA8Pj3vOxYsXuXnzJna7nV27dpGenj5v617I1EoGAgMDaWtrY8uWLcTFxQGj09fefvttZfqaXq/H5XJx7NgxOjs7yc/PV8006OnpoaqqipSUFDIzMzl16hQXLlyQBt5CiHkVsPcbPPnZz7F98CGGmBhcAwME7ts7s405huH0j2DvT+HGSc2naWXInDp1SsmWjIqKIisrC6PRSEhIiHLzxGg0UlZWpgw/6OrqUnrkzRW1rEyA/v5+6uvrWb9+PXV1dR4d4+Dg4LSzQoVYiiTQIxaUiaNK3RdLycnJ9PX1UVFRIRdJQixxWqUDvizT8iT7xOVycffuXdauXTvuOZGRkQQEBFBTU+Oz9S9UE0sG3CV6LS0tXL16lRUrVuByucZNXwO4fv06PT0947Y18fPDPdHGYDBgNBrR6XTKtEYhxMLV0dHBxx9/rAR3b968SXV1NTabjZCQEPLy8pRA8GLg9+yzhL71n7H+0+jULfOf/SkBL++f2caq34YVkZDxCtwoGn3M5UDtkk4tQyYtLY2cnBy6uro4c+YM/v7+dHZ2YrPZ8Pf3Z926daxZs4Zbt25x/PhxAOLi4ti+ffvM1jtLly9fJi0tjfDwcNXvqx3juXPnvJIVKsRiJ4EesaCojSq12+20tbVhMpkYHBzk8uXLZGVlcf78edra2hgZGSE6Oprdu3cr9bxCCO/RymYxGo2LvnZe69g6OjooKyujv7+fhIQE8vPzPco+iY+PH/ecsLAwIiIiuHv3rs+OcSGbWDLgvmvs5+dHfn4+n3/+OTabjdTUVGX62vDwMFVVVWRmZvLll18q21L7/NiyZQsNDQ00NzcTGhrKtm3b5vX4hBBf8zSAU1lZOS64W1FRQVBQEAUFBZw4cYLKyspF15/N/L3vYv7ed2e/oc7rcPcivDWm39hPI+HH46egaWXIpKSkAKPZMDAaKPnmN785aTcvv/zy7Nc6S11dXbS0tBAaGkpjYyMwOnUTGJe1Mzw8rLzGZrNN+lwWYrmSQI9YULRGlQIcP36cR48eKR/8QUFBvPjii9jtdj777DOuXLnCnj175mupQiwbWtksqampi752Xu3YEhMTuXjxIrGxsezatYuTJ09SVVVFamrqU7NP0tLSqKqqGtfUUqjr6OhgcHBwXMmAOzim0+nQ6XQMDAzg7++PwWBQXldbW0tkZCQJCQnjAj1anx/Z2dlzehxCCM94EsDJzs7GarWSnJysBHfDw8NxuVyEhoZiNBqXd1P17T+ErDdGvy75W7h+Av7o/LQ2ceTIEUZGRggODmblypXeX+MMTczKHBgYYHh4mI6ODuU558+fJzw8nCtXrihZO9evX+f69esAWK2jAa+xn8u5ubmL7iaUEN6g9/UChPBEc3Oz8sHvlpubS0xMDAkJCZhMpkV9sSnEQubupRIaGjouU2Vs7fxsdHd389FHH/HLX/6So0ePcvHiReV7w8PDvPfee/ziF7+Yk6wYtWMbGhpiYGCA1atXEx0dTUxMDK2trUr2yZYtW7h7967SnNLPz4+XXnoJh8NBbW3tuOdcu3bN62teKgYHByktLeXDDz+ktraWdevWkZ6ezvPPP4/dbufjjz9Gr9crZXHu19TX15OTk6M85nK5lK+FEAuT2nlceHg4JpNpXADn8uXL5OTkjAvuZmRk8PjxY44cOcLQ0NDyzswLTYT4LaP/vX4c/pNr9OtpOHToEC+88AL9/f1cvnx5jhY6fR9++KGSuVNUVMStW7coKCigoKCA2NhYADZv3oy/vz9tbW2sX7+euLg4goKClG34+/sD4z+Xz58/P+/HIsRCIBk9YsFzOp3KB39bW9uk71dXV2O322d9sSmE0KY2frW8vHzK2nk3rfKoBw8eUFFRgdVqJSQkhG9961vcuHFDyaqJj4+fl4yhicfmPlF03zU2mUzYbDYePHigmX1iMBiUu9R6vX5cU0ur1ar0hrHZbPT19RESEjKnx7QYJCQk8Prrr096PCoqioKCAioqKvjqq6+UPhEA77//PiMjI3z00UfKY2VlZfj7+yuldUKIhUXrPC4jI4OSkhKOHDmC0WgkNTWVBw8ekJKSwp07dwAYGRmhtLSUmJgYcnNzKS0t5fz587z66qu+OpxFQ63vZXt7O1FRUR5Nr6ytraW+vh6Xy0V6ejpbt25Vep/Nhamy+leuXMmDBw9Ys2aN0nOnpaWF+vp65Tl9fX2EhYWN+1x2/ynEciSBHrHgNTY24u/vP+6D33339osvvqCmpoYdO3aQmJjoy2UKsaSpjV9tbW3ltddeU34vtaiVRyUkJFBRUcHKlSvZs2cPJ06c4Pr168THx9PQ0MDQ0NCU0zbm4tiuX7/Ol19+yb1794DR95fg4GClyePJk6MTTnQ6nZJ9YrFYKCsr49ixY6xYsQIYvSs5tqllUVER7e3tAFRVVdHY2Kga4BDjqU1fe+mll5QLk8ePH1NeXs6mTZuU3khCiIVH7TxOLYBz7do1hoaGeOedd5TXvvvuu+h0OvR6vXLh7i7PEVOb2LcsMTERq9WqBH3i4+PJyclRfe2DBw+4dOkSW7duxWw2c+7cOSIiInjmmWfma/maxmbtjL0R5O7N9vzzzyufy2FhYdLWQSxbEugRC15vby+PHj0a98F/9OhRpcncpk2bSEpKwmazKRdaQgjvUeul4nA4sNvtvPfee8rzTp48SUFBARaLZdzr1cZoBwcHY7fbSUxMxGKxEBERQWtrKw8fPiQkJISkpCTKyso8yhjy1rG5M3ieffZZ6urqePDgAc3NzZhMJpKTk1m/fj2FhYWkp6ezY8cO4Ovsk6kcOHBgzta/lGlNX3OzWCysW7duvpclhJgmtfM4tQCO0+lU3k/dwd2CggI6OzupqamhsLAQs9nMzp07fXUoi8pUGTJP8/DhQwCSk5MJCQnh3Llz3LlzxyeBnomZSWOzdl599VXOnDnDyMgIhw8fVl7ztM9lIZYDCfSIBU/tru6BAwc4ffo0MHrX/YsvvsBsNstdciHmgLuXysTxq+6JFq2trdTU1JCXl6cZlFEr/QLo7OzEbrfT29urnMgdPHiQvr4+jzOGvH1smzdvxm63c/XqVb744gsSExPZvn27kiIuZaJCLF1Op5Pjx4/T0dGBw+FQLh5//etfj3tecHDwuAtLoU3tPE4tgJOfn6/cKBgb3LVYLKxdu9Yna1+u3H1vOjs7lalWvuqFqTZRUbJ2hHg63XQaGG7ZssV15cqVOVyOEEKIpWhkZASr1aqUfm3fvp2hoSFqampwuVxK3f/LL79MSEgI9+/fV22gqJYx5E0TA1L79u3DZDLhdDr5l3/5F0JCQjh48OCc7V8I4VtOp5Pa2lo6Ozu5ffs2hw8fJigoSBnrbLPZKCwsZO3ateTn5/t4tULMDafTyeeff05bWxtGoxGXy8WaNWvmJJtKLbgaHBzMqVOnuHfvHi6Xi6ioKHbv3j3j/nZa+xg7pt3lcpGdnU1WVpaXj1AIz+l0umqXyzW9DusaJKNHCCHEnFIr/TIajcTExJCcnExLSwvV1dUAnDhxAoANGzYoqdeeZAx5i1ovog0bNtDc3IzNZlve016WiY6ODj7++GNcLhdvvvkmer0el8vFsWPH6OzsJD8/X7ILljC9Xq+Uho99zF1G2NTUBEhm36Jyvwbe3gouJ/xkGAxy+eOJ7OxsNm/eTEdHBxcuXGDNmjVztq+kpCSCgoKUnngAaWlp5OTk0NXVxZkzZ6ivrycvL8+r+zh37pwypt1gMCiNnoVYCuSdTgghxJzSKv06c+YMbW1tBAQE8Oyzz7Jlyxb0ev2k11ssFrZs8crNjSlpBaQAGhoaCAwMJCUlZc7XIXyrsrISvV6vTFEDuH79Oj09Pb5blFgQnE4nTU1NxMbGEhkZ6evlCE99/pegN4HDN6VHi5HD4aC4uJj+/n5CQkLYs2cPMTExc7IvteAqoHzeOp1OACIiImacmaO2D5vNRltbG1u2bFFK0YVYSiTQI4QQYk5pjdF+4YUXfLAabVoBqa6uLtrb28nOzlYNRImlo7m5GavVSnJyMrdu3QJgeHiYqqoqMjMz+fLLL327QOFTktm3CDV+DL2tkFEAX73v69UsGiaTaUH0oDpy5AgjIyMEBwezcuVKwHuZOe7pbS0tLVy9epUVK1aQm5tLQkLC3ByMEPNMAj1CCCF8bmhoiMLCQqxWKwaDgaSkJLKzs8c1YYS5bYCqFZCKiIiY1fQSMV53dzfFxcX09PRgNBpJT0/nueeeo7a2lvr6eoaGhoiIiCA/P39esyacTieXL18mJyeHtrY25fHa2loiIyNJSEiQQM8yMXHKj8FgYMWKFZLZt9g4huH0j2DvT+HGSV+vRszAoUOH6O7u5syZM1y+fJkXXnhBMzNn8+bNVFVVTZnt424sDePHtOfn51NcXMz58+d544035v04hZgLEugRQgjhc3q9nq1btxIZGUljYyN1dXUkJycrgRd3A9T4+Hgfr3RxUQug5efnc/HiRW7evIndbmfXrl2kp6fP25ocDgdpaWkkJiZy9epV6urqiImJ4dKlS8THx5OTk0NhYSHV1dXzmvXV2NiIv78/KSkpyqS3wcFB6uvrOXjwoHLh73K5xjUQF0uP2pSfrKwsyexbbKrfhhWRkPEK3CgafczlQC5/Fh614Gp7eztRUVGYTCZ0Op1SSj3R2Myc3t5eDAbDuNJbd7bPtm3baGlpoaWlZdKYdoPBoPwpxFIh73RCCCG8Siu4YDQaGR4e5oMPPsBms7F//34lRdpkMpGamgqA2WzGYDAQFhYmDVBnSSuAFhkZSUBAADU1NfO+pqioKKKiogCIi4ujoaEBl8uF2WzG39+f0NBQ9Ho9JpNpXtfV29vLo0ePeOedd5TH3n//fUZGRvjoo4+Ux8rKyvD391f+vYqlRyuDTzL7FpnO63D3Irw15r3kp5HwY6vv1iRUTQyuJiYmYrValaCP+yaAGndmjr+/PwcPHqSoqEj53tg+PGMnecqYdrEcSKBHCCGEV2kFF1JTU6mtrWVoSL0h5sTR5sHBwYA0QJ0NrQBaREQEd+/e9ena7HY7NTU1hISEkJSURG9vL1VVVTQ3NxMYGDgvDbjHysrKIi0tDYDq6mru3LnDSy+9pNxFfvz4MeXl5WzatEkyy4TQoNUs98GDB5w7d44nT54A8G/+zb/hyZMnlJWV0dvbS3R0NLt27VKC+16x/YeQ9bsynJK/hesn4I/Oe2/7wmumE0SdmP2jlpnjNjbbJzAwULUPz7e//W3l3+wnn3wio9fFkiGBHiGEEF6lFVzo7++nvr6e9evXU1dXN+l1sx1trpVJ9ODBAyoqKujv7ychIYH8/HzlDuByoBVA8yW73U5RURGDg4McPHgQq9VKVVUVKSkpZGZmcurUKS5cuMCLL744b2sym83KRabafi0WC+vWrZu39QixWE1sljsyMsLp06dxuVzo9XqcTidXrlzh7t27+Pv78+1vf5vPP/+c0tJS9u/f772FhCaO/gfw+nHvbXeRcvdBc7lcpKens3Xr1nFBEa0gXXl5uc9KfdWolVaOzczx8/NTvu9pHx4ZvS6WIgn0CCGE8Dq14EJ5eTlpaWmEh4dPer43RpurZRIlJCRQUVHBypUr2bNnDydOnODy5cvs3LnT+we9QGkF0HzFHeTp6+tj37596PV6pUGmwWDAaDSi0+no7+/32RqFEDOjNsb60aNHDAwMEBAQQEpKCrdu3aK1tZX+/n42btxIREQE0dHR3L59G6fTKT2Q5sCDBw+4dOkSW7duxWw2c+7cOSIiInjmmWfGPU8t4OHLUl81Wtk/BQUF9PT0UF9fT2Njo5LtExoaSkdHB4WFhTidToKCggAmZexYLBZlWzJ6XSwFEugRQgjhdWrBhdbWVl577TWl0e1Y3hhtrpZJFBwcjN1uJzExEYvFQkREhOr+lyqtAJrValUCKTabjb6+PkJCQuZtTY8fPwbgxIkTAGRnZ7NlyxYaGhpobm4mNDRURlgLsUS4y2fWrl2LzWYDYGBgAKPRSFdXFw6Hg56eHlwuF0NDQwQGBvpyuUvSw4cPAUhOTiYkJIRz585x586dcYEetSAdQEZGhs9LfT2llu2zZ88eTp06pdxQcH+2TMzYGZtpLKPXxVIggR4hhBBepRZccDgc2O123nvvPeV5J0+epKCgAIvF4rXR5lplSp2dndjtdvr6+rDb7bM8Qt/p6Ojg448/xuVy8eabb9Lb20tJSQnd3d1ER0eze/du5W4laAfQioqKaG9vB6CqqorGxkbVn/9ciIuL0/w7zc7Onpc1CCHmT0dHBzD6fu4OMAcGBrJ582YuXLjAkSNH8PPzw2AwEBAQ4MulLnpa5Vnuz4XOzk4l4KHVL28x0/pseeONN6iqquKLL77AYrGoZuyEhYUpz5fR62IpkECPEEIIr9IKLrhPplpbW6mpqSEvL0+1jMtTav0ELBYL69evp7Gxkbt37/Lb3/6WjIwMmpqaaGpqwmQyebfZ5zyrrKxEr9cro2PPnj2LyWTi4MGDnDp1irKysnH9ZbQCaAcOHJi3NQshlpeJzXLdgYXi4mLlOTabDYvFQkFBAU+ePKGqqoqYmJhxPWPE9ExVnpWamsqNGzcoLi7GaDRiMBgW9WfhbE3M2PH391eyWmX0ulgqJNAjhBDCq7SCC+50fIvF4rWJSmP7CXR1deF0OqmrqyMkJITh4WFsNhs9PT288sor9PX1UVFR4fNGkjPV3NyM1WolOTmZW7du8eTJE7q6usjNzSUyMpLExEQaGxs97nHR3d1NcXExPT09GI1G0tPTee655wAYHh7mgw8+wGazsX//fklXF0J4bGL5TEpKCjt27KC2thar1YrL5WL//v1cu3aNxsZGTCYTycnJbLh5kwd/+Ve4hocJ+sPXCf6rH0rgZxqeVp6VnZ3N5s2b6ejo4MKFC6xZs2bSNiYG6QwGA06n02elvtOh1Uza3YuntbUVgKamJuXY3Rk7x44do6enB5DR62LpkECPEEKIRWliP4GhoSHl6/7+ftLT07l58yZ9fX0UFhYSEBBAWlqa5nhUrcDHgwcPKCsrGze1a+xUj/ngdDq5fPkyOTk5tLW1ASgTQEwmE93d3dy+fRuXy8X//J//U1m7O41/aGiIiIgI8vPzlRH1DoeDtLQ0EhMTuXr1KnV1dSQmJhIfH09tbe2STOsXQsw9rfKZiVPr4uPj2b59OwD2ujoe/8e/IeTH/wFDTAzd3/9zTOvXE7j/pTlf71IxVXmWw+GguLiY/v5+QkJC2LNnDzExMZO2odbj5smTJz4r9Z0urelZjx8/Jj4+njt37qDX6zEajeMydsLDwxkZGeHw4cPjtldQUDDfhyCE10igRwghxKKgNT7d7eLFiwwODrJmzRquX7/O9evXCQwM5ODBgx6NFFcLfMTFxVFSUkJsbCy7du3i5MmTVFVVkZeXN5eHOkljYyP+/v6kpKQozaTdvSyGh4dxOByEhoYyODhIamoqdXV1xMTEcOnSJeLj48nJyaGwsJDq6mpeeOEFAKKiooiKigJG++Y0NDQwNDREf38/9fX1rF+/flxzSiGWKrVMAIBf//rX454XHBw86UJQjJrYP6yzs5OysjJ6e3uJjo5m165dU5YKDZ46DcCK7/wB+vBwen701wx8fkoCPdMwVXmWyWTy6N/udHrizZfp/n6ObSbt7sUDKJ+d1dXVPHnyRDJ2xJIngR4hhBCLgtr49OTkZOX77ibL169fJyUlhczMTE6dOsWFCxfG9a3Rohb4GBkZYWBggNWrVxMdHU1MTAytra3zHujp7e3l0aNHvPPOO8pjx44dIzw8nJaWFhISErDZbCQlJREfH09jYyMulwuz2Yy/vz+hoaHo9XpMJtOkbdvtdmpqaggJCSEpKYmysjLS0tJm1T9JiMVmYiZAUFCQkrVgs9koLCwkPj7el0tc0NT6h/n7+/Ptb3+bzz//nJKi03zj2vj3FMOzsZgOrgXA8Xi0YbM+KAidTofObMbZ8Xh+D2IJ8KQ8azGa6e+nuxdPVFQU/f39k6ZnScaOWMok0COEEGLaJt69rays5MaNGzgcDsLDw9mxYwfR0dFe3afa+HS9Xq+cyK1atYrm5mYADAYDRqMRnU6n9Bbw1NjAh9PpVPbt/tNdMjWfsrKySEtLA0bvRt65c4cDBw5gMBgoKSnhk08+ITo6mpycHIqLi5WgTW9vL1VVVTQ3NxMYGDipN5LdbqeoqIjBwUEOHjxIX18fra2tvPbaa8tqDL1Y3tTGSuv1eiUboqmpCYD169f7ZH0L3cT+Ye4Jhxs3biQiIoLo6Ghu376N6fvfHA0GNXUw8tlN9KlfB34MltEgu9NqRe/nh8tqRR9l8dUhLShavWcmlhVv27bNo/KsxWY2v58yPUssZxLoEUIIMW0T794mJiaSkZHB4OAgn376KTU1NR5l0UzXxPHpp06dUr7nDvKsWbOGu3fv0tzcTGhoKNu2bfN4+xMDH+4gkrvfwfDwsNJUej6ZzWblpHbiz9V9R1Jt7VVVVZrZTe7n9/X1sW/fPvR6PR0dHdjtdt577z1l+ydPnqSgoACLRS661Gj1djp+/DidnZ24XC5WrlzJnj17lIsOsXg4nU6ampqIjY1V+luJr6n1D/Pz88NoNNLV1YXD4aCnpweXy4XdX0dgYACOxscQZEKf8fV7SsDeb/DkZz/H9sGHGGJicA0MELhvr68Oa8GZmNEyMjLC6dOnx5UVf/nll8uutHDi7+fEZtIyPUssZxLoEUIIMS0T797C6EkojKZQ63Q6IiIi5mTfFouFQ4cOcfv2ba5cucL27du5cuWKkoFSWlrKM888w+7du6e9bbXAR1RUFAEBAdy6dQuz2czDhw+VCSYLidra3cEpreymjo4OHj8eLY04ceIEABs2bFACR62trdTU1JCXl6daxiVTu0ZpNbUODw9n27ZtdHZ2UlJSQn19vdemzYn509zcjM1mm1bAeDlR6x/mcrnYvn07Fy5c4MiRI/j5+WEwGAgICMD5qB9Xay+GnavQGb6eDuj37LOEvvWfsf7TP+MaHsb8Z39KwMv7fXVYC4paRsujR48WRFmxr038/VRrJi29eMRyJYEeIYQQHlO7ewujQYNjx44Bo02Ce3t7effdd6cs5ZpY/lVaWsr169eV72/YsEGZyOJ+/uDgICEhIRiNox9fDofDaxkoaoGP7Oxs9u7dy4ULFygqKiIhIYGtW7dOa7vzQWvtW7ZsoaGhQTW7KS4ubsrGmxaLZcrAhEztGqXV1HrHjh0ASiaW+y6zWJjUxkqvWLGChoYGAgMDSUlJ8fEKFya1/mFHjx7l29/+NgUFBTx58oSqqipiYmLQ6XSMXLkHeh3GzXGTtmX+3ncxf++787n8RctmswG+LyueL57+fmp9pkkvHrEcSaBHCCGWsekGW7Tu3rpLudw9bTo7Ozl48OCUpVwTy78AoqOj2bt3NF1/4gjzwcFBSktLsdls+Pv7s27dOtasWUNc3OgFw9MyUJ5mqsDHq6++Ou3tzSe1tbszboaGhjAajSQkJBAXFzflyPXpWKpTu6zvvov1H0ezCoL+8HWC/+qH6HS6p75uYlNrGP3duHjxIgaDgYyMjLleupgFtUyArKws2tvbyc7ORq/XT/Hq5Uurf9i1a9dobGzEZDKRnJzMc889h2toBEfdQ/TpkehCpIxxNlasWAGMKSt+0kHg0CP4Tzr4yTAYltYlnvx+CjF9S+tdQAghxLRMN9iidvf23XffxWg0Eh8fT1tbGzqdjsDAQCIiIjRLudTKv2A0QPTRRx8RERFBXl7euNclJCQoUzbGcvfMeVoGynKjlnHztJHrM7GUpnbZ6+ro/fFPCPnxf8AQE0P39/8c0/r1BO5/acpePB0dHYyMjKDX6zl48CBGoxGXy0V5eTk3b95k79690t9lgdMK8i7EcdMLiVb/MIvFMu4mAcBI1T2wOzBskell06XWe2ZcWfHjDp6xX/XxKueO/H4KMX0S/hRCiGVqbLBlLHew5dy5c0ozYresrCwKCgooKChQshYCAgLw8/Pj7t27AISGhrJz505+85vf8N5772E0Glm1apWyjbHlX2ObIiYnJ/Pyyy+zb98+uru7KSkpmaMjXx6ioqLIysoiPDxcyXrydOS6p8Y2gN6/f78ytSs7OxuXy+WtQ5k3g6dOA7DiO39A4CsF6AIDGfh8tOG3O3D2yiuvsHr1aurq6rh37x4hISHKFDiHw8GNGzew2+2UlZXR2NjI9u3biYqKWtJlFUJ4wrg1noC/2Y0hZfEFgX3tww8/pLGxERjNaKmqqmLv3r309PRQdPy3xI/cZmvc0zMPhRDLh2T0CCHEMqTVayc5OZm1a9fidDo5c+YMJSUl42rbJ969vXr1Kjdu3ODgwYOUlJRw48YNXn75ZQwGAy+++CJ9fX2cPXuW8vJyDh06BGiXf40NOMXFxdHa2joPP4mlb2LGzdNGrk9nu0ttapfjcQcA+qAgdDodOrMZZ8do7yOtUrW0tDSuXbumbOOrr77Cz89PGflbXl4OwMqVKzlw4MB8Ho4Qi9JMyyeXMs2y4ld+D/5xPTz/d3Dj5LjvDQ0NUVhYiNVqxWAwkJSURH5+Pp9++int7e3K87Zt20ZmZuZcLt+rtMbNDw4OUl5ezr1793C5XGRnZ5OVleXr5QrhMxLoEUKIZchbwRatRpzbt28nNjYWk8mETqdTmidP9Zp169YpDRXb29vnbHLXcjLdkevTMdupXQuRwTIayHFarej9/HBZreijxgeoJgbOjEYjf/zHf0xJSQm3bt3i937v94iMjJQyQiFmYKrySaGi+m1YEQkZr8CNotHHXA7AiF6vZ+vWrURGRtLY2EhdXZ3yGZ+amqpMSfT3X3z9kiaOmwc4d+4cHR0d7Nu3D4PBIFmUYtmTQI8QQixD3gq2aDXiLC0t5cKFC8qI8rEjX7Vec/XqVT777DNGRkawWCzs3LnT24e9rMxk5PpEWneEOzo6uHDhAn5+fiQkJJCfnz+pn9Ni7JkUsPcbPPnZz7F98CGGmBhcAwME7turfH9i4Ex68QjhXWPLJ/Xh4fT86K8Z+PyUBHq0dF6HuxfhrTEluD+NhB9bMZlMpKamAijlpWFhYQDcuXOH+/fvK5+1synhnW9q4+ZtNhttbW1s2bJFKVUWYrmTQI8QQixD3gq2aDXidJdpTec1u3fvnunhCBUzGbk+kdod4cTERC5evEhsbCy7du3i5MmTVFVVjQvmLVZ+zz5L6Fv/Ges/jZaNmP/sTwl4eT+gHjiz2+1cvHiRpqYmduzYofTicTcIF2IxmziV0el0UllZSUtLCyMjI2RkZChZIZNNDPL+Ckh/6j6nKp8UKrb/ELLeGP265G/h+gn4o/PKt9vb2zl58iQOh4OEhASCg4PJyMggNzcXm83G2bNnqaysZN++fb5Zv5e4+wm2tLRw9epVVqxYQW5uLgkJCT5emRC+I4EeIYRYhmYSbJl40l9ZWcmNGzdwOByEh4ezY8cOoqOj53rpwkNTjYvPzs72aBtqd4SHhoYYGBhg9erVREdHExMTQ2tr65II9ACYv/ddzN/77qTHtQJn0otHLFUTpzK63/Off/55zGYz3d3dT9nCDwB3RpxnmW6elE+KMUITR/8DeP34pG9bLBYOHTrE7du3uXLlCteuXWPDhg3K9yMiIujq6pqv1c4Zd/mZn58f+fn5FBcXc/78ed544w0fr0wI35FAjxBCCI9MPOlPTEwkIyODwcFBPv30U2pqambU60UsbBPvCLtPqN2p/iaTaVn0QtAKnC228jQhPDF2KuOtW7cAuH79Os8884zS58XdnFzb28B7QD7w7zza79PKJ4XnOjo6GBwcJCQkROmTZzQaKSsrUz67u7q6lAmai4nauPmwsDB0Oh0Gg0H5U4jlTAI9QgghnkrtpN99cmiz2dDpdNI8eYmaeEc4MXH07rG738/w8LCUKgmxhKhNZRwcHMThcPDo0SPee+89TCYTmzdv5plnntHYyo+AjUAZ8M/AauC1p+57qvJJMT2Dg4OUlpZis9nw9/dn3bp1rFmzhlu3bnH8+Gj2T1xcHNu3b/fxSqfvww8/VL4uKipizZo1PP/885SVlXHs2DHCwsLYs2ePD1cohO9JoEcIIZah6ZRhaY1iB/jNb35Dd3c3AQEBrFq1CoDu7m6Ki4vp6enBaDSSnp7Oc889x4MHDygrK6O/v1+zga9YWNTuCOt0OgICArh16xZms5mHDx9OcbEnhFhs1KYyjp3M9M1vfpOKigqKi4spKSkZ16i9uLhYGW8dFdXE7t0HCAn5Z+CGx/vXKp8U05OQkMDrr78+6fGXX37ZB6vxLq2yZPfURyGEBHqEEGJZmk4ZltYodhjt79PX18fZs2cpLy/n0KFDOBwO0tLSSExM5OrVq9TV1REXF0dJScmSbOC7lKndEV63bh0RERFcuHCBoqIiEhIS2Lp1q6+XKoTwEq2pjPHx8QwMDGAwGNDr9ej1el555RWuXbumjO5OS0tj27Yw7PYKzp59TGdnLSEhAGk+Ox4xc7W1tdTX1+NyuUhPT2fr1q3odDpfL0sI4QEJ9AghxDIz3TIsrZP+7du3Exsbi8lkQqfTKRkfUVFRSu+GuLg4GhoaGBkZWdINfOfTxGys3t5eSkpK6O7uJjo6mt27dxMUFOSVfWndEY6Li+PVV1/1yj6EEAuL1lTGgIAASkpKKCwsJCgoiL179xIeHj5udPfoZ4eOkZGLvPJKM7ACeBV4xYdHJGbiwYMHXLp0ia1bt2I2mzl37hwRERGSwSnEIiGBHiGEWEamW4YF2if9paWlXLhwAb1eT1RU1KSgjd1up6amhpCQEJxOJ7D8GvjOhYnZWGfPnsVkMnHw4EFOnTpFWVmZNMUWQsyY1lRGgG9961vK1+3t7fzyl78cN7ob4MiRYkZGnic4OJiXXnqJsLCweVu78J6HDx8CkJycTEhICOfOnePOnTsS6BFikZBAjxBCLCPTLcMC7ZN+9/fV2O12ioqKGBwc5ODBg1itVkAa+HrK4XBQWlpKa2srLpeLuLg49uzZw71798ZlYz158oSuri5yc3OJjIwkMTGRxsZGnE4ner3e14chhFjCtEZ3Hzp0iO7ubs6cOcPly5d54YUXfL1UMQPuzNDOzk7ls3toaMiXSxJCTIMEeoQQYhmZbhnWTLiDPH19fezbt0/J+JEGvp5ra2vjxo0bPPvsswQHB1NWVkZTUxONjY3jsrHcWVFjM6VcLheDg4OsWLHC4/15s4H20NAQhYWFWK3WcU1ar169Sn19PUNDQ0RERJCfn09kZOQMf0JCCF9Sa9TucDi4desWUVFRXvksEd7hdDo5fvw4HR0dOBwODh8+THBwMMePH6e9vV15nslkUt7/t27dSmpqKjdu3KC4uBij0YjBYFBu+gghFj559xVCiGVkpmVY09HR0cHjx48BOHHiBADZ2dns3btXGvh6KCQkBL1ePy6bqqOjY1I2VkBAADA+U8o9FWs6vNlAW6/Xs3XrViIjI2lsbFSatF66dIn4+HhycnIoLCykurpa7vQLsUipNWpPS0tTgvwGg0H5fRe+l5SURFBQELdv3x73eGpqKqtXr+b06dNkZmYSGhqq9OJJTU0lOzubzZs309HRwYULF1izZo2PjkAIMV0S6BFCiGVkJmVY0xUXF6c5+nSqBr5qmSDZ2dl8+OGH454XHBzM4cOHvbbehSgkJITExETKy8vR6XTExsbi7+8/KRvr2LFjhIeH09LSQkJCAm1tbSQmJk67bMubDbRNJhOpqakA45q0ms1m/P39CQ0NRa/XK1lIQojFR6tR+3w2adfKHhw/4j2K3bt3EzI6+mtZ0uv1bNq0iaqqqknfu3PnjnLjYOXKlcTGxiq9eFatWkVxcTH9/f2EhISwZ88eYmJi5nv5QogZkkCPEEKIBUErE8R9MWGz2SgsLCQ+Pt7HK517169fp7W1la1btxIcHExxcTErV66koKAAGJ+NZTAYKCkp4ZNPPiE6OpodO3bMeL9aDbS7u7t5+PAhdrudo0ePKqVdMJpF9MEHH2Cz2di/fz8JCQm0t7dz8uTJcU1aMzIyqKqqorm5mcDAQLZs2TL7H5QQYk5NnPLX2dlJWVkZvb29REdHs2vXLp+V82h9ZqSlpZGTk0NXVxdnzpyhvr5+SU149NbI84yMDHJzc7l16xb19fVcvnxZ+TkNDQ1hMpmW/E0VIZYyCfQIIYTwmNYd1AcPHlBRUTGul4u/v/+0tj1VJghAU1MTAOvXr/fuQS1A7pN2o9Go9LgYHh7GYrEAkyfhuANAszFVA22Hw8GKFSswGo0kJydTV1dHYmIi8fHx1NbWTmrQqdaktaqqipSUFDIzMzl16hQXLlyQ6WBCLHBqU/78/f359re/zeeff05paSn79+/3ydq0PjNGR7yjBKvd/78UeHPkufs1UVFRXLt2jcePH3PixAnpxSPEEiGBHiGEEB5Tu4OakJBARUUFK1euZM+ePZw4cYLLly+zc+fOaW9fLRMERk/Ym5qaiI2NXRYNfNPS0rh37x7V1dU4nU7i4uLIysqas/09rYH2xo0bsdlsPPPMM0pp19DQEP39/dTX17N+/Xrq6uoA9SatBoNB+dNoNKLT6ejv75+z4xFCzF5zc/O4KX92u52+vj42btxIREQE0dHR3L5926dT/rQ+M44cOcLIyAjBwcGsXLnSJ2ubC56OPJ/YgPnll19mcHAQQCmH9vPzY+fOnUogLy4ujtTUVOnFI8QSIYEeIYQQHlO7gxocHIzdbicxMRGLxUJERIRS8z9dWuN6m5ubsdlsbNu2zZuHs2AZjUb27t07b/vztIH2xo0b+fzzzwkJCSEpKYmysjLS0tIIDw9XtqXWpDU9PZ2BgQEaGhpobm4mNDR02fxdCrEYOZ1OLl++PG7Kn5+fH0ajka6uLhwOBz09PbhcLoaGhggMDPTJOpfbiPfpjDwf24C5qKhIedzhcLBy5Ura29s5c+aMEqRrb2/HZrNJLx4hlggJ9AghhJgWrTuonZ2dyh1fu90+7e2qZYK4/2xoaCAwMJCUlBTvHYhQeNJAe2JpV19fH62trbz22mvjAntaTVqzs7PJzs6emwMQQnhVY2PjpCl/LpeL7du3c+HCBY4cOYKfnx8Gg2HaU/68ZTmOePd05PnEBszukepuXV1d/Ou//it5eXlkZGTM2/qFEPNn6bzzCSGEmBcT76C2tLSwefNmampqaGpqwmQyzai+Xy0TZM2aNXR1ddHe3k52drbPygOWO7XSro6ODux2O++9957yvJMnT1JQUKD0EhJCLE69vb2TpvwdPXqUb3/72xQUFPDkyROqqqqIiYmZUSNgb1iuI95nO/J8qZa1CSHG07lcLo+fvGXLFteVK1fmcDlCCCEWsrF3UFtaWrh48SL5+flERkai1+vp6+ujoqKC9evX8+yzz/p6ucJL7t+/r5R0uW3YsIG0tDQAWltbqampIS8vj/T09CV1B12I5chqtTIwMAB8PeWvoKCAGzdu0NjYiMlkIjk5mW3btmEymXy82uVjeHiYf/3Xf1VGnmdnZ0/ZiLmqqoovvvhiXEZPb2+vUtaWlJS0ZMrahFgKdDpdtcvl8spYUjkTE0II4TGtrJszZ87Q1tZGQEAAaWlpM2ocrDXRy2g0qo7wFvNnqtIuGM3yknHpQiwdZrNZycwcOx3PYrGwfft2Xy1r2ZvOyPOenh6lAbM7w6m9vX3JlrUJIcaT324hhBBTetpI9aNHj5KQkMAbb7wx7ZHqY6lN9EpOTiY1NVV1hLcQQggh1LmnawEUFRWRmJiI1Wpd0mVtQoivSaBHCCHElOZ6pLqb2kSvsLAw1RHeYnHr6Ojg448/xuVy8eabb9Lb20tJSQnd3d1ER0eze/duZbqMEGKhm5jN9ysgfW52db8G3t4KLif8ZBgMcimjZaosTCHE0ifvjkIIIaY01yPVx1Kb6FVeXj5phLdY3CorK9Hr9TgcDgDOnj2LyWTi4MGDnDp1irKysnHlImJx6e7upri4mJ6eHoxGI+np6Tz33HPcvXuXiooK+vv7SUhIID8/f1ZZgGIh+QGw93dfR87dbj7/S9CbwCEZnkIIMRUJ9AghhHiqmYxU17rYq62tpb6+nqGhISIiIpRmzjB5ote1a9dUR3iLxau5uRmr1UpycjK3bt3iyZMndHV1kZubS2RkJImJiTQ2NuJ0OmXK2iLlcDhIS0sjMTGRq1evUldXR2xsLOfPn/dqFqBYSN4G3gPygX83N7to/Bh6WyGjAL56f272IYQQS4ScQQkhhHgqdwBmy5Yt3L17Vxmp3tTUxNGjR3E4HJNGqrsv9l555RVWr15NXV0dzc3NXLp0ifDwcA4ePEhHRwfV1dXAaDnPgwcP0Ov1SoNIh8OhjPAuLS0FRkd4P378eH5/AMIrnE4nly9fJicnB4PBAKBM9nFP7jGZTLhcLqWJqFh8oqKiyMrKIjw8nLi4OGB00s9cZAGKheBHwC+A3wc+Ao5N69XdLY2c/ps/5MN/k03h/3sXj5uqJz/JMQynfwR7fwoGyQITQoinkYweIYQQUxo7Ut0dgDEajcTExJCcnKyMVE9PH9+TISoqiqioKGB0alNDQwMulwuz2Yy/vz+hoaHo9XrlAl9ropf7QnHsCG8p45q+mWRYeVtjYyP+/v6kpKQoF/kBAQHA6Nhg9586nU55XCxedrudmpoaQkJCWL16NZcuXZoyC1AsVq/+7s8k4J+BGx6/cnign3P/5Y8Jjl3Fvrd+Tf/jexj8VH73q9+GFZGQ8QrcKBp9zOVALmWEEEKdvDsKIYSY0mxHqo+92EtKSqK3t5eqqiqam5sJDAxUxnInJCTw+uuvT3p9YGAgsDRHeKtNNMvOzh43LQUgODjY45G6WtTKaWJiYrh06ZIyfaWwsJDq6mpeeOGFWe1LS29vL48ePeKdd95RHjt27Bjh4eG0tLSQkJBAW1sbiYmJUra1yNntdoqKihgcHOTgwYOYzWY2b95MTU0NTU1NmEymSVmAYjFqBK4AO4Dy3z2W5vGr79ecZ6i3kx0/+Dnhq9IJX6XRxLnzOty9CG+Zvn7sp5HwY+sM171waU26LC4u5t69e7hcLqKioti9ezchISG+Xq4QYoGSQI8QQogpaQVgPAkGTLzYs1qtVFVVkZKSQmZmJqdOneLChQvLtvGu1kh598/bZrNRWFhIfHz8rPc13QyrsSZOySotLeX69evK9zds2MD27dufuoasrCzS0kYvAqurq7lz5w4HDhzAYDBQUlLCJ598QnR0NDt27Jhy/zKla2Fz/9739fWxb98+9Ho9drudpKSkKbMAxXzy1qSsQOAz4L//7utXgVc8fnV/x30Aav/lH3jyoJWwVenk/MlbmKMnvOdt/yFkvTH6dcnfwvUT8EfnZ7DehU/rcyEtLY2cnBy6uro4c+YM9fX15OXlzevanE4nx48fp6OjA4fDweHDhwkODmZwcJDy8nIlEJWdna1580cIMT8k0COEEMuU1l3DBw8eeGUyjtrFnrs8x2AwYDQa0el09Pf3e/vQFg2tkfLuTIempiYA1q9f77V9epphNdbEKVkA0dHR7N07OmXHz8/Po32bzWbl2CYG9woKCjRfJ1O6FpeOjg6lj9aJEycAyM7Opqury6MsQDFfvDEpK5nRINHM+JvDRveetpGNh39A8Vvfpe79n7H9+38//omhiaP/Abx+fMb7Wwy0PhciIiKA0WALoPz/fEtKSiIoKIjbt28rj507d46Ojg727duHwWBQeq8JIXxHAj1CCLFMqd01TEhIoKKiwiuTcbQu9rZs2UJDQwPNzc2Ehoaybds2rx7XYqM10czpdNLU1ERsbKzXeubMJMNq4pQst87OTj766CMiIiLIy8ubs4sOmdK1+MTFxfEnf/Invl7GsjVkHaLw33/Ok8f9GEwGkrbEsetPt2H0M0x45jxMynqK2Kzt6Iz+NNlDqKn4Euee7zPiuAvAqVOnlm2pktbnwpEjRxgZGSE4OJiVK1fO+7r0ej2bNm2iqqpKecxms9HW1saWLVuUnnpCCN+TQI8QQixTancNg4ODvTYZZ6qLvezs7Bmve6lRGym/YcMGmpubsdlsXguEzSTDauyUrLa2NuXx5ORk1q5dy9DQEKdOneJf//Vf8fPzU7LCLl68yM2bN7Hb7ezatWvGJTpq+59qSteKFStmtB8hlhK9Qc/WN54lMiWchs+uU1fYSMpzSaRuTxrzrB8BG4EyRhsorwZem/e1Blniyflf/o5L505h0AfitKwm48B3AZRSpUcdnfzVB1/xv39RyYgTjv1FPnHhgfO+1vmk9blw6NAhuru7OXPmDJcvX56zfmrTYbWO9klqaWnh6tWrrFixgtzcXBISEny8MiGWNwn0CCHEMqZ111Am48wPrYlmAA0NDQQGBpKSkuK1fU03w0ptSpbL5SI5ORkYnZBlsVjo6Ohg7dq1Si+JyMhIAgICqKmpmdWaZUrX4qQ14e348eN0dnbicrmUrMGZlIWKqZkCTUpQJ9gShMGkJzQueMKzZj4py9tSd3yL1B3foqqqii+++IKA0NFeYu73vuERB4kBA6wKjaCqzeazdc4Xtc8Fh8PBrVu3iIqKwmQyodPplO/5mvt32M/PT2kaff78ed544w0fr0yI5W1hvEMIIYTwiYl3DVtaWmQyzjzSmmjW1dVFe3s72dnZHpUjeTo6/dChQ6plYFoZVmpTso4ePcq6deuUi7AnT54QGRk5qZfE3bt3Z/QzGdt4ef369TKlaxFSm/CWmJhIeHg427Zto7Ozk5KSEurr65fcJL2Fov3qI4r+01kcdgcJz64kJGbs+/jsJmXNJ3ep0o64YB6GhC2LQI/a50JaWpqSkWkwGJRJib7Q09PD4OAgAH19fYSFhREWFoZOp8NgMCh/CiF8SwI9QgixTGllk8TExMhknHmiNdEsIiJiWj1OvDk6fWzQyGAw8Mwzz5CZmUlFRQUPHz4E4Pbt2zQ1NeFwOAgNDaWzs5PHjx+PywqbqbGNlzMzM1mxYgWDg4PU19cDeDSlS/iW2oS3oaEh5e/JHTx2XywK77M8E8Ghn+2nueIOVb+qpenMLTIPrP3dd2c3KWs+jS1Vum+97+vlzAutz4VXX31V5dnz78MPP1S+LioqYs2aNTz//POUlZVx7NgxwsLC2LNnjw9XKIQACfQIIcSypZVNcubMGZmMs8jMZnT6RBODRg0NDTzzzDP09fWRkpLCxo0bOXnyJGlpaeTl5TEyMoLVap3US2ImJjZeNpvNbNq0id7eXurr68nOzsZisQBTT+kSc8+T8qyYmBj6+/uVCW8wWvp38eJFDAYDGRkZPj6KpanjdheDfUOExJox+v8uiO8/NsMimdlMypoLE7NEDAYD7e3t40qV9Do94PTtQoXmTQh5TxZiYZFAjxBCLFNadw0XQnNH8bWhoSEKCwuxWq0YDIYpGx7PZHT6RGpBo5GREQYGBli9ejXR0dHExMTQ2tpKenq6alaY1WpVmjrbbDb6+vqeOi1Hq/GzWJieVp718OFDLly4gMlk4tChQxiNRlwuF+Xl5dy8eZO9e/d6bZqcGG+gd5DSf7yErXsAf7Mf6/evYc3zq329rClNzBJJTEzEarUqQR+/iHjMrmi484B73Tb8jHqigqW/kxBCaJFAjxBCiFlRC0RkZ2ePO3EHCA4O5vDhwz5a5eKl1+vZunUrkZGRNDY2ajY8nsno9KmMDRo5naN30cdOuhoYGNDMCisqKqK9vR2AqqoqGhsbVYOKY2k1fhYL01TlWXa7nZKSEuV7er0eu93OxYsXaWpqYseOHURFRTEwMEBg4NKenuQLiZvi+MN3Fld2xdNKVZ/73z4HHgDwZ0evsP/ZOP6mIHMeVrawOZ1Ojh8/TkdHBw6Hg8OHDxMcHCyNz4UQEugRQggxO1qBCPeFvc1mo7CwkPj4eB+vdHEymUykpqYCaDY8HhkZmfbo9KmoBY1g/KSrwMBAzaywAwcOTPs4tRo/f+c73+HJkyfAaLlhT08PYWFh096+mBsTs8gAHj9+TGdnJwCtra20traSnZ1NU1MTAOXlow2AV65cOaN/K2L5ufi335yT7Y5t/v7mm29y8+ZNqqursdlshISEkJeXR1xc3Jzs21uSkpIICgri9u3bymPS+FwIIYEeIYQQs6IViHA3XHVf3K1fv95na1zs2tvbOXnyJA6HQ7XhsdVqnfbodC3uIM/YoFFUVBQBAQFK35yHDx/yzDPPePUYs7KySEsbnfxTXV3NnTt3OHDgAJcvX+b69evA6Mj5hoaGaTWqFnNnYkDQXZ51+/Zt9Ho9e/fuJTk5WXm+XGiKueJJzyi1zJaxzd8BKioqCAoKoqCggBMnTlBZWcmhQ4d8dVhPpdfr2bRpE1VVVeMen6rx+cTgltPppLKykpaWFkZGRsjIyOC5556bv4MQQswJCfQIIYSYNa1AhNPppKmpidjYWOnHMQsWi4VDhw5pNjwOCwvTDH5ojU7X0tHRoRo02rt3LxcuXKCoqIiEhAS2bt06w6NRZzablYuSseVlu3fvZvfu3V7dl9vEC57e3l5KSkro7u4mOjqa3bt3ExQUNCf7nk9a5R21tbXU19czNDREREQE+fn5Hv+eqgUEpTxL+MrTekapZbZMbP4Oo5kwLpeL0NBQjEajRw3s54NWr7bi4mLu3bunBKqePHmifP5qNT6fGNyqrKzkxo0bPP/885jNZrq7u+f/AIUQXieBHiGEELOmFYhobm7GZrN5lEki1HV0dHit4bEn4uLiNINGC2W8r7dMvOA5e/YsJpOJgwcPcurUKcrKyjzuabTQTSzvGBoa4tKlS8THx5OTk0NhYSHV1dUeN2PXCghKeZbwhal6RsHkzBat5u+ZTddwvnuU1n//1yRs3cLan/3DPB+JOq0S6bS0NHJycrh8+TItLS1cu3aNuLg4zcbnasGt69ev88wzzyjZd+6foxBicZNAjxBCiFnRCkTAaKlNYGAgKSkpvlzioubNhsfiaxMveJ48eUJXVxe5ublERkaSmJhIY2MjTqcTvV7v6+XOilp5h8lkwmw24+/vT2hoKHq9flrZC1oBQSnPEr6k1jNKLbNFrfn7wBdf4P9//l/cLfg9krKzWf2Tv6H+//y/+MbfveWz43GbqldbT08POp0OgICAAGw2G1euXJmUWefv7z8puDU4OIjD4eDRo0e89957mEwmNm/e7PXSXCHE/JNAjxBCiFnRCkR0dXXR3t5Odnb2or9QnomJZUHuMpnBwUGlf44nvNnwWIxSu5s/MDAAjJ8s5nK5GBwcZMWKFT5b61zR6/VkZGRQVVVFc3MzgYGBEqQRi5pWzyi1zBa15u+X//7/YA3QtSOP1G98g4G3/o6QL7/0zcGo0CqRHjvh0l2K6e5rNjazLjU1dVJwa2y/om9+85tcvHiR8+fPs2rVqgVTtiaEmBkJ9AghhJgVrUBERETElE1ztXoOPHjwgIqKCvr7+0lISCA/P39RjoWdWBYUEhLCpk2bqKys9PHKhNrd/ICAAGD8ZDGdTqc8vtT09PRQVVVFSkoKmZmZnDp1igsXLiyZUjWxvEy3Z5Ra8/e10dE4gR67nd/+9rd8w9+PmAUU7NAqkf6DP/gDuru7OXPmDElJSZp9zSoqKlQnG8bHxzMwMIDBYECv16PT6ZblzRkhlhoJ9AghTrW4PQABAABJREFUhPAJtZ4DCQkJVFRUKNNRTpw4weXLl9m5c6evlzstan0QVq9eTW9vrwR6FgC1u/nHjh0jPDyclpYWEhISaGtrIzExcclc8PT09Cj9Sfr6+pRSD4PBoGQ+3Lt3j1/+8pfjphbdvXt3SQRel4PulkauHPk7ulsa8VsRTN5f/AOWtZt9vax5Md2eUWrN3/v+/v/gCfDayy+jj4ig/f/7vxOQtGqej0SdWom0w+Hg1q1bREVFYTKZ0Ol0yvcARr5sZ+STa8r/ZwOZa3MY3BM/brJhQEAAJSUlFBYWEhQUxDe+8Q0MBsN8H6IQwssk0COEEMJjWlk4Fy9e5ObNm9jtdnbt2kV6evpTt6XWcyA4OBi73U5iYiIWi4WIiAgl42Kx0GryKRYOrVHuBoOBkpISPvnkE6Kjo5VGrkvB2PKOoqIi1qxZw5YtW2hoaKC5uZmgoCBWr17N+vXrlalFsbGxnD9/ftEHXpeD4YF+zv2XPyY4dhX73vo1/Y/vYfBbmtloarzRMypg7zd48rOfY/vgQwwxMbgGBgjct9eby5wxtRLptLQ0JYvJYDAojdXdDOujMaSGA+Bo6mDks5usWLeSYItlUubet771rXk9HiHE3JNAjxBCCI9pTf6IjIwkICCAmpqaaW1Pq+dAZ2cndrudvr4+7Hb7XBzKnFErC3K5XD5elRhLa5Q7QEFBgS+WNOe0yiizs7MnPeaeWtTb27voA6/Lxf2a8wz1drLjBz8nfFU64au+DrZP7Bem1+txuVwcO3aMzs5O8vPzWbt2rQ9XvzD4PfssoW/9Z6z/9M+4hocx/9mfEvDyfl8vC9AukZ5qEqLOZADTaGaOo/ExBJnQZ1jmbI2zcr8G3t4KLif8ZBgMcokqxGzJb5EQQgiPTTX54+7du9Pe3sSeAy0tLWzevJmamhqampqUyUCLiVpZ0NGjR/nOd77DkydPgNG7sz09PYSFhSnP8Wa2lBAzNXZq0erVq7l06dKiDrwuF/0d9wGo/Zd/4MmDVsJWpZPzJ29hjo6f1C8MRkdq9/T0+Gi1C5f5e9/F/L3v+noZXuV81I+rtRfDzlXoDAu0FPXzvwS9CRxDvl6JEEuGBHqEEEJMi1YWznRpjWWPiYkhOTmZvr4+KioqFl1gQ6ss6PLly8oklIaGBhoaGsZlWXg7W2oxm5iB0NvbS0lJCd3d3URHR7N7926CgoJ8vcwlZ+LUIrPZvOgDr8uFvzkMgMi0jWw8/AOK3/oude//jJUH/u2kfmHDw8NUVVWRmZnJlwtoqpSYG44r90Cvw7g5ztdLUdf4MfS2QkYBfPW+r1cjxJIhgR4hhBDTojX5Y7q0xrKfOXOGtrY2AgICSEtLIysraw6OYu5olQVpTUJx83a21ELS3d1NcXExPT09SqPf9evX8+tf/3rc84KDgzl8+PCkDISzZ89iMpk4ePAgp06doqysTKZDeZnW1KKkpKRFHXhdLmKztqM3mNAbTRj8/EGnQ2/0U+0XVltbS2RkJAkJCRLoWeJcQyM46h6iT49EF7IAm6g7huH0j2DvT+HGSV+vRoglRQI9QgghPKaVhWO1Wunv7wfAZrPR19dHSEiI6gX+c889x6lTp7h37x4ul0vJ0AgJCQHghRde8Nnx+Zq3sqUWGofDQVpaGomJiUqj38TERKXnhM1mo7CwkPj4+EkTy548eUJXVxe5ublERkaSmJhIY2MjTqdzyUzEWgi0phZ1dXUt6sDrchFkiSf33/5X6j/4b9z47FfEZm4jIHs//g86xvULGxwcpL6+noMHDypT2FwuFy6XS5nEJpYOR91DsDswbImf9L2JmZM3b96kuroam81GSEgIeXl5xMXNcRZQ9duwIhIyXoEbRaOPuRzIJaoQsye/RUIIITymlYVTVFREe3s7AFVVVTQ2NvL6669rXuCnpaWRk5NDV1cXZ86cob6+nry8PB8fne95K1tqoYmKiiIqKgr4utHv0NCQkvnkHoGckZFBcXHxuAyEgYEBYDTjyf2ny+VicHCQFStWTLlfKQHznNbUIrF4JOe9THLey8r/V1RUTOoX9v777zMyMsJHH32kPFZWVoa/v7+SUSgWB62+bsXFxcqNlKioKHb/xW4CfncjZayJmZMVFRUEBQVRUFDAiRMnqKys5NChQ3N7EJ3X4e5FeMv09WM/jYQfW+d2v0IsAxLoEUII4TGtyR8HDhxQfb7WBb77gsLpdAIQERExRytePKabLbUYjW30m5SUBIz+G2hqaiI2NpZHjx5NmlgWEDA6Inp4eJihoSEaGxuB0XHhSUlJZGdnjxsdDk8vAdu5cyfFxcX86le/kgCQWLLU+oW99NJLyvvL48ePKS8vZ9OmTcTHT874EAubVl83T26kTMycBAgPD8flchEaGorRaFSC63Nq+w8h643Rr0v+Fq6fgD86P/f7FWIZkECPEEKIOad2gX/kyBFGRkYIDg5m5cqVPl6hb4zNOHnxxRcpLi5WphpZLJYps6UWm4mNft0Xm83NzdhsNrZt28bDhw8nZSAcO3aM8PBwWlpaiI2NRa/Xs3LlSiwWi3JhM90SsMbGRnQ6HS6XC6fTueh7AGmVSN69e5eKigr6+/tJSEggPz8ff/8F2KdDzAmtfmFuFouFdevWzfeyhJdM1dcNtG+kOJ1O1d5NGRkZlJSUcOTIEYxGI/v27Zv7gwhNHP0P4PXj4751p+8O/8eV/517/ffxN/izN2kv393w/5r7NQmxREigRwghxJzSusA/dOgQ3d3dnDlzhsuXLy/L3jxjM04SEhLYuXMnNpuNyspKEhMT0ev1mtlSi4lWo18/Pz8aGhoIDAwkJSWFmJgY1YllBoOBkpISTp48SXR0NLt27aKlpUW5sJlOCVhvby9Wq5XQ0FB6enro6OhY9D2A1EokY2NjOX/+PCtXrmTPnj2cOHGCy5cvs3PnTl8vVwiPOZ1Ojh8/TkdHBw6Hg8OHDwNoNnJfbrT6uk11I6WxsXFS5uTIyAilpaXExMSQm5tLaWkp58+f59VXX533Y3IbdtrZk/QNsqM3U9R8nI9vHiM7ZgsbLRt9tiYhFhMJ9AghhJgzahf4T5484dGjR0RFRWEymdDpdErwZzlRS51fvXo1vb29VFZW+nh13qXV6Dc1NZX29nays7PR6/VTZiAUFBQAoxc2H3zwwaQLG09LwFpaWsjLy+PKlSvK62BmPYAWCrUSyd7eXux2O4mJiVgsFiIiIpSfhZtkAi1eagGQ4OBgjh8/TmdnJy6XSwnyLfa/u6SkJIKCgrh9+zYAQUFBqll8y5FWX7epbqT09vZOypx899130el06PV6DAYDOp0Oq9W3fXJWhz3D6rBnANgY9SyfNp/Ean/i0zUJsZgsvzNrIYQQ80btAn/dunW0t7fT19eHwWAgPj6enJyceV/bdEd+e5NW6vxSNVWj3+k2ANa6sPGkBCwwMJCRkRFCQkKUvkfuYM7w8LDyp06nUwJDi4ndbqeqqgq9Xq8EshoaGnjmmWfo7u7GbrfzP/7H/1CCN5IJtLhNDIDAaJ+Vbdu20dnZSUlJCfX19WzZssWHq5wdvV7Ppk2bqKqqGvfYxCy+9evX+2R9vqTW183hcHDr1q0pb6So9W4qKCigs7OTmpoaCgsLMZvNC+Z3vn+4n/ev/ZqVQXFsiVm8/5aFmG8S6BFCCDErEycblZaWcv36deX7GzZsYPv27dPertpEkaka707XdEZ+e5ta6rzL5fL6fpYarYbVgEclYPX19dy8eZOPP/5Y2ebYHkAJCQm0tbUpZXOLiTt7bmhoSLmQO3/+PI8fP+bdd99VnudwOGhubqa3t5ff//3fZ3BwkNOnT/Pkyeid8s7OzqdmAgnfUwuAAOzYsQNACYS4R6gvRWOz+CIjI329nHmnNgUzLS1NyaLVupGilTlpsVhYu3btvB7D0/QP9/O/VfxH+ux9/NedP8XfuPgC8EL4igR6hBBCzMrEyUYA0dHR7N27FwA/P78ZbVdrooi3AjGejvyeizvFaqnzR48e5Tvf+Y5ywT04OEhPTw9hYWFe3/9ipXZhs2bNGrq6ujwqAcvJySEzMxNQ7wH0ySefEB0drVwsLxYTSyRDQkLw8/MjJSWFx48fk5aWxo0bNwgPD2fv3r2cOHGCrq4uWlpaOH/+PDExMTidTvr6+ujt7QW+Dvj09fUpDcLF4uByubh48SIGg4GMjAxfL2fOjM3iW460pmD6sq+ON9mGbfzNhf9Ie/99/tecH2PSm7AN21hhWhwltUL4mgR6hBBCzJhanxkYvUj86KOPiIiIIC8vb0bj07Uming7EPO0kd+e3inW6ndSW1tLfX09Q0NDREREkJ+fr5o6f+DAAS5fvqxkQzU0NNDQ0DDt0qalTOvCJiIiwqOfkyc9gBYjtRLJjRs30tDQAMDdu3cBiImJISgoSAnKuvv4dHd343Q6iYyMpL29nc2bN1NTU0NTUxMmk0n5mYmFz+VyUV5ezs2bN9m7d++SyHTp6elRMpPcmSorVqwYl8Unlp5bPTe50TP6efjjC/8rAN9Jf53XM/7Qo9e7P3tdLhfp6els3boVnU43Z+sVYqGRQI8QQogZ0eozk5yczNq1a3E6nZw5c4aSkpIZX0RrTRTxVsq+JyO/PaVWChYTE8OlS5eU9PnCwkKqq6t54YUXVAMOu3fvZvfu3TM+HrE8TeyB5P53bTKZOHToECEhIVRXV1NTU8O1a9eA0Yy5pKQkLl26hM1mY9++fZSUlGC320lKSiI5OZm+vj4qKipIT0/31aGJKagFQK5cuUJTUxM7duwgKiqKgYEBAgMDfbzS2RlbrltUVMSaNWvIysoal8Unlp5MSxaf/F7RjF774MEDLl26xNatWzGbzZw7d46IiAjCwsLGlZo7nU4qKytpaWlhZGSEjIwMnnvuOS8fiRC+IYEeIYQQM6LVZyY5OVl5TlxcHK2trTPehyeNd2fK05HfnlIrBXO5XJjNZvz9/QkNDUWv1yvTnYSYC1r/rpOSkoiPj+f8+fM8efKEzMxMZeS80+nk888/B8Df358vvviCtrY2AgICSEtLIysry5eHJDSoBUDcGYHl5eUArFy5kgMHDvhkfd7irUbui5Vav7r8/HyKi4u5d+8eLpeLqKgodu/eTUhIiK+XuyA8fPgQGL3xFBISwrlz57hz5w6NjY3jSs0rKyu5ceMGzz//PGazme7ubl8uWwivkkCPEEKIGdHqM7Nu3TolQNLe3j6jsi3wvPHuTHk68hu0T7Q//fTTSaOMdTrduFKw3t5eqqqqaG5uJjAwcFFPwBELn9a/687Ozv8/e38eFOed5/m+79xYxCog2UEICSSEVrRvaLHkcsklqWiV3WW340Z3l09HTJzpme6ZO9O3b/X0TN86cTp6Ynp6TsTM7b5V1eryva4q21WWVZaQZRkhsWkBCVtgNkkIEJLQwpqCBJJc7h84H0PyPKxJZgLfV4RDOMnllyiV5PN9vr/PVym6rlq1ipycHEJCQigoKECv1yvdO7m5uWzevNlfyxczoFbokI7AxUcrry4rK4sdO3bQ3d1NUVERtbW17N2719/LDQhhYWHA6DZy90RF9+TBsVvN7969y+rVq5UTVO6TNUIsBlLoEUIIMW1Op5Nz587R2dmJw+Hg1VdfJSwsjIqKCp4/f47L5eLBgwc0NjbicDgwm82zHtE63eDd2ZrJyG+tD9qeo4y/+uornjx5omwF6+/vp6qqipUrV7JhwwYuXbpERUXFhHwYIbxF63X95MkTpdDT3NxMc3MzeXl5dHd3S/fOIuI5BbGrq4uysjL6+vqIj4/nwIEDkrm0wGjl1blPojidToBZn1RZjDIzM7l37x7FxcUYjUb0ej0vX75k//79ylbzoaEhHA4Hz58/5/3338dkMrF161ZWr17t59UL4R1S6BFCCDEj6enphIWF8eDBA2JjYwkJCaG3t5cVK1aQl5fH+fPnWb169awLPG5zDd71Jq0P2mMvA2hqasLlcilbZtxnEg0GA0ajEZ1Ox8DAgE/XLgRMXtgUi4fnFMTLly8THBzMyZMn+fzzzyktLeXYsWN+XqWYKa28utOnT2O324mIiCApKcnPqwwseXl5bN26lc7OTsrLywkLCxu31Tw4OFi57ne+8x1u3LjB1atXWbFihWyxFouCFHqEEEJMm16vZ8uWLVRVVSmX9fb2YrPZSEtLw2w2ExMTo3yQWky0Pmi7Rxnr9XolGHXslplt27ZRX19PS0sLUVFRs8oV8uykeuutt4iIiODRo0dcu3aNgYEBUlNTyc/PH/fhVSwsWpPbAEZGRvjwww+xWq0cO3aM1NRUP69WBBrPKYg2mw2LxcKmTZuIiYkhPj6eBw8e4HQ6JcB4gdHKqzt16hQ9PT0UFRVRWVnJq6++6u+lBgSHw0FxcTEDAwNERkaSmprKo0ePJmw1T0lJYXBwEIPBgF6vR6fTyb8NsWhIoUcIIcScuCe6dHV1KQcWNpvNz6vSPmh++vQpZWVl44ojQUFBU96f2gft3NzccaOMxwZRj5WXlzfn5zO2kwpGD/yLioqUbKDz589TWVk5504q4T9qk9vS0tJISUnhzp07DA8P+3uJIkCpTUEMCgrCaDTS3d2Nw+Ggt7cXl8vF8PDwgp/EtZSo5dU5HA6am5uJi4vDZDKh0+mU74nRLty33npL+f/+/n4lfP727ds8fPiQ48ePExISQklJCWfPniUsLIxXXnkFg8Hgr2UL4VXyjiCEEGJOwsPD2bp1K9XV1TQ2NmIymQIiA0LtoDk5OZmSkhISExM5cOAAFy5coKqqasoAS61g6LKyMp+MMl7KnVQz5ZlR0tfXR0lJCT09PcTHx3Pw4EElqDPQqE1uGx4eZmBggNraWnJzc6mpqfHzKkUg0pqCuGfPHioqKjh9+jRBQUEYDAZCQkL8vFoxE2p5dVlZWcp0PYPBQEpKCjt27PD3Uufkzp071NbW4nK5WLNmDdu3b0en03nlvsPDw5XPJZ4Zed/73ve88hhCBBop9AghhJiR3t5eZYuS+0Nmeno6GRkZyuSeNWvW+HmV6gfNdrudwcFBVq1aRXx8PAkJCbS1tU1Z6NEKhi4tLQX8M8o4UDup/E0to8RkMnHixAkuXbpEWVlZwIdh22y2cZPbysrKyMrKYvny5f5emggwYwubwLitKf/8z/9MXl4eBQUFvHz5kqqqKhISErx28Cx8Qyuv7o033vDDaubH06dPuXnzJtu3byc8PJwrV64QExMjwchCzIEUeoQQQszIRx99pHxdWFhIdnY2NpstYCf3jD1odk8ncQctmkwmpZ17MloftP0ZbhuonVT+5JlR8vLlS7q7u1m/fj1nzpzB5XLx8uVLqqur+frrrxkaGlJylAKFzWajsLBQmdxmsVhoa2vjzTfflI4tMcHYwubJkyfR6/WUlJTQ3d0NwLNnz7hz5w4mk4mMjAwl80mIQPLs2TMAMjIyiIyM5MqVKzx8+FAKPULMgRR6hBBCzMhCmtzjedDc398PoEzDGhkZ8VtWxfDwMGfPnqW/v1/pisrPz6euro7a2lqGh4eJiYkhPz+f2NjYBdNJ5S9qGSXuIt7Dhw/HdfmEhISwZcsWrl+/7rf1qnG/Xi0WizK5rbOzE5vNxvvvv69c78KFCxQUFGA2m/24WuFvnoVNs9mMw+FgaGiIzZs389VXX7Fq1Spef/11fy9ViEm5t9N2dXUpv58lk0yIuZFCjxBCiEVJ7aA5Li6OkJAQmpubCQ8P59mzZ347Y6jX69m+fTuxsbE0NDRQU1NDRkYGN2/eVPIWzp49y+3bt3n11VcXXCeVr6lllLizSIaHh5WDYYC1a9fy8uXLgCv0dHZ28uLFC+DbyW3r16+noKAAgLa2Nqqrq9m7d69s41ri1AqbMJpzEhsbS2pqKl999ZX/Fij8RuskQnFxMY8fP8blchEXF8fBgweJjIz093IByMzM5N69exQXF2M0GjEYDEu+Q1WIuZJCjxBCiEVJ7aA5Ly+PI0eOUFFRQWFhIampqWzfvt0v6zOZTGRmZgKj27AMBgPR0dGEh4cTHBxMVFQUer1e2Wa2kDqp/KGvr4/nz5+Pyyj5+OOP0ev1hISEYLfbAUhLSwvY8bnJycmT/j2bzeaA2mYm/EetsDk0NERtbS0nTpxQuv9cLhcul0tyeZYQrZMIWVlZ7Nixg+7uboqKiqitrZ0yn86X8vLy2Lp1K52dnVRUVJCdne3vJQmxoEmhRwghxKI02UFzoIRYdnR0cOHCBRwOB6mpqURERJCTk0NVVRUtLS2EhobKgf00bdy4kaysLODb8bnr16+nvb0dvV6vHAzv2bPHn8sUwivUCpsffPABdrudjz/+WLmsrKyM4OBgpagsFj+tkwgxMTE4nU6KiooAqKurY+PGjURERFBeXs79+/ex2WwcOHDA59uAHQ4HxcXFDAwMEBkZyaFDh0hISPD648znZC8hAo0UeoQQQswrtTbyvLy8cVuRACIiInjrrbf8tEr/MJvNnDp1igcPHnDr1i2ampqoqqpi5cqVbNiwgUuXLlFRURHwU6ICgdr43GvXrimhtG4ff/wxP/zhD3n58iUw2gXR29tLdHT0vK9Ra0vFjRs3/HqQJRYetcLmd7/7XYzG0Y/2L168oLy8nC1btpCSkuLPpS5JY6ehvfvuu9y/f5/bt28rOXEmk4l169bNW6FB7SQCwC9+8QvsdjtGo1HpcgSIjY0lJCSE6upqr69lOkwm07z//pfJXmKpkUKPEEKIOdM6gH369CkVFRX09/eTlJREVFQUdXV1ZGRkKFOsrFYrZ8+eXXIHI52dnQwNDREZGakcnBkMBuVPo9GITqdjYGDAn8tc0NQOho8fP05lZSV3794FoL6+nvr6ep9sjdPaUuHvgyyx8KgVNscym82sW7fO18sS3xg7DQ1Gi87BwcG4XC6MRiNBQUF89dVXSqHB250maicR1q9fz6lTp+jp6eGLL74Yd/2cnBwePXo0p+cc6GSyl1hqpNAjhBBiztQOYFNTU7l27RpJSUkcPnyY8+fPMzIyMi6LBqCxsRGA3Nxcfz4FnxsaGqK0tBSr1UpwcDDr1q1jzZo1DA4OUl9fT0tLC1FRUezevdvfS12wtA6GDx48yMGDB32+nsm2VMz1IKunp4fi4mJ6e3sxGo2sWbNGGaU9MjLChx9+iNVq5dixY6Smps75uQgh1HlOQwNYvny5UrQPCgoiLCyM/v5+Hj58SHh4uFc7TdROIjgcDpqbm4mLi1Ny3wKdt4tfMtlLLDVS6BFCCDFnagewERER2Gw20tLSsNvtjIyM8OzZs3Ft5E6nk8bGRhITE4mNjfXJWrW6jz777DO6urpwuVwkJSVx6NAhgoODgYlt+H19fZSUlNDT00N8fDwHDx5UPkROV2pqqtLVNFZeXh55eXleea4i8GhtqZgrh8NBVlYWaWlp1NXVUVNTQ1paGikpKdy5c0cOaITwAa1paDk5OVy9ehUY/R20Y8cOnj17xvDwsNc7TdROImRlZSlTKN2/ny0Wy5yf73yZzTYrp9PJuXPn6OzsxOFw8NZbbxEREcG5c+fo6OhQrieTvcRSIYUeIYQQXqF1ANvV1UVGRgbBwcHYbDYePXqktJG3tLRgtVp92rWitX1m+fLl7N69m66uLkpKSqitrVWCkD3b8C9fvozJZOLEiRNcunSJsrIyydER06K1pWKu4uLiiIuLA0aDyOvr6xkeHmZgYIDa2lpyc3OpqamZ8+MIMVd2h5N/9S9VNHVYsNmdnPmzfJKXh/p7WV6hNg3NbrdTWlpKQkICLpeL58+fc/XqVaXQ4O1OE62TCO4hBL29vdTW1mKxWJTCj9PpVDqOrFYrFovFr6PXZ1v8Sk9PJywsjAcPHoy7PDMzk507d9LV1UVQUBC9vb0y2UsselLoEUII4RWeB7Ctra1s3bqV27dv09DQgMlkIjg4mKGhIaWdvL6+ntDQUFauXOmzdWptnxl7GaCMJ/Zsw3/58iXd3d3s3LmT2NhY0tLSaGhowOl0BuzYbhEY1LZUGI1G+vv7vXaQZbPZqK6uJjIykvT0dMrKysjKymL58uVeex5CzNXebDPxkcFcrnvm76V4ldo0tF/84hfodDr0ej3Z2dkMDQ0xMDCA0+kkOzsbs9nMvXv3fNZpMnYQQmFhIdnZ2bx8+VLpeqmqqqKhoUG1WOQrsyl+6fV6tmzZQlVV1YTvPXz4kMePH2O323E6nURFRc3bZC8hAoUUeoQQQsyZ1gFsQkICoaGh3Lp1SymcrFu3juzsbLq7u+no6CAvL8/nBRKt7iOXy8WNGzcwGAzk5OSotuEPDg4C8PXXX3Pr1i3ldkNDQ+j1esrLy3n8+DEul4u8vDw2btzo0+cmApfalors7GwKCwu9cpBls9koLCxkaGiIEydOYLFYaGtr480331S6C4TwN6NBzx/mZ/JPl+/5eylepxYAX1BQQFdXF7dv36akpASAZcuWsWvXLhISEnA6neTl5bF161Y6OzvnvdPEF8Hzc5WZmem14ldOTg47d+7EarVy+fJlMjIyOHr0qJdXLETgkUKPEEKIOdM6gC0qKqK9vZ2QkBA2b97Mtm3blKJOTEyM3z5wqm2fyc3Npby8nPv373PkyBFiY2Opq6ub0IYfEhICjI6j3blzJ0VFRfT09NDd3U1tbS2dnZ0cPXoUg8GgFIWEAO0tFcePH5/zfbuLPBaLhaNHj6LX6+ns7MRms/H+++8r17tw4QIFBQWYzeY5P6ZYGhbzVitv0wqAN5vNrF27VvU2DoeD4uJiBgYGiIyMlE6Tb3ir+DV2u1dMTAzd3d3eWqIQAU0KPUIIIeZM6wD21Vdf9cNqJqfVfVRWVkZjYyP79u0jLi6OwcFB1Tb8M2fOsHz5coaHh3E6nUoxZ3BwkPb2drZt20ZycrJfnptYujo7O3nx4gUA58+fB2D9+vUUFBQA0NbWRnV1NXv37pVtXGLG1LZaTRZ+29XVBX1PWXa/FHo7CAqLZO+f/XfMa7f68VkEJpPJxFtvveXvZQSUyYpfk03j6u3tVbqH3flDt2/fJicnh6GhIbq7u0lPT5/1uuxfdWD/tGncZYbNiZhOqBfxhPAnKfQIIYRYUrS6j0pLSwEoLy8HUCZvebbhHz9+HIPBQElJCb/73e8AxgVqtra2UldXx7Jly9i5c6eMsl7CtCa83bhxg/v372Oz2Thw4ABr1qyZ82MlJydP2iFnNpuVcHEhZmKyrVZq4bfLly9n+5ZNlP31mwwZw8h86z+xIj4aQ1CIcp3WF/30WUezVx73WAky6omLCJ7/JyMWBK3i11TTuNTyhwYGBjh37hww+j65Z8+eWa/LkBuPIXO0UO5o7MR+8T76TCmci8AkhR4hhBBLilb3kdZBslobPsDrr79OYWEhAwMDfO9738PlcgEQFBREfn4+xcXFXL16lXfeeWfCfWoVAJ4+fcq1a9cYGBggNTWV/Px8ZcS7WHi0JrzFxsYSEhJCdXW1v5coxKxphd/u27ePtopC7AN92PJexRUZT+q2feOu88P/WaF8/afv3eLY5mT+umCDT9YtFq6ppnHN93ZwnckAJgMAjoYXEGZCnyPbYEVgkkKPEEKISfX09FBcXExvby9Go5E1a9awa9cunj59SllZ2biiRFBQkL+X6xNqeShBQUFER0ej0+kwGAzKn2rUCgCpqalcu3ZN6SQ6f/48lZWV7N+/38fPbmpqr4nc3Fx+/etfj7teRETEkt6SoDXhLSYmhkePHvl5dULMn4EXTwAIflDB07tfUHw1hx1/8hPC41MAuPE33/Hn8gKa1pY4gJGRET788EOsVivHjh1bch2j3h5FP1vO5wO42vow7F+BziDTNkVgkkKPEEKISTkcDrKyskhLS6Ouro6amhqSk5MpKSkhMTGRAwcOcOHCBaqqqti7d6+/l+sTankoeXl5HD58mLKyMs6cOUN0dDSHDh1Svb1aASAiIgKbzUZaWhpms5mYmJiAnZSk9ppIS0tTOqWsVitnz54lJSXFzyv1P60Jb0IsJDPZauVyuXj4dPT9MXXDLtYd/j7FP/kjaj74B/b8m//mszUvZGpb4mA0n8YfhY1AoTWNa7LcnvnguPUY9DqMWyWPTwQuKfQIIYSYVFxcHHFxccDo/vb6+nrsdjuDg4OsWrWK+Ph4EhISaGtrW7CFnpl2qEyWh+IOv52KVgGgq6sLm82GxWLBZrPN7YnNE7XXxPDwsLLNrbGxEYDc3Fy/rTFQqE14W79+vb+XJcSMaG21Ugu/vXXrFu3DQSzTB/H/seTx6DfdjGT/Pf9Jd9Nfy19QtLbEDQwMUFtbS25uLjU1NX5anf95TuOKjY2lvLxcM7fH21zDdhw1z9CviUUXKVurReCSQo8QQohpsdlsVFdXExkZidPpBEY7U9x/LuRR4v7oUPEsALS2trJ161aqq6tpbGzEZDIphZNANfY14Z5k4nQ6aWxsJDExkdjYWD+v0L+0Jrz19/czMDAAjL62LBYLkZGR/lyqEJPS2mqlFn579+5dCI1kcO0REp7ewGg0cy9iE+uO/5GvlrsoVVZWkpWVtaSn5qlN43K/l2rl9nh9DTXPwObAsE06VkVgk0KPEEKIKbkzaYaGhjhx4gT9/f0Ayh75kZERQkND/blEzYDjzz77jK6uLlwul5J/4xlw7OsOFa0CQEJCAhkZGVgsFq5du+aVaUzzxfM14X4eLS0tWK1Wdu/e7ecV+p/WhLfCwkI6OjoAqKqqoqGhQTUgXIhAp9bZePDgwXH//0+X73Gv9AHBUXE+WtXi093dTVtbG2+++WbAbun1BbVpXPfv3wd8l9tj3J6CcbsUeUTgk0KPEEKISakFD8fFxRESEkJzczPh4eE8e/Zs3s6eTZfWhKPly5eze/duurq6KCkpoba2VnPMtK86VLQKAEVFRbS3txMSEkJWVhYbN270yuPNllbxzGq18sknnzA8PIzJZOLu3bts3LiRoKAg6uvrCQ0NZeXKlX5deyDQmvB2/PhxP6xGCLFQeG6JGxwcxGaz8f777yvXuXDhAgUFBZjNS3vqk1ZujxBLnRR6hBBCTEorePjIkSNUVFRQWFhIamoq27dvB7SndA0NDVFeXs7jx49xuVzk5eV5tZChNeFo7GWA8uHZky87VLQKAK+++qrXHsMbtIpnYwNBR0ZGlDHhmZmZdHR0kJeXh14vk0i0aBXQbty4wf3797HZbBw4cCCgO7qEEPPHc0vcihUrlPy3trY2qqur2bt374LfxqU1YezcuXNK16Ob+3ueEz/37ds3IbcnOzvbT89IiMAhhR4hhBCTmix4+I033phwmVbeTU1NDZ2dnRw4cICKigoqKyuprq6e0RarqWgFHLtcLm7cuIHBYCAnJ2fC7dS6lmw225LuUPEsRkRGRmIwGDAYDLx48UL52rNgp/Va8YXOzk4++eQTXC4X7777rjKJZWhoiLy8vAmdXFpFSZjfMcZaBbTY2FhCQkKUwpkQC91MpnWJb032Pmo2mzW7UhcirQljK1euJDIykp6eHmW7mt1u54svvpgw8fPRo0fjcnsSEhL88VSECChS6BFCiEVEq1PAbrfPazfNWGp5N1arlfb2drZt20ZKSgq7d++e0xYrLWoTjnJzcykvL+f+/fvs2rWLq1evTjiwf/LkiWrX0lLuUHEXI5xOJ1euXKGzs5OYmBilCBEeHs7Q0BB1dXXExMR4tRAyW9evX0ev1+NwOACIjIxky5YtXL9+XfX6WkXJlJSUeR1jrNV9FhMTw6NHj+blMYWYnOd77S+BuXeUaU3rWsweWh7y97f+K48HnhBsCOZI+hH+aP2P/L2sgKQ1YQygvb0do9FIUFCQctnz588nTPxsb2+XjDMhVEihRwghFhGtToGmpiY6Ozs5evQoBoPBJxOyxubdhIWFAdDa2kpdXR3Lli1j586ds9pipUUr4LisrIzGxkb27dtHeHg4GRkZZGZmjjuwb2hoIDQ0lFdeeUX5+WRkZADjz6x6doxcv36de/fu4XA4WL58Ofv27SM+Pn6uP7qA4C5G2O12Nm3axJ07d+ju7la+HxYWxiuvvEJxcTFXr17lnXfe8eNqR7fY9ff3k5GRQXNzMwCrVq2ir69Ps9CjFcLtizHGWt1nQvjPnwNHvvnaO3lkWtO6FrMRp41D6a+QF7+VwpZzfHL/DHkJ29hk3uTvpS0YOTk57Ny5E6vVyhdffKFcbrVagdHfT3fu3OHJkyc4HA4qKyvZvn07Op3OX0sWIuBIoUcIIRYRtU6B0NBQpZsmOTnZJ+vwzLtxuVwABAUFkZ+fz6VLl7hw4QLAjLZYTUYr4Li0tBSA8vJyAJKSkti6datqt9FUPx/PjpG0tDRycnIYGhris88+o7q6mtdee21G6w5kjY2NlJeX43Q6iYqKoq+vDwCDwcDTp0+5fPkyDocDg8Hgk/VobbX68ssvuXXrFjD62pspzxDusrKyeR9jrNZ9tn79+nl7PDH/PAvBXV1dlJWV0dfXR3x8PAcOHAjwkNifAe8D+cB/8PNaFq5V0atZFT06nGBT3GY+a7lAv+3lrO5Lq0u3uLhY6dCNi4vj4MGDREZGevNp+NXY4Q6hoaFKgWfZsmUAvHjxglu3bhEVFcXQ0BBfffUVMTExfh8KIUQgkUKPEEIsMp6dAu4ii7ubJigoSDlgf/fddyktLeXu3bvK7devX8+ePXtm/fhqeTdBQUFER0ej0+kwGAwYjUbCwsLIyclR3WJ15MiRGU+40go4Vss6mE63kedWJLWOEfdkLqvVik6nIyQkhJ/97GeLpuMnNDSUkJAQBgcH6evrIzExkadPnxITE4PdbqenpwedTuezKVJqW60SEhKoqqrCZDJx7Ngxfve73wEor/upeBYlLRbLvI8x1uo+6+/vZ2BgABh9TVkslkV18LbYeRaCL1++THBwMCdPnuTzzz+ntLSUY8eO+XmVWv4C2ASUAf8IrALe9OuKFrqBkQE+aPo1SWHJbEuYXaaOVpduVlYWO3bsoLu7m6KiImpra9m7d6+Xn4FveE4YMxgM3L59m5ycHDo7O5UOZIvFQnR0tDLxE0bfJ1evXk1DQwMPHz6UQo8QY0ihRwghFhnPTgF3McLdTeM+EB4rPj6evLw8Ll68yNdff82uXbtmXaTQmtJ1+PBhysrKOHPmDGFhYezevZv+/n5g4haruLg4BgcHCQ0N9daPRaHVbQQwODjI4OAgV65cIS0tbVwBLCgoiP3799Pe3j7u/n7zm9/Q09NDSEgIXV1di6bjp7OzE4PBwIkTJ2htbeXGjRtkZWUxNDREUFAQBw8epKioCLvdTmJiok/WpLbVyuVyYTKZGBkZGffafu+99/jhD3/Iy5ejZ9KHhobo7e0lOjpauY5aUbKzs3PexxhrdZ8VFhYqk2aqqqpoaGiQ7IkFwrMQbLPZsFgsbNq0iZiYGOLj43nw4AFOpzNA877cwfrpjBZ67vlxLQvfwMgA//naX2GxWfjb/X9HsDFkVvczWZ4XjE6tApT/X4g8J4xlZ2czMDDAuXPnGBkZmfC9I0eOcOXKFQCio6NZuXIlDQ0Nk2aqeXbbOZ1Orl+/TmtrK3a7nZycHCWIX4jFQgo9QgixiKh1Crg/GOp0Op4+fYrL5cJoNGK325XbdXV18fnnn4+7r9kWKSab0lVQUMCjR48oLS2luLh4yi1W3u4UmazbyGKxoNfrcTqdylak+Ph4jhw5wt27d2ltbWXlypVKl4e7QPTaa69hsVi4dOkSPT09rFy5UrPjZyF9GFcrRqxZswaz2awU7KKjozl06JDP1+a51SonJ0fJ0nH/HR4/fpzKykqlWFdfX099ff2EzCXPouT69evnfYyxVveZrzqjhHc5nU4qKyvZsWOHUggOCgrCaDTS3d2Nw+Ggt7cXl8vF8PDwvBSw3etQG1XtOY46Pz9/XMAtNAC3gH1A+TeXZc3LGpcC64iVv674KzoGnvCXO36MSW/COmJlmWnZrO5PK8/r9OnT2O12IiIiSEpK8uZT8KnZTGp86623+Pzzz2lvb+fSpUsYDIZJt0V6dtu5T2QdPnyY8PBwenp6Zr1+IQKVFHqEEGIRmezgvLS0lGvXrhEeHs7y5cuVA5KMjAwiIyOpq6tjeHhYOUM4X0WKmWyx8jatbqM1a9ZQVVWlFG8OHjzI3bt36erq4uOPP8ZgMGC1Wvn5z3+u3Nd7773Hnj17SExMxGAwYLfblRHkY43t+FmxYsW8P0dv0fp7iouLUwoh/uDZkdXf309NTQ0rV65kw4YNXLp0ifj4eMxmMwcPHuTgwYOa9zVZURIW3xhjMT8aGhoIDg6eUAjes2cPFRUVnD59mqCgIAwGAyEhs+vsmC7PUdVa46jHb/MJBS4C//TN128Avzev61zMmnvvc693tMD844q/BOCHa97m7Zw/mNX9aeV5nTp1ip6eHoqKiqisrOTVV1/12nNYCPLy8ti6dSudnZ1UVFSQnZ2tej21bdd3795l9erVytAFd6eoEIuJFHqEEGIRmezgfM2aNej1ek6cOEFJSQkwejCSnp7OzZs32bNnDzdv3lS2U8H4IkVSUhIffvjhtEa3r1mzRjVA0mg0MjIywocffojVauXYsWM+HcutdmDvdDr5zW9+w6FDh2hvb+fu3bskJiZis9lYu3YtTqeTL774QuleuX37Ng8fPuT48eOUlpZSUVGhdEm98sor1NbWKj9b+Lbj5/Lly5SXl3Pq1CmfPd/FRq0jy93a785+0ul0Ss6NEL7Q19fH8+fPJxSCT548SUFBAS9fvqSqqoqEhIR5nQqkNqpabRx1W1ubR6Eng9Fx6sIbNpg38un3C71yX2pdug6Hg+bmZuLi4jCZTOh0OuV7S4XD4aC4uJiBgQEiIyM5dOgQCQkJE66n1m03NDSEw+Hg+fPnvP/++5hMJrZu3Sr5PmLRWVrvCkIIsYRpHYzEx8ej1+sJCwtTQg/VihQ3b96c9uh2rQDJzMxM7ty5M+leel/TOhvvPtMHkJKSQltbG2azedz2NXfR5tq1a3z99decOXNG+d7Yjp+l+mHc27Q6srZt20Z9fT0tLS1ERUWxe/dufy5TLDEbN24kK2t0q9PYQnBTUxMNDQ2YTCYyMjL8kgEydhy1+0/3+7wIfGpdullZWUrB22AwkJKSwo4dO/y9VJ8ymUy89dZbU15P7fd7cHCw8v3vfOc73Lhxg6tXr7JixQrl34kQi4F84hRCiCVC62DkypUr9Pb28umnnyrXVStSzHR0u1qA5MDAALW1teTm5iqZKv6mVQBbt24dK1euBEYzEibbuqb1s3V3/Oj1euLi4hbEVBStcb5qnVsbN2706dom22qVl5fn07UI4RYeHq7kg4wtBJvN5jlNMPQG9zhqd+fbyMjIvGUECe/T6tJ94403VK4tPGn9fk9JSWFwcBCDwYBer0en0wVoSLoQsyeFHiGEWCK0DkaOHTumnOGdqkgx1ej2saPJ1QIky8vLycrK8mqw7VxpFWnq6uq4ePEidrsds9nM/v37Ne9D62e7ELdpaXVjqXVuCSECi+eo6rHjqMPDw3n27JlsURFLhtbv95CQEEpKSjh79ixhYWG88sorE/L1hFjodGPHyk5l27Ztrlu3bs3jcoQQQgQyu91Of3+/Egq5Z88erl27RnJyMrt27aK4uBibzcY777yjet1bt27x5ptv8vDhQ0pLS+c1o6enp4fi4mJ6e3sxGo2sWbOGXbt2MTQ05PfOlIXi66+/5ubNmxw7doxz586xbds26ZwRIoD99Kc/Hff/2dnZZGdnU1FRQX9/vzJ1a+z2FSGEEIFBp9PddrlcXpkCIR09QgjhZ1pbZTo7O6cYietbU41uNxgMyp9aAZI2m433339fuc8LFy5QUFCA2Wz2+nodDgdZWVmkpaVRV1dHTU0NaWlp1NTUSGfKFGbSuSVGqf07zsvL46OPPhp3vYiIiGllSwgxG1pbGw8dOsQnn3xCS0sLr7zyCi9evKCsrIy+vj7i4+M5cODApOOphRBCLCxS6BFCCD9T2yqTlpbGjRs3phiJ61uTjW4vKyvjzJkzymQqtetmZ2crOT5tbW1UV1ezd+/eedvGFRcXp4xMTU5Opr6+HqvVqpkpJL7lOc43PT0dgKCgIPLz8ykuLubq1au88847fl5p4NDa8ubO17BarZw9e5aUlBQ/r1QsRdevX0ev1+NwOAC4fPkywcHBnDx5ks8//1zpsBQi0DidTs6dO0dnZycOh4O33nqLiIgIAL9O8RQi0EmhRwgh/Ewt5Hh4eHgaI3F9a7LR7QUFBRMuV7uuOwTUbDazbZtXOlOnZLPZqK6uJjIykrCwMEA6UyYzk84t8S21f8fR0dFKl0RjYyMAubm5flujWJpaWlro7+8nIyOD5uZmbDYbFouFTZs2ERMTQ3x8PA8ePMDpdEogrQhI6enphIWF8eDBg3GXB9oUTyECiRR6hBAiAHhulXHnJyzVkbha+TpPnz6d0XY2m81GYWEhQ0NDnDhxQtmCJJ0p2mbSuSXGUwsgh9Ez0o2NjSQmJhIbG+vnVYqlxOl0UllZyY4dO2hvbwdG3/+MRiPd3d04HA56e3txuVwMDw/LRC4RcPR6PVu2bKGqqmrc5YE4xVOIQCKFHiGECACeW2XS0tKApTsSVy1fJzk5mZKSkmlvZ3MXeSwWC0ePHkWv1xMUFCSdKVOYaeeW+Jbnv+OmpibWr19PS0sLVquV3bt3+3uJYolpaGggODiYlStX8vDhQwBcLhd79uyhoqKC06dPExQUhMFgICQkxM+rFWL6KisrA26KpxCBRAo9QgjhZ2pbZXQ63ZIeiauWr2O322e0na2zs5MXL14AcP78eQDy8vI4fPiwdKaIadHqLCsvL+f+/fvYbDYOHDjAmjVrVP8du/+sr68nNDSUqqoqrl69GtCh62Jx6evr4/nz5/z85z9XLnvvvfc4efIkBQUFvHz5kqqqKhISEtDpdH5cqRDT193dTVtbmzLFUwgxkRR6hBDCz9S2yqxbt46YmBgqKiooLCwkNTWV7du3+3upPjc2X8fpdALT386WnJysOYFGOlPEdGhNbouNjSUkJITq6mrluloB5N3d3XR0dLB582bi4uICPnRdLC4bN24kKysLgNu3b/Pw4UOOHz9OU1MTDQ0NmEwmMjIy2LVrl59XKoS23t5ehoaGALBYLAwODvp0iqcQC5EUeoQQws+0tsokJyfzxhtv+GFFgcEzX6e/vx9YutvZhO+pdZYNDw+Tk5PDo0ePxl1X699xTEzMhIJjIIeui8UlPDxcCQR/7bXXlMvNZjN79uzx17KEmJGPPvpI+bqwsJAVK1YoJ2x8McVTiIVICj1CCCFmRGs7y9DQEOXl5Tx+/BiXy0VeXh4bN26c1WOo5evExcUt6e1s/qT1dw5LY7zt2M4y96j52ZDQdSGEmDmt7lzw7RRPIRYSKfQIIYSYEa3tLDU1NXR2dnL06FEMBsOcDli18nWOHDmy5Lez+YPW33lKSsqiH2/r2Vnmzt2ZjemGrjudTs6dO0dnZycOh4O33nqLiIgI7ty5Q21tLcPDw8TExJCfny9TvIQQC5rW+503Tx4JsRRJoUcIIcSMqG1nsVqttLe3s23bNpKTk+f8GJPl6yzl7Wz+orWFyVvjbX3RJTYbap1lNpsNm83GwMAAAFarFYvFQmRk5KT3NdPQ9fT0dMLCwnjw4AEAw8PD3Lx5k5SUFHbs2MHZs2e5ffs2r7766jz+BIQQYv55vt8BXLlyxWsnj4RYiqTQI4QQPtbZ2cknn3yCy+Xi3Xff5erVq7S3t2O324mPj+fgwYNERET4e5lTGrudJSwsDIDW1lbq6upYtmwZO3fuXJTbeOZieHiYs2fP0t/fP27y0tOnT7l27dq4yUvubT2BxHMLU1lZmVfG2/qiS2w2tDrLOjo66OjoAKCqqoqGhoZx+TxqhauYmBiuXr067v5ra2tVu9T0ej1btmyhqqpKua7JZCI8PJzg4GCioqLQ6/XKli8hhFio1N7vvH3ySIilSAo9QgjhY9evX0ev1+NwOAAICwvjtddew2azcfHiRW7duhXwI789t7O4XC4AgoKCyM/Pp7i4mKtXr/LOO+/4eaWBRa/Xs3379nGTl1JTU7l27RpJSUkcOnSI8+fPU1lZyf79+/293HE8/84tFovXxtv6oktsNibrLJuMWuHq9ddfV4pBVquVs2fPkpKSMu3Qdb1eT05ODlVVVbS0tBAaGiq5FGJyT6rhZ9vB5YT/NAIG+dgvfE/rBEdxcbHSrRkXF0dMTIxyG/fwBTl5JMTsyTu+EEL4UEtLC/39/WRkZNDc3AzAzp07AXC5XJhMpoDPO1HbzhIUFER0dDQ6nQ6DwaD8KcYzmUxkZmYC305eioiIwGazkZaWhtlsJiYmZs6FE29T+zvv7Oz0+njbQOkS8+y6Ky0t5e7du8r3169fP+nEIq2tbu7pR42NjQDk5uZOe029vb1UVVWxcuVKNmzYwKVLl6ioqBg3SUmIcT7/d6A3gSOwf6eIxU3tBEdGRgZZWVns2LGD7u5uioqKsNvtym3cHa1y8kiI2ZNCjxBC+IjT6aSyspIdO3bQ3t4+4fu3b9/GZrPN6OBvtrQyUcrLy7l//z42m40DBw6wZs2aCbfV2s5y+PBhysrKOHPmDNHR0QHfleQvnpOX3Nv0urq6sNlsWCwWbDabn1c5ntrf+fr167063jaQusQ8u+4A4uPjOXLkiLKm6VCb1uV0OmlsbCQxMXHSIOXe3l6GhoYAsFgs6HQ6AAwGA0ajEZ1Op+QECTFBwyfQ1wY5BfD1B/5ejVjC1E5wREdHKx08TqcTQMkus1gsREdHy8kjIeZICj1CCOEjDQ0NBAcHs3LlSqVjw30w++WXX1JdXc2+ffuUSTzzSSsTJTY2lpCQEKqrqzVvO9l2FveBvz8EaqCvJ8/JS62trWzdupXq6moaGxuVLJZAMtUWprmOtw2kLjG1rjsYLcR9/PHHxMTEsHfv3nHbDNRoTetqaWnBarWye/fuSW//0UcfKV8XFhaSnZ3Ntm3bqK+vp6WlhaioqCnvQyxRjhH44i/gyN/BvQv+Xo0Qmic4Tp8+rXTyPH36FPj2/U5OHgkxN1LoEUIIH+nr6+P58+f8/Oc/Vy577733lBDCLVu2kJ6ejtVqZdmyZfO6Fq2tJTk5OTx69GheH3u+BGqg71hqk5eMRiMJCQlkZGRgsVi4du2aaifVYjZZl1hJSQm/+c1vgNGfVXFxMfn5+Xz22Wd0dXXhcrmUfKO5Blhrdd1lZGSwdu1anE4nRUVFlJSUTFrU1JrWFRQURH19PaGhoaxcuXLStWgV1vLy8mb35MTScftnsCwWcn4P7hWOXuZyIB/7hb94nuBoampi/fr1nDp1ip6eHoqKikhPT58wRdCfJ4+EWOjkHV8IIXxk48aNZGVlAaPbtB4+fMjx48f54osvgNGuni+//JLw8PBxE3zmk9rWkoUqUAN9xxoaGqK0tBSr1UpwcDDr1q0jOzuboqIi2tvbCQkJISsry68dR/4wWcfQiRMnaG9vn5DvsHz5cnbv3k1XVxclJSXU1tbOOZxYq+suIyNj3Frb2tomvR+twlVmZiYdHR3k5eWh1+vntFYhNHXdhUc34CdjprL9XSz8uN9/axJLltoJDofDQXNzM3FxcZhMJnQ6nfI9IYR3yL8oIYTwkfDwcGVLztgAVV8VdTxpbS0Zy9/jwGcTihsogb5qUlNTVf++Pc9iim9p5TuMvQxQ8mzmQqvrbt26dUoHTkdHx5TbtiYrXM1mipcQM7Ln38PGb7KsSv4G7p6HP7zq1yWJpUvtBEdWVpbS9WgwGEhJSWHHjh3+XqoQi4oUeoQQYgnS2lpis9mUgFer1Up/f79fx4HPNBQ3kAJ9/U2rSGe32wMqs2g6tPIdXC4XN27cwGAwkJOTM+fH0eq6q6ur4+LFi9jtdsxm87y81oXwmqi00f8A3j7n37WIJU/rBMcbb7zhh9UIsXRIoUcIIZYgra0lHR0ddHR0AFBVVUVDQ4PyAc3X48BnGoobSIG+gUBrpG1TU1PAZBZNl1q+Q25urjIl7siRI+j1ej7++OMJYdx37tyhtraW4eFhYmJiyM/P15x2pdV1d/DgwTmtX6vo1tnZSVlZ2bjOOHfxUu02eXl540KaASIiInjrrbfmtD6xuHh2QnZ1dVFWVkZfXx/x8fEcOHAg4ALfhZith5aH/P2t/8rjgScEG4I5kn6EP1r/I38vSwi/k0KPEEIsQVNNURrLH+PAZxOKK2Pfx1Pb8hQaGhpQmUXToRVgXVZWRmNjI/v27SMuLo7e3t4JYdwJCQncvHlT2RZw9uxZbt++7fOtcmpFt7S0NG7cuEFiYiIHDhzgwoULVFVVsXfvXs3bZGRkKIVXq9XK2bNnSUlJ8elzEYHPsxPy8uXLBAcHc/LkST7//HNKS0s5duyYn1cphHeMOG0cSn+FvPitFLac45P7Z8hL2MYm8yZ/L00Iv5JCjxBCiEn5Yxz4bEJxA3Xsuz95FuncW9kCJbNoOrQCrEtLSwEoLy8HICkpiePHjwPfhnG7XC7Cw8MJDg4mKioKvV6PyTQaUNvT00NxcfGEDiB3l5DNZuPAgQNemYCmVnQbHh5mcHCQVatWER8fT0JCAm1tbUqhRyubyP1vrbGxEYDc3Nw5r08sHp6dkO5i/KZNm4iJiSE+Pp4HDx7gdDolEFwsCquiV7MqejUAm+I281nLBfptL/28KiH8Two9QgghNPlrHLi3QnGXOs8inXuy2kLKLNLKd9Aq6nlOkuvr66OqqoqWlhZCQ0OVyVwOh2NCB1BaWhqxsbGEhIRQXV3t1efhWXRzh5e7C08mk2nCNjqtbjqn00ljYyOJiYma29DE0qPWCRkUFITRaKS7uxuHw0Fvby8ul4vh4WFCQ0P9vGIhvGdgZIAPmn5NUlgy2xLmNoFRiMVACj1CCCE0+WscuITizp1akc7dFbJYM4s8w7j7+/upqqpi5cqVbNiwgUuXLlFRUcFrr71GXFwccXFxwLcdQMPDw+Tk5PDo0SOvr82z6JaWNhqWOzIyovzpeeCtlk20fv16WlpasFqt7N692+vrFAuXVifknj17qKio4PTp0wQFBWEwGAgJCZn2/TqdTs6dO0dnZycOh0PJhPr1r3897nqSFyXchvuHOfsfP+fliwEMJgPp25I58K93Ywya+PtGK8OsuLhYGRoQFxfHwYMHiYyMVH28gZEB/vO1v8Jis/C3+/+OYOP0X99CLFZS6BFCCKHJX+PA5ysUdylRK9KtWbMGs9m8KDOL1MK43UUUg8GA0WhEp9MpU+XG3m5sB9B8UCu66XQ6QkJCaG5uJjw8nGfPnrF69epJb+P+s76+ntDQUKW7TQjQ7oQ8efIkBQUFvHz5kqqqKhISEtDpdDO67/T0dMLCwnjw4AEAYWFhkhclNOkNera/s5nYlcupv3iXmrMNrNyVTuaeie+xWnlkWVlZ7Nixg+7uboqKiqitrVW2to5lHbHy1xV/RcfAE/5yx48x6U1YR6wsMy3zxVMVImBJoUcIIcSC4ctcFW/TOmtpNBoZGRnhww8/xGq1cuzYMa9k5mgV6eLi4gIys0jr73ZoaGha4+C1wri3bdtGfX09LS0tREVFjeuC8ewAchdSvE2t6LZu3TpiYmKoqKigsLCQ1NRUtm/fPultsrOz6e7upqOjg7y8PMlYEeNodUI2NTXR0NCAyWQiIyODXbt2zeh+9Xo9W7Zsoaqqatxl4eHhOJ1Ofve73wGjuVFbtmwhIiKCc+fO0dXVhcvlIikpiUOHDinbFcXiZwo1KUWdCHMYBpOeqOQI9etq5JG5t2Y7nU4AIiMj+fDDDyf8Di28WEjm09WscmVx/nwhdyKqOZH7fd7O+QMfPFMhApcUeoQQQiwYvs5V8Sats5aZmZncuXOH4eFhfy/Rr7T+bmtqaqY1Dn6yMO68vLwJl6l1ANlsNmw2m9L1Y7VasVgsmtsFpkur6JacnMwbb7wxo9vExMRMe2KeWFq0OiHNZjN79uyZl8d0Op3YbDZCQkIYGhpSLl++fDm7d++mq6uLkpISamtrlXwssTR01D2n8L9cxmFzkLo5icgE7aENWnlkp0+fxm63ExERQVJSEmFhYRN+h25bv43l+5crnT9/mvRn7M2Z2PkjxFIjhR4hhFjitDpN7Hb7tDopfMnXuSrepHXWcmBggNraWnJzc6mpqfHzKv1H7e/WarUq4+BDQ0Pn1PHjSasDqKOjg46ODgCqqqpoaGhQLbgIsRip5fG4O3SeP38OQElJCUePHiU4OJi2tjZGRkZISUmhtbVVuZ99+/YBKIWnsUUgsTSYV8dw6h+O0XLtIVW/vENjUTMbjq9Vv65GHtmpU6fo6emhqKiI6upqZdv4ZJ0/MqRBiFFS6BFCiCVOq9OkqalpWp0U/uCLXJX5oHbWsry8nKysLJYvX+7v5QWEsX+3YWFhwOg4+NraWgwGA3v27KGrq2vGHT+eJusAEmIp88zjgdFMnvT0dFpaWnjy5AnV1dXs3r1byYuKjo6ecD8ul4sbN25gMBjIycnx4TMQ/tb5oJshyzCRieEYg7/JGAtWD/5XyyNzOBw0NzcTFxeHyWRCp9NhNBqn3fkjhJBCjxBCLHlqnSahoaFKJ0VycvKE22h1AX322Wfznsvgq1yV+aB21rKtrY0333xTmZKzlHn+3bpcLmB0RPTrr79OcXExt2/fZs+ePRM6ftRep96ilR8EzEu+khD+opbHA3D//v1x///w4UPWrFmj5EW5/626uVwuJTvtyJEjxMbGzvvaReAY7Bui9H/dxNozSHB4ELnHssk+vEr1ump5ZFlZWcrWWoPBQEpKCjt27CAkJGTKzp/Kysp5HxghxEKwcD4dCyGEmDOttvw7d+5w8+ZNAEJDQ5W8mNbWVurq6li2bBk7d+5UDmK1uoDmO5fBl7kq3qZ11tJms/H+++8r17tw4QIFBQWYzWZ/LdUv1P5ug4KCJoyD1+v1qh0/aq9Tb9HKD0pJSZF8JbEk/Mmf/Akul4uSkhKam5s5cuSIkhfV29tLbW0tgHJgfuvWLRobG9m3bx9xcXEMDg4SGhrq52chfCVtSzJ/8PPphf5r5ZF55pd1dnbS29s7rc4fIYQUeoQQYsnxbMsfGRnhyy+/JDk5maioKBoaGrhy5Qow+sHqO9/5Djdv3uTy5cuEhoYyMDBAamoq+fn5BAcHj9srP7YzCLyfy7CQc1W0pii5O1Ha2tqorq5m7969S3Ibl9bf7eHDh5Vx8O4P+GodP/n5+RQXF3P16lXeeecdr65NKxtK8pXEUjFZh85HH32kfF1YWEh2djZ3794FoLy8HICkpCSOHz8+rceyO5z8q3+poqnDgs3u5Myf5ZO8XIpES91MOn/Gsn/Vgf3TpnGXGTYnYjqhnhckxGIhhR4hhFhC1NryW1tbsdlsJCUlYTKZgNHOH5PJxMjICAbD6L764eFhEhMTOXToEOfPn+fq1as8evRowl75+cxlWMi5KlpnLd1nuc1m85KeSjPZ321BQcG0O37cr9f54JkNVVZWJvlKYtHp7e1VivTT6dBR+3d78ODBOa1hb7aZ+MhgLtc9m9P9iMVjup0/ngy58RgyR9+jHY2d2C/eR58p79li8ZNCjxBCCABu375NSEgIRqMRl8tFZmYmTU1NXLx4USnipKWlYTabiYmJobOzc8Je+dzcXMllEPNiOh0/0dHRHDp0aF4e3zM/yGKxSL6SWJS83aEzU0aDnj/Mz+SfLt+b9HpaW5FnO4lPBA6tHMDi4mLl7zUuLo6DBw9OuU1bZzKAafQEgKPhBYSZ0Ocsra3RYmmSQo8QQixxWVlZWCwWqqurGR4exmQyERoayrJlywD4/d//fXQ6Hb/61a/o6urCZrPR29vLyMgIer1e2Q9vNBopKyuTXAYNWoG+586dU7adAezevZsNGzb4caWBaaqOn/mk1k3U2dkp+UpiUZqPDp35ojYh7MqVKwE7MVJMj1YOYFZWFjt27KC7u5uioiJqa2vZu3cvMI3ikNNJzGAQ+9fnEWLQ+/kZCjH/pNAjhBBLjFpbfnp6OhkZGVgsFq5du8aaNWsYGRlRbhMeHs7WrVuprq6msbERg8GAy+Xio48+Gpc3U1paCnx71hcgODh43qcUdXZ28sknn+ByuXj33XcpLS1VzkADrF+/nj179sz5ceZCK9AXIDMzU/n5eHtKmZg7tW6i9evXKwWmpZ6vJIQ/qG1F9tUkPjG/1KaBRkdHExYWxtmzZ3n58iUAz58/x263U1xczKNHj3A6ncTExBAXF0djY+O44tDzoq+56qinIaiLff58ckL4iBR6hBBiiVFry7fZbLS3txMSEkJWVhbp6enU1dUB2sWg3NxcNm/ePO6+3WeCOzs7efLkic+mFF2/fh29Xo/D4VAui4+P58iRI8BoWK+/aQX6wmihoKWlRQkXdm9BePToEdeuXZsQgC18a6psqKWeryQmcjgclJaW0tbWhsvlIjk5mUOHDgXEe5HalieAX//61+OuFxERoXzP11pf9NNnHT3Z8LjHSpBRT1zE1O99/f39o7ef50l8Ynom67J59OgRdrsdGC3srFixYlwHjtPpVH6nj80B7O/vx+l0EhQUxIsXL3j48OGETh+j0agUh2JiYnAN2xlsGYQoiE2Ujktv0tpCeenSpRlvsxPeJYUeIYRYYqYTZvzTn/5U+VqrGDRZ5oEvpxS1tLTQ399PRkYGzc3NyuVdXV18/PHHxMTEsHfvXmJiYrzyeHPlGejrdDrZvn079fX1NDc3K8WekZERioqKSEpKUgKwKysr2b9/v5+fgRBiKu3t7dy7d4/NmzcTERFBWVkZTU1NAbMt03PLU1hYmBJ0a7VaOXv2LCkpKX5b3w//Z4Xy9Z++d4tjm5P564Kpf3buQvh8T+IT0zPZFqwtW7Zw//59amtriYyM5P79++M6cF68eMGVK1eIj4/n0aNHNDU1sX79en7wgx/Q09PDF198AaAUcwCePRsN73769Om44tC//P/ewx7lICI0jKSkJP/8MBYxtS2Uk22zE74hhR4hhBATeGuy1XxPKXI6nVRWVrJjxw7a29uVyzMyMli7di1Op5OioiJKSkrmPcdlOjwDfY1GI6tXrwYgMTGRJ0+eYLVagdEtdjabbVwAtoT+zoxWLtLTp08pKysb1ykVCJ0WYvGIjIxEr9cTHh5OeHg4gDLV0N/Utjy51wrQ2NgIQG5url/WB3Djb74zret5bkWOjo726SQ+MTmtLVjuwozBYKC2tpaoqCh6e3uV73V2dirdPuHh4Tx//hyHw6GcDLl69Soul4tly5YpxZzTp09jt9tZtmwZmZmZfP3110px6NQbo8WhoqIiKisrefXVV/3zA1mE1N5PAFauXAmMfk4DAuZk21IihR4hhPAirQNL9yQqm83GgQMHiI+Pn3C93NzcgGqdnytfTClqaGggODiYlStXKvfpcrnIyMhQrpOcnExbW5tXHm8u1AJ9bTYbN2/eJCcnh6GhoXGhoe4Qa3cAtsViwWaz+Wv5C5JaLlJycjIlJSUkJiZy4MABLly4QFVVlZxpFF4VGRlJWloa5eXl6HQ6EhMTyc7O9veypuR0OmlsbCQxMXFBTE1U24rsq0l8Yno6Ojq4cOECDodjXJeNuzAD8ODBg3HfO3v2rFIgePLkCevWrSMrK0v5HWowGIiIiKCvr08p5hw+fJiuri6qq6t5/PgxgFIciouLw2QyodPplAESYv65/44jIiKkk8oP5JUuhBBeNPbA8tatW9TU1FBTU8O+ffsIDw+nu7ubkpISYLSD4/d+7/fGZdgEUuv8XPhqSlFfXx/Pnz/n5z//uXLZe++9x7p165SzSR0dHQFxJklrPLjFYuHcuXPAaGHPYrEAEwOwTSaTcsZdTI/aFkK73c7g4CCrVq0iPj6ehIQE2trapNAjvOru3bu0tbWxfft2IiIiKC4upra2lk2bNvl7aZNqaWnBarWye/dufy9lWvw1iU98azqj0JcvX05iYuL4LptTp+js7KS4uJioqKhx27PeeOMNpQMnMTGRfftG45MPHTrE0NAQkZGRtLa2cuPGDaWYY7fbqa+vx+VyYbFYVItDKSkp7Nixw88/saXj1KlT0knlR1LoEUIILxp7YNnT06NcvnbtWlpaWgDYtWsXmZmZBAUFERQUNC7DJpBa5+fCV1OKNm7cSFZWFgC3b9/m4cOHHD9+nLq6Oi5evIjdbsdsNgdErs1Ugb69vb3U1tZisVgmnYY2Fa0P3Xa7nfLycuWDd15e3qQ5S4vJ2C2E7rPE7m00JpNJxi8Lr9PpdAAYjUalg2BgYMCfSxpHbfrismXLqK+vJzQ0VCmUCzGVyXJ4srKy6Ozs5Msvv1S20LkLMwaDgf7+/nFdNlN14AwNDVFaWorValUmfnoWc9LT05WTawBvvPGG738oS4za+0lHR4d0UvmZ/MSFEGIe3Lt3D4vFgtFoVFqT3W7duqV0EISHh4/LsIGF1zqvxldTisbmX7z22mvK5QcPHpzzffvadKahTacwo/Whu6mpic7OTo4ePYrBYFgyxQ3PLYTuqTwjIyPKn+5tckJ4S1ZWFo8fP+b27ds4nU6Sk5MDqrCq9n6zceNGOjo6yMvLQ6/X+3F1YiGZLIfn0aNHyomr7u7ucYWZvr4+pfDe398/rQ6c1NRUpfN5LCnm+Jfn+0laWhr9/f3SSeVnUugRQggvc59xMhqNJCcn09raCoxu1Xr8+DG5ubk0NTVx5coV9Hr9uGBeWHit88I7vBWArfahOzQ0lPb2drZt20ZycrJXHmchUNtCGBcXR0hICM3NzYSHh/Ps2TMlEFsIbzEajRw5csSnjzncP8zZ//g5L18MYDAZSN+WzIF/vRtj0MQwYq33G2+9D82FjGteeLRyeC5duqRktHz3u98lOjoamLww46+ijdbrzjNjcTqdtUtNILxviImk0COEEF5ks9k4c+YMTqeT73znOzQ0NADw8uVLpfMkKCiI2NhYHj9+THBw8Lhg3qCgIGmdF3Pm+aHbPbK9tbWVuro6li1bxs6dO0lNTfXzSueXVi7SkSNHqKiooLCwkNTUVLZv3+7PZYoA5nA4KC0tpa2tDZfLRXJyMocOHfLplLbOzk4++eQTXC4X7777Lnq9HpfLxZkzZ+jq6iI/P5+1a9eiN+jZ/s5mYlcup/7iXWrONrByVzqZe9J9tlZv0RrXvG3bNr744guePn3KBx98IEWgAGE2mzl16hQPHjzg1q1b43J4FlJGi9rrLjY2lpCQEKqrq/24MiFmTgo9QgjhRZ2dncrWkAsXLiiXf/TRR8rB9tgRlMPDw+MOQDMzM6V1XsyZ54du97bAoKAgJSTz6tWrvPPOO35e6fyabAuhtPqL6Whvb+fevXts3ryZiIgIysrKaGpqYsOGDT5bw/Xr19Hr9TgcDuWyu3fv0tvbO+56plCTUtSJMIdhMOmJSo7w2Tq9ZbJxzU6nk9TUVPr6+sZ9Lysrix07dtDd3U1RURG1tbUSsO4jnZ2dSkDydLN25msdYwuiY4c0uE3WeaL1usvJyeHRo0deX68Q800KPUII4UXJycm8/fbbSv6JOyD4+9//PnV1dbS1tY0LCFYLIpYWWDEXah+63ZkJOp0Og8Gg/CmEmFxkZCR6vX5cHpg7yNsXWlpa6O/vJyMjg+bmZmA0V6qqqooNGzbw1Vdfjbt+R91zCv/LZRw2B6mbk4hMWFyT+n7xi19gt9sJCgrCZrMpl7s7YN2ZL4EwaXGpmE5Asi8yWjwLou4sH4fDwZkzZ0hISJjXxw9kWtvSzp07R0dHh3K93bt3T1rEtn/Vgf3TpnGXGTYnYjqxdt4fW8ycFHqEEMLLFlNAsFh41D50r1mzBrPZTFlZGWfOnCE6OppDhw75e6lCBLzIyEjS0tIoLy9Hp9ORmJhIdna2Tx7b6XRSWVnJjh07aG9vVy6/c+cOsbGxpKamTij0mFfHcOofjtFy7SFVv7xDY1EzG45rH4QtNO6tQJcuXZrwvdOnTyt5MElJSX5Y3dIUCAHJagVR9+ewpqYmRkZGWLdunc/WE4jUtqUBZGZmsmvXLgCCg4MB7eLM1Y5aOpOe4nK5WGuIJ+dxKPrMqSenzuSxhfdIoUcIIYRYJJxOJ7dv32ZwcBCn08n3v/99IiIiePToEdeuXWNgYEAZty4fqoSY2t27d2lra2P79u1ERERQXFxMbW0tmzZtmvfHbmhoIDg4mJUrV/Lw4UNgtJBbW1vLiRMnlHHGLpcLl8tFV0sPQ5ZhIhPDMQaPfsQ3Bs+xc+9JNfxsO7ic8J9GwDDx0GGyM/ZdXV24XC6SkpI4dOjQtN93pjOu2b0d2m2h5cEEioXecaFVEHWrr68nIiKCFStWTHlfaq87p9PJwMAAAFarFYvFsuDyn7S2pQE8fPiQJ0+eKJ3m7o5FteLMqrVZbNs9ukVS/2EjI0EQnGP2+mML75BCjxBC+FhPTw/FxcX09vZiNBpZs2YNu3btWjAfqkRg8/xwNjIyQlFRkXKgdf78eSorK9m/f7+fVypE4NPpdMDoFC33Vkj3Qd986+vr4/nz5+OyRj744APsdjsff/yxcllZWRnBwcGY+kIo/V83sfYMEhweRO6xbLIPr5rbIj7/d6A3gWN40qupHRQuX76c3bt309XVRUlJCbW1tWzbtm1aD+s5rjkhIUEpROh0OkJDQ5WDbrUi0HznwSw2891xMTw8zNmzZ+nv78dgMCgnHIqLi+ccoq1WEHUXAZ8/f86LFy/YsWOH8m95Mp6vu+zsbF6+fKl8NquqqqKhoUG1g2khysnJYefOnVitVi5fvsz169eVASFaOVkArhdWQkeW0ZMTRoRhdnmSWo8tvEfeBYUQwsccDgdZWVmkpaVRV1dHTU0NaWlpgLSxirlR+3DW29uLzWYjLS0Ns9lMTEyM8mFYCDG5rKwsHj9+zO3bt3E6nSQnJ7Nx40afPPbGjRvJysoCvs17++53v6sUMV68eEF5eTlbtmwhJSWF4Mxg/uDnBd5bQMMn0NcGOQXw9QeaV9M6KNy3bx/w7RYad6fEdCQlJfH8+XMcDgfvvvsuH330ETExMRw4cIDf/va3WK1WYPRgPC0tjf7+fp/mwSwmvui40Ov1bN++ndjYWBoaGqipqSEjI8MrIdpqBdH33nuPP/7jP6a+vh6DwcDatdPbvrjUMhJXr16tfB0TE0N3d/eUtzl9+jRbu2LIIorQ3VN3SXnzscXMSKFHCCF8LC4ujri4OGA0vLm+vp7h4dGzpdLGuvj4u4MrNDQUgK6uLmw2GxaLZVyI6WKk9TMvLy/n/v372Gw2Dhw4wJo1a/y9VBHgjEYjR44c8ctja+W9uZnN5vnLHXGMwBd/AUf+Du5dmPr6GlwuFzdu3MBgMJCTkzOt23jmrbjftzZt2kRMTAyrVq3iwYMH/OhHP5LplPPImx0XJpOJzMxMYPR17R4Q4A7NnkuItlpB9Pjx4wwNDdHc3ExmZiYhISGzWvdiorYt7fbt2+Tk5DA0NER3d7cyoXMyp45/H9PPa2kPeUlb/Ve8mjr1FklvPbaYGSn0CCGEn9hsNqqrq4mMjCQ9PR2n0yltrIuQvzu4wsPD2bp1K9XV1TQ2NmIymZSDx8VK62ceGxtLSEgI1dXV/l6iEIHt9s9gWSzk/B7cKxy9zOVgJocOLpdLKa4eOXKE2NjYKW+jlrcSFBSE0Wiku7sbh8NBb28vLpeL4eFhpZAtvM/bHRcdHR1cuHABh8NBamoqERERwNxDtCcriP7oRz+a05oXE7VtaQMDA5w7dw4YPfG4Z88e5TpaOVnxj13oR1zci+pjmXF6f18zfWzhHVLoEUIIP7DZbBQWFjI0NMSJEycwGo3SxjpLgd694esOLrUPZ+np6WRkZGCxWLh27dqi72TR+pnn5OTw6NEjP69OiAWg6y48ugE/GfOe9Hex8ON+1aurve/cunWLxsZG9u3bR1xcHIODg1MWZrTyVvbs2UNFRQWnT58mKCgIg8Hg1S6Nzs5OPvnkE1wuF++++y5dXV2UlZXR19dHfHw8Bw4cUC2QawUZDw0NUV5eruTP5OXl+WzL32z4ouPCbDZz6tQpHjx4wK1bt2hqamL9+vUSou0jM92W5lmcGbdFMt1AYmLitLdILrUtcYFCCj1CCOFj7iKPxWJRQu9sNhs3b96UNtZZWCjdG77q4FI7c2az2WhvbyckJISsrKyAPuDwJs+fuRCByLPIcP/+fW7fvo3VaiUyMpK9e/eSnJzs20Xt+few8Z3Rr0v+Bu6ehz+8qnl1tfedu3fvAlBeXg6M5u4cP3580ofVyls5efIkBQUFvHz5kqqqKhISEqYVrjtd169fR6/X43A4ALh8+TLBwcGcPHmSzz//nNLSUo4dO6Z6W7Ug4ytXrtDZ2cnRo0cxGAwMDg56ba3zYb47Ljo7OxkaGiIyMlLJmHI4HDQ3N0uIdoCS4szCJ/+ahBDCxzo7O3nx4gUA58+fByAvLw+LxSJtrLOwELo3fNnBJR/ORqn9zIWYK63pQZ999tmsM7c8iwzXrl0jLCyMgoICzp8/z/Xr1zl16pTXn0tPawO3Tv8f9LQ2ELQsgr1/9t8xr906+s2otNH/AN4+N+V9qb3vHDx4cMZr0spbaWpqoqGhAZPJREZGhrLl1RumygSKj4/nwYMHOJ3OCZlAakHGVquV9vZ2tm3b5vsC3SzN9++NoaEhSktLsVqtBAcHs27dOrKyspSTXhKivbBpdbYFSmf1UiWfeoQQwseSk5PlYHweBGr3hnRw+Z7Wz9xmsymjsd2jmWc6ylcsbVrTg2B2mVueRQYYHUvucrmIiorCaDTOSyj/yOAAV/7P/42IxBUc/cmvGXjxGEPQJFuhnlTDz7aDywn/aQQM83MIoZW3Yjab5+Xkx3xkAvX3j25va21tpa6ujmXLlrFz505SU1O9vv6FIjU1VXUk+RtvvOGH1Yj5oNbZNraz2ul08rvf/W5CMQhgZGSEDz/8EKvVyrFjx5b0vxVvkkKPEEKIBS+Quzekg8v3tH7mHR0dStdFVVUVDQ0NqgcfQmjRmh4EM8/cUisywOi0o5KSEk6fPo3RaJyXUP4n1VcZ7uti35//D5avWMPyFVOcaf/834HeBI5hr6/Fn+YjE8hd5AsKCiI/P5/i4mKuXr3KO++8M2/PQwh/UutsA8Z1Vut0OtViEMCdO3eU7ELhPYHzSVgIIYSYBbXujefPn3P16lUsFgsATU1NJCUl0dLSQm1tLcPDw8TExJCfnz+tSTBzIR1cvic/czGf1KYHzWYUtVqRwW63U1paSkJCAjt37qS0tJSrV696vfNhoPMJAHd+9d95+bSN6BVr2PEnPyE8PkVloZ9AXxvkFMDXH3h1Hf7mjUwgzyDj6OhooqOj0el0GAwG5U8hljKdTqdaDBoYGKC2tpbc3Fxqamr8tLrFSQo9QgghFqzh4WF++9vfKttx3N0boaGh48Ivnz59ytmzZxkaGlJyAM6ePcvt27dlwocQYka0pge5TTdzS63I8Itf/AKdToder1eKBO6tQN4UHB4NQGzWJja99ecU/+SPqPngH9jzb/7b+Cs6RuCLv4Ajfwf3Lnh9HVOZ7TSs6fJGJpBakPHhw4cpKyvjzJkzREdHc+jQoVmvcSY8s1J+//d/n6tXr/Ls2bNx19u2bRvd3d0LZiqYWLwqKyvJyspi+fLl/l7KoiOFHiGEEAuWXq9n9+7d4/Iyjhw5wpMnT1i7di1dXV2UlJQAsG/fPm7cuEFwcDBRUVHo9fp5yb4QQixeatODjEYjZWVlM87cUisyFBQU0NXVRXV1NWfPniU8PJz9+/d7/XkkbtyD3mBCbzRhCAoGnQ69SSVX6PbPYFks5Pwe3CscvczlwFeHEHOZhjUd3sgE0uoeLCgomPW65sJze0x6ejp9fX0MDQ1x8uRJwsLCKCkpobu7e8FMBROBz7OzzWAw4HQ6J+TijdXd3U1bWxs/+MEPlBN1Fy5ckPweL5FCjxBCiAVLKy/DfVlQUJDyp/vDblVVFS0tLYSGhrJt2za/rV0IsfCoTQ/Kzs6mubl5xplbkxUZ1q5dO+Xt5zKWPcycws7//W+p/fD/4t7FX5K4YTebfvhvJ16x6y48ugE/GVMU/7tY+LH3u4w8zWUals/4KKR6ujyzUtz/39DQAMDFixeJjY3lyZMnC2oqmJg/nu8jYzsM3aazFVqts+3ly5fjcvHq6upYsWIFMFoMGhwcxGaz8atf/WrcfXV1dSmFHsnvmT0p9AghhFjQ1PIy4NsxyACHDh2iv7+fqqoqVq5cyYYNG7h06RIVFRXjDrCEEGIyWtODXn/9dZ+vZa5j2TP2vk7G3inWveffw8ZvQoRL/gbunoc/vOqlZ6BtPqZhzYsFElIdGxtLf38/O3bsoKKiAgjMqWD2rzqwf9o07jLD5kRMJ6YufIrZ8Xwfcb+/ORwOzpw5Q0JCwrTuZzrFoJ/+9KdK0bGwsJAVK1YonW9tbW1UV1cDEBUVBUh+z1xJoUcIIcSCppaXkZWVxYcffsjQ0BDbtm0jNjZWOSNkMBgwGo3odDqlpVgIIRYSn41lj0ob/Q/g7XNzv79pmo9pWN5f5MIJqY6JiaGtrY3U1FSio6Pp7u4eNxUsNfX7Hrf4JTDFJLZ5YMiNx5A5mtXiaOzEfvE++kzJbpkvnu8jjjtPMZ67C4wWCd5iJdag0a7Dh5aH/P2t/8rjgScEG4I5kn6EP1r/oxk93mTFILPZjMvl4ssvv1S2xUp+z9xIoUcIIcSCpZWXUVxcrOwVv3XrFrdu3SIvL49t27ZRX19PS0sLUVFR7N6925/LF0Iscp7bIpxOJ9evX6e1tRW73U5OTs6kQb9q/DmWfbI1jQ0BdmdsnDt3jq6uLlwuF0lJSRw6dEgZPz4Zb0zDmld+DqmejGdWSn9/Py0tLQA0NzfT19eHyWQaNxVs1J8DR775en6nUWrRmQxgGp1Q5mh4AWEm9Dlmv6xlsVN7H9GvM2NcFQPAlx8Ws65jGZEbRwu9I04bh9JfIS9+K4Ut5/jk/hnyEraxybxpXtbnzu958803lWKvmBkp9AghhFiwtPIySktLx10vKSlJyePJy8vzx1KFEEuQ57aI69evc+/ePQ4fPkx4eDg9PT0zvk9/jmWfjGcIMIx2Ge3evVsJxq+trZ1WNpo3pmHBt1t4+/v7MRgMpKenk5+fz2effaZkhwDs3r2bDRs2TP/J+jmkejKeWSljVVZWEhYWRn5+Prdu3VKmgo36GfA+kA/8Bx+tVp3z+QCutj4M+1egM/gpf2mRU3sfwahHF2zi+fPnxHSCPVhH8LrRQtuq6NWsil4NwKa4zXzWcoF+20uvrcezQOnO73n//feV61y4cIGCggLMZin+TYf/342EEGIR6enpobi4mN7eXoxGI2vWrGHXrl08ffqUsrIyBgYGSE1NJT8/XwkKFrOnlZcxnb3i3qZ1QGE0GhkZGeGDDz5gcHAQvV6P0+lUzng/evSIa9eujXttTOeMtxAisKltr7p79y5paWncvn1b+T3R1dXFrl27NLtfelobuHX6/6CntYGgZRFEvvLHPO8Z9stYdi2eIcBu+/btA1BCp90HclPxxjQs97q2b98+bjJjRkYGAJmZmUqhaMbvuX4MqZ7KdH//paWljfk/J7AJKAP+EVgFvDmNe/Es2nlny5fj1mPQ6zBulbDo+aLVNffHf/zHtFTVs2lkGexOnlBoGxgZ4IOmX5MUlsy2BO8NtPAsUKrl9+zdu1e2cc2AFHqEEMKLHA4HWVlZpKWlUVdXR01NDcnJyZSUlJCYmMiBAwe4cOECVVVV7N2719/LFV6kdUCRmZk5bmpEfHw8T58+BUbHhhYVFSkHdefPn6eysnJexikLIXxHbVvE0NAQDoeD7u5uBgcHWbZsGZGRkdTU1JCWlqba/bIpN4cr/+f/RkTiCo7+5NcMvHgMoZFsiB+dXOPrseyz4XK5uHHjBgaDgZycHJ8+ttZkRoCHDx/y5MkTzGYz+/fvn1mmkZ9CquePu/MrndFCz70Z3Na7W75cw3YcNc/Qr4lFFyknPeaLVtfc0NAQoY19OHWRhO5MH3ebgZEB/vO1v8Jis/C3+/+OYKP38rGmyu+RKakzJ4UeIYTwori4OOLi4oDREbv19fXY7XYGBwdZtWoV8fHxJCQk0NbWJoWeRUbrgMI9NWL9+vXU1NQQFham3Ka3txebzUZaWhpms5mYmBjZiy7EIqC2LcLdNWIwGPje977HjRs3ePLkCTDaEajW/fKk+irDfV3s+/P/wfIVa1i+Yny3xGzGsvuSy+WivLyc+/fvc+TIEWJjfZ/9ojaZMScnh507d2K1Wrl8+TLXr1+fWaaRn0Kq50cDcAvYB5R/c1nWDG7v3S1fjppnYHNg2JYy5/sS2rS65lzDdrJHYtCvXT6u0GYdsfLXFX9Fx8AT/nLHjzHpTVhHrCwzLfP52sX0SKFHCCHmgc1mo7q6msjISJxOJ4ByttBkMjE4OOjP5Yl5onZAUV5erjk1wj0SuKurC5vNpuxL/+d//udx2786Oztl658QC4jWtoiUlBQGBwcxGEYDZ10uFxEREaSnpyv/P7b75Vn5GQDu/Oq/8/JpG9Er1rDjT35CeHzgHQR7ZmwYDAZu3bpFY2Mj+/btIy4ujsHBQZ+PQlebzLh+/Xrl+zExMXR3d/t0TYElFLgI/NM3X78B/N40b/sXzG7Llzbj9hSM2wPv9b1UaBXamnvvc693dCLXjyv+EoAfrnmbt3P+wOdrFNMjhR4hhPAym81GYWEhQ0NDnDhxQslJGBkZUf709Qdd4RtqBxSTTY0IDw9n69atVFdX09jYiMlkYtmyZRw/flzZ/pWWlsaNGzdk658QC4jWtoiQkBBKSko4e/YsLpeLoKAgXn/9dYxGo2r3S294NACxWZvY9NafU/yTP6Lmg39gz7/5b358duo8Mzays7O5e3f0wLC8fLRTJCkpiePHj/tsTVqTGcvKysjJyWFoaIju7m6l0BYI7A4n/+pfqmjqsGCzOznzZ/kkL5/PzwwZjGbrzMZctnyJQKRVaNtg3sin3y9UuYUIVFLoEUIIL3IXeSwWC0ePHkWv1xMXF0dISAjNzc2Eh4fz7NkzVq9e7e+lCi9TO6BwOBwTpka4Q1ndZ7zT09PJyMjAYrFw7do1cnNziYqKUrZ/DQ8Py9Y/IRYYrW0RAK+++uqE3xM2m40bN25M6H5J3LgHvcGE3mjCEBQMOh16U2DmlqhlbBw8eND3CxlDazJjc3Mz586NbrlKTk5m165d/O53v5swHh5GT858+OGHWK1Wjh07Rmpq6ryve2+2mfjIYC7XPZv3x5q9uW75EkLMJyn0CCGEF3V2dvLixQsAzp8/D4yO8z5y5AgVFRUUFhaSmprK9u3b/blMMQ+0DiiSk0enhrinRri5z3jbbDba29sJCQkhKysLs9nMP//zPyvbv9y5HrL1T4jFQev3RGNjIzCx+2Xn//631H74f3Hv4i9J3LCbTT/8t/5Z+AKkNZnx9ddfH/f/TqdTdTw8MC5M3xeMBj1/mJ/JP10O9O6YuWz5Umf/qgP7p03jLjNsTsR0IrCypxaqh5aH/P2t/8rjgScEG4I5kn6EP1r/I7+tx+l0cu7cuQkF1nPnztHR0aFcb/fu3WzYsMFv61yopNAjhBAatMZlP336VHMcdnJysubkgDfeeEP1crFwTfUauXfvnvIa2bZt27SmRtjt9nHbv9wjcGXrnxCLg9bvCa33h4y9r5Ox93XV7y1UWgd4T58+9UsemdZ4eHeYfm5uLjU1NfO+joUlg9lv+VJnyI3HkDmaZ+do7MR+8T76TBmn7S0jThuH0l8hL34rhS3n+OT+GfIStrHJvMlva9IqsGZmZrJr1y7g2yB7MTN6fy9ACCEClXtc9g9+8APWrl3L/fv3efDgAUVFRURFRfG9732PR48eUVlZ6e+lCj/x9muks7OTp0+fotfrle1fOp1O2fr3/Plznj17FlB5EkKIpWSbx39Nk199Eunp6axYsUL5f7vdzhdffEF0dDTHjh3j0aNHEwovvlZZWUlWVpbyfnzhwgWcTicvXrzgzJkz/Mu//AuFhYVKFp+YG53JgC4yBF1kCI6GFxBmQp9j9veyFo1V0av5/uoC0iPT2RS3GYB+20u/rcddYI2KiprwvYcPH3LmzBnKysp82lG3mEhHjxBCaFAblx0REYHNZuP58+d8+umnOJ1O7t69y+7duyft9BH+odVx89lnn9HV1YXL5SIpKYlDhw7N6u9qsteI1sj0ybqASkpKGBgYACAkJIR169axbt06YmJiJmz907qfGzducP/+fWw2GwcOHGDNmjWqaxdCBJaenh6Ki4vp7e3FaDSyZs0adu3axblz57zyfuU9fw4c+ebr2Y1LV+ugef78eUDlkXV3dyth+u48H7fLly8THBzMyZMn+fzzzyktLeXYsWNe2xrT+qKfPutoF+fjHitBRj1xEUvn84Tz+QCutj4M+1egM0hfgrcNjAzwQdOvSQpLZlvC1J3GvpaTk8P27dspKSmhvb2dX/3qV7Ktaxak0COEEJNQG5cNo+NYd+7cyaeffordbufBgwdcu3ZN+RB+/vx5Kisr2b9/v5+fwdLm7riJjY1VplhlZGSwfPlydu/eTVdXFyUlJdTW1k5rW5UardfI2JHpNptt0jWlpqYqr59XX32V8+fPs3LlSvbt2weMbvXw3Po3MjKi+txiY2MJCQkZlwckhAh8DoeDrKws0tLSqKurU6buefP9aqz+X/yC/v/1j7hGRgj7g7eJ+L//e3Q63TRu+TPgfSAf+A9zXoeb1WoF/JdH5jkefnBwcEKYPsCTJ0+wWCxs2rSJmJgY4uPjefDgAU6n02tbY374PyuUr//0vVsc25zMXxcsnYNZx63HoNdh3Jrs76UsOgMjA/zna3+FxWbhb/f/HcHGEH8vaYLVq1fjdDrJzs6mpqZmQkePbOuaHin0CCHEJDzHZbe2tirjsD/55BP0+tEzTVN1cQj/UOu4iY6OHncZoHy4n43JXiPukenux9Fa02xeP1rPLSYmhkePHs36+Qgh/CMuLo64uDhgtLhbX1/P8PCwUvD1fL+aS86NraaGvh//JyJ//P/EkJBAz7/5t5hycwk99t0pVvkXwCagjNFx2quAN73y/JctWwb4L4/Mczz8ihUrOHnyJEVFRcTHx9PS0gJAfHw8RqOR7u5uHA4Hvb29uFwuhoeHWRW9mlXRo1M1N8Vt5rOWC7PaGnPjb77jnScVQDo7O/nkk09wuVy8++67OJ1Orl+/TmtrK3a7nZycHHbt2oVr2I6j5hn6NbHoIuUg3pNWN29xcTGPHz/G5XIRFxfHwYMHiYyMHHdb64iVv674KzoGnvCXO36MSW/COmJlmWmZn57NxAKrwWDg9u3b5OTkYDabx50oc3v48CFPnjzBbDazf/9+pTgsxpNCjxBCaFAbl200GklISCA8PJyysjIcDgeRkZFTdnEI/9HquHG5XNy4cQODwUBOTs6s7nuy18jYkeme26dm2gUE6h/usrOzuXjxIk6nE51OR1VV1bhxxna7nQ8//HDCB0K73U55ebnyoTAvL4+NGzfO6mcghPAum81GdXU1kZGRSh6X1vuVZ5Cp3W7n888/V86AP3r0iC+++AKLxYLVaiUyMpK9e/cSfukLAJb98PfRL19O71/8Pxj8/NI0Cj3uzsJ0Rgs9s58M5XmAFx0dreSRhYeH8+zZM1avXj3r+58ptYDsuro6wsLCeOWVVygpKeHevXsYDAb27NlDRUUFp0+fJigoCIPBQEjIt50Rgb41xh+uX7+OXq/H4XAo/3/v3j0OHz5MeHg4PT09ADhqnoHNgWFbij+XG7C0OpWzsrLYsWMH3d3dFBUVUVtbO2HbY3Pvfe713gXgxxV/CcAP17zN2zl/4PPn4eZZYM3OzmZgYEDZLhkREYHFYlGuk5OTw86dO7FarVy+fJnr169z9OhRn697IZBCjxBCaNAal11UVMTDhw8JDg4mKiqKp0+fTtnFIfzHs+OmqamJ3NxcysvLuX//PkeOHCE2dnY5E5O9RsaOTPcsosy0CwjUP9wlJSUpHUF9fX20t7dTWVnJypUrgdEgZ7UPhE1NTXR2dnL06FEMBoOMaxciQNhsNgoLCxkaGuLEiRMYjUZcLpfq+5VWzs3w8DB6vR6n00l8fDyPHj1i+fLlFBQUcP78ea5fv87hF52j9xEWhk6nQxcejrPzxRSrawBuAfuA8m8uy5r1c1U7wDty5MiEPDJ/6uvr4/nz5/z85z9XLnvvvfc4efIkBQUFvHz5kqqqKhISEpRtbwtha4yvtbS00N/fT0ZGBs3NzQDcvXuX1atXk5GRAaB0sxm3p2DcLkUeLZN188Jopx+g/P9YG8wb+fT7hb5b7DRoTap1q6qq4ssvv1T+f2zxNyYmhu7u7nlb20InhR4hhNCQmprK22+/PeHyvLw81q1bR2RkJK2trTx9+nRaXRzC97Q6bsrKymhsbGTfvn3ExcUxODg4qy0CWq+RV199dcZrmur14/nhTq/XMzIywsjICDExMfT19REWFkZrayvx8fHAaBdQamoqkZGRygfC0NBQ2tvb2bZtG8nJkn8gxHwY7h/m7H/8nJcvBjCYDKRvS+bAv96NMcigeRt3kcdisXD06FH0ej02m40bN25M+/2qra0NgISEBDo6OpQtWyaTiaioKIxGIyaTCYN59KDa2d+PPigIV38/+ripphuFAheBf/rm6zeA35vRz2UsrQM8zzwyf9q4cSNZWaPFrNu3b/Pw4UOOHz9OU1MTDQ0NmEwmMjIylLyQQNwa429Op5PKykp27NhBe3s7MHqSxOFw8Pz5c95//31MJhNbt271aQfXQqbVFXz69GnsdjsREREkJSVN+/60toG6C8z+Gu7g2fWn0+n45JNPGB4exul0otfrWbFihQQ0a5BCjxBCzNBsuziE72n9XZWWlgJQXj56VjopKYnjx4/7dU3Tef2M/XAXFxdHTU0NMLpffe3atbS2tjI0NERJSQkweibs66+/xmazKR8IXS4XAK2trdTV1bFs2TJ27txJamqqT56/EDPhmeuh1+txuVycOXOGrq4u8vPzWbt2rb+XOYHeoGf7O5uJXbmc+ot3qTnbwMpd6WTuSde8TWdnJy9ejHbVnD9/Hhg9sdDY2AhM/X7ldDqVLVxuNpuN4OBgXrx4wenTpzEajRw9epSQ9HRe/sP/wPrhRxgSEnANDhJ69MiE+xwvA/jl9H4AC5Dd4eRf/UsVTR0WbHYnZ/4sn+Tl4Up35WuvvaZc12w2s2fPngn3EYhbY/ytoaGB4OBgVq5cqWTPjQ3Q/c53vsONGze4evUqK1askLyVaVDrVF6/fj2nTp2ip6eHoi+KuPH/vcDB3m9P5hg2J2I6of1e6bkNFJiX4Q5aRaWhoaEJW8pv3Lih3K6wsJCsrCxMJpNS/ElISGDPnj0UFxdLQLMKKfQIIcQU1Ebevv3228rI23v37tHf3x8AI2+FJ62Om6lahefTbLqA3Dw/3O3Zs4fh4WGqq6tpampSztr//u//vnIbu91Of38/d+/e5auvvuLx48cADAwMcPToUYqLi/nss88wmUyawa1C+ItnrgeMbvno7e3136KmwRRqUoo6EeYwDCY9UckRk94mOTlZ9b1psglbY894f/3114SEhOBwOHj5cjQA+NmzZ4yMjJCYmMjOnTspLS3l6tWrvPHGG0T95P9F//97dOpW+J/+a0JePzbbp7to7M02Ex8ZzOW6Z7O6fSBujfE3re1vKSkpDA4OYjAY0Ov16HQ6ZcCF0KbWFexwOGhubiYuLg6TyYROryNojZng3btwNHZiv3gffeZyzftU2wYKo3k48zHcQa2odOXKlQlbyrU+q7m3cx08eJCwsDBAAprVSKFHCCGm4OuRt0Komc2Wr7G3cRdvtm7dSm1tLYODgzx9+pSBgQEMBgPHjh3jwoULVFVVTQhwFMIf1HI9RkZGqKqqYsOGDXz11Vf+XeAUOuqeU/hfLuOwOUjdnERkgvdz28bm3NTV1U34vnuClV6vx2AwoNPp6O/vByD8j/+I8D/+I6+vaaEyGvT8YX4m/3R59gHTYiKt7W8hISGUlJRw9uxZJfDaYNDe2ihGqXUFZ2VlKds+DQYDKSkp7Ni7G114CI6GFxBmQp8z1dZM31ArKlmt1jltKZeAZnVS6BFCiCnMdOStEPNhNlu+1G6zefNmRkZGuHPnDrdu3cLlcrF582bi4+NJSEigra1NCj3C79RyPQDu3LlDbGwsqampAV/oMa+O4dQ/HKPl2kOqfnmHxqJmNhz37jazsWe8+/v7lWB19wF1QUEBXV1dVFdXc/bsWcLDw9m/f79X1wDw0PKQv7/1X3k88IRgQzBH0o/wR+t/5PXHCUyeJ3h+CUhGn1t4uPr2N4Dvfe97/ljSgqbVFayWbeV8PoCrrQ/D/hXoDIHbLeUuPs92S7kENKuTQo8QQkzTTEbeCuFts9ny5Xmbjo4O/uVf/kXJ68nKyuLKlSuYzaNn+kwmk0zgEgFBLddjaGiI2tpaTpw4oRTWXS4XLpdLmXgUKDofdDNkGSYyMRxj8DcdeMHz262gdUBtNpvnPcdoxGnjUPor5MVvpbDlHJ/cP0NewjY2mTfN6+MGjj8H3BlHs5viKIS3OW49Br0O49apu2TGbgP97LPPsFgsOJ1Odu7cCUB1dbWS/wfeDTx2xx4EBQWRn59PcXExV69e5Z133pl0ne4Optu3b5OTk8PQ0BDd3d3KZ/SlTgo9QggxDTMZeSsWr+HhYc6ePUt/fz8Gg4H09HTy8/MxGo2MjIzw4YcfYrVaOXbsWECGG3tm/KSlpQHfbu8YGRmZ1fQxIbxNLdfjgw8+wG638/HHHyuXlZWVERwcrEykCxSDfUOU/q+bWHsGCQ4PIvdYNtmHV/l7WfNmVfRqVkWPnlXfFLeZz1ou0G976edVzVzri376rKPvh497rAQZ9cRFTCd772fA+0A+8B/mcYVCTI9r2I6j5hn6NbHoIqd+DY/dBtrb20t4eDj9/f3cvHkTgJcvX2I0GnnzzTeBuQUeexZroqOjiY6ORqfTKVtMtbbxjV1nYWEh2dnZDAwMcO7cOWC0814tKH0pkkKPEEJMYaqRt6GhoVy+fHncgb/dbp8wPUCmcC18er2e7du3ExsbS0NDAzU1NWRkZJCZmcmdO3cYHh729xI1qWX86HQ6QkJCaG5uJjw8nGfPnsl4WxEQ1HI9vvvd7yqv3RcvXlBeXs6WLVtISUmZ8f2rhezv2rVLCdl3uVwkJSXNOmQ/bUsyf/DzghnfbqEbGBngg6ZfkxSWzLaEmWfWeU5Z6+rqoqysjL6+PuLj4zlw4IDStTQffvg/K5Sv//S9WxzbnMxfF0zVtfAXwCagDPhHYBXwJvavOrB/2jTumlNNPhJLm9ZEqkuXLimfJ+Pi4jh48CCRkZHA6HbW2tpaXC4Xa9asYfv27eh0Ohw1z8DmwLBteu+PnsHH7sBj9xrOnTvHixcvOHPmzJwDj9WKNYcPH6asrIwzZ84QHR3NoUOHprVOoU0KPUIIMYWpRt66t7qEhIRw//59MjIyaGpqmjA9QCx8JpNJ6RwIDw/HYDAQHR3NwMAAtbW15ObmKiPPA41aXs+6deuIiYmhoqKCwsJCUlNT2b59u9/WqPUh1/1Bdnh4mJiYGPLz86WDbpGbLNcDRrvT1q1bN+v791fI/nD/MGf/4+e8fDGAwWQgfVsyB/71boxBCz+EdmBkgP987a+w2Cz87f6/I9gYMuP78JyydvnyZYKDgzl58iSff/45paWlHDs2f9PBbvzNd2ZxK3c2SjqjhZ7RMGdDbjyGbyYdTWfykRCgPpEqKyuLHTt20N3dTVFREbW1tezdu5enT59y8+ZNtm/fTnh4OFeuXCEmJobVq1dj3J6CcfvMi+Ba3IHHD7secqv8FlVnqmiKa5hVHpdWsaagYOkVx+eTFHqEEGIK0x15+/XXX3Pz5k1CQ0PnND1ABLaOjg4uXLig5NxERERQXl5OVlYWy5cH7od4rYyf5ORk1RDH+abWUZGTk8OzZ9+ONf71r3+ttI+npKSwY8cOzp49y+3bt6c1jl4ILf4K2dcb9Gx/ZzOxK5dTf/EuNWcbWLkrXRnFvlBZR6z8dcVf0THwhL/c8WNMehPWESvLTMumfR+eU9ZsNhsWi4VNmzYRExNDfHw8Dx48wOl0BtAY7gbgFrAPKP/mstFONJ3JAKbRAl6gTT4SgUlrzPnKlSuB0ZMhMBo4DCi/LzMyMoiMjOTKlSs8fPhwXjpz3ff5MshCUHgQmWRiTomd1zyu2XQ4iW9JoUcIIebI88Df5XIBs58eIAKbZ85NU1MTbW1tvPnmm0porK/MNDPIc1vE9evXuXfvHg6Hg+XLl7Nv3z7i4+N9snatjoq3336bO3fuKKOiU1JSePz4McHBwURFRaHX62fdLi6EJ1+H7JtCTUpRJ8IchsGkJyo5wquP4Q/Nvfe513sXgB9X/CUAP1zzNm/n/MG0bq82ZS0oKAij0Uh3dzcOh4Pe3l5cLhfDw8MBlCUWClwE/umbr98Afm/cNRbK5CMR2E6fPo3dbiciIoKkpCQAwsLCAOjq6lKy9ryxhXyywOPgoRBcVhcJ6Qlkxq2c9zyumXQ4ifGk0COEEHPkeeDvPliYzvQAsbCo5dw4HA5sNhvvv/++cr0LFy5QUFCgTLNy0yrMPH36lGvXrjEwMEBqair5+fnTygWZaWaQ57aItLQ0ZVLFZ599RnV1teo2mfmg1VERHh5OUFCQcr3169cTGRlJVVUVLS0thIaGenUrjVi6/BWy31H3nML/chmHzUHq5iQiE+Yvc8ZXNpg38un3C2d9e7Upay6Xiz179lBRUcHp06cJCgrCYDAQEjLzLWHzJ4PRceraZjL5SAgtp06doqenh6KiIiorK3n11VfJzMzk3r17FBcXYzQaMRgMXsmwmk7g8ebtm/lJ9d/MOo8LtDt2gHEnq1atGh9kr9XhJMaTQo8QQsyB2oG/O7dlOtMDxMKilnOTnZ2tbNFra2ujurqavXv3qm7jUivMpKamcu3aNSX49fz581RWVrJ///4p1zOTzCDPbRGAUpS0Wq3odDq/fFjS6qiA0WKQwWCgqqqKlStXsmHDBi5dukRFRYXPClKePLui+vr6KCkpoaenh/j4eA4ePKicZRWBa6qQ/X379hEXF8fg4KDXu0fMq2M49Q/HaLn2kKpf3qGxqJkNx5d2QK/alLX33nuPkydPUlBQwMuXL6mqqiIhIQGdTufHlc7MTCcfCaHWTdPR0UFcXBwmkwmdTqd83oTRzMitW7fS2dlJRUUF2dnZ4+5PK6x5MlMFHnsjj8tNrWPHve7JupPUOpzEeFLoEUKIOVA78F+zZg1ms3la0wPEwqKVc+M+EDSbzZN2m6gVZiIiIrDZbKSlpWE2m4mJiZnRFrDpZAapbYtw+81vfkNPTw8hISGsWLFi2o/rDWodFb29vUr4eVpaGlarFRgtoBqNRnQ6HQMDA9O6f60Oqhs3bnD//n1sNhsHDhxgzZo1016zWlisyWTixIkTXLp0ibKyMr8VocT0TRWyv27d/83jFr8Epv860XzcB90MWYaJTAzHGDz6MdwYLCcC1KasHT9+nKamJhoaGjCZTGRkZLBr1y4/r3RmZjr5SAjPbpq0tDT6+/uVoo87rw5GO4qLi4sZGBggMjKSQ4cOkZCQoNx+srDm2fJGHpebVibRdAZcnDp1iq6uLoqKipSf2dgJYR0dHcp1d+/ezYYNU03PW3yk0COEEHOgdeAfFxcn0wOEKrXCDIzusXeHj9pstmnf33Qyg9rb21W3RcDoRCOLxcLly5cpLy/n1KlTXn7G6rQ6KsZ+yP3yyy8ZGBhg27Zt1NfX09LSQlRUFLt3757WY2htbYuNjSUkJITq6uoZrdmzK+rly5d0d3ezc+dOYmNjSUtLo6GhIcDCYoWaqUP2fwr8OXDkm//3zhauwb4hSv/XTaw9gwSHB5F7LJvsw6umvuEipzVlzWw2s2fPHuX/H1oe8vdl/5XHA08INgTPauLPXM1kBLy3Jx+JxW8m48NNJhNvvfWW5vfnI6x5rnlc01FZWTnuZJU7f0irwyksLIz+/v5x95GZmakUhqezFX4xkkKPEEII4UOehZnW1la2bt1KdXU1jY2NmEymae+xn25mkDvY2HNbxJ49e0hMTFRtB59vWh0VP/jBD/jtb39LXl7euO6ovLy8GT+G1ta2mJgYHj16NKP7UuuKGhwcVB7H/afL5WJoaIhly2Z+dlMEmp8B7wP5wH/wyj2mbUnmD34uJwFma8Rp41D6K+TFb6Ww5dysJ/7MZZqPv0fACzFd8xHWPNc8rql0d3dPOFnl/lOtwyktLY2IiAjlc47bw4cPefLkCWazmf379y/JIQ5S6BFCiCVAbZS1+0yH2nQmMT/UCjNGo5GEhAQyMjKwWCxcu3Zt2luJppsZtG3bNpKTk5XJGe5tEaWlpVRUVKDX64mLi/Pp1AqtjgqY2RnNqWh1UM2UWlisOxTW/QF6ZGQEnU4XYGGxYnb+AtgElAH/CKwC3vTrisby7CpxOp1cv36d1tZW7HY7OTk5C26b03Ssil7NqujRboRNcZvnNPFnNtN8FuYIeLFUzVdYszd5ZhINDg5OOFkFqA64cPPc+pWTk8POnTuxWq1cvnyZ69evc/To0fl5AgFMCj1CCLEEaI2yTklJmTLwTniPVmGmqKiI9vZ2QkJCyMrKYuPGjdO6v9lkBo3dFuGrbVr+pLa1bf369TO+H7Ww2DNnzrB8+XJaW1tJTU2lvb2dtLQ0OcCbpcAKun7jmz/TGS303PPR406PZ1fJ9evXuXfvHocPHyY8PJyenh4/r3B+DYwM8EHTr2c98UcrG2SyaT4LdwS8WMq0wprn0tU2W2qP6ZlJFBwczNGjRwkPD59ywIWWsVvTYmJi6O7u9sr6Fxop9AghxALneXBUWlrK3bt3le+vX7+ePXv2qI6ynk7gnfAercLMq6++6ofVLH5aHVT9/f1KoLPVasVisUz5QVYrLNZgMFBSUsKnn35KfHw8+/btm98ntYgFTtB1A3AL2AeUf3NZlg8ed3rUJujdvXuX1atXk5GRAaC83y9G3pz4o0Zrms/CHQEvlqqpwpqn09VWU1NDV1eX1wpCno/5J3/yJ7S0tLB8+XLlMZ88ecLevXunHHAB6lPKbt++TU5ODkNDQ3R3dysTPZcaKfQIIcQC53lwBBAfH8+RI6MhokFBQcrlnqOsy8rKxgXeCd+bj8lQYpRWB1VhYaEykaOqqoqGhgbVAtxYWmGxgASve0FgBV2HAheBf/rm6zeA35vnx5weta6SoaEhHA4Hz58/5/3338dkMrF169Y5Ba4GKm9O/NFy6tQpenp6KCoqorKyUinEL9YR8GLxmiyseSZdbWFhYTPe5jjXx5wuz46g7OxsBgYGOHfuHDB6cnNsoPtSIoUeIYRYwNTO7MJo8N7HH39MTEwMe/fuJSYmZsIoa4vFMiHwTvietydDiW9pdVAdP37cD6sRWmYadG0wGOa5OJrB6Dj1wKPWVTJ2osx3vvMdbty4wdWrV1mxYsWiCyD15sSfsZ0A3b19/Ntf1XLvaT8jThf/+Fb2hID6xToCXghPY7vakpOTiY6O9mpxZqrHHNtJNxVvZvotNlLoEUKIBUrt4AhGx2iuXbsWp9NJUVERJSUlvP766xNGWXd2dk4IvLtw4cKkgXfC+7w5GUqIhWimQdcOh2PJFke1ukpSUlIYHBzEYDCg1+vR6XSLMivKmxN/xnYCfHbhAomGNOzhDu5aTJSVlZOdlsKOHTuU60x3BLwQC51WV5un2RZn5vKYYvqk0COEEAuUVl6AO6MBRltW29raVEdZr1+/XtlyohZ4pzWpq7y8XLYUeZm3JkMJ4S+eWWF37tyhtraWoaEh8vLyJs1ZmGnQtV6vX7LFUa2ukpCQEEpKSjh79ixhYWG88sorGAwGP682sKl1AvzT5XvcLX3AD37wA5KXS5iyWNzU8m06OjqIi4vDZDJN6GrzNJvizFwfU0yf/BSFEGKB0jqzu27dOqWltqOjg5iYGM1R1p4HZ9evX+f999/H4XAQERFBamoqhw8fHjepaymcNfc1z8lQX375Je3t7crknObmZtasWcO5c+eUbBmA3bt3s2HDBn8te1bUCoi5ubn8+te/Hne9iIgIzWyB+VrHrl27lALF8PAwMTEx5OfnExsbO2/rWCw8s8IiIyPZsmUL169fn/K2swm6XqrF0cmyor73ve/5Y0lCiAXKM98mLS2N/v5+pQCTkjLa1ebN4sx0H1PMnRR6hBBigdI6OKqrq+PixYvY7XbMZjP79+/XvA/Pg7O0tDRlUsFnn32GxWJh+fLl4yZ15eTkLPqz5r6kNhlKp9ORnp7OypUruXXrFo8ePVImqWVmZioZEGOzORYKh8NBVlYWaWlp4wqI7iwdq9XK2bNnSUlJ8fk6EhISuHnzpnKdFy9ecPnyZYaGhqbVmbJUqWWFrVq1ir6+vmkVemYTdO1ZHG1qamL9+vUzXvtw/zBn/+PnvHwxgMFkIH1bMgf+9W6MQUu1G8bz9f1LQLo2hViMpptv89Of/lT5eq7FGcnU8R0p9AghhBdpdQnMdgzlZLQOjg4ePDit26sdnLlHUFqtVnQ6nRLiPHZSl/AutclQW7duHTcZCuDGjRssX76chw8f8uTJE6WIN5uwVa3XqS86huLi4pTRz2MLiO7XcmNjIwC5ubmzeh7T7Q5SW4fL5UKv1+NyuZSvQ0JCyMnJmVbBYinSygqbT2rFUaPRSH9/PwMDA8Doe5jFYpnyfVZv0LP9nc3ErlxO/cW71JxtYOWudDL3LOX3uj8Hjnzz9dLpZmt90U+fdTQT6nGPlSCjnriIhVdMF8Lb3MUZp9PJuXPnePLkybhR6+Xl5fz2t7+VLf0BRgo9QgjhRVrdCrMZQzkXnluyrl69Snt7O3a7nfj4ePLz8zUPzn7zm9/Q09NDSEgIycnJ4yZ1yb5p75tsMpTNZuPTTz/Fbrfzgx/8gNbWVnbu3InVauXy5ctcv36do0ePqt7vdIs5cXFxyusUfNcxpFZAdDqdNDY2kpiYOK3tUt7oDhq7DqfTidFoxGazAaPP/9ChQzidTin0aNDKCptPasXR7OzsccXRqqoqGhoaVP9tjWUKNSlFnQhzGAaTnqjkpbENTNvPgPeBfOA/+HktkxseHladwFZcXDzjkys//J8Vytd/+t4tjm1O5q8LFtbWWCHmW3p6+oRR67KlPzDJJ3YhhPAirW4Fd3Cot8ZQTsVzS1ZYWBivvfYaNpuNixcvUlxcrHlw9tprr2GxWLh8+TKXLl3CYDAok7psNhs2m23GZ83FzNlstglFttWrVyvfj4mJobu7W/P2WkUQGF/M6ejo4NmzZwwPDwN4pWNoNs8NRrvMrFYru3fvntb9zLU7aOw6vve973HhwgVsNhvh4eH09/fjdDqpqKiY9nqWIq2ssB/+8Ie8fPkSGC3M9Pb2Eh0d7ZXHnKw4Ohsddc8p/C+XcdgcpG5OIjIhfK5LXMD+AtgElAH/CKwC3vTriiaj1+tVJ7DN5uTKjb/5jo9WLcTCpNfr2bJly4RR67KlPzBJoUcIIeaBWreCN8dQTkZtS9bOnTvp7Ozks88+A2BwcJD+/v4JB2d79uwhMTERk8mEy+XCbrdjt9uVSV15eXl0dHTM+Ky5mBl3AcJisYwrst28eVPJUOru7p50K51WEQS+LebExsYyMDCgvE6dTue0O4a8/dyCgoKor68nNDRUCROfyX3OtDvIcx0PHjwgKCgIGM1Icv/pLmrOxWIOftbKCqusrFRyperr66mvrw/YbAbz6hhO/cMxWq49pOqXd2gsambD8bX+XpafvPHNn+mMFnru+XEtUzOZTJoT2MB3J1eEECLQSKFHCCG8TKtbYTZjKGdqsryM69evo9PpcLlcbN26lSdPnpCUlERDQwMvXrwgKSmJuro6Kioq0Ov1xMXFsXfvXvmA7AednZ28ePECYFyRzWKxcO7cOWC0eLNnz54p78uzCOIu5vT19XHlyhUMBgNvvPHGjDuGvP3cMjMz6ejoIC8vD71eP+37m213kNo63MZ2ouzbt2/OnSmTBT+7QyzPnj3L7du35+V9YT5NlhWmlRc2nVHscxnXPqPHedDNkGWYyMRwjMHf5P0EL9Ug5gbgFrAPKP/msiz/LWeatCawzffJFc/XV1dXF2VlZfT19REfH8+BAweUfxtCiLlzZwR1dnaOywgCGBkZ4cMPP8RqtXLs2DFSU1P9vFr/k0KPEEJ4kVq3wsuXL3n+/PmsxlCqmSzw+eHDhzidTr7++mtCQ0OB0S1ZLS0tdHV1KduzsrKylLC8NWvW8N5776HT6Th16tTcfwhizpKTk73S/aC1/ctms1FRUYFOpyMkJGRWHUOzNdlzm+lznkt3kOc6+vv7GRwcBL7tTCkoKKCurm7OnSlawc/h4eEEBwcTFRWFXq+fl21ygWg6o9jnMq59Jvcx2DdE6f+6ibVnkODwIHKPZZN9eJU3nuYCFApcBP7pm6/fAH7PryuaDq0JbJ4nV44cOaJ6kPj06VPKysoYGBggNTWV/Px8pbtvMp6vr8uXLxMcHMzJkyf5/PPPKS0t5dixY/P99IXwKbVR606n02db+tUyggDu3LmjdC2LUVLoEUIIL1LrEli3bh0dHR2zGkOpZrLAZ5PJxL1793j27Jly/ffeew+DwYDNZlO6NKxWq3Km8fbt29hstmlNORILx2Tbv+Li4pTX6cDAAL/61a9m3THkT97sDppNZ8pMeXZX9fX1UVVVRUtLC6GhoQEzun0+t5pNZxT7XMe1z+Q+0rYk8wc/Vx/hvvRkMDpOfeFQm8DmcDhobm5WPbnieZBot9v54osvSExM5MCBA1y4cIGqqqop83w8X182mw2LxcKmTZuIiYkhPj6eBw8e4HQ6Z9ShKESg++ijj5SvCwsLyc7O5uXLlz7Z0q+VETQwMEBtbS25ubnU1NR4/XEXKin0CCGEF3mrE2MykwU+m81m0tLSKC4uVoo669at4+uvvwZQtuJ8+umnvP3223z55ZdUV1ezb98+JahXLAye2wZKS0uVzhOAlStXam7/un//PiaTicTERPLz8wkLC/PLc5grb3YHzTfP7qr+/n6qqqpYuXIlGzZs4NKlS1RUVIwrMvnLfG01m84odpfLNedx7f4Y+S78Q20CW1ZWllLkHntyRe0g8fnz5wwODrJq1Sri4+NJSEigra1t0kKP2usrKCgIo9FId3c3DoeD3t5eXC4Xw8PDSnetEItBoP1uBaisrCQrK4vly5f7eykBRQo9QgixQKkF0H700UdKJsGRI0eIjo7m2rVrSiCl29DQEF9++SVVVVVs2bKF9PR0rFYry5Yt88dTEbPguW0AID4+niNHjgCjBx7eDlIWs6PWXTUyMgKAwWDAaDR6LfjZG+Zrq9l0RrF3dnbOeVy7P0a+C//QmsD2xhtvqFx7IqvVCqC8lk0mk7KFU4vW62vPnj1UVFRw+vRpgoKCMBgMhISEzOTpCLGgaWXonDt3Tun4Adi9ezcbNmzwymN2d3fT1tbGm2++qfx7FKOk0COEEAvQTAKftabifPHFFwB8+eWXfPnll4SHh8v0rAVCbVsKQFdXFx9//DExMTESpB1AtLaYbdu2jfr6elpaWoiKigq4Me7e3mo2nVHsAwMDdHd3z2lcuz9GvouFyX1yw114HRkZmbIDR+v1dfLkSQoKCnj58iVVVVUkJCQoE/yEWCq0MnQyMzPZtWsXAMHBwZPeh1bB6NKlSzx69Eg5wfX48WO++uorbDYb77//vnL7CxcukJKSQmdnJy6Xi7y8PDZu3OjlZxr4pNAjhBALzEwDn7WyR6SoszBpbUvJyMhg7dq1OJ1OioqKKCkpoaBAskcCwWRbzPLy8ny8mumZj61m0xnF7t5eWlBQMOtx7Yth5LuYH55BstHR0YSEhNDc3Ex4eDjPnj0bN31Qjdbrq6mpiYaGBkwmExkZGcpBrRBLhVaGDsDDhw958uQJZrOZ/fv3T9kRqlYwysrKorW1Vfn/0tJSoqOjSUlJ4fHjx+Tm5lJXV0d0dDTd3d0cPXoUg8EwZZfeYiWFHiGEWGB8EfgsApfWtoGMjAzlOsnJybS1tflphd6nFQ4MMlJ1PszXVrOZBl7PNhTbF8HaYmFSC5I9cuQIFRUVFBYWkpqayvbt2ye9D63Xl9lsDvgAe7H0BMJI8pycHHbu3InVauXy5ctcv36dV155RXNdDoeDuro6ZWul28qVK/mTP/kTuru7+e1vf8v+/fvJycmhqqqKx48fs3HjRrZs2cL777/Ptm3bSE5Onpfns1BIoUcIIXzMM0R3bPu322RnmX0R+CwCl9a2gXXr1imjxDs6Ohbstq2pQqajoqKUSXMpKSk+Gak6n1OoAtFC3WomxFS0fndON9NHiIXI3yPJx3bJuQeFzHZdp0+fVrIok5KSJny/v78fgNbWVurq6li2bBk7d+5ckieBpNAjhBA+5hmi695C5XA4OHPmDAkJCf5cnghwWtsG6urquHjxIna7XWmNDkTDw8OcPXuW/v5+DAYD6enp5OfnYzQaGRkZ4ezZsxOCc8eGTD958oSrV68yPDzss5Gq051C9emnn+J0Ohd819FC3GomhBBiIl+PJPfcHmkwGLh9+zY5OTkMDQ3R3d1Nenr6rNellkU5ljv/JygoiPz8fIqLi7l69SrvvPOO157jQiGFHiGE8CG1EF13C3hTUxMjIyOsW7fOn0sUAW6ybSkLgV6vZ/v27cTGxtLQ0EBNTQ0ZGRlkZmZSUlIyYUIcfBsyHR0dzeDgoBIOXFZW5pORqtOZQqXT6YiKiuLQoUNKMciXXUdCCCHEdM3XSHK17ZEDAwOcO3cOGP0dOtkWR/e6xgY2uwtGHR0dE7Io1XK3oqOj0el0GAwG5c+lSAo9QgjhI1ohum719fVERESwYsUKP6xOCN8wmUxkZmYCo0Urg8FAdHQ0L1++5MGDB6Snp48bkeoOmR4eHubSpUvodDrefPNNLBbLrEaqanUUffbZZ3R1deFyuUhKSuLQoUMTJoNMNYXq6NGjREREKMUgX3YdCSGEENMxnyPJ5xItMHZdY6doFRYWkpaWRn9//4Qsyl/96lfjrpednc3hw4cpKyvjzJkzREdHc+jQoTk9p4VKCj1CCOFlnhkj169f5969e9jtdvR6PWFhYcrWFPefz58/58WLF+zYsUPGsYp5pVXoePr0KdeuXWNgYIDU1FTy8/OnHIE6Wx0dHVy4cAGHw0FqaioRERGcP3+e0NBQMjIyJoRMu8OB3f829Ho9nZ2dqiNVCwoKMJvNmo+t1VG0fPlydu/eTVdXFyUlJdTW1o4bHz7dKVSHDx8eVwzyVdeREEIIocaz62VwcHBWvz/9sS5g0nVpFZY8p45qhVIPDQ1RXl7O48ePF90odin0CCGEl3lm8KSlpZGTk8OtW7dobW3l008/Va773nvv8cd//MfU19djMBhYu3atv5Ytlgi1QkdqairXrl1TOlnOnz9PZWXlvOX8mM1mTp06xYMHD7h16xZNTU10dXXhdDopLS1VrucOmY6IiFDCgQF+9atfsX79euWDXFtbG9XV1ezdu3fKgopWR9HYywDlwydMfwpVf3//uGLQbLuOhBBCCG/x3E61YsUKzd+fvpzSNZN1eYNa+POVK1fo7OxclKPYpdAjhBBepJbBk56eDsCWLVtob29n1apVDA0NKSG6Q0NDNDc3k5mZSUhIiD+XL2ZIaxrT06dPKSsrG9cdExQU5O/lAuqFjoiICGw2G2lpaZjNZmJiYuatMNHZ2cnQ0BCRkZEYjaMfQxwOh2o2z759+3jy5AlNTU0YDAYlZNrzQ5/ZbB7XfTMVtY4iGO0gunHjBgaDgZycnHFrnmoKVWRkJA6Hg4GBAY4ePYrdbqewsFD1rKm7MKS1RWyp8eyCdE8zGxoaUn7OYnGb6zRKIYS2yf7teP7+dDqdPpvSNZN1zZVa+LPVaqW9vX3RjmKXQo8QQnjJZBk8v/nNb+jp6SEkJIScnJwJk7V+9KMf+XKpwkvUpjElJydTUlJCYmIiBw4c4MKFC1RVVbF3715/L1ehVejo6urCZrNhsViw2WzTvr+pRqKvX79eCV8cGhqitLQUq9VKcHAw69atIzs7W/mQNfYsXmZmJtnZ2XN6rp5ru3r1qvLvMzw8nEePHtHU1ERubi7l5eXcv3+fI0eOjBuTPnYKlbu4d+fOHaW4l5uby69//Wvl+u5iUFpaGmvXrqWlpYX79+8DkJiYyI4dO+jr61PdIrbQeP583c+rp6eH+Ph4Dh48SFhY2KT34dkFGRkZyZYtW7h+/bovnoIIAAt9GqUUqsRi4espXf602EexS6FHCCG8pKGhgeDgYFauXDkuYwRGpyNZLBYuX75MeXk5p06d8udShZeoTWOy2+0MDg6yatUq4uPjSUhIoK2tLaAKPZ5bp1pbW9m6dSvV1dU0NjZiMpmULUzT4XmQBuNHoo/tZkpNTVUO4sYKDQ1V1ubNwofn2vR6PXl5eQQFBVFSUgKA0WikrKyMxsZG9u3bR1xcHIODgwwNDU3o2Fq9ejVhYWEMDAwwPDxMTU0NUVFRynOyWq2cPXv2/8/ev0dFfeB9vu+7qiguUtxvigUCigga8kgQ76jpmMe2W/txTPpJMs/curNy9vnjrDWzz23PembPPuuctfas2evsWfPHXmvvcdKZJ2e6n54k3canVZIYY6KAKAi2EEDjBQUUxeJeFFUFRZ0/6Pq1YBUiAgX4ef2DllW/3/dXoZsf3/pe2LBhA+Xl5cDEDXJbWxtHjhwhOTkZgMTERGByi9hSNPX9/eabb7BarRw+fJgzZ85QWVk5aTvcVMGqINeuXcvAwIASPS+JxbyNMlBd5vf7KSgoYMuWLUHn6C31RJXIs8zXlq5wWu6r2JXoERGZpalDbWNiYhgYGJj0Sd5HH33E9u3bsdvtk9ZByvLy5DamQAuS1Wo1vi6mnu9grVMRERFkZGSQk5PD4OAgFy9epKCgYNLrQg1xrqys5OHDh8bzenp6jK+///3vSU5OZufOnUaCI5hQLXCBChuv18uePXueiulZgv0CuW7dOi5cuMDw8DAw8Qvl+vXrjdlAVVVVAKxatYrt27c/VbGVkZFBe3s7q1evZvXq1dTW1nL9+nWj1ev69esAbNy48blbxJaaqe/v0NAQvb29bN26lZSUFLKysmhtbWV8fByz2fzU65+1iVCWv8W8jfLhw4dcvnyZLVu2YLPZ+Pbbb0lOTmbdunWTnreYE1Uic2E+t3QtpJdtFbt+2xARmaVgQ21LSkpobGw0frk3m83U19dz+fJlzGYzqampi6qyQ15csG1MgDGsd3R01KhWWQxCtU6dPXuWjo4OoqOjyc/Pf2rrRLDv9+zsbG7evElSUhJxcXG0t7fT0NBAQUEBq1evpr6+nq6uLn7/+9/zyiuvsG3bNk6ePElXV5dx3O3bt7Nq1aqnEipZWVmkpKQQHR1NQ0PDc19nqF8gAxVFV65coaGhgd27d2M2m0O2VUyt2PL7/dhsNqxWKzdv3gQmWo0C57x+/TorV64kJSWFsbGxp4ZOT9ciNp0XWQs/H4K9v4GE5pNJTr/fj9vtZsWKFU8dY7oqSHk5TPc9EO5tlI8ePQIgJyeH+Ph4vv32W9rb2yclehZzokpktuZiS9di3HI1dfjzcl/FrkSPiMgsBRtqu3r1ahoaGti2bRtFRUX8wz/8A06nk3/2z/5ZmKOV+RBsG1NqairR0dHcvn0bm83Go0ePnvoEOJxCtU69+eab074u2Pd7X18fZrOZxMREo2IjIiKCnJwcHA4HRUVFdHZ2cv/+fSN5A5CXl8e2bduAidJpq9X6VELF4/FQWFhIZ2fnrK5zul8gr169SkNDA7t27TJims6TFVvZ2dn09PRw9epVAKKjoykrKwMmPtl3uVxs3749ZOVUsBaxmSQCZ7sWfr4Ee38Dw+SfTHKaTKaQQ+YHBgbo7u6eVAX58ccf88477zA0NARMJCb7+/uNVjdZXkJ9DyyGbZSB2VI9PT3G9/TUQbSLOVElMltztQ1rsW25mukq9uVCiR4RkRcw10NtZWkJtY3pjTfeoLq6mtOnT2O329myZUs4w5wzU7/fPR4P4+PjtLW1Gc+5d+8ely5dIjc3l4yMDK5du0ZcXByDg4PGL0nt7e08ePDA2KIVqACZmlB5EaF+gQwMmdy8eTPZ2dm4XK6g1SYBUyu2+vr6uHr1KmazmW3btlFfX09lZSUHDx6kpaWFmJgYcnNzefDgQdDKqWAtYocOHXrm9cxmLfx8Cvb+Hj9+nKSkJO7evYvdbqejo4OsrKygbVsAxcXF5OfnA1BfX29sIqytrTWGebe0tNDS0qJBtstUqO+BxbCNMi8vj5s3b3Lu3DkiIiKwWCxPzS5bzIkqkdmai21YL+OWq8XG9DwlsqWlpf4rV67MYzgiIkvL2NgYTqfTaM3YsWMHHo/HaDWxWq3ExMTw13/912GOVOTFTf1+f+2116ivryczMxOfz8ejR4/IyMggISGBe/fuMTY2RmpqKm63G7/fz1tvvcXdu3eJj4/H5XLxzTffkJ2dzf79+42EyvDwMIcPHzbaoTo7O6moqHjuGT1Op9P4pDDwC+SRI0f4+uuvjfY6mEiQBKtwgqcrtuLj4+ns7DSSNQExMTH85Cc/4Xe/+928rgOfmmjbv3+/0R51/vx5bt++zV/91V/NuB0slJls0vL7/UHfX4vFwvnz5+nv7yc9PZ09e/Y812BvkcVifHycx48fYzKZcDgcVFdXc/jw4UmDlUP9/0xcXBy/+c1vyMvLW1atICLPq66ujqtXr/Luu+8yMjLCiRMnSE1NZXh4+JlbrkK1f8FExegnn3yCy+Xi4MGDy2ZTlslkqvf7/XNyE6GKHhGRWZrtUNv5GjwrMp+Cfb8HKmFWrFhBcXExX3zxBWNjY+zduxf4c6JkdHSUw4cPExERMamNLTk5md7e3qAtcF6vF6/XawxNdrlcDA4OGgmgZ7HZbEaC4cmtT6GSOqGuOVjFVmlpqdFelpCQwI4dO0hOTp73qpOp29JeZObPdGa6SSvY+wvLtwxeXg5PJjptNhvDw8OYzWbMZjNXrlyZlLwM9f8zAL/85S8XPHZZGqYm08fHx6mpqeHu3buMjY1RWFhotDcvJ4H5cYEPQXt6eqioqDASOFNn+G3bti1o+xdMbMSb2ko5E6GSR8HmB77yyiuzvNLFQYkeEZFZmu1QW5/PN+eDZ0XmW7Dv94KCAkZGRmhpaaGtrY2EhAS2b98OBJ9f5PV6uXz5MoWFhbjdbnp7e8nOzg6ZUOnq6jJuvOrq6mhtbX2uRM2LyszMDJm8KSkpWbA4IHRiebYzf0IJtUlr06ZNHD9+HL/fz9DQEA0NDXz//fe43e55rWISWWhPJjrfeecdPv30U6KiotizZw9fffUVFy5c4ODBg+EOU5awqcn0mpoabt68yeuvv47NZqOvry/MEc6N6bZcZWZmMjQ0ZMy/Cgg2w+/J9i+A4eFhmpqa2LhxI42Njc8dV6jk0dRzL3VK9IiIzNJsh9qmpqbO+eBZkfkW6vu9pKQkaNIjVPJmcHCQkydPAhPf/zt27CA2NlYzWJ4hVGJ5tjN/gpluk1Z7e/ukX0yio6PZvHkzNTU1L3ppIovG1ERnYNbeq6++SnJyMunp6dy5c4fx8fGQs6dEpjP1ewzghx9+YN26deTk5AB/3va41E235aq1tZXIyMinEj2hZvg9qba2lvz8/GcOgw4m2Oyg5zn3UqJEj4hImMzl4FmRxWa6ahh5fqESbXP5Hk+3Scvj8Uz6xWTDhg0MDQ0p0SPLRrBEZ2RkJBEREfT29uLz+ejv78fv9/PRRx8xPj5utH2o9VpmItj3mNvtxufz0d3dza9//WusViuvvfbaotrWOVvP2nIVmN8TUFhYyNatW40ZfjU1Nezfv3/Sa3t7e7l37x4///nPjZ9Tc2Em515qlIoWEQmDJzf5HDx40GjFEBEJlyc3CN28eROA3//+95jNZqKjoxkbGwOYdpOWyFL1ZKIzsKzG7/ezY8cOHjx4wEcffYTT6cRsNrNmzZpJr01JSWHTpk3hCFuWkGDfY0+2CP3lX/4lK1as4Lvvvnuq0uVlsG7dOtLT08nJyTFm+E1t/wrM9fv1r39tVLRWVFQYFcRzee6lTr9ZiIgssPkYPCsi8qKCrbretGkTHR0dmM1m49PTHTt2hDNMkXkRalX6z372M44cOcLQ0BB1dXVkZGQ8tfJdrdcyE6G+x1avXs3IyAgWiwWz2YzJZFr2yfSpCRyLxUJ9ff1TM/ymtn+tWbPGqAi6d+8eDQ0N7Ny587nauGZ67qVO69VFROZAqE1aDx8+pLKykuHhYex2O+Xl5TgcDmNmScDUwbMw/dpnEfmzUFs0rl27RlNTEx6Ph+TkZMrLy+dkK9XL5OLFi3z//feTHouIiOCdd96ht7eXiooKioqK2LRpE4mJieEJUmQOhFqVfvPmTVpbW7FareTk5LB9+3b++Mc/GiujA+ueOzs7qaioUOuWhBT4HvN6vXz11VeMjY1htVrJzMzE6/Xy8OFD/H4/ZrOZ9PR09u7du2w/8Dt27Nikv69fv57h4WG6u7sBWLlyJeXl5cTGxi6rcz+L1quLiCwywTZpZWZmcv78eVauXMn27dupqKigra0Nq9XKunXrKC8vZ2xsjKqqKpqbm/H7/Wzbtu2pLV0ii5HH4+HEiRM4nU4sFgvZ2dlGInNqcjMyMnLe45m6RcPj8XD58mVWr15NWVkZJ06coL6+/pnD0mWyYFU+hw4dora2lh9++AGAlpYWWlpaNJNJlrRQq9LT0tJUxSZzIvA9Njo6yt69e0lJSaG1tZXGxkbeeOMNNm3aRFJSEr29vZw9e5ampiZ27twZ7rDnRTh/XrwsP6uU6BERmQPBNmmNjY0xMjLC2rVrycjIICUlhZGREdatW0djYyM5OTncuHEDh8PB/v37sVgsxqeJIoud2Wxmy5Ytk25Us7KyuHTpEitXrmTPnj1UVFRQV1c37zeqwbZoWK1WbDYbUVFRJCQkYDabl/wGjXCw2Wy43W4+//xz/H4/77//PteuXTPauAKr1fv6+vj973//VFVjZ2cnFy9enJT4Ww5ra+XlFaztY3x8XK3XMmNWq5W8vDxg4v9jLRYLiYmJJCcnAxNVqoDxd5HZUKJHRGQOPblJK/CD2mq1YrVaSUhIoL+/3/ihHhMTQ0dHB6WlpWRmZoY5cnnZharQiYiIYHR0lE8++QSXy8XBgwex2+1Bb1Q9Ho+R3ExPTycjI4N79+6F5RNJs9lMYWEhdXV1tLW1ERMTQ2npnFRDv3RqamomrVaPj49/arV6sKrGlStX8t1337Fq1Sr27dvHqVOnqK2tZffu3eG6FJEXFmxl9NDQkNF6XVdXR2trq1qvZVpdXV1UVFTg8/mw2+1GC+BHH33E2NgYcXFxrFq1KsxRylKmRI+IyBx5cpPW4cOHcTqdAMbmBKfTic/n4+LFi9jtdmPjwt27d2lubmbFihVs3boVu90etmuQl1ewCp2cnBzy8vK4du0aHo/nqddMvVENVGoEKmesVmvYqtT6+/upq6sjNzeXV155hTNnzlBdXT2pJUOera2tDafTOWm1+tq1axkYGJiU6AlW1TgwMIDX6yUrK4u0tDSSk5PndB2uSDi8LG0fMr/S0tI4evQod+7c4cqVK9y4cYNNmzZx9OhR+vr6OHv2LLW1tWo3lllTokdEZA4E26SVmppKdHQ0t2/fxmaz0dfXx9q1a0lKSuLKlSvGRP++vj78fj8jIyN89dVX/OQnP+EPf/jDpOPHxsbi9XpJT09nz549xhwBkbkSqpR8eHiYpqYmNm7cSGNj46TXTL1RzcrKAv6c3BwdHSUmJmZB4p/aTmEymQCwWCxERERgMpmM1ooXEaryqbm5edkNfh4fH6e2tpaysjI6Ojpm9JonqxrXrl3L5cuX6enpwev1Mjg4iNfrneeoRUQWN4fDgdvtJj4+noiIiV/HfT4ft2/fJjU1FavVislkMv5NZDb03SMiMgccDgePHz8GMDZqlZSU8MYbb1BdXc2pU6dITU3l1Vdf5cGDB8DEL6AJCQlYLBbKysr49ttv8Xg8XLhwwTjuO++8w6lTp4iOjubHP/4xX331FRcuXODgwYMLf5Gy7AUrJa+qqiI/P/+p1aXBblRNJtOk5OajR49Yt27dgsQerJ2itLSUlpYW2traSEhIYPv27S98nlCVT8tx8HNraytRUVHk5uYalTjTbWudWtVos9l47bXXaGho4Pr168bcJBGRl5nb7ebChQu4XC6ioqIoKioiPz/f+MDQYrEYP09EZkuJHhGROZCZmRmynPvtt9+ms7OTCxcucOLECeOHekFBAWlpaVRWVvL1118THR2N1+vF6/USHR1tDED1eDxkZWWRnJxMeno6d+7cYXx8HLPZvMBXKcuBw+GYNFi3pqaGmzdv4vP5SExMZO/evQwMDBil5Pfu3ePnP//5Uy03wW5Ui4qKSE5Oprq6mtOnT2O329myZcuCXFeo//2VlJTM6XlCVT4tx8HPAwMDdHd38+GHHxqPffzxx7zzzjsMDQ0BE98H/f39rFix4qmqRq/XS3Z2Njk5OQwODnLx4kWtnRaRl57dbg86w+ntt98OQzSyXJmm+2RmqtLSUv+VK1fmMRwRkZfPk1UUERER7Nq1i+vXr/Pw4UPeeOMNzp49S0xMDO+99x4nTpygp6eHf/JP/smCtcTI8nLy5Em6u7vx+Xy8//77dHZ2GtU31dXVrFy5kuzsbC5dusTWrVu5fPnyU8c4cuQIaWlpYYh+cZha+bR//36+//576urqMJlMxMTE8LOf/cwYrrlUOZ1OY8ZSYLX6kSNHaG5uNlarB/z0pz81qhkDSkpK6O3tpaOjg+joaPLz8yktLX0qSd3X18e5c+ee2tjldrupqqri/v37+P1+SkpKKC4unt+LlrAY843zf/4vddzoGsQ7Ns7xf1lOZpJ+xonIy8VkMtX7/f452Ryhih4RkTALzDm5dOkS7e3teDwe4uLiePjwIWvWrMFms+F0Ovnoo4+IjIzEYrEQHR0d7rBlCQo2WDcwK6q3t5fx8XEePHhAX18fRUVFrF+/3tgId+/ePRoaGti5c+dTbVwvm2BDNOd78PPUSqyBgQHOnz9PX18f6enp7N27l9jY2Dk7H0xULAVarZ68lr1797J3796nnj/bIbXBNnZlZWXR2NiIw+Fg//79WCyWsA32loWxc30a6fFRfNP8KNyhiCyo8fFxTp48icPhwOfz8e677xofFATbevkyCfXenDx50th0B7B9+3ZeeeWVWb9mOVKiR0QkjJ6ccxIYUvrkJpu/+7u/w+/3s3r1ajZt2kRdXR0ZGRnGoFmRmZpusO5nn31GX18f0dHR/OVf/iUZGRnGvwUqx9LS0rSenOCziSwWi/F1Lgc/P2nqivNvvvkGq9XK4cOHOXPmDJWVlUt2o1iwjV0ul4uOjg5KS0uNZKMsXxEWM/+8PI//45ub4Q5FJCyys7OJjY3lzp07kx4PtfXyZRLqvcnLy2Pbtm0AxtbPZ70mNzeXoaEhent7qampIScnZ9kmgZToEREJoyfnnFitVnJzcykuLuarr74yNgitWLGChw8f0tPTQ05OjvFDTeR5TDdY98CBAwwODvLNN99QVVXF0aNHgx5juvk+SUlJ7Nq1i/T09AW7pnAINpuooKCAkZGROR/8HDC1Eitwk7p161ZSUlLIysqitbV1yc/uenJjV6A66e7duzQ3N7NixQq2bt360n2aLSLLn9lsZvPmzdTV1U16fLqtly+LUO8NQHt7Ow8ePCAtLY3du3cbs/Gme03gg66oqKinqkSnSxwtRUr0iIiEUaiBfP/0n/7TMEQjy1mowbo7duxg5cqVM1rnOrWqJCsri8LCQtxuN1988QUNDQ1LtqpkpkL9b7akpGTOBz9D8EqswM1p4KbWarXi9/txu92sWLFizmNYCFM3dgWSkJGRkZSXl3Pu3Dm+++47/uZv/ibMkYqILIza2tqgWy8FCgsL2bp1Ky6Xi2+++Yaamhr2798/49d8/fXXT/17qMTRUqVEj4iIyEuguLiY/Px84M+DdQ8dOsSFCxeorq7GbDaTmprKzp07g75+uvk+LpcLk8lEcnLywlzMSyRYJVZgRtfo6KjxNbDafikKJHme3NgVGRlJYmIiJpMJi8VifJXl6+5jJwOuie/p+30uIiPMpMYt/U/VRWajt7c35NZLgXXr1hl/Tk5Opre397leExMTg8vlMv4+m8TRYqdEj4iIyEsg1GDdUG1aT5rpfJ81a9bMbdAStBLr+PHjJCUlcffuXex2Ox0dHWRlZS3Zti2Hw8Hjx48BjM1dJSUlvP7661RWVnL8+HESExPZt29fOMOUefbO/1Zt/Pn/8vEVDv5FJv/2yNKekSEyU/39/UbL/uDgICMjI3i9Xn79618bz6moqHgpt15OfW8sFgv19fVGRXFvb6/xwdNMXzO1bWs2iaPFTuvVRUREZFrNzc3cvHmTw4cPc/78eW7evMkvf/lLLBYLQ0NDxnyf2NjYGSWOZOZCrTi3WCycP3+e/v5+0tPT2bNnj5HIE1kKxv7Yxdgfbkx6zPIXK7Ee3hCmiETC59ixY5P+vmbNGqMd+MmtlwUFBdO2WC9HU9+b9evXMzw8THd3NwArV66kvLx80ubJ6V7j9/uJiopieHiYn/zkJyQlJU1KAn311VdkZ2eHpaJnLterK9EjIiIi07p48SLff//9pMciIiKM+T5er5evvvqK+Ph4fvazn4UpSpnK4/Fw4sQJnE4nFouF7OxsysvLcTgcVFZWMjw8jN1up7y8nMjIyHCHK8ve5N9d/KP/FUZyAfBddzD25S2s/6gQy6aMYC8WEZkTU5NA+fn53Lt3z9h+m5mZyb59+zh37tyCb+Kay0TPy5UOFBERkef2ovN9JDzMZjNbtmwhJSWF1tZWGhsbWbVqFVVVVfj9fiIiIrh79y7R0dH09fXR09OD3+9n1apV7Nu3b1lsHZHF5l8BbwBgsqaAdeJXEV/rY4i1Yi58uVpSRGThffDBB5P+Pj4+zrVr1+jp6eHOnTvs2bPHqA5aypu4lOgRERGRab3IfB8JH6vVSl5eHjDx39BiseD1evH7/ezYsQOn00ljYyN37twhLy+P7du309PTw/nz52lqaqK0dE4+VBR5wn8Gfg2UA/93AMa7h/HfG8Cyew0my9KcMyUiS9dsVrgvBUr0iIiIiCxTXV1dVFRU4PP5sNvtxvr1hIQEAEwmE16vl127dgEYCb3AEEuRufP/BF4FKoH/HVgL/BzflftgNhHxWmZYoxMRedJS38SlRI+IiIjIMpWWlsbRo0e5c+cOV65cISsrC4CvvvqK8fFxoqOjjbXlfr+fS5cuYbFYKCwsDGfYsiy9/aev2Uwkem7i94zha3yEuSAFU/zSaosQkeVtqW/iUqJHREREZBlyOBy43W7i4+ONLS0mk4no6GhSU1Ox2Wxcv36dlStX4vf7qaqq4tatW7zxxhukpKSEOfrpORwOPv/8c/x+P++//z7Xrl2jqakJt9tNSUmJ2s4WnVbgCrALqPrTY/n4Gh+B14eldHX4QhORedU+2M7/euV/4f7wA6IsUbyR/Qb/YtMvgz73RbbxjY+Pc/LkSRwOBz6fj3fffZe4uDjjZ5vX62XPnj0UFBQ89drZrHBf7JToEREREVmG3G43Fy5cwOVyERUVRVFREenp6bz66qtcv37d2CaSl5dHZWUl169fZ9euXaSmpjIyMkJMTEyYryC0mpoazGYzPp8PgPj4eDZv3kxNTU2YI5PgYoAvgf8DiMHheIfPPx/A7z/N+//mfXp6eqg8fpyBgQHS09PZs2eP0UYoIkvb6LiXfdk/oiT9NU63neTzW8cpySjl1bRXn3quZWM6lrwk4M/b+Mx/+vtMZGdnExsby507d4zHUlJSiI6OpqGhIeTrPv30U+PPp0+fNtaxnzx5EpjYxLVjx44Zx7EYKNEjIiIisgzZ7Xbee++9SY91dnbS3NxsJH9yc3MpKiriww8/BKCqaqLaYtWqVRw6dGjBY56JtrY2nE4nOTk53L59G4C1a9cyMDCgRM+ilQP8xvhbTc1JzOZuI1H3zTffEBUVxc9+9jO++uorLly4wMGDB8MTqojMqbWJ61ibONEG9WrqX/BFWwVO71DQ55qsFrBOtBM/7za+UEOVCwsL6ezsnPa1UzdxLQdK9IiIiIi8JIIlf2Dp3OSOj49TW1tLWVkZHR0d4Q5HZmFqos7r9TI4OMirr75KcnIy6enp3Llzh/HxccxmbeESWS6GR4f5bzd+y6rYTEozpm+v1Ta+F6dEj4iIyEuur6+Pc+fO0d/fT0REBAUFBWzbto0zZ85w//59/H4/qamp7N27l/j4+HCHKy+x1tZWoxKpvb0dmBgiLUtDsERdZGQkERER9Pb24vP56O/vx+/34/F4FnX7oIjM3PDoMP/TxX/DoHeQf7f73xMVET3t87WN78UpPSYiIvKS8/l85Ofn84/+0T9i7dq1NDY2cv/+ffLz8zly5Ah79+7l4cOHNDU1hTtUeckNDAzQ3d3Nhx9+yM2bNwH4+OOPcblcDA1NtAK43W76+/vDGKWE8mSiLpCg8/v97NixgwcPHvDRRx/hdDqxWCxER0//i6CILA2uURf/tvrf8MD5gP/ra/83rGYrrlFXyOe/yDa+qUOVXS4XTqeT4eHhiVhcLgYHB2d/MUuI6Xk+BSktLfVfuXJlHsMRERGRcLpz5w5nz57ljTfeIC8vD4De3l5+97vfsXv3bq3dlrByOp2MjIwAUF9fT3t7O0eOHKG5uZkffvhh0nOXSjvay+TixYt8//33kx6LiIjgZz/7GSaTiaGhIerq6sjIyGD37t1hilJE5lLT40b+tvpfT3rsnYL3eK/wHwd9/ljdfca+uIn1n7yKJXfmg5gBjh07Nunv69evZ2hoyFg+AGCz2YK2MC8GJpOp3u/3z8naSCV6REREBACv18sf/vAHxsbGeOutt4iIiOCjjz5ibGyMuLg4fvzjH5OYmBjuMOU5eTweTpw4YVRKZGdnU15ejsPhoLKykuHhYex2O+Xl5URGRoY7XFnGQiXqbt68SWtrK1arlZycHLZv347Vag1ztCKyXIRavQ4wOjrKJ598gsvl4uDBg9jt9rDFOZeJHs3oEREREbxeL6dPn8btdnP48GEiIiZuEY4ePUpfXx9nz56ltraWN998c0HicTgcfP755/j9ft5//31qamq4efMmPp+PpKQkdu3aRXp6+oLEstSZzWa2bNlCSkoKra2tNDY2kpWVxaVLl1i5ciV79uyhoqKCuro6du7cGe5wZRmz2WzG2vQDBw4Yj6elpS251cUiS8XUn6eBLYtPehkqIIOtXge4du0aHo8nTFHNH83oEREReckFkjyDg4O8/vrrmM1mhoaGjNXVVqsVk8lkJH8WQk1NzaSNO1lZWRw+fJgf//jH9PX10dDQsGCxLHVWq5W8vDwSEhKw2WxYLBY8Hg8jIyOsXbuW9PR0MjIyuHfvXrhDFRGROTb15+l7773He++9x1//9V9jtVrDWsGyUAKr1xMSEiY9Pjw8TFNTExs3bgxTZPNHFT0iIiIvOYfDwePHjwE4deoUAEVFRXR1dTE4OIjFYmH16tWUlZUtSDxT1y/DxCdxMDFI0WQykZycvCCxLBddXV1UVFTg8/mw2+1ERU0MuAy0x1itVqOlRkRElodgP08DVXU3btxgdHSUoqKicIYYVrW1teTn55OU9HyzgJYCJXpERERecpmZmYumbDvY+uWAzz77jL6+PqKjo1mzZk2YIlya0tLSOHr0KHfu3OHKlStkZWUBE7MJAl+1ylpEZPmY7ucpQEtLC3FxcS/tz9Pe3l7u3bvHz3/+c9rb28MdzpxTokdEROQlEWwob0lJCZ9++umk58XFxfHuu++GJcYn1y8HbrwCiyMOHDjA4OAg33zzDVVVVRw9ejQsMS41DocDt9tNfHy80X5nMpmIjo7m9u3b2Gw2Hj16xLp168IcqYiIzJXpfp52d3fz+PFjysrKMJlMc3bOUEOPq6qquHXrFl6vlz179lBQUDBn55ypqavXR0ZG8Hq9/PrXvzaeU1FRwZEjR0hLS1vw+OaaEj0iIiIviWBDeXNycow1oy6XixMnTrB69eqwxTgwMEB3d/ekYZEff/wxO3bsYOXKlWGZF7TUud1uLly4gMvlIioqiqKiIoqKikhOTqa6uprTp09jt9vZsmVLuEOVZWQx/8In8jII9fP0F7/4BS0tLVgsFjZs2DDn5w029DglJYWVDgv2Rg980oWbiXXnlr9YifXw3McQzJMfap0+fZo1a9Zw5MgRAO7du0dDQwPbt2+nurqanp6eRb2dayZ0lyQiIvKSCAzlBYyhvImJiUa//vXr1wFmNZSwr6+Pc+fO0d/fT0REBAUFBWzbtg14vpuj4uJi8vPzgT+vXz506BAXLlyguroas9lMamqqtkM9B7vdbiTznpSZmcnbb78dhojkZRHqF77o6GgNVBeZZ6F+nrrdbm7fvk1eXh7R0dFzes7A0OO6urpJjxcWFtIZ087vus6ybds2ckbjGfvyFua8hZuNM12LelpaGqWlpYyPjzM2NobNZlvy27mU6BEREXmJTB3KG/ikanx8nOvXr7Ny5UpSUlKe+7g+n4/8/HyysrJobm42VnivXr36uW6OQq1fVpuWwNNrgq9du0ZTUxNut5uSkhJKS0vDHaL8ybS/8HV2hikqkZdHqJ+nAL/85S8XPqAIMy7LGL4VFnyXHkOsFXPh4mqRCvX/W09u52psbAxTdM9HiR4REZGXyNShvDdu3GDTpk20tbXhcrnYvn37rI6bmppKamoqMFEp0tLSgsfjWXQ3R1MTBRcuXOCHH34w/n3Tpk3s2LEjjBHKdAJrgn0+HwDx8fFs3ryZmpqaMEcmIiIzYR0YxX9vAMvuNZgs5me/YBGYbjtXqDbVkydP0tXVZTxv+/btvPLKKwsWsxI9IiIiL4lgQ3kDX1taWoiJiSE3N/eFzuH1emloaCA+Pp7s7GwqKysX1erSqYkCgPT0dN544w0AIiMjwxWaPMO1a9cm3TQ7HA6uXbtGf38/MPG9JyIi0D7Yzv965X/h/vADoixRvJH9Bv9i08JU8UwdemyxWBgfH2d4eBiAmNZB/GaIeC1zQeJ5UTPZzhWsTRUgLy/PaGOPioqa91iftDRSaCIiIvLCAkN5P/30U65du0ZRURHr16+nt7eXrq4uCgsLMZtnf2vg9Xo5ffo0brebgwcPMjg4yL179ygpKTE2fYRTW1sbTqeTnJycSY/39PTw+9//nm+//Ran0xme4GRa4+PjXLlyZdL357lz5zCZTPzoRz8CWJbrcZe6qb/wuVwunE6n8Qufy+VicHAwnCGKLEuj4172Zf+I/7DnP7Jr9S4+v3Wca4+vLci5P/30U1pbW4GJoce1tbV8++23nD9/nohxE7HtIzyIGcEUv7CJj5ma+v9bvb29xnauCxcuABPbuR4/fgz8ud0rISHhqWO1t7dz/PhxKisrF3y+jyp6REREXhKhhvImJydPO6RwJgJJnsHBQfbv34/ZbMbhcCya1aXj4+PU1tZSVlZGR0eH8XhOTg4bNmxgfHycs2fPcv78eWMLhyweVVVV+P1+cnJyjE9MBwcHefXVV0lMTDT+Pj4+/kLJSplbU7fcrF+/nqGhIaMyq66ujtbW1qD/vyQis7c2cR1rE9cB8GrqX/BFWwVO79CCnHu6+4mxuvuMfXGTnH+0bUFimY2ZbOfauXPnMyuVCwsL2bp1Ky6Xi2+++Yaamhr2798/r7E/SYkeEREReWEOh8P4dOvUqVPAxLyb2dwczYfW1laioqLIzc01Kj8CiYOAzMxM7t27t+CxyfTGx8e5c+eO8fVJjx8/ZuXKlcbfu7u7J/1dwutFE8gi8mKGR4f5bzd+y6rYTEozwj+sPmLLaiK2rJ6TY4WajQMvtgp9Jtu5ZmLdunXGn5OTk+nt7Z1xDHNBiR4RERF5YZmZmXN2czQfBgYG6O7u5sMPPzQe+/jjjykqKjLmEnV1dZGcnByuECWE1tZW4uLi2LVrFw0NDUZF1ubNm/njH//IgwcPjOf+4Q9/UHJBRISJJM//dPHfMOgd5N/t/vdERcztKvXFINRsnPlYhR4qsVRVVcXNmzcZHR0F/jyXqL6+nsLCQtxuN729vWRnZ89pPM+iRI+IiMgy5/F4OHHiBE6nE4vFQnZ2NuXl5URERLzQp159fX2cO3eO/v5+IiIiKCgoMIYOvshx50NxcTH5+fkA1NfX097ezqFDh2hububLL79kbGyMlJQUXC4Xv/rVrya9Tw6Hg8rKSoaHh7Hb7ZSXl2to8wIaGBigp6eHf/iHfzAei4iIIC8vj7Vr1zI0NERdXR0ZGRns3r07jJGKiCwOrlEX/7b639A1/IB/Xfa3WM1WXKMuVlhXLFgM8z0QOhyr0IMlllJSUmhpaTH+HmhTHR4e5uTJk8DEh2ELvdFTiR4REZFlzmw2s2XLFlJSUmhtbaWxsZGcnBzy8vJe6FMvn89Hfn4+WVlZNDc309jYSFZWFqtXr56XT9NehM1mw2azAXDgwAHj8b179xp/Hh0dpaOjY9L7lJWVxaVLl1i5ciV79uyhoqKCuro6du7cudCX8NIKlaS7ceMGra2tWK1WcnJyjCSjiMjL7nb/LW72/wDA31b/awDeKXiP9wr/8YLFEBgIXZL+GqfbTvL5reOUZJTyatqr83re6Vahv4hQiaXCwkLi4uKoqKhgz549FBQUzOl5Z0uJHhERkWXOarWSl5cHTCQ8LBYLiYmJL/ypV2pqKqmpqcDEp1UtLS04nU5++9vfMjQ0ZAzF9fl8nDx5kp6eHvx+P6tWrWLfvn0Lvmr0WYK9Tx6Ph5GREdauXUt6ejoZGRncu3dPiZ4FFCpJl5aWtuCfkIqILGYOh4PPP/8cv9/PifdPTmpXdnYPc6zy2IK1t4ZjIPRMVqG/LJToEREReQl0dXVRUVGBz+fDbrcbfeVz8amX1+uloaGB+Ph4srOz+eGHH0hPT8fj8dDZ2Ul3dzdJSUls376dnp4ezp8/T1NTU1hn9oQy9X0KJKOsVqvxdWRkJJwhioiIBFVTU4PZbMbn8wEYG+18Ph/Hjx8nIyNjwWOabiD0i7Z3TV2FPjIysmi2fYab9k+KiIi8BNLS0jh69CilpaV0dnZy48YN7t27R0lJCX6/f9bHDaxVd7vdHDx4kJGREXp6eti+fbuRJImNjWXXrl2kpqayZs0aAOPGbLGZ+j4F4gwMWRwdHSUmJiacIYqIiDylra0Np9M5aZtkoCLy4cOHjI6OUlRUtKAxPTkQ+v+14//91EDoQHvXf9jzH9m1ehef3zrOtcfXZnz8Tz/9lNbWVmBiNs7t27c5cuQIR44coaSkBGBOt31OTSy5XC6cTifDw8MAuFwuBgcH5+RcL0oVPSIiIsucw+HA7XYTHx9PRMTEj36fz/fCn3oFkjyDg4Ps378fs9mMw+F46rhVVVWkpaWRmprKpUuXsFgsFBYWzu1FzoFg75PJZCI6Oprbt29js9l49OjRpJWpIiIi4TY+Pk5tbS1lZWXGZsIntbS0EBcXZ3zYshBmMhD6Rdu7Fnrb56effmr8OTB0eWhoiK6uLgDq6upobW01KqnCSYkeERGRZc7tdnPhwgVcLhdRUVEUFRWxfv16MjMzAbh37x4NDQ3P/amXw+Hg8ePHAJw6dQqATZs2cfjwYVwuF83NzXR1dZGbm0tiYiJVVVXcunWLN954g5SUlLm/0BcU7H0qKioiOTmZ6upqTp8+jd1uZ8uWLeEOVUREXgLTrfS+desWXq+XPXv2MDY2RlRUFLm5ucZsmkC1bnd3N48fP6asrAyTyWQce763Yj3PQOjp2rsWk4WabzQXTM9Trl1aWuq/cuXKPIYjIiIiS9mTVTF3797l0qVLlJeX093dzfXr19m1axfZ2dlYLJYFb4EKtQ5+6g3zYtmYISIiL7fx8XGuXbtGT08Pd+7cMRI9ra2tDA8P09DQwJ49e+jp6eH777+f9NqIiAh+8Ytf8N1333H79m3+8T/+x0RH/7l16nb/LZocTcZWrC/aKvj/7Pyf530r1lSB9i7HSA//bve/Z1XsqgU9/2JiMpnq/X7/nGS6VNEjIiIicyZU9dCFCxeAiTYugFWrVnHo0KEFjS3UOviUlBSio6NpaGhY0HhERESmM91K787OTuPvxcXF5OfnA1BfX097ezuHDh3C7XZz+/Zt8vLyJiV5IDxbsaaaSXvXQgtVRXXy5EmjRQtg+/btvPLKK2GL81mU6BEREZE5Y7fbg/amL4Zy52Dr4D0ez1M3zCIiIktJYOgywIEDByb92y9/GbwdK9C61em8j88/Rqw1dsHbpp6nvWshZWdnExsby507dyY9npeXx7Zt2wCMhROLlRI9IiIi8lKZug5eRETkZTM67mXn6t1U3r/Aw+EuhkeHud53Y0Fbt15JK+YPf3V6wc43E6GqqADa29t58OABaWlp7N69G6vVGoYIZ0br1UVEROSlMXUdfGC7loiIyGI0Xyu9V8VmcrnrEj0jDv7RurcA6BlxzF3gy0xhYSE//elPKS8v58GDB9TU1IQ7pGnp7kZEREReCsHWwXu9Xrxe71M3zPHx8WGOVkREZP5Wej/ZNvXbG78B4P6Q2phDWbdunfHn5ORkent7wxjNsynRIyIiIi/E4/Fw4sQJnE4nFouF7OxsysvL+eKLL+jp6cHv97Nq1Sr27dsX1p72YOvgS0pK6OrqeuEbZhERkfkwXzPuXkkr5rc/+VQbr4KYWkVlsVior6+nsLAQt9tNb2/vom/91np1EREReSGjo6N0dHSQkpJCa2srjY2NvPHGGzx48IANGzbQ09PD+fPnKSkpobR0YQc9ioiIyNNcoy7+x+q/NTZerbJlsiJiRVg3Xi0Wx44dm/T39evXMzw8THd3NwArV66kvLyc2NhY4zlzsa1L69VFRERk0bBareTl5QETmz8sFguJiYmTHgOMT8dEREQkvBbrxquFEioxc+bMGaxWK36/n9TUVPbu3Tvjdu7FtK1LiR4RERF5YV1dXVRUVODz+bDb7cTFxQHg9/u5dOkSFouFwsLCMEcpIiIisDg3Xi20YImZ/Px8ysrK6O3t5ezZszQ1NbFz585nHmuxbevS1i0RERF5YWlpaRw9epTS0lI6Ozu5ceMGfr+fqqoqbt26xY9+9CNSUlLCHaaIiIiIkZhJSEiY9Hhubi6JiYkkJiYCE4OXX0S4tnWpokdERGSZcDgcfP755/j9ft5//32+++47Ojo6GBsbIz09nb179xqVNnN9XrfbTXx8vLGuPCIigsrKSq5fv86uXbtITU1lZGSEmJiYOT+/iIiIyFz56KOPGBsbIy4ujlWrXmxAdbi2dSnRIyIiskzU1NRgNpvx+XwAxMbGcuDAAbxeL19++SVXrlxh3759c35et9vNhQsXcLlcREVFUVRUxPr167lw4QIAVVVVAKxatYpDhw7N+flFRERE5srRo0fp6+vj7Nmz1NbW8uabb87odYtpW5cSPSIiIstAW1sbTqeTnJwcbt++DcDWrVuBiTk5VqsVj8czL+e22+1B15HP10pYERERWRpCDT0OtHZ7vV727NlDQUHBgscWLDHT1dVFamoqVqsVk8lkVCrPxKeffmr8+fTp08a2rpMnTwKQmZnJjh075vYiQlCiR0REZIkbHx+ntraWsrIyOjo6nvr3+vp6vF4vGzduDEN0IiIi8jILNvQ4JSWF6OhoGhoawhbX1MRMVlYWTqfTSPqsXr2asrKyGR9vMX3ApUSPiIjIEtfa2kpUVBS5ubm0t7cDE1U8AFevXqWhoYFdu3aRlZUVzjAXlanzjGpqarh58yY+n4+kpCR27dpFenp6uMMUERFZ0kJtoyosLKSzs5MhyyD/4c7/l54feoiyRPFG9hv8i02/XJDYFlNiZq4p0SMiIrLEDQwM0N3dzYcffmg89vHHHxs3Vps3byY7OxuXy8WKFSvCGOniMXWeUVZWltFD/8UXX9DQ0MCBAwfCHKWIiMjyNm4a57WEUg5s+jGn207y+a3jlGSU8mraq+EO7YWFals7efIkXV1dxvO2b9/OK6+8MqfnVqJHRERkienr6+PcuXP09/cTERFBbm4uR44c4dKlSzx8+BC/309CQgLNzc3ARFXP1atXsdlsQWfpLGdT36uCggIyMjJwOBxG1dMPP/zAhg0bAHC5XJhMphdepyoiIiLPljCWyJ6UPWTHZ/Nq6l/wRVsFTu9QuMOaM8Ha1gDy8vLYtm0bAFFRUXN+XiV6RERElhifz0d+fj5ZWVk0NzfT0tLC2rVr2bRpE7t376a3t5ezZ8+yceNGdu7cGe5ww2rqe9XY2EhsbCxr166lu7t70prTzz77jL6+PqKjo1mzZk0YoxYREVk+gg09Hh8fZ3h4GJj4kOVh70P+243fsio2k9KM0nCGO2dCta0BtLe38+DBA9LS0ti9ezdWq3VOz61Ej4iIyBKTmppKamoqMLHBoaWlBY/HQ15eHjBRKgyoKoXg71VERAS7du3iiy++AP48z+jAgQMMDg7yzTffUFVVxdGjR8MWt4iIyHIRbBvV0NCQ0b508cpF6u5chlj4d7v/PVER0eEKdUEUFhaydetWXC4X33zzDTU1Nezfv39Oz6FEj4iIyBLl9XppaGggPj6e7OxsAD766CPGxsaIi4tj1apVYY5w8Qi8V5GRkQwMDEyaZ1RZWUlNTQ15eXkUFRVhMplwOp18/PHH+P1+SkpKKC4untf4PB4PJ06cwOl0YrFYyM7Opry8nEuXLoV9/ayIiMiLmG7osWvUxf9Y/bd4hz3869f+FqvZimvUxQrr8p0puG7dOuPPycnJk6qL54oSPSIiIkuQ1+vl9OnTuN1uDh8+TETExI/0o0eP0tfXx9mzZ6mtreXNN98Mc6Th9+R79Zd/+ZfGe/Xdd9/R19dHYWEhbW1t/PDDD9y+fdv49/3792OxWBgZGZn3GM1mM1u2bCElJYXW1lYaGxvJyclZFOtnRURE5svt/lvc7P8BgL+t/tcAvFPwHu8V/uNwhjVngrWt1dfXGwsgent7jQ/r5pISPSIiIktMIHExODjI/v37MZvNDA0N0d3dTWpqKlarFZPJZCQs5svUFeXfffcdHR0djI2NkZ6ezt69e4mLi5vXGJ5l6nsVFxdHZGQkXq+X4uJizp8/j81m47XXXqO6upry8nK+/fZbSktLyczMXLA4rVar0Xpns9mwWCwkJiaSnJxMZ2fngsUhIiIy36beP/y0+2eT/t3ZPQyFYQpujgVrWxseHubkyZPARFv5jh075vy8SvSIiIgsMQ6Hg8ePHwNw6tQpAIqKiujq6jI+LVq9ejVlZWXzGsfUFeWxsbEcOHAAr9fLl19+yZUrV9i3b9+8xvAswd6rkpISurq6jNkAdXV1mM1m4uPjiY2NBeDu3bs0NzezYsUKtm7dit1un/dYu7q6qKiowOfzYbfbw54kExERmQ9T7x8CG0F9Ph/Hjx8nIyPjqddMTQ492YIdMF2LWLiEKyYlekRERJaYzMzMsN/MtLW14XQ6ycnJ4fbt2wBs3boVmBhubLVa8Xg84QwRePZ7Faj4GR4e5uDBg8Zg5sjISMrLyzl37hzfffcdf/M3fzPteeaiuiktLY2jR49y584drly5wo0bN9i0adPzX7SIiMgiFez+wWazAXDjxg1GR0cpKip66nWzSQ69zJToERERkecyPj5ObW0tZWVldHR0PPXv9fX1eL1eNm7c+Mxj9fX1ce7cOfr7+4mIiKCgoIBt27Zx7do1mpqa8Hg8JCcnU15eTkpKypxeR7AWuMjISBITEzGZTFgsFuPrs7xodZPD4cDtdhMfH2+03EVEROB0Oietnx0cHCQ+Pn4Orl5ERGRhPev+oaWlhbi4ONasWTPp8dkmh+Yy7pMnT+JwOPD5fLz77rvExcVRVVW1aBcmKNEjIiIiz6W1tZWoqChyc3Npb28H/ryi/OrVqzQ0NLBr1y6ysrKeeSyfz0d+fj5ZWVk0NzfT2NhIRkYGly9fNtrPTpw4QX19/ZwPlg7V1vX6669TWVnJ8ePHSUxMfGb72VxUN7ndbi5cuIDL5SIqKoqioiLWr1/P6dOnJ7WYtba2Gp9iioiILCXT3T90d3fz+PFjysrKMJlMxmtmmxyaa9nZ2cTGxnLnzh3jsdkuTAiVOPof/of/oeDYsWP+J576rz744IP/OJt4legRERGR5zIwMEB3d/ek/viPP/6YzZs3U1dXx+bNm8nOzsblcrFixfTrUVNTU0lNTQUm2qxaWlrw+/3YbDaioqJISEjAbDZjtVrn/Dqma+s6cuTIjI4xV9VNdrs9aALn0KFDM4pDRERksQt1//CLX/yClpYWLBYLGzZsmPSa2SSH5prZbDbucZ5UWFg464UJwRJHf/IZ8N//6c/9szo4SvSIiIjIcyouLiY/Px+YSGS0t7dz6NAhvv76a2Ciqufq1avYbLYZV594vV4aGhqIj48nOzubgYEB6urqaGtrIyYmhtLS0nm7nhcxm+omj8fDiRMncDqdWCwWsrOzKS8v59KlS4u2BFxERORFhbp/cLvd3L59m7y8PKKjoye9ZjbJocUuVOLoTw4C+4Ba4L8DnLM5hxI9IiIi8lxsNpvRG3/gwAHj8dm2FAVm5bjdbg4fPozT6aSuro7c3FxeeeUVzpw5Q3V19aRzLRazqW4ym81s2bKFlJQUWltbaWxsJCcnZ9Yl4CIiIktBqPsHgF/+8pdBXzOb5NBSVV1d/Tg3N/cdYCXwCfAfgLdncywlekRERCRsgg1EHh0dBcBisRAREYHJZDIGEi82s6luslqt5OXlARM3vRaLhcTERJKTk2ddAi4iIrIczSY5NB/6+/txu90ADA4OYrFYGB8fn9OFCZWVlb0XLlyoBTh27FgTUDzbYynRIyIiImETaiByaWkpLS0ttLW1kZCQwPbt2+fl3C+6En221U1dXV1UVFTg8/mw2+3PPE+o7WSdnZ1cvHiR4eFh7HY75eXlREVFzfAdEBERkZn49NNPjT+fPn2a9evXMzQ0NOuFCcESR7/4xS+yjx07VgKkApuA07ONV4keERERCZvpBiKXlJTM67lfdCX6i0hLS+Po0aPcuXOHK1eucOPGDTZt2hTy+cG2k61cuZLvvvuOVatWsW/fPk6dOkVtbS27d++el5hFREReVqHuVWYrWOIoNTU1GvgWMP/p67+c7fGV6BEREZGXzlysRIfQlTZVVVUhBys7HA7cbjfx8fFEREzcikVEROB0OkOWgAfbTjYwMIDX6yUrK4u0tDSSk5ONgdBL2dRKq2vXrtHU1ITb7TaqvYKtpgX47W9/O+lYcXFxxr+JiIg8S6jV59P9XJ+NYImjffv2/eD3++dk+4QSPSIiIrJsBWvPam9vx+v1kpiYyPj4+FOvmelKdAheaZOVlTXtYGW3282FCxdwuVxERUVRVFTE+vXrOX369DNLwJ/cTrZ27VouX75MT08PXq+XwcFBvF7vLN+pxaOmpgaTyYTf7+fDDz9k3759pKWl0dHRYbyfJSUlT62mjY2NNd4vl8vFiRMnWL16ddiuQ0RE5l+oxIzb7aaqqor79+/j9/spKSmhuHhmI2+CrT5fagsTlOgRERF5SYSqPjlz5oxxI5SamsrevXtnPUww1DkARkdH+eSTT3C5XBw8eBC73T6XlxdUsPasDRs20N7eTn9/v/H4s1aihxKs0sbj8VBYWBhysLLdbg/aw3/o0KFpzzV1O5nNZuO1116joaGB69evY7VajXlBS1Wg0ioyMtKYXbB27VrcbjcdHR3G84KtpjWbzcb1X79+HWBGyToREVnagiVmvv32WxwOB/v378disTAyMjKjY4VafT7dz/XFSIkeERGRl0So6pP8/HzKysro7e3l7NmzNDU1sXPnzjk9x+rVq7l27dqM2qHmSqj2rIsXL9Lf3w/A0NAQ8OyV6M/yZKVNdnb2nF9LsO1kXq+X7OxscnJyGBwc5OLFiy9cSh5O4+Pj1NbWkpOTQ2tr66R/y87OpqamZsbHuX79OitXriQlJWU+QhURkUUiWGLG5XLR0dFBaWkpmZmZc37Oq1evUlVV9VRr1w8//MDY2BgA77//Ph9++OFTr53rWT+hKNEjIiLykghVfRJY9R1oY0pOTp7zcwwPD9PU1MTGjRtpbGx8wSt5tkDSoKysbFIlCEysRB8dHeXGjRukpaXx+PHjZ65En87USpvA3J25FGo7WW9vLx0dHURHR5Ofnz/jsvTFqLW1laioKO7du0dqaioPHz6c1XHa2tpwuVzzsqlNREQWP6fTCcDdu3dpbm5mxYoVbN26dc4qiZOTk0lNTQ3a2hU4N/x5A+fjx4+Ne4zx8fEFSQAp0SMiIvKSCVZ98tFHHzE2NkZcXByrVq2a83NUVlaSn59PUlLSCx97JgJJg9zcXGNAcaA96+bNm9y4cYNdu3ZRVFRkvGamK1GfFKrSxuv1hhysPBvTbSdbLgYGBuju7gYmVs0GDA8PG5VXgNHSFWw17YoVK2hpaSEmJobc3NwFjF5ERBaLqKgoACIjI9m1axcnT56koqICYEbDlYP9fBkfHzd+rqelpRl/DoiOjjbawQMC7cSnT0/ekh643/D5fBw/fpyMjIy5unSDEj0iIiIvkVDVJ0ePHqWvr4+zZ89SW1vLm2++OWfnGBwc5N69e/z85z9fsK1QgaTBk5+avWh7VjChKm26urqeOVhZJisuLmZkZMRoswv4b//tvxlJOoCWlhY2bdoUdDVtcXExXV1dlJSUYDabFyx2EREJn6mJmcTERBITEzGZTFgsFiIjIxkdHTXaqmD64crBfr4MDQ1N+rlutVqN5wSqiDds2MDVq1cnHautrY2hoSEiIiKM8wcSQDdu3GB0dHTSh05zRYkeERGRl0Sw6pOhoSG6u7tJTU3FarViMpleqPUo2DkcDgder5df//rXxvMqKio4cuQIaWlpc3FpTykuLiY/Px+Y2KLV3t7+Qu1ZobwMlTYLxWazsXXrVqP9LPDf7a/+6q9obm7mhx9+MJ776aefhnzf9d9DROTlEiwx8/rrr1NZWcmJEydITEwkOTmZmzdvGs+bbrjyTH6O1NXVGUmdQBXxk5U5fr+f8fFxLl68yPj4OMnJyTgcjknHaGlpIS4ujjVr1jzX9c6EEj0iIiIviWDVJ0VFRXR1dRmlyatXr6asrGxOz7Fp0yaOHDkCwL1792hoaGDnzp3z2sZls9mMT8wOHDhgPB7OqppQG8mmKx9/2YT677Z371727t0bpqhERGQxC5WYCdx7AE9t0XoRUyuIenp66O7u5ssvvzSe8/HHH7N161Z8Ph9ms5n4+HgcDodRodrd3c3jx48pKyvDZDLNWWwBSvSIiIi8JBai+uRZ50hLS6O0tHReY1isQm0km658XERERBaXqRVEubm5pKSk0NPTYzxutVrp7e01EkKBwc0ff/wxv/jFL2hpacFisbBhw4Z5iVGJHhEREZEFEGoj2XTl4yIiIvJiQg1XDmzIunDhAufPn5/RoGaYeYuw0+k0EjlPtpG73W5u3bpFZGQkv/nNb4w17QDHjh37DtjzxGH+1QcffPAfn/ealegRERERWUDBtp6Njo4CEzebNTU1ZGdnU15ejsPhoLKykuHhYex2O8XFxVRWVj7V/nXmzBnu37+P3+8nNTWVvXv3vtCWLxERkeXiWcOVnxz4D9MPan4eodqRAX7xi19w7do1enp6Jq1p/5PPgP/+T3/un825legRERERWSChtp4FNkRt2bIFt9tttHVdunSJlStXsmfPHioqKjCZTEHbv/Lz8ykrK6O3t5ezZ8/S1NTEzp07w3mpIiIii8KzKnCeHKwM0w9qnitms9nYBBrEQWAfUAv8d4DzuY//YuGJiIiIyEw8uZHs9ddfx2w24/V6cTqdRkm53+8nIiICi8WCx+NhZGSEtWvXkp6eTkZGBo8ePaK4uJikpCQyMzMB8Hg85ObmGutkAZKTk8N1mSIiIjJ7/wl4Hfjln77+h9kcRBU9IiIiMmsOh4PPP/8cv9/P+++/z7Vr12hqasLtdlNSUvLSDl4OJthGspKSErq6uozy8cAne3a7naioKGBioGPg68jICBC8/eujjz5ibGyMuLg4Vq1atXAXJiIiMsem3l98+OGHTz1naqXO+Pg4J0+exOFwGHNvZjJzZzpXr16lqqrKOF5sbCy/+c1vjJ/HW7du5dVXX52zDZoffPDBbwN/PnbsWBNQPJvjKNEjIiIis1ZTU4PZbMbn8wEQHx/P5s2bqampCXNki8+zNpKNjY3hdDq5c+cOV65cISsrC/jz/J7R0VFiYmJCtn8dPXqUvr4+zp49S21tLW+++eb8X5SIiMgLCJWcOXXqlDE758MPP6SkpIQNGzbg8/k4fvw4GRkZQY+XnZ1NbGzspLk3083cCTWoeXh4GIC4uDgSEhLo6OgwXpOens7AwAD9/f0zOkcoU8+dnJxsPXbs2P8O/GcgFdgEnJ7xAZ+gRI+IiIjMSltbG06nk5ycHG7fvg3A2rVrGRgYeCkSPX19fZw7d+6pwciz+VTP4XDgdruJj483Ejcmk4no6Ghu376NzWbj0aNH5OXlGe1f+/fvx2w2MzQ0RHd3N6mpqVitVkwmk3EMERGRxW5qcqatrQ2fz0dsbCzDw8O8++67REdHY7VauXHjBqOjoxQVFT11nFBzb6abufOsQc33798nMjJy0jn+8i//km+++WZSomc2c32mnvvtt99eDeQD3zIxZudb4F8+10H/RHcBIiIi8tzGx8epra2lrKxs0qdcLxOfzxd0MPJsPtVzu91cuHABl8tFVFQURUVFFBUVkZycTHV1NadPn8Zut7NmzRquX78O/Ln9q6ioiK6uLuOTyNWrV1NWVjYv1ywiIjKXpiZnAvcXcXFxDAwMAFBVVcXu3buxWq20tLQQFxfHmjVr5uT8M1mVPnVY81yZem6TyXTX7/e/MRfHVqJHREREnltraytRUVHk5ubS3t4OPL2edLlLTU0lNTUVmGjLamlpwePxzOpTPbvdznvvvffU45mZmbz99tuTHpvJTamIiMhSdOvWLaKioti4cSO3bt2io6ODBw8eUFNTw6uvvsrjx48pKyvDZDKFO9RFTYkeEREReW4DAwN0d3dPGo748ccf88477zA0NARMVKn09/cbm6CWq2CDkUVEROT5BdqRu7u7jcd8Ph+9vb20tLRgsVjYsGFDyNc/a+aOy+VicHCQ+Pj4GcUT7Hh9fX24XC7j3x89emS0mc3mHPPB9DyfvpWWlvqvXLkyj+GIiIjIUuB0Oo2NE/X19bS3t3PkyBGam5v54YcfJj337bffDjrLJrChy+PxkJycTHl5OSkpKcDT2zYGBgY4f/48fX19pKens3fvXmJjY+f1GkPN4HG73VRVVXH//n38fj9WqxW/38/hw4eNm7rOzk4qKipeaPOGiIjIy6C/v5+mpiZaW1t5/fXXiYqK4vr164yMjPDw4UMsFgt2u53Ozk7y8vLYt29fyGMdO3Zs0t+nztwBiImJwe12z2ijV7DjTb3PiYiIIC0tbdI5bDZb0Erd6ZhMpnq/3z8n60qV6BEREZF55XA4ePDggTHLpqWlhf379/P1118b82ROnDjBmjVrjE1RJ0+epLu7G5/Px/vvv8/x48exWq3s2rWLM2fOkJSUxIEDBxY87p/85Cc0NjbicDgoLy/n8uXLOJ1ODhw4QHx8PJGRkXi9Xu7fv8/58+fZsmULa9euDeuneiIiIotZsGTK8PCwUdWzcuVKysvL5+wDnqn3GIHqnNHRUX73u98ZrejPs559Nuvgp5rLRI9at0RERGRWPB4PJ06cwOl0YrFYyM7Opry8nObm5qcqdYqLi4E/z7Lx+/3YbDaioqJISEjAbDZjtVqBp7d5DQ0N0dvby9atW0lJSSErK4vW1lbGx8cxm83zdn3BZvC4XC46OjooLS3FarUaGzcCg5FLSkpob2/H4XAAEwMc//jHP2IymfD5fCQlJbFr1y7S09PnLW4REZGlZCFnzwXbGGqz2QC4fv06fr+fjIwMHj16ZLxmJksWampqMJvN+Hw+AKOa58nk0bFjx4zk0cmTJydVAG3fvh2AY8eOWYHzQAkQBeR+8MEHd48dO/YdsOeJU/6rDz744D+GikeJHhEREZkVs9nMli1bSElJobW1lcbGRnJycrh8+fKkSp36+nrefPPNp2bZDAwMUFdXR1tbGzExMZSWlgbd5hVoEQskggKtUm63mxUrVsz7dT4Zd+DTxLt379Lc3ExKSgpbt27Fbrcbz+/q6sJisRifFHZ2dmKz2XC73XzxxRc0NDTMezWSiIiITPasjaGtra3ExcWxatWqSYmeZy1ZmE3yCCAvL49t27YBEBUVFXjYD5wCOoHJ2xjgM+C//9Of+6e71vn7GExERESWNavVSl5eHgkJCdhsNiwWC4mJiUErdbxeL6dPn8btdnPw4EGcTid1dXXk5uaye/duXC4Xv/3tb2lpaWFsbIzq6mqjBz46OhqY+FQs8NVkMhmPz6epcQcSPZGRkfz4xz/G5/Px3XffGc9/8mYvIDs7m+TkZBITEzGZTCQnJ8973CIiIjLZkxtDA+1Zga/d3d08fvyYwsLC59ro9WTyyGKxBD1nIHk0VXt7O8ePH6eyshKPxwPABx98MPbBBx/8z8APT70ADgJXgf8EJE0Xlyp6REREZNa6urqoqKjA5/Nht9uJi4ujsLBwUqXOq6++yunTpxkcHGT//v2YzWYjaWOxWGhpaTGONzAwYGytCDh+/DhJSUncvXsXu91OR0cHWVlZ89q2BX9O8jwZd2RkpJGwsVgsxleY/pPCzz77jL6+PqKjo1mzZs28xi0iIiJPC7Ux9Be/+MWkjV5NTU0zPuaTyaP29nbg6eRRWVmZcd8TUFhYyNatW3G5XHzzzTfU1NQ861T/Cfh/ACuBT4D/wNMVPwYlekRERF5iU4cHBjZhud1uSkpKKC2dfiZgWloaR48e5c6dO1y5coUbN24YlTqvvPIKZ86c4cKFCzx+/BiYPMumtLSUxsZGvF6vMcT4lVdeYf369TidTr7++msADh06hMVi4fz58/zhD38gPT2dXbt2ze8bw8R7Eyzu119/ncrKSo4fP05iYqKx/WO6m70DBw4wODjIN998Q1VVFUePHp33+EVEROTPiouLyc/PB/68MfTQoUO43W5u375NXl4ebrf7udazzzZ5tG7dOuPPycnJ9Pb2Thv7Bx988NvAn48dO9YEFE/3fCV6REREXmJThwfGx8ezefPmmXyyhMPhwO12Ex8fT0TExC1FoLrFYrEQERFhDCEONmhxfHycmzdvsnv3bjo6Ovjhhx+w2WzG9iqYSKykpaUBcOTIkTm55pnKzMwMOSAyWCyhbvZ27NjBypUrsVqtmEwm470SERGRhWOz2YzZOVNn5f3yl78EJm8AO3369FPr2evq6mhtbTWGLc82eVRfX09hYSFut5ve3l6ys7ON8x47dmwDkPKnv649duyYB/i3wH8GUoFNwOnprlV3GiIiIktQqI1XDoeDyspKhoeHsdvtlJeXG0mTqYIND1y7di0DAwMzSvS43W4uXLiAy+UiKiqKoqIiCgoKGBkZoaWlhba2NhISEoxNElNNVwGzFPT19XHu3Dn6+/uJiIggNzeXI0eO8Mc//pG7d+/i9/spLi6mubmZ6upqzGYzqamp7Ny5M9yhi4iILFpzsap8tp513PHxcU6ePMmvfvUrfD4f7777LmlpaVRVVfHw4UMAent7SUtLmzZ5NDw8zMmTJ4GJD5Z27Njx5Glan/jzWeBjwA58y8Sc5W+BfzldnEr0iIiILEHBNl5lZWVx6dIlVq5cyZ49e6ioqKCuri5oYuFZmydmwm63G59oPamkpISSkpJnvj5UBcw777zD0NAQMJFM6u/vJzExcVYxziefz0d+fj5ZWVk0NzfT0tLC2rVrsdvtJCUl0dDQQFxcXMg2rak3st999x0dHR2MjY2Rnp7O3r17iYuLW+CrEhERCa9Qq8p9Ph/Hjx8nIyMjnOGRnZ1NbGwsd+7cMR6bbgX78yalPvjgg5lPgw5BW7dERESWoGAbrzweDyMjI6xdu5b09HQyMjK4d+9e0NdPt3lioRQXF3PkyBGOHDlilCwfOnSI2tpaKioqAGhpaeHTTz9d0LhmKjU1leLiYpKSksjMzAQmKq0KCwtZuXLlM18fuJGFiaTPrVu38Hg8vPHGG3R1dfHJJ5/wq1/9ipMnTxqJLxERkeUs2PbKQMvVw4cPGR0dpaioKGzxmc1mNm/eTEJCwqTHZ/qzf6GookdERGSJmrrxKioqCphIAgW+joyMBH3tYqimCdUrv3fvXvbu3Tsv55wPXq+XhoYG4uPjJ/XYT2dq29ylS5ewWCzGf0uz2UxaWholJSV8+eWXXLlyxRj6LCIishw9q9q4paWFuLg4ba+cASV6REREwmzqrJeCggK2bdvGyZMnjeF/ANu3b+eVV14x/j5141VWVhaAscJzdHSUmJiYoOcMNTywtraWH374AZi4oWppaZm3PvjlILCC3e12c/jw4RkNWg52I/tk0qehoYHx8XFKSkqw2+1YrVY8Hs98X4qIiEhYzXRVucn0/J1Ngdk6DofDmK0TFxdHVVUVt27dwuv1smfPHgoKCub0msJFiR4REZEwmzrrJTBvByAvL49t27YBGBU7EHzjlclkIjo6mtu3b2Oz2Xj06NGk9Z1PWi7VNOEUSPIMDg6yf/9+zGYzXq8Xr9cbcg0rTL6RDbTWbdmyhfv37wNw9epVdu3aRVZWFleuXMHr9bJx48aFv0AREZEFNNNV5bP1PLN1pksM3bx50/hQbSYr2MNBiR4REZEwS01NJTU1FZjYvNDS0mJUcLS3t/PgwQPS0tLYvXu30ZYVbONVUVERycnJVFdXc/r0aex2O1u2bAnbdS13DoeDx48fA3Dq1ClgYhB1V1dXyDWsEPxG9sKFC0aL3Kuvvkp2dja1tbX88Y9/NJI+IiIiy9lMVpVHR0fP6tiB2Tp1dXWTHi8sLKSzszPoa0IlhlpaWoy/z2QFezgo0SMiIrJITJ31Mj4+ztatW3G5XHzzzTfU1NSwf/9+IPTGq8zMTN5+++2FDn1ZCtVS96Jl3k/eyJ49e5ahoSF8Ph89PT0AXLt2jWvXrgGwefNmsrOzcblcrFixYm4vUEREZBEJVW0MGKvKF8p0iaG4uDgqKioWdauXEj0iIiKLQLBZL0+2XSUnJ9Pb2xvGCF8+oVrqpluhOhNP3sgeOnTIGJgd+PTyyJEjfP311zidTq5evcrVq1ex2Wxh/WRQREREZsbhcPD555/j9/t5//33J1XwBsz3/EMlekRERMIs1KyXy5cvU1hYiNvtpre3l9WrV/PJJ5/gdDqxWCxkZ2dTXl7OpUuXFu0gwVBVMdeuXaOpqQmPx0NycjLl5eWkpKSELZ5gVTqhWuqmK/N+XqE+vVRSR0REZG719/fjdruB+Z2tU1NTg9lsxufzAX/+me7z+Th+/DgZGRkvdPyZUKJHREQkzELNehkcHOTkyZPARKJh27Zt9Pb2kpKSQmtrK42NjeTk5Lxwhcl8ClYVk5GRweXLl1m9ejVlZWWcOHGC+vp63nzzzbDE86wqndmsTw8IlVhyu91UVVVx//59/H4/JSUlFBcXz9VlioiIyBSffvqp8eeZzNaZTWKora1t0iZNwPhA58aNG4yOjlJUVDTv16pEj4iISJhlZmbOuIQ3MLDXZrNhsVhITEwkOTl5zipMZuJ5kxdTq2L8fj82m42oqCgSEhIwm83GkOn59rxVOrNZn/6kUImlxsZGHA4H+/fvx2KxGO1bIiIiMj+et13qeRND4+Pj1NbWUlZWRkdHx1PHa2lpIS4ujjVr1rzAVcyMEj0iIiJLSFdXFxUVFfh8Pux2O3FxcQsew2ySF1OrYgYGBqirq6OtrY2YmBhKS0uDnsvj8XDixImn2tUcDgeVlZUMDw9jt9spLy8nMjJyxtcwkyqd2a5Pf1KwxJLL5aKjo4PS0lIyMzNnHLOIiIgsnOdNDLW2thIVFUVubi7t7e0A+P1+ALq7u3n8+DFlZWWYTKY5j3Uq87yfQUREROZMWloaR48epbS0lM7OTm7cuLHgMaSmplJcXExSUpKRqAgkLzZu3EhmZiYZGRnk5OQAk6tiDh48iNPppK6ujtzcXA4dOsT4+DjV1dVBz2U2m9myZQtvvfUWGzZs4NatW7S1tfH111+TmJjIwYMH6ezsfGorxnSmxhOqSifQUufxeDh16hR///d/T2NjI99++y3nz58HJj7NC7TbPeucgcRSbGwsAHfv3uW//tf/yu9///sFrcgSERGRuTcwMEB3dzcffvghN2/eBODjjz8GJqp5LBYLGzZsWJBYVNEjIiKyQEJVpzx8+JCLFy9Oqk6Jiop66vUOhwO32018fLyRnIiIiMDpdM75IMGZCJW8aG5uZsWKFWzdupX09PSnqmJGR0cBsFgsREREYDKZjPinslqt5OXlAX9uV/N4PIyMjLB27VrS09PJyMjg3r177Ny5c0Yxz7RK53la6mZyzkD7V+DTvcjISMrLyzl37hzfffcdf/M3f/PC5xIREZH5Nz4+zsmTJ3E4HPh8Pt59912Ki4sZHh6mo6ODsbExYGK7ptvt5vbt2+Tl5REdHb0g8SnRIyIiskAC1SlPDlO22+1cvHiRVatWsW/fPk6dOkVtbS27d+9+6vVut5sLFy7gcrmIioqiqKiI9evXc/r06ZD94vNlpsmL119/Peig6dLSUlpaWmhrayMhIYHt27eHPNfUdrVAEiww18dqtc54xk2owdddXV3z8h4GSyxFRkaSmJiIyWTCYrEYX0VERGTpyM7OJjY2ljt37gATH0jZ7XaSkpJoaGhgz549pKWlAfDLX/5yQWNTokdERGSBBKtOiYuLw+v1kpWVRVpaGsnJyUZf91R2uz1o8uHQoUPzGvdUz5O8mK4qpqSkZEbnC7Sr3blzhytXrpCVlQVgVAaNjo4SExMzo2PNVZXOTIVKLL3++utUVlZy/PhxEhMT2bdv36yO/fnnn+P3+3n//ff57rvvjE8R09PT2bt3b1hmOImIiCx3ZrOZzZs3P9U6Hmq5w0JTokdERGQBhRqm3NPTg9frZXBwEK/XG3Sz1caNG/ntb3876XhxcXG8++67C3oN85m8CHauqe1qJpOJ6Ohobt++jc1m49GjR6xbt+6FzzUfpkssHTly5IWOXVNTg9lsxufzARAbG8uBAwfwer18+eWXXLlyZU7+G4iIiMjSokSPiIjIAppanXL37l1ee+01GhoauH79OlarFZvNFnKzVaCix+VyceLECVavXj3jc4eaEfTFF1/Q09OD3+83WsiCzQgKmC55cfDgQU6cOEF/fz9ffvmlcY7m5maamprweDwkJydTXl5OSkrKM2MO1q5WVFREcnIy1dXVnD59GrvdzpYtW2b8PiwHbW1tOJ1OcnJyuH37NgBbt24FJjZ8WK1WPB5POEMUERGRMFGiR0REZIGEGqYc2FA1ODjIxYsXKSgoCLqW2+PxYLPZALh+/ToAGzdunPH5g80IysnJISkpie3bt9PT08P58+dpamoKue58tue4fPkyq1evpqysjBMnTlBfX8+bb775zOOFalfLzMzk7bffnlWMS934+Di1tbWUlZXR0dHx1L/X19cbVWG/+tWvjIqwbdu24Xa7qaqq4v79+/j9fkpKSiguLg7DVYiIiCxt/f39uN1uAAYHB7FYLIyPj89oQUawYc4Ax44d+9+AvwESgH/xwQcf/N1sYlOiR0REZIGEGqZ89uxZOjo6iI6OJj8/f9Iv3k9utsrOzgYmbg6uX7/OypUrZ1QVExBsRlBiYuKkxwCGh4f55JNPnqr8cTgcVFZWTtoOFhkZOaNz2Gw2oqKiSEhIwGw2G4OU5fm1trYSFRVFbm6uMc8pMAz76tWrNDQ08Bd/8RdER0c/VRHW2NiIw+Fg//79WCyWGQ+xFhERkck+/fRT48+nT59m/fr1DA0NzXi5w9Rhzn9yDegF/scXiU2JHhERkQUSqjolVGXL1M1WgSqgtrY2XC7XtJuqQgk1I8jv93Pp0iUsFgsbNmwgKytrUlVOVlYWly5dYuXKlezZs4eKigrq6uqCrjQPdo7CwkLq6upoa2sjJiZm1hVDAgMDA3R3d/Phhx8aj3388cfGUMjNmzdTWFiI2WxmxYoVRkWYy+Wio6OD0tJSMjMzw3gFIiIiS9+LLHcINcz5gw8++M/Hjh1740VjU6JHRERkEQq22crr9RIZGUlLSwsxMTHk5uY+93Gnzgi6ceMGGzdupKqqilu3bvHGG2+QkZFhPD9QlePxeBgZGWHt2rWkp6eTkZHBvXv3giZ6gp2jrq6O3NxcXnnlFc6cOUN1dTUHDhx4offoZVVcXEx+fj4w0abV3t7OoUOH+Prrr4GJqp6rV69is9l46623jIqw2NhYAO7evUtzczMrVqxg69at2O32sF2LiIhIuAVrowp8EDY6Osonn3yCy+Xi4MGDS+ZnphI9IiIii1CozVZ5eXl0dXVRUlKC2Wx+7mMGmxFUWVnJ9evX2bVrF6mpqYyMjNDf3z+pKicwnDnQcmW1WoO2/QQ7h8ViMb5GRERgMpmM/nV5fjabzWizezJZNrVabGpFWKC9KzIykvLycs6dO8d3333H3/zN3yxc8CIiIotQiDYqrl27tiSXGyjRIyIisghNt9lqtqXCoWYEXbhwAYCqqioAVq1axY9//ONJVTlZWVnAxCdbga9RUVH8/ve/n7QC3m63c/78eSORE6hEKi0t5fvvv+fWrVsAjI2NcenSJQ0InifBKsIiIyNJTEzEZDJhsViMryIiIi+zUG1Uw8PDNDU1sXHjRhobG+f8vFOHOScnJ1uPHTuWDWT96Smrjh07tvaDDz64/bzHVqJHRETkJRFqRtDUxJHD4eDhw4eTqnJMJhPR0dHcvn0bm83Go0ePWL16NRkZGU8N/E1OTmZ8fJwf/ehHxsDfnJwcsrOzefDgwaTnJyYmUlNTw+joKFarlTVr1hAfH/9SJ/qjU9EAAJbuSURBVH8cDgeff/45fr+f999/n4GBAc6fP09fXx/p6ens3bvXaMOa7hjBKsJef/11KisrOX78OImJiezbt2/er0dERGQpqq2tJT8/n6SkpHk5/tRhzm+//fZq4P8H7PnTw/8z8H8Ccp732Er0iIiILAIej4cTJ048temqubmZpqYmPB4PycnJlJeXP9emrdkIVvlTVFREcnIy1dXVnD592ti6FWjpmsnA32Ar410uF6Ojo2zcuBG/309LSwsFBQV8++23OBwO9uzZQ3V1NbW1tTQ0NBjvyxdffEFPTw9+v59Vq1axb98+I5b5FOq/00w2ks1UTU0NZrMZn88HwDfffIPVauXw4cOcOXOGysrKZ843mq4i7MiRI7OKS0RE5GXR29vLvXv3+PnPf25suJxrU39Om0ymu36/f+9cHFuJHhERkUXAbDazZcuWSZuucnJyuHz5MqtXr6asrIwTJ05QX18fckvXXAlV+ZOZmcnbb7/91ONProCfycDfJ5+/cuVKAB49esTg4CAAnZ2dRrJo9erVbN++/an3JSkpie3bt9PT08P58+dpampakE1ewf47Pc9Gsmdpa2vD6XSSk5PD7du3GRoaore3l61bt5KSkkJWVhatra2Mj48/94wmERERCW5qG9XIyAher5df//rXxnMqKio4cuQIaWlp4QpzxpToERERmSN9fX2cO3du0syamc6gsVqt5OXlAX/edJWYmIjNZiMqKoqEhATMZrMxDHmxeN6Bv6GeHxERQUxMDGNjY1y/fh14Oln05Pvy5HsFGDdn8y3Yf6fn2Ug2nfHxcWpraykrK6OjowPAGHj95BBsv9+P2+1mxYoVk14frNqopKRkUmk4TLzXFouFsbExoxUssF1ERETkZTS1jWrNmjVGBey9e/doaGhg586d89bGNdeU6BEREZkjPp+P/Pz8p2bWNDY24nA42L9/vzGzJpiurq5Jm67i4uIoLCykrq6OtrY2YmJiFqRqZaaed+BvqOcnJCTQ09ODxWLBZrPh8/nweDxGsujMmTNUVFQAGO8LgN/v59KlS1gsFgoLCxfsuqf+d5rpRrJnaW1tJSoqitzcXKNMPDo6Gpg8BDswL2mqUFVh6enpOBwOxsfHAYiPj2f37t14vV6+/PJLrly5olk9IiLyUptu0UVaWtqiuv+aCdX8ioiIzJHU1FSKi4tJSkoy5tMEZtZs3LiRzMxMMjIyyMnJCfr6tLQ0jh49SmlpKZ2dndy4cYO6ujpyc3M5dOgQ4+PjVFdXL+AVTS8w8Nfj8XDq1Cn+/u//nsbGRl5//XW8Xi/Hjx/HbDYbSYRgz7969Spms5mxsTG8Xi9ms5ny8vJJyaKIiAhiY2MnvS9+v5+qqipu3brFj370o3mfW/Skqf+dAtVETyZjYmJinvu4AwMDdHd38+GHH3Lz5k0Ajh8/TlJSEnfv3qWnp4eOjg6ysrKCtm0Fqo0SEhKMaiOn08nIyAi5ubnG8/bt20dGRgZ2ux2r1bok18aKiIhIaKroERERmWPPO7MGJpIgbrd70qarQCVMINlhMpmMteWLwfMO/A32/AcPHnDt2jVgokKnv7+fR48eTdoOFRsby/bt23E6ncBE61FlZSXXr19n165dpKamMjIyMqvkyvMK9t8p2EaydevWPfexi4uLyc/PB6C+vp729nYOHTqExWLh/Pnz/OEPfyA9PZ1du3aFPMaT1UarV6+mpaWFsrIyo0Jo5cqVRlKsvr4er9fLxo0bnztWERERWbxMgd74mSgtLfVfuXJlHsMRERFZ2gLtScPDw8YMmk8++YTMzEy2bdvGuXPn8Hq9xsyagM7OzkmbrnJzc9mxYwd//OMfaWlpwePxkJCQwI4dO57aZrWchXpfPvzww0nPW7VqFYcOHQpbPA8fPqS6uhqn0/nURrKFNDY2htPp5M6dO1y5cgWbzcY777zDqVOnePjwIfv27SM/P5+rV69SV1fHrl27KCoqWvA4RUREljqHw8Hnn3+O3+/n/ffff+reBKZvCZvKZDLV+/3+OekRU6JHRERkjkydQRMfH09kZCQnTpwgNjaWHTt2cPbsWcbGxnj33XfDHa4sM09WG929e5dLly499RyLxUJJSQl1dXVs3ryZwsJCzGbzU4OdRUREZHonT56ku7sbn8/H+++/j8vlAiZmNh4/fpyMjAwOHjw44+PNZaJHrVsiIiJzJDCDBuDUqVMAlJSUTGpDSkxM1OBbmRdut3tStVF+fj5FRUU4nU6++eYbAA4fPszXX38NwNWrV7l69So2m4333nsvnKGLiIgsKW1tbTidTnJycrh9+zbw502gN27cYHR0NKwVs6roERERWUBTy3xramq4efMmPp+PpKQkdu3aRXp6erjDlBkKttK8vLyc5uZmmpqa8Hg8JCcnU15evqADo0VERGR+jI+P89lnn1FaWkpHRwc//PAD77//vrEo4fPPP8ftdvPOO+9gMplmfNy5rOjR1i0REZEFVFNTM2ljUlZWFocPH+bHP/4xfX19NDQ0hDE6eV6BleZvvfUWGzZs4NatW7S3t3P58mWSkpI4fPgwDoeD+vr6cIcqIiIic6C1tdWY0xconAl87e7u5vHjxxQWFj5XkmeuqXVLRERkgQQr883OzgYm1rCbTCaSk5PDGeK8mVrJNDAwwPnz5+nr6yM9PZ29e/caG8qWksBKc8BYaZ6YmIjNZiMqKoqEhATMZjNWqzXMkYqIiMhcGBgYoLu7e9Lw5Y8//phf/OIXtLS0YLFY2LBhQxgjVKJHRERkQYyPj1NbW0tZWRkdHR2T/u2zzz6jr6+P6Oho1qxZE6YI51egksnn8wHwzTffYLVaOXz4MGfOnKGyspIDBw6EOcrZeXKlud1uJy4ujsLCQurq6mhrayMmJobS0jmpxBYREZEwKy4uJj8/H4D6+nra29s5dOgQbreb27dvk5eXR3R0dFhjVOuWiIjIApiuzPfAgQP85Cc/AaCqqipsMc6XJyuZAIaGhujt7SUnJ4eUlBSysrLo6OhgfHw8vIHOUlpaGkePHqW0tJTOzk5u3LhBXV0dubm5HDp0iPHxcaqrq8MdpoiIiMwBm81GWloaaWlpHDhwgA8++IC0tDSio6P55S9/uSiWbijRIyIisgCeLPO9efMmMFHme/36dXw+H1arFZPJRETE8iq2fbKSyWKxADAyMgJgtDNZrVb8fj9utztscc6Ww+Hg4cOHmM1m479d4DotFgsRERGYTCaGh4fDGaaIiIi8RJbX3aSIiMgiFarM98KFC1RXV2M2m0lNTWXnzp1hjnRuPVnJ1N7eDmCUM4+OjhpfTSZT2MucZ2PqSvOioiIKCgoYGRmhpaWFtrY2EhIS2L59e9DXT51ddOHCBX744Qfj3zdt2sSOHTsW6nJERERkGdB6dREREZk3Fy9e5Pvvv5/0WEREBHFxcURGRrJr1y7OnDlDUlLSkp3R8yJOnjxJd3c3Pp/PSPT09/fzxhtvABPtfRUVFVrfLiIissxpvbqIiIgsCcXFxRw5coQjR44YG8YOHTrEj370I8bHx/nDH/5AfHw8u3btCnOkC2/q7KKAnp4efv/73/Ptt98yMjKi9e0iIiLyXNS6JSIiIvPGZrNhs9kAnqrYOXLkSDhCWhRCbWHLyclhw4YNjI+Pc/bsWaqrq433SevbRUREZCaU6BERERFZIB6PhxMnTjA4OAjA3bt3MZlMAPzmN7/B6/UarViZmZncu3dP69tFRETkuah1S0RERGSBmM1mtmzZwrp16/D7/dy+fZtbt24BE4Odc3Nzefz4MdXV1XR1dZGcnKz17SIiIvJcVNEjIiIis9LX18e5c+fo7+8nIiKCgoICtm3bxpkzZ7h//z5+v5/U1FT27t1LfHx8uMNdFKxWK3l5eaSnpxMTE8P3339PRkYGXV1dZGZmEliS8ejRIzIyMti0aRMPHz4kPj5e69tFRERkRpToERERkVnx+Xzk5+eTlZVFc3MzjY2NZGVlkZ+fT1lZGb29vZw9e5ampqZltzb+RUxtxdq/fz/ff/89dXV1mEwmVqxYwc9+9jPi4uLo7Ox8ofXtIiIi8vJRokdERERmJTU1ldTUVAAyMzNpaWnB4/GQl5cHTAwcBkhOTg5bjItRoBXrzp07XLlyZVIr1iuvvMKZM2eorq7mwIED2O123nvvvaeOUVJSQklJSRiiFxERkcVOiR4RERF5IV6vl4aGBuLj440V6h999BFjY2PExcWxatWqMEc4ITAI2el0YrFYyM7Opry8nC+++IKenh78fj+rVq1i3759REVFzUsMDocDt9utViwRERGZN0r0iIiIyKx5vV5Onz6N2+3m8OHDRvLi6NGj9PX1cfbsWWpra3nzzTfDHOmfByGnpKTQ2tpKY2MjOTk5JCUlsX37dnp6ejh//jxNTU3ztsXK7XarFUtERETmlRI9IiIiMiuBJM/g4CD79+/HbDYzNDREd3c3qampWK1WTCaTkfwJt8AgZACbzYbFYiExMXHSYzCRjJkvasUSERGR+bY47rxEREReEqHahx4+fMjFixcZHh7GbrdTXl4+b+1Dc8XhcPD48WMATp06BUBRURFdXV0MDg5isVhYvXo1ZWVl4QxzkqmDkOPi4gDw+/1cunQJi8VCYWFhmKMUERERmT1TYI3nTJSWlvqvXLkyj+GIiIgsb6Ojo3R0dExqH9q7dy8XL15k1apVlJSUcOrUKdatW8fu3bvDHe6yMzY2htPpNAYh79ixg40bN1JVVcWNGzd44403yMnJCXeYIiIi8pIxmUz1fr9/TnrHVdEjIiKygIK1D8XFxeH1esnKyiItLY3k5GTa29vDHOnyE2wQckREBJWVlVy/fp1du3aRmprKyMgIMTExYY5WREREZHaU6BEREVlgodqHenp68Hq9DA4O4vV6wxzl8hNsEPL69eu5cOECAFVVVQCsWrWKQ4cOhTNUERERkVlT65aIiMgCC9Y+5PF4aGhoACaqfmJiYvjrv/7rMEcqIiIiIgtBrVsiIiJLVKj2oYyMDHJychgcHOTixYsUFBSEOVIRERERWYqU6BEREVlAodqHzp49S0dHB9HR0eTn51NcXBzuUEVERERkCVLrloiIiIiIiIhIGM1l65Z5Lg4iIiIiIiIiIiLhp0SPiIiIiIiIiMgyoUSPiIiIiIiIiMgyoUSPiIiIiIiIiMgyoUSPiIiIiIiIiMgyoUSPiIiIiIiIiMgyoUSPiIiIiIiIiMgyoUSPiIiIiIiIiMgyoUSPiIiIiIiIiMgyoUSPiIiIiIiIiMgyoUSPiIiIiIiIiMgyoUSPiIiIiIiIiMgyoUSPiIiIiIiIiMgyERHuAERERJa7vr4+zp07R39/PxERERQUFLBx40Z++9vfTnpeXFwc7777bpiiFBEREZHlQIkeERGReebz+cjPzycrK4vm5mYaGxvJysrivffeA8DlcnHixAlWr14d5khFREREZKlTokdERGSepaamkpqaCkBmZiYtLS14PB5sNhsA169fB2Djxo1hi1FERERElgfN6BEREVkgXq+XhoYG4uPjyc7OBmB8fJzr16+zcuVKUlJSwhyhiIiIiCx1SvSIiIgsAK/Xy+nTp3G73Rw8eJCIiImi2ra2Nlwul6p5RERERGROKNEjIiIyzwJJnsHBQV5//XXMZjNerxeAlpYWYmJiyM3NDXOUIiIiIrIcaEaPiIjIPHM4HDx+/BiAU6dOAVBSUkJeXh5dXV2UlJRgNuuzFxERERF5cSa/3z/jJ5eWlvqvXLkyj+GIiIiIiIiIiLxcTCZTvd/vL52LY+njQxERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZUKJHhERERERERGRZSIi3AGIiIgsJx6PhxMnTuB0OrFYLGRnZ1NeXs7Dhw+5ePEiw8PD2O12ysvLiYqKCne4IiIiIrLMqKJHRERkDpnNZrZs2cJbb73Fhg0buHXrFnfu3OHs2bMkJCTw05/+lM7OTmpra8MdqoiIiIgsQ6roERERmUNWq5W8vDwAbDYbFouFuLg4vF4vWVlZpKWlkZycTHt7e5gjFREREZHlSIkeERGROdbV1UVFRQU+nw+73U5cXBwAPT09eL1eBgcH8Xq9YY5SRERERJYjJXpERETmWFpaGkePHuXOnTtcuXKFu3fv8tprr9HQ0MD169exWq3YbLZwh/lS6+vr49y5c/T39xMREUFBQQHbtm0DYHR0lE8++QSXy8XBgwex2+1hjlZERERk5pToERERmUMOhwO32018fDwRERM/ZiMiIsjIyCAnJ4fBwUEuXrxIQUFBmCN9ufl8PvLz88nKyqK5uZnGxkaysrJYvXo1165dw+PxhDtEERERkVlRokdERGQOud1uLly4gMvlIioqiqKiItavX8/Zs2fp6OggOjqa/Px8iouLwx3qSy01NZXU1FQAMjMzaWlpwePxMDw8TFNTExs3bqSxsTHMUYqIiIg8PyV6RERE5pDdbue999576vE333wzDNFIKE+2bo2PjxMZGUl2dja//e1vGR0dNZI8bW1tat0SERGRJUXr1UVEROSl4/P5yMvLIz4+HovFgtfr5datW7jdbrKzs9m6dSsAWVlZYY5URERE5Pko0SMiIiIvnfj4eO7evYvL5eLVV18FJuYr+f1+2tvbuXz5MgBnzpzh8ePH4QxVRERE5LmodUtEREReOg6Hw0jg1NfXG4+XlpYSGxtLR0cHd+7cISUlhaSkpHCFKSIiIvLclOgRERFZJjweDydOnMDpdGKxWMjOzqa8vByHw0FlZSXDw8PY7XbKy8uJjIwMd7hhlZmZyT//5/+c06dPMzw8zOHDh4mPjzf+vaCggM8//xyv12tsTxMRERFZCtS6JSIiskyYzWa2bNnCW2+9xYYNG7h16xZtbW18/fXXJCYmcvDgQTo7O6mrqwt3qGHn9Xo5ffo0g4ODvP7665jNZrxeL5WVlTgcDjo7O+nt7SU5OTncoYqIiIg8F31EJSIiskxYrVby8vIAsNlsWCwWPB4PIyMjrF27lvT0dDIyMrh37x47d+4Mc7Th9WTr1qlTpwAoKSlhcHCQkydPAhNVPzt27AhbjCIiIiKzoUSPiIjIMtLV1UVFRQU+nw+73U5UVBQwkQQKfB0ZGQlniItCZmYmH3zwQbjDEBEREZlzat0SERFZRtLS0jh69CilpaV0dnbidrsBGB0dNb7GxMSEM0QRERERmUeq6BEREZlDoQYiX7p0iVu3buH1etmzZw8FBQWzOs50g5UdDgdut5v4+HhjgLDJZCI6Oprbt29js9l49OgR69atm/f3QURERETCQ4keERGRORQYiJySkkJrayuNjY3k5OSQkpJCdHQ0DQ0Nsz5OVlYWly5dYuXKlezZs4eKigrq6uqMeTtut5sLFy7gcrmIioqiqKiIoqIikpOTqa6u5vTp09jtdrZs2TKfb4GIiIiIhJESPSIiInMo2EDkxMREYmNj+d3vfgdAVVUV9+/fp7y8nIiICEZHR/nkk09wuVwcPHgQu90+q8HKdrud995776mYMjMzefvttxfoHRARERGRcFKiR0REZI5NHYgcFxcHwPr167l69SqrVq3i1q1b5OTkkJeXx7Vr1/B4PM88jgYri4iIiMizaBiziIjIHJs6EPnGjRtYrVZWrVoFQHR0tFHpMzw8TFNTExs3bnzmcTRYWURERESeRRU9IiIicyjYQOSIiAicTif3798H4NatW6xcuZK4uDiqqqrIz88nKSnpmcfRYGUREREReRYlekREROZQsIHI69ev5/Tp03R1dRnPe/jwITdu3ODevXv8/Oc/p729/ZnH0WBlEREREXkWk9/vn/GTS0tL/VeuXJnHcERERJanJyt07t69y6VLl9i6dSuXL19+6rlHjhwhLS0tDFGKiIiISDiYTKZ6v99fOhfHUkWPiIjIAghV6ZOZmQnAvXv3aGhoYOfOnU+1cYmIiIiIzJQqekRERJYIh8PB559/jt/v5/3336empoabN2/i8/lISkpi165dpKenhztMEREREXlOqugRERFZJDweDydOnMDpdGKxWMjOzqa8vJyHDx9y8eJFhoeHsdvtlJeXG+vRZ6umpgaz2YzP5wMgKyuLwsJC3G43X3zxBQ0NDRw4cGAuLktEREREliitVxcREXkBZrOZLVu28NZbb7FhwwZu3brFnTt3OHv2LAkJCfz0pz+ls7OT2traFzpPW1sbTqeTnJwc47Hs7GySk5NJTEzEZDKRnJz8glcjIiIiIkudKnpERERegNVqJS8vDwCbzYbFYiEuLg6v10tWVhZpaWkkJyc/tVXreYyPj1NbW0tZWRkdHR2T/u2zzz6jr6+P6Oho1qxZ80LXIiIiIiJLnyp6REREXlBXVxe/+tWvuHjxIqtWrSIuLg6Anp4evF4vg4ODuN3uWR+/tbWVqKgocnNzCczWC3w9cOAAP/nJTwCoqqp6wSsRERERkaVOFT0iIiIvKC0tjaNHj3Lnzh2uXLnC3bt3ee2112hoaOD69etYrVZsNtusjz8wMEB3dzcffvih8djHH3/Mjh07WLlyJVarFZPJRESEfqyLiIiIvOx0RygiIvICHA4Hbreb+Ph4I9ESERFBRkYGOTk5DA4OcvHiRQoKCmZ9juLiYvLz8wGor6+nvb2dQ4cOceHCBaqrqzGbzaSmprJz5845uSYRERERWbq0Xl1EROQFdHZ2cuHCBVwul9FetWPHDs6ePUtHRwfR0dHk5+dTWlqK2Tz3HdMLufVLRERERObHXK5XV6JHRERknoVKxly6dIlbt27h9XrZs2fPrKp+RkdH6ejoICUlhdbWVhobG9m7d68xL6ikpIRTp06xbt06du/ePQ9XJyIiIiIvai4TPWrdEhERmWeBFexPJmNycnJISUkhOjqahoaGWR97IbZ+LUd9fX2cO3eO/v5+IiIiKCgoYNu2bcBE8uyTTz7B5XJx8OBB7HZ7mKMVERERmTklekREROZZsGRMYmIiycnJdHZ2vvDxu7q6qKiowOfzYbfbg2798nq9L3ye5cTn85Gfn09WVhbNzc00NjaSlZXF6tWruXbtGh6PJ9whioiIiMyKEj0iIiILIFQyZi7M99av5Sg1NZXU1FQAMjMzaWlpwePxMDw8TFNTExs3bqSxsTHMUYqIiIg8PyV6REREFsDUZMyNGzfYtGnTCx93IbZ+LWder5eGhgbi4+PJzs6msrKS/Px8kpKSwh2aiIiIyKwo0SMiIjLPQiVjnE4nw8PDALhcLgYHB4mPj3+uY7vd7klbv4qKili/fv1TW7+Ki4vn/LqWOq/Xy+nTp3G73Rw+fJjBwUHu3bvHz3/+c800EhERkSVLW7dERETmWagV7KdPn6arq8t4ns1m47333gtjpMtHqE1nY2NjVFVV0dnZyejoKGazmR//+MfEx8fz4MEDvvvuu6eOdeTIEdLS0hb+IkREROSlofXqIiIiMiuhtk2dOXOG+/fv4/f7SU1NZe/evc9dXbSYBFs7/8Ybb3Djxg0cDgevvvoqly5dmvSaTZs2kZ+fD8C9e/doaGhg586dFBQUGJVYIiIiIvNB69VFREQWqVCVJA6Hg8rKSoaHh7Hb7ZSXlxMZGbng8YXaNpWfn09ZWRm9vb2cPXuWpqYmdu7cueDxzZVgm85iYmLo6OigtLSU4uLikO1sDoeDq1evAlBYWEhNTQ03b97E5/ORlJTErl27SE9PX7BrEREREXkeSvSIiIjMIbPZzJYtWyZVkmRlZXHp0iVWrlzJnj17qKiooK6uLiyJlFDbpgJJkfHxcQCSk5MXPLa5NnXTWaCK+e7duzQ3N7NixQq2bt2K3W6f9LqamhrMZjM+nw+Hw0FzczMABw8e5IsvvuAPf/gDJpOJ9PR09u7dO6cb1ERERERelDncAYiIiCwF4+Pj/MM//AO/+tWvOHbsGENDQ8DE/J1PP/2U//Jf/gtff/014+Pj5OXlkZCQYFSSeDweRkZGWLt2Lenp6WRkZHDv3r2wXs/UbVMAH330Eb/73e+Ii4tj1apVYY1vLgQ2nZWWltLZ2Ulvby8AkZGR/PjHP8bn8z01k6etrQ2n00lOTg4Aly5dwmKxAJCYmIjJZGLt2rW8+eabPHz4ELW0i4iIyGKjih4REZEZys7OJjY2ljt37gATc2DOnj3LqlWr2LdvH6dOnaK2tpZ169ZNqiSJiooCJtqJAl9dLheffPLJUy1ezc3NNDU14fF4SE5Opry8nJSUlDm9jqnbpgLzZ44ePUpfXx9nz56ltraWN998c07Pu5CCbTqzWCxGssZisRhfA8bHx6mtraWsrIyOjg4AI+lz+/Zt/v7v/57o6GiKiopIT0/HarXi8XjCcn0iIiIioaiiR0REZAbMZjObN28mISHBeKy/vx+v10tWVhZpaWkkJyfT3t7+VCWJ2+0GJhJDga8xMTFs2bKFt956iw0bNnDr1i3a29u5fPkySUlJHD58GIfDQX19/ZxeRyDJMzg4yOuvv47ZbGZoaIjbt28DE0kok8m05IcPB9bOf/rpp1y7do2ioiIKCgp4/fXX8Xq9HD9+HLPZzL59+4zXtLa2GlvRAm1eW7ZsMZJBBw8eBKCqqor6+nq8Xi8bN25c+IsTERERmcbSvosTEREJo5iYGAB6enrwer0MDg7i9Xp5+PDhpEoSk8lEdHQ0t2/fxmaz8ejRI9atW/fUsODExERsNhtRUVEkJCRgNpuNKqC54nA4ePz4MQCnTp0CoKioiK6uLgYHB7FYLKxevZqysrI5Pe9Cs9vtQVfVp6amcuTIkaCvGRgYoLu7mw8//NB4rLKy0phXFBERgclkwuPx0NDQwK5du8jKypqfCxARERGZJa1XFxEReQ51dXVcvXqVd999l7i4OOrr62loaAAmqmECFTEul8uoDtmxYwcPHz6kuroap9NpbN3q7e2d1OK1f/9+vv/+e+rq6jCZTMTExPCzn/1Mw34XiNPpZGRkBICzZ88ac5ieFBcXx9DQEJs3b6awsBCz2cyKFSsWOlQRERFZZuZyvboSPSIiIjPU399PU1MTra2t/OQnPyEpKYnh4WHMZjODg4NcvHiRjRs38hd/8RczOt7Y2BhOp5M7d+5w5coVduzYwcWLF8nNzeWVV17hzJkzpKenc+DAgXm9rr6+Ps6dO0d/fz8REREUFBSwbds2qqqquHXrFl6vlz179lBQUDCvcTwPh8PB559/jt/v5/333+e7776jo6ODsbGxOdmG9WTSp76+nvb2do4cOcLXX3+N0+k0nmez2YJWDomIiIg8j7lM9Kh1S0REZIY+/fRT48+nT59m/fr1eL1eOjo6iI6OJj8/n+Li4hkdK9Sw4MDXQJvQ8PDw3F/IFD6fj/z8fLKysmhubjZWwqekpBAdHW1ULC0mT65AB4iNjeXAgQN4vV6+/PJLrly5Mmn+zvOy2WzYbDaASYk2JXVERERksVOiR0REZIY++OCDOTtWYFhwoMUrMCx4ZGSElpYW2traSEhIYPv27XN2zlBSU1NJTU0FIDMzk5aWFjweD4WFhXR2ds77+Z/XkyvQA0Okt27disPh4Msvv8Tv9+N2u6murubmzZv4fD6SkpLYtWsX6enpzzz+1GqhmpqaWR1HREREJByU6BEREQmDUMOCS0pKKCkpCUNEExu5GhoaiI+PJzs7OywxPEuwFegBNTU1xp+LioowmUwUFhbidrv54osvaGhomFEb3NRqoaysrFkdR0RERCQclOgRERERY+262+3m8OHDi3a9+pMr0Nvb2wHw+/20tbXR09NjrEXPysrCbDYD4HK5MJlMxvas6QSrFgokvZ7nOCIiIiLhsjjv4kRERGReeTweTpw4gdPpxGw2Yzab8fv9lJaW8sUXX+ByuVi1apWxPtzlcjE4OEh8fHxY4w62Av3v/u7viIiIwOv1kpycTG9vLy6XC5vNxmeffUZfXx/R0dGsWbNm2mNPVy30PMf5/7f351FR53me//uMjTXYVxEQUCRxIVNEXBM1U7MsMzWbsrOqzKo+v66snPzd351epn89d3p6qrunp6vPr0/PmenuOXd6Zm5NlpWezloyu1PpQsxNTQV3BFNIwQ1QEFEMVoMgCIiI+wdNpGiEggKB+nqckwf4xnd5fzEk/b55f95vERERkWDS1C0REZFJehKnVN1reHiYtrY2EhISqK6uprm5edzreXl5XLp0ibv/nTAbJkz5m4b1/PPPU19fj8fj8e0XGRnJ9773Pe7cuUN/fz8HDx4kMjKSHTt2BDz3+fPnuXz5Mtu3b+fIkSNcvnyZH/7wh5hMpkmdR0RERGSyNHVLREQkiB51StXdVTQmk4nMzExKSko4efLkjCeILBYLOTk5AKSmpnLt2jVWrlzJ8ePH2bRpEzk5OTgcDnp6eoKe3Lmbv2lYx48fH5fkgdHv9YULF0hNTcVisWAwGB66HM1ftdDu3btZs2bNpM4jIiIiEkz6l4qIiMgkPeqUKqPRyIoVK0hISKCxsZG6ujqysrKCNsa8o6OD/fv343a7SU9PJzQ0FBhNAo19HKuemc0KCgrIzc0Fvq7y2bZtG5WVlRw7dgyj0UhiYiJr166dkfM86TweD+Xl5dhsNtxuNzt37iQqKopz585RX1/P0NAQ8fHxlJSUkJCQEOxwRURE5B5K9IiIiDyiyU6puruKxmq1YjKZiI2NJT4+PihjzJOSktixYwfNzc2cOXPG149neHjY9zE8PHzG4xoTaImc0+nk6NGjtLe34/V6KSwspKCgAGDcNKzJLq/yVy30KOd5GmRmZhIZGelb0jc0NMSpU6eYO3cuxcXFlJWVUVNTwyuvvBLkSEVEROReSvSIiIg8gkedUnVvFU1UVNQ0R+qfzWbD6XQSHR3ti91gMBAWFkZTUxNWq5Vbt26xYMGCoMQHgZfI1dXVYbPZ2Lx5MyaT6YmoOnqSGI1Gli1bRnV1tW+bxWLBarUSGhpKTEwMRqPRV/klIiIis4sSPSIiIpM0luTp7+9n8+bNGI1GXC4XLpeLgYEBIPCUqnuraC5evMiSJUtm/B6cTieVlZU4HA5CQ0NZtGgRixYtIj4+nmPHjlFRUUF6ejorVqyY8djG+Fsi53A4aGtro6ioiLS0tKDF9qwxGo3k5+f7GncbjUaampq4fPmylnaJiIjMMkr0iIiITJLNZuP27dsA7Nu3D4DCwkI6Ojro6OgAoLq6msbGxnGNjP1V0ZjNZux2+0MTRFMtPT3db5PltLQ03njjjWm99mTdvUQuMjISgKtXr3L+/HkiIiJYuXIl6enpQY7y6dbb20t1dTXZ2dksXryYTz75hJCQEBwOB6ClXSIiIrOJEj0iIiKTlJaWxjvvvDPp4/xV0SxcuJCKiooHJoieZfcukRsb9x4SEkJJSQmHDh3i8OHDfP/733/ka9hsNvbu3YvX6+Xtt9+msrKSS5cu+V5fsmQJa9aseex7mW0e1HT53LlzDA0NAXD9+nViYmIAMJlMhISEYLFYfH8WoKVdIiIis4kSPSIiIjMkUBXNtm3bghDN7OdviVxISAixsbEYDAZMJhNerxen08lPf/pT38j6wsJCPvzww3HnioqKYufOnX6vc+LECYxGI26327ctOTmZTZs2AaNJpadVoKbLd6uqqmLhwoUUFRXR0NBAS0sLMTExxMXF0dTUxAcffIDH46GgoIC6ujqam5sxGAw0NTXR29urJVwiIiIzTIkeERERmRWGhoYoKyvDbrdjMplITEy8b4kcwLp167h48SJ79uwhJiaGwsJC5s+fP25k/VhCzeFwUFZWxty5c/1es6WlBbvdTlZWFk1NTb7tXV1dfPTRR8THx7N27Vri4+On8c6D40FNl5OTkykpKeEf/uEfyMnJYcOGDcDoEsUxp0+fBmDu3Lm0tbVRV1fHvHnzuHbtGgaDgaSkJDo7O7WES0REZIYp0SMiIiKzgtFoZMWKFSQkJPiSNps2bSInJ4czZ85w7tw53G430dHRlJaW3nf83SPrx8akX7hwAYDFixfft7/H4+H06dMUFxfT1tbm2z537lza29sZHByko6OD8vJyvve97zEyMhJwrPvT4u6myy0tLYSHh1NUVHTffr29vb6lXaGhob7tZrOZ8PBwXC4Xw8PDWsIlIiISBEr0iIiIyKxgsVjIyckBxidtBgYGqK+vZ/HixdTV1d13XKCR9R6PhwsXLpCamup36VBjYyOhoaFkZ2fT2toKgNfrJSsrC5PJREJCAp988gl9fX20trZy8eLFp36s+91Nl5cuXcpnn33GsWPH2LJly7j97l4ad+XKFQAKCgq4cuUKTqcTr9dLd3c3ERERfhNFIiIiMn2U6BEREZFZw1/S5ujRo+Tm5hIXF+f3mEAj61taWnA4HKxevdrvcX19fXR2dvLuu+/6tu3evZtFixaRnZ2N0+n0TZUKDw9/Kse69/b24nQ6Aejv78dgMACjTZfNZjMGg8E3Ee5udzcjr66u5uzZsyxevJjnnnuODz/88KGJIhEREZk+SvSIiIg8RE9PD4cOHaK3txez2UxeXh6rVq3is88+8y3jSUxMZMOGDdM+Fv1p5y9pc+3aNb797W/7qm7uFmhkPUBDQwPh4eFkZ2f7vVZBQQG5ubkA1NTU0NrayrZt2zh//jz79+9neHgYgJSUFN+EqadtrPvdlTkVFRV+my4HSpTBoyeKREREZPoo0SMiIvIQbreb3NxcMjIyOH/+PHV1dWRkZJCbm0txcTHd3d0cOHCA+vp61q5dG+xwn1j+kjZutxuXy8X777/v22///v2UlpaSlJQUcGR9d3c3HR0dFBYWYjQa/V7ParX6evncXXGyYcMG1q1bh91u9yWcuru7gakd6z5T/CUqFy9ezC9/+ctx+0VFRfltugyBR7HfmyiaN2/epBJFIiIiMvWU6BEREXmIxMREEhMTAUhLS6OhoYGhoSFfPxmPxwNAWFgYH330kSp/HlGgpM3YUqlr165RW1vL2rVrfcu4Ao2sj4+PH7e8aDL8JZzG+gWNjXUf+zgbBKo4Ky8vp6Ojw7ff888/z/DwsC9ROdHJZGP8jWKH0ebVxcXFlJWVYTAYKCwsvC9RJCIiIjNHiR4REZEJcrlc1NbWEh0dTWZmJgC7du1iZGSEqKgo4uPjsVgsE678uXeceGZmJiUlJZw/f576+nqGhoaIj4+npKTEbzPhp02gpE14eDgwuqxrJhr7+ks45eXlkZSURFVVFXv27CE2NpaNGzdOeywTEajiDCAnJ4dVq1YBo9Ox2traaGhoYHBwkDNnzvgqdGB0Mtm5c+f8vvceNIo9NDSUmJgYTdgSERGZJZToERERmQCXy0VFRQVOp5Pt27f7Kj127NhBT08PBw4c4PLly7zyyivAgyt/4uPjAf/jxLOysjh16tS4KomamhrfeWeKzWZj7969eL1e3n77bQ4fPkxbWxsjIyMkJyezYcMG33Srp02ghFNiYqLfse7BFqjiDKC1tZUbN26QlJTEypUrxyUq79y5Q0REBC0tLSQmJmK1Wif13pvoKHYRERGZWf4XrYuIiIjPWJKnv7+fl156CaPRyJ07d2hqagJGKxsMBoMv+ROo8uef/umfiIqKYs6cOb7jcnJyiImJGTdOfDZUSZw4cWJcb5vIyEi2bNnCK6+8ws2bNzlz5syMxyQPdu/7Lj8/n9dee42SkhLa29spLy/H6XSydetWQkJCWLZsma/J9MKFCyddoXP3KPZt27bh8Xg4duzYTN2uiIiIBKCKHhERkYew2Wzcvn0bgH379gGwaNEiOjo66O/vx2Qy+aogJlL5c/r0aV+VhL9x4sGukmhpacFut5OVleVLZq1cuRIAr9eLxWLxVYw8yQL1tgEYHh7mgw8+wOFwsHXr1lk/Xcvf+27BggW+14xGIy6Xi1dffdX3eUhICDabDYCMjIyHVuhowpaIiMiTQYkeERGRh0hLS5tQY9+7K382b97sq/zp7OwkMTHxvsof8D9OfKxKYunSpXz22WccO3Zs3FSo6eTxeDh9+jTFxcW0tbXd93pNTQ0ul4vFixfPSDzTKVBvm7lz53Lu3LknJpnl733ncrk4deoU+fn5tLe3MzIyAnydqCwsLCQnJwe73Q6MLsO6u0LH33vvcUexi4iIyMxQokdERGSKTKbyZ2x/f9Odxj4Go0qisbGR0NBQsrOzaW1tBfAt7zl79iy1tbWsW7fO1+x3trq3x1BlZSWXLl3yvb5kyRLWrFnjt7fNwMAA9fX1LF68mLq6umDdwoT5e98VFhbS399PeXk5MFqxU1JSQmRkpO+43t5e8vPzaWxsnFCFTqBkpyZsiYiIzC5K9IiIiEyRiVb+jAk03WlwcDBoVRJ9fX10dnby7rvv+rbt3r3bN3Fp2bJlZGZm4nA4iIiImLG4Jmusx9DYRCmA5ORkNm3aBEBISIhv+729baqqqsjNzfWNcJ/tJvu+G6MKHRERkaeTEj0iIiLT6EF9YFJSUvB4PHg8HjZu3OjrA1NYWBi0KomCggJyc3OB0WVara2tbNu2jc8//xwYreo5e/YsVqvV72Sq2cBfjyGArq4uPvroI+Lj41m7di3x8fH39bbp7+/n2rVrfPvb3/ZVND2tVKEjIiLydFKiR0REZBpNZR+YQEmjzz77jPb2drxeL4mJiWzYsIHo6OhHitdqtWK1WgHG9QWarUmdewXqMZSVlcVzzz2Hx+PhwIEDHDlyhFdfffW+3jY2mw2Xy8X777/vO3b//v2UlpaSlJQUjFuach6Ph/Lycmw2G263m507dxIVFcX169c5fvw4AwMDpKenU1JSQmhoaLDDFRERkUnSeHUREZFplJiYSEFBAXFxcaSlpQHc1wdmosaSRt/61reYP38+dXV1tLe3k5ubS2lpKRs2bODmzZvU19dP1+3Menf3GBrrLeT1esnKyiI1NZW0tDTS0tLo7u729bYZGhpi3759/OIXv8Bms1FaWkppaamvsmXt2rVPzDKuicrMzGTevHm+r4eHhzlw4AAxMTG89tprXL9+ndOnTwcxQhEREXlUqugRERF5RIEqbMrLy+no6PDtt3r1avLy8h67D0xiYqLf5sE5OTnAaKUGQHx8/BTe5ZMlUI+hRYsWkZ2dDYyOtI+Pj39ob5ukpKQZH20/E4xGo6/n0pje3l5cLhcZGRkkJSURHx//1C9dExEReVop0SMiIvKIAi3LAsjJyfH14jEYDFPaB+be5sEAu3btYmRkhKioKObMmTN1N/mECdRj6Pz583zyySeMjIyQlJTEiy++GORIZ5fw8HBgtI+Ry+Wiv78fl8sV5KhERETkUSjRIyIi8ogCVdgAtLa2cuPGDRITE3E4HAwMDExJH5h7mwePjWXfsWMHPT09HDhwgNOnT/PKK69M013PboF6DG3YsCFIET0ZrFYry5cvp7a2lgsXLmCxWHzfRxEREXmyKNEjIiISgM1mY+/evXi9Xt5++20qKyu5dOmS7/UlS5awZs2a+ypsPB4PK1euxOFwcODAAd+Sqn379vmOKy0tBeDatWvU1tZOqA/MWJLn7ubBd+7cobOzk8TERCwWCwaDwZf8EQmkt7cXp9MJQH9/PyaTiczMTLKysujv7+f48ePk5eUFOUoRERF5FPqXoIiISAAnTpzAaDTidrt925KTk9m0aRMAISEhfitsFixY4Ns/ISEBl8vFd77zHb/XmEwfmLHmwfB10mjRokV0dHT4Htbnzp1LcXHxI92vjBoaGqKsrAy73e5LgBQWFvLhhx+O2y8qKoqdO3cGKcrHc/e9VFRUsHDhQlwuF21tbYSFhZGbm0tBQUEQIxQREZFHpUSPiIiIHy0tLdjtdrKysmhqavJt7+rq4qOPPiI+Pp7i4mKOHz8+rsLG5XJx6tQp8vPzcTqddHd3+/roPK6HNQ+WqWE0GlmxYgUJCQk0NjZSV1dHVlaWb8S8w+GgrKyMuXPn+j0+UJNuGJ1u9cEHH+BwONi6dSvp6elTHv9Erg9M2/VFREQkuJToERERuYfH4+H06dMUFxfT1tbm256VlcVzzz2Hx+PhwIEDHD58mL6+PuDrCpvCwkL6+/spLy8HRpMza9asmfmbkEdmsVh8k8ysVismk4nY2Fhfz5oLFy4AsHjxYr/HB2rSPXfuXM6dO+fr4zRdgn19ERERCS4lekRERO7R2NhIaGgo2dnZvqlYXq+XrKws3z5paWlcu3ZNFTZPqY6ODvbv34/b7SY9PZ2oqChgNAl44cIFUlNTSUhI8HtsoCbdAwMD1NfXs3jxYurq6qYt9mBfX0RERILLGOwAREREZpu+vj46Ozt59913uXz5MgC7d+/m5MmT3Lp1i1u3btHR0UF8fHyQI5XpkpSUxI4dOygqKuL69etcvHgRGF3S53A4Albz3O3eJt2nT58mNzf3oU23p0qwry8iIiLBoUSPiIjIPQoKCigtLaW0tNTXX2fbtm04nU4++eQT9u3bR0xMjEZ2P6VsNhs3b97EaDT6JpiNfWxoaCA8PJzs7OwHnuPuJt1bt26lv7+fa9euUVhYiNfrnfZ7CPb1RUREJHgMk/mffVFRkffMmTPTGI6IiMjs5W8aU0lJCSdPnuTKlSu4XC7Wr1+vsdRPuOvXr1NZWYnD4fAt4VuzZg29vb380z/9E4WFhQ+clDaWZBlr0h0dHc2NGzc4fPjwffuWlpaSlJQ0pfEH+/oiIiIyeQaDocbr9U5sFOvDzqVEj4iIyMQMDw/T1tY2bhrTpk2bfP1PamtrlegRbty44WvOPWbJkiXk5uYCcO3aNWpra1m7di15eXm+aqGn5foiIiIyeVOZ6NH/2UVERCYo0DSm+Ph4rl+/PqFzBBp9ffToUVUFPWECVXgZjUbi4uIYGBggPT2dkpISQkJCfMclJSU9sCLocaWlpT2wSfh0X19ERESCS4keERGRSQg0jWmiAo2+TkhIICwsjNra2mmKXKaa0WhkxYoV4yq8MjIyOHnyJKmpqaxfv579+/dTXV3N2rVrgx2uiIiIPCOU6BEREZmEsWlMzc3NnDlzhosXL7JkyZJx+wSq2rl58yZVVVUMDAzQ2dlJZmamb/R1fn7+hKuCZOrZbDb27t2L1+vl7bffpq+vjyNHjtDT00NycjIbNmwgMjJy3DH+KryGhoYYHBxk/vz5JCcnk5KSwrVr15ToERERkRmjRI+IiMgE2Ww2nE4n0dHR46Yx2e12BgYGAHA4HPT19d1XtZOWlsaRI0d8lR4VFRW0t7f7Rl9LcJ04cQKj0Yjb7Qbg4MGDWCwWtm/fzmeffUZVVRVbtmy577h7K7xCQ0OB0STQ2MfBwUHf/oGSgDDaA+qDDz7A4XCwdetW0tPTA8YbaNnYxx9/TFdXF16vlzlz5rBx40ZfTCIiIvJsUKJHRERkgpxO57hpTIsWLWLhwoVUVFTQ0dEBQHV1NVarlTfffBMY7ZfS0NDAyMiIr9IjNjYWg8GAy+WitLRUzXCDrKWlBbvdTlZWFk1NTdy5c4fu7m5WrlxJQkICGRkZNDY24vF4MBqN4469t8IrIyMDGE3ajH0MDw/37R9o6d7cuXM5d+4cQ0NDE4rZ37KxrKws4uLiWL16NV1dXRw5coT6+nr14xEREXnG6F+WIiIiE5Senu5L4Nxt27Ztfvd3uVzU1tYSHR2Nx+Pxba+oqGBkZASDwYDRaMTlcuFyucZVBfX39xMdHf1IcbrdbiorK7l27Rper5e0tDQ2btw4riGwjPJ4PJw+fZri4mLa2toAfBU4d1fleL1enE4nERERvmP9VXgZDAbCwsJoamrCarVy69YtFixY4DsmMTGRxMRE4Osk4NjUtvr6ehYvXkxdXd1D4w7UGPzubTCanJxKem+JiIjMfkr0iIiITAOXy0VFRQVOp5Pt27djt9sB6O7u5vbt2779fvGLX1BYWEhHR8e4qqDGxka/SaWJaGtr4/Lly7zwwgtERUVRVVXFxYsXWbp06ePf2FOmsbGR0NBQsrOzaW1tBSAsLAwYX5UzlsC5m78Kr0WLFhEfH8+xY8eoqKggPT2dFStW3Hfdu5OAmZmZVFVVkZubS1xc3IRjD9QY3Ov1cvLkSUwmE/n5+Y/0fQlE7y0REZHZT4keERGRKTaW5Onv72fz5s0YjUYSExMJCwujp6eH3/iN32D//v0sWLCAdevWTfn1o6OjMRqNWK1WX2XHWHWKjNfX10dnZyfvvvuub9uePXuIi4vj6tWrpKen09bWRkZGxn3LtgJVeKWlpfHGG28EvOa9ScD+/n6uXbvGt7/9bV+yaSL8NQZfvHgxR48e5cqVK2zatImEhIQJn28i9N4SERGZ/ZToERERmWI2m81XtbNv3z4ACgsL2bRp00MrPaZCdHQ0GRkZHD16FIPBQGpqKgsXLpyWaz3pCgoKyM3NBaCmpobW1la2bduGyWTiyJEj/PrXvyY5OXnKEnL+koA2mw2Xy8X777/v22///v2UlpaSlJTk9zyBGoNXVVVx4cIF1q1bR2JiIoODg+N6BD0uvbdERERmP4PX653wzkVFRd4zZ85MYzgiIiLyuBoaGjh69CgrVqwgKiqKQ4cOsXLlSp5//vlghxZUgSZVnTx5kitXruByuVi/fj15eXnTFsONGzd8yb8xS5Ys8SWbrl27Rm1tLWvXriUvLy9go+7r16+PWzaWnZ3NmjVrxlUmAcyZMydgD6lHofeWiIjI9DAYDDVer3dKJiiookdEROQxzMYx1waDARit8BhLFIw1en6WBZpUlZCQQFhYGLW1tX6P8/dnXFhYyIcffjhuv6ioKHbu3PnAGNLS0njnnXcCvp6UlDShKVmBlo096NxTQe8tERGR2U+JHhERkccwG8dc5+bm0t7eTk1NDR6Ph7S0NAoKCmbk2rNZoElV8fHxXL9+PeBxgf6MxxItDoeDsrIy5s6dOyP3EUx6b4mIiMx+SvSIiIg8hmCNuX4Qs9nMpk2bZux6T5JAk6oeJNCf8dif7YULFwBYvHjx9AU+S+i9JSIiMvsp0SMiIvKYgjHmWh6Nv0lVS5Yseehxgf6MPR4PFy5cIDU1dconXM12NpuNvXv34vV6efvttzEajXi9Xvbs2UNXVxclJSU899xzwQ5TRETkmaNEj4iISACB+u+YzWaGh4f54IMPcDgcfOMb37gvebBgwQJ+9atf4XK5MJvNnDt3jpKSEmw2G1VVVQwMDJCenk5JSQkhISHBvtVnQqBJVXa73ddnxuFw0N/fT3R09LhjAyWIWlpacDgcrF69esbvJ9hOnDiB0WjE7Xb7tl26dIne3t7gBSUiIiJK9IiIiAQSqDdLTk4O586dY2hoCICenh7i4uLGJQ9OnjyJy+Vi+fLlOBwOGhsbycjI4OTJk6SmprJ+/Xr2799PdXU1a9euDeZtPrXurTg5efIkN27cAEb/bHNzc1m4cCEVFRV0dHQAUF1dTWNj47hGx4ESRDA6hSo8PJzs7OwZvrvgamlpwW63k5WVRVNTEwDDw8NUV1ezdOlSvvzyy+AGKCIi8gxTokdERCSAQL1ZBgYGqK+vZ/HixdTV1VFXV8eZM2cIDQ1l0aJFLFy4kMrKSgBqamp85xsaGmJwcJD58+eTnJxMSkoK165dU6JnmtxbcZKUlMSKFStwuVx88sknwGjC52Hjx51O57hR5mN/xt3d3XR0dFBYWIjRaJz2+5ktPB4Pp0+fpri4mLa2Nt/2c+fOkZCQQHp6uhI9IiIiQaREj4iIyAP4681y9OhRcnNziYuLA+Cll14iPT193HHvvPPOfceOjVe3WCy+j4ODg9MSt9vtprKykmvXruH1eklLS2Pjxo3PzDIxfxUnK1euBEZ7J1ksFl9F1sMEGmUeHx8/7ePMZ6PGxkZCQ0PJzs6mtbUVGE2G1dfXs337dl/jca/Xi9fr9Y1kFwmGc+fOUV9fj9frJS8vjxUrVug9KSJPPSV6REREHsBfb5Zr167x7W9/2/eQO9FjMzIygNElLmMfw8PDpyXutrY2Ll++zAsvvEBUVBRVVVVcvHiRpUuXTsv1ZpNAFSdjampqcLlcz8SUrOnQ19dHZ2cn7777rm/br371K0ZGRvjoo49826qqqggNDfVVxYnMlLuXbQIsWrSItrY2vvzyS65du8Y3v/lN39Q8EZGnkRI9IiIiAfjrzeJ2u3G5XLz//vu+/fbv309paSlJSUkPPNZgMBAWFkZTUxNWq5Vbt26xYMGCaYk9Ojoao9GI1Wr1PdCMVRI97fxVnIw98J09e5ba2lrWrVvnS7zJ5BQUFJCbmwuMJs1aW1v55je/6Xuf3759m6NHj7Js2TLmzp0bzFDlGXXvss1r164RHh7OnTt3uHPnDpWVlWzdujXIUYqITB8lekRERAII1JslLS0NGH14qK2tZe3atb5lXA86dtGiRcTHx3Ps2DEqKipIT09nxYoV0xJ7dHQ0GRkZHD16FIPBQGpqKgsXLqTnaiNndv0lPVcbCYmIYu2/+RuSnls+LTEEi7+Kk927d7Ns2TKqq6tZtmwZmZmZOBwOIiIighjpk+nu5OGWLVvuez0pKYlFixbNdFgigP9lmwMDA6SlpWGz2QgJCaG9vR2Px/NM9dYSkWeLYew3XBNRVFTkPXPmzDSGIyIiIlOhoaGBo0ePsmLFCqKiojh06BBFhcu4+pM/ICp1HkU//DMGbrcTEZ9CfM7TtYTJbrf7eh+NVZyUlpby+eefY7fbfftZrVa/vXdklMfjoby8HJvNhtvtZufOnURFRfl6ngwNDREfH09JSQkJCQnBDlcEj8fDP/7jP1JUVERbWxuXLl0iPT2d69evYzAYMBgMvv5cv/VbvzVtS2dFRB6FwWCo8Xq9RVNxLlX0iIiIPIXGmo2azeavl9RcvcRQXxfr/uDviJuXR9y8vGCGOG0CVZwoqTN5mZmZREZG0tzcDIxOjjt16hRz586luLiYsrIyampqeOWVV4IcqYj/ZZsvvPACSUlJfPnll3g8HjweDyaTibCwsCBHKyIyfZToEREReQrl5ubS3t5OTU0NHo+HtLQ0kl036AbO/eJvuHPzGrHz8ih+58dYk9VHRe5nNBp9y93GWCwWrFYroaGhxMTEYDQan5neTzL7+Vu2uW/fPgwGA1arlezsbK5fv05KSoomb4nIU02JHhERkaeQ2Wxm06ZN47Y1HfxHABJyn+f5nX/AoR//gLpf/S1rfu+/BCNEeQIZjUby8/Oprq6mpaWF8PBwioqmpMpcptndk6jefvttrly5Qk1NDQ6Hg+joaNauXevrP/ak8tcovLS0lMuXL9PY2MilS5fIyspi1apVQY5URGR6KdEjIiISwL0PRpWVlVy6dMn3+pIlS1izZk0QI5yc1II1GE0WjGYLppBQMBgwWkKDHZY8QXp7e6muriY7O5ulS5fy2WefcezYMb9NmWX2GHF7+D93VXNzIAM3Bl7tHeT48eNERkZSWlrKvn37OHHiBDt27Ah2qBMWqIeU2Wzm6NGj3Lp1i5CQEDo6OlizZs0T9bNaRORxKdEjIiISwL0jegGSk5N9lTIhISHBCi2gnp4eDh06RG9vL2azmby8PFatWoXT6eTEuUYGXv5dqvuGqH/3b8hYuprnv/v7wQ5ZZrHe3l6cTicA/f39vuUuJpMJs9mMwWBgYGAgmCHKBFy9epXMcCdx4ZF8ZRsdxBIXF4fX6yUmJgaz2fxELsG7t4cUwBdffIHNZmPz5s2YTCZfY3YRkWeJEj0iIiJ++BvRC9DV1cVHH31EfHw8a9euJT4+PohR3s/tdpObm0tGRgbnz5+nrq6OjIwM6urqsNlsbPnmq//y8PMaWVlZwQ5XZrkPP/zQ93lFRQULFy6kqKiIhoYGWlpaiImJYfXq1UGMUB7G4/FQc6aa39laxLtHWsA2mrjOz8/nyJEj7Nq1C7PZzObNm4Mc6eT46yHlcDhoa2ujqKjoiV+GJiLyOJToERERuYfH4+H06dMUFxfT1tbm256VlcVzzz2Hx+PhwIEDHDlyhNLS0ke+zoOqb44ePUp7ezter5fCwkIKCgomdM7ExEQSExMBSEtLo6GhQQ8/T5F7lxOOjTp3Op0UFhZOeb+cd955x+/2wsLCKb2OTJ+7J1FxeLTyZWRkhMrKSlJSUli5ciWVlZUcPnyYN954I8jRPh673Q6MVjCdP3+eiIgIVq5cSXp6epAjExGZWUr0iIiI3MPfiF6v1zuuAiYtLY1r1675PX6ivX0eVn3zOEsPXC4XtbW1REdHExkZCejh50njLxF4+/Zt33LCn//85wwODpKfn09jY2Oww5VZ6u5JVN39sUAM//RP/0S0xYvRaMRkMmEwGHxJkidZaOhoz7GQkBBKSko4dOgQhw8f5vvf/36QIxMRmVlK9IiIiNzD34je3bt3s2jRIrKzs7lz5w4tLS14vV52797tq8Q5evQoV65cweVy3Te6119vn4lW3wwNDfHBBx9gt9sxmUxkZmZSUlLC+fPnqa+vZ2hoiPj4eEpKSkhISMDlclFRUYHT6WT79u14vV7fdfXwM7s8qKrr+PHj9Pb2YjQaiY2Npa6ujvDwcN9ywqGhIWD0vaVEjwQyNomqrcdJ3YHzcAeeX7UBq2GQ5sY6ysrKsFqtvPjii8EOddLu7SEVGxtLbGwsBoPBl8AymUxBjlJEZOYp0SMiInIPfyN6t23bxvnz5/nkk08YGRnBarWyZs0a2trafJU4CQkJOJ1OmpubSUxM5Pbt275zPqi3z8Oqb4qKilixYgUJCQk0NjZSV1dHVlYWp06dYu7cuRQXF1NWVkZNTQ0bNmygoqKC/v5+Nm/ejNFoJCQkRA8/s9SDqrq6u7v55je/iclkorm5mVu3brFgwQJf8+PFixdTX18f5DuQ2c5qtWK1Wtn23z8FRhPQf/LPV9j6Qhp/9uabwQ3uMfnrIfXSSy9RVVXFnj17iI2NZePGjUGMUEQkOJToERERucfYgxEwbmz0hg0b7tvX7XbT0NDA0NAQeXl51NTUAKONQsc8qLfPRKpvqqqqfNU3VqsVk8lEbGwsVquV0NBQYmJiMBqNWCwWbDabL8G0b98+YLSfih5+ZqeJVHW5XC6ampowmUwsX76cjz76CIDY2NggRi5PmpP/6RvBDmHKBeoh9Ti900REngZK9IiIiDyiuytxMjMzaWxsJCQkBIfD4dvnQb19xpI8E6m+6ejoYP/+/bjdbtLT04mKiiI/P5/q6mpaWloIDw+nqKiIqKgoPfw8gQJVdX311VcMDw8Do0nF9957z3fMsWPHgNFqMYCzZ89SX19Pfn4+q1atAmB4eJgPPvgAh8PB1q1b1ZdJRETkGWB8+C4iIiJyr7srcbZu3YrZbKavr4/e3l4Abt26BYz29jl58iS3bt3i1q1bdHR0+JZtjVXfDA0NsW/fPn7xi19QV1fHSy+9hMvlYs+ePRiNRjZu3EhSUhI7duygqKiI69evc/HiRaqrq8nOzmbbtm14PB7fg788We59L40lesxmM2FhYXg8HkwmE9/4xjfGTbvyeDwAfPXVV8BoUnF4eJi6ujra29sBOHfunK+Xj4iIiDwbVNEjIiIySf4qcVwuF/PnzyckJITa2lpiYmLo6+u7r7dPUlISL774IjabjYqKCgDefvttDh8+TFtbG+fOnaOjo4NNmzYRFRUFjCaEbt68SXR0NGbz6P+6x3rsmEwmzGYzBoPB17tFnhwPquoaGRnxJQ5dLheffvopS5Ys8VVmXbt2jdraWtauXUteXh5ms5nm5mYOHDjA0NAQAwMD1NfXs3jxYurq6oJ4lyIiIjKTlOgRERGZpEB9cDo6Oujo6ABGJ3dZrVaSkpL89vYpLy/3jckGiIyMZMuWLbhcLj755BPOnDnj66PjdDqprKzE4XAQGhrKokWLyMvLY3BwkIaGBlpaWoiJiWH16tUzcPcylR7WU2msH9PatWtJTU0dd2xSUhJFRUW+r+9dSlhVVUVubi5xcXEzd0MiIiISdIaxpo8TUVRU5D1z5sw0hiMiIvL0a2lp4eTJkyQnJ9PU1MTbb7/ta948NrI9NTV1XCNokUBsNht79+7F6/USHh7OggULuHTpEkNDQyxdupS4uDgqKyvVo0dERGQWMxgMNV6vt+jhez6cKnpERERmkMfj4fTp0xQXF9PW1nbf6zU1NbhcLhYvXhyE6ORJdOzYMd+0to0bN9LX10d6ejpNTU3jxq/v37+f0tJSkpKSghWqPCM8Hg/l5eXYbDbcbjc7d+4kKioKp9PJ0aNHaW9vx+v1UlhYSEFBQbDDFRF56ijRIyIiMoMaGxsJDQ0lOzub1tZWAN9D+tmzZ6mtrQXg448/Hte7Z2RkhOTkZDZs2ODr3SPS0tJCf3+/7+v9+/cDkJubC8Bzzz1HRESEr5ePlnHJTMnMzCQyMpLm5mbfti+++AKbzcbmzZsxmUwMDg5O6TXvrm57++236erqoqqqir6+PpKTk1m/fj1Wq3VKrykiMhtp6paIiMgM6uvro7Ozk3fffZfLly8Do5O5zp49S3V1NVar1beMC77u3fPKK69w8+ZNtIRaxoxVh61Zs4aFCxcCo42933nnHd90roiICIqKinjnnXdYvHixr5m3yHQyGo0sW7aMmJgY37a2tjba2toYHBwkNTUVo9FIbW0tP/vZz6ioqMButz/2dU+cODHu5+fBgwcxGAy8/vrr9Pf3U1lZ+djXEBF5EijRIyIiMoV6enr46KOP+OlPf+obrT5meHiYK1euALBmzRoyMzMB2LZtG42NjQDY7Xbf2GyAlStXkpKSQnp6OhaLRaOyxefu6rCxqrDJ9F4UmUmnT5/2ff7+++/zz//8zwwPD09ZEqalpQW73U5WVhYw2py8v7+ftLQ04uPjSU5Opr29fdzPVxGRp5USPSIiIlPI7XaTm5vLt771LebPn09dXR3t7e0AnDt3juHhYQBiY2PZsmUL77zzDklJSXz3u98lJiaGl19+2VedcTf17pF7BaoOczgc3LlzBxid2DY2ol0kWNra2nA6nb6vX375ZTweDwMDA1OShLm795nJZAIgJCQEs9lMd3c3breb3t5evF6vkuUi8kxQokdERGQKJSYmUlBQQFxcHGlpaQAMDQ0xMDBAfX19wETNg6ozxnr3rFu3joyMjJm5EZn1CgoKKC0tpbS0dFx12OnTp329ehoaGvjwww+DGaY8o3p7e33JnbNnz7Js2TIsFgsAYWFhwOjPuKlIwgT6+blmzRpu3LjBrl27sNvtmEwm37VFRJ5mGq8uIiIyDVwuF7/+9a8ZGRnhN3/zN6mqqsJisZCUlMSRI0fuG3V9/Phxvvrqq3HnMJvNLFu2jOrqapYtW0Z+fj5Go5GIiIiZvh0RkUn5yU9+Mu7r3NxchoaGaG1txWg0Eh4ezuDgIF6vl5CQEEZGRnjrrbcwGAyTvlagn5+vv/46BoOBO3fuUF1dTUpKCi+++OJj3ZeIyHTReHUREZEZNjQ0RFlZme+3wpmZmZSUlHDz5k2OHz/OwMAA6enplJSUYDAYqKiowOl0sn37dvr7+7l27Rovv/wyH3/8MTC61KCnp4cjR47Q09NDfHw8W7ZsITw8nJqaGlpbW9m2bRuff/45MPob8bNnz2K1WnnzzTeD+a2QJ0hPTw+HDh2it7cXs9lMXl4eq1at4ty5c9TX1zM0NER8fDwlJSUkJCQEO1x5irzzzjvA10mYseWFMNqsecuWLfclYR4lyQOj1W1jk+bu/vl58eJFGhsbsVgsZGVlsWrVqse/MRGRJ4AqekRERCZgeHiYtrY2EhISaGxspK6ujg0bNnD8+HHmzJlDYWEh+/btIzs7m56eHvr7+9m8eTPR0dHcuHGDw4cP33fO6OhowsPDWbduHZ999hlxcXFs2bJl5m9Onlo2m40bN26QkZHB+fPnaWhoYPPmzXz++efMnTuX4uJiysrKmDdvHq+88kqww53VRtwe/q+fVXOxox/XiIc9/6aEtLjwYIc169ntdt8Y9bEkTGlpKZcvXx6XhFm9ejX29iuc2fWX9FxtJCQiirX/5m9Iem55kO9ARGRmqKJHRERkhlksFnJycgCwWq2YTCaioqJwuVxkZGSQlJREfHw8ra2tvr4U+/btA2DJkiUUFxfz1VdfERoaSk9PD4WFhdTW1pKfn09CQgIZGRk0Njbi8XjGjQcWeRyJiYkkJiYCkJaWRkNDA16vF6vVSmhoKDExMRiNRl/vFHmwtQuTSI4O5eD5W8EOJSg8Hg/l5eXYbDbcbjc7d+4kKiqKzz77jPb2drxeL4mJiWzYsIHo6Ghg9Oel1WoFGJfITkpKYs2aNb6vhwcH+OL/+VdEpc5j849/ycDtdkwh6qcjIvIolOgRERGZoI6ODvbv34/b7SY9PZ2oqCgAurq6fKN8h4eHfUsWxng8Hv7xH/+R1atX09bWRk9PD+np6dTW1voesN1uN16vl5/97Gfjltg4nU6OHj3qe4gqLCykoKBgxu9dZjebzcbevXvxer28/fbbvqVZTqfT956pra0lOjqazMxM+vr6qK6upqWlhfDwcIqKpuQXiE81s8nIb5fk8L8OXn74zk+xzMxMIiMjaW5u9m3Lzc2luLiY7u5uDhw4QH19PWvXrp3UeW/UHmaor4t1f/B3xM3LI25e3lSHLiLyzNCvDEVERCYoKSmJHTt2UFRUxPXr17l69SrLly/nwoUL7N69G7fb7fvN9d38TYQZm/wyNm7d5XIB8Bu/8RvjxrJ/8cUXdHR0sHnzZr75zW/6fksucrcTJ06MqwSLjo5m2bJlwGgScaxn1NatW7Hb7VRXV5Odnc22bdvweDwcO3YsWKHLE8RoNLJs2TJiYmLGbc/OziY2NpbY2FgA4uPjJ33uAdsNAM794m/Y86/Wcugv38Le2f7YMYuIPItU0SMiIjIBNpsNp9NJdHQ0ZvPo/z7NZjMpKSlkZWVx48YNTp06xZ07d9i9e7evIufo0aNcuHABj8fDu+++6zvfnj17iIuL4+rVq6Snp2Oz2cjMzCQhIcG3xMbhcNDW1kZRUZFvVPvd/DWILiwsvG+cdlRUFDt37pzeb5AETUtLC3a7naysLJqamgCYP38+fX19nDhxgsuXL+N2u9m8eTNGo9GXXDSZTJjNZgwGAwMDA8G8BXkK7Nq1i5GREaKiopgzZ86kjw+1xgKQkPs8z+/8Aw79+AfU/epvWfN7/2WKIxURefop0SMiIjIBTqeTyspKHA4HoaGhLFq0iIULF3LgwAHa2tqwWCykpqayevVqX7PmjIwMEhISWLRoEV999RWFhYXYbDbfRBiTycSRI0f49a9/TXJyMuvWrcPlcvmW2ERGRgJw9epVzp8/T0REBCtXrvSNZTcajaxYsWJcg+isrCzfVC6Hw0FZWRlz584N2vdNppfH4+H06dMUFxfT1tbmdx+HwwF83TOqsLCQoqIiGhoaaGlpISYmhtWrV89YzE+yq7ft9DlGE2XtPQ5CzEYSo0KDHFXweTweYmJi6Onp4c6dOxw/fpytW7dy8+ZNqqqqxk0lDAkJ8XuO1II1GE0WjGYLppBQMBgwWvS9FRF5FEr0iIiITEB6errfseb+JhWNVeQMDQ2Rn5/P9evX+eqrr4iKirqvF0ppaanvc5fLNW4s+9gyr5CQEEpKSjh06BCHDx/m+9//PuC/QXRsbKxv+diFCxcAWLx48RR8B2Q2untZYGtrKwD3TlQdS+zcq7CwcEZifJp8979/vcTtd3efYesLafxZ6dIgRjTzent7fQ3n+/v7MZlMtLe3k5aWRkhICB0dHZhMJkZGRvj8889JTU1l/fr17N+/n+rq6oC9eyKT5rLyX/8V9R/8Ny5/8nNSl67m+e/+/kzemojIU0OJHhERkSl0d0VOZmbmpI6rqKjwjWU3Go2EhIQQGxuLwWDAZDL5Pt4tUINoj8fDhQsXSE1NJSEhYUrvUWaPvr4+Ojs7xy0L3L17N9/97ne5c+cOMFqN1tvb6+ufIoHd29T6ypUr1NTU4HA4iI6OZs//udbvMspnyd1LQysqKsjIyMBut9Pf3+/b/sILL9DZ2cng4CDz588nOTmZlJQUrl279sAmzVlrXyVr7avTGr+IyLNAiR4REZEpcm9Fzlgvn4mw2Wzcvn0bGL/E5qWXXqKqqoo9e/YQGxvLxo0bxx031iC6ubmZM2fOcPHiRZYsWUJLSwsOh0NLcp5yBQUF5ObmAlBTU+NbFnj69GkuXboEQENDAw0NDfdNg5P7jTW1drvdABw/fpzIyEhKS0vZt28fJ06cYMeOHUGOMrge9D6qrq7m7NmzREREcOvW6Aj6scmCFouFwcHBGYlRRORZp0SPiIjIFPBXkeNyuXC5XL5Gtw6Hg/7+fr+Ts9LS0gI+QN29vOtugRpEw+jDfXh4ONnZ2VNxezJLWa1W31K9LVu2+LZv2LCBDRs2BCmqJ5O/ptZxcXF4vV5iYmIwm82+pMVsc28lUldXF1VVVfT19ZGcnMz69ev9TgScThEREcDXkwWHh4cJDw+f0RhERJ5VSvSIiIhMgUAVOR0dHXR0dACjv+1ubGz02+vnUQRqEN3d3U1HRweFhYXjRm6LiH+Bmlrn5+dz5MgRdu3ahdlsZvPmzQD09PRw6NAhent7MZvNvil75eXldHV14fV6mTNnDhs3biQ0dPobCt9biXTw4EFCQ0N5/fXX+fTTT6msrGTr1q3TGsO9vXtiY2MJCwujqakJq9XKrVu3WLBgwbTGICIio5ToERERmQIPqsiB8aPQ33vvPTIzMykpKeHjjz9+5AfDQA2i4+PjtUxHZBL8NbUeGRmhsrKSlJQUVq5cSWVlJYcPH+aNN97A7XaTm5tLRkYG58+f903Zi4uLY/Xq1XR1dXHkyBHq6+v9NsKeSvdWIrlcLvr7+3n++eeJj48nOTmZ5uZmPB7PtCZ+7+3ds3DhQjZt2sSxY8eoqKggPT2dFStWTNv1RUTka0r0iIiIzIBAo9CD8WAoIuP5a2r93nvvYTAYMBqNvmbodrsdgMTERBITE4HxU/bWrVsH4FsmNVbhMl38VSKFhIRgNpvp7u7G7XbT29uL1+tlaGhoWpdOBUouv/HGG9N2TRER8U+JHhERkRkQaBT63dtg+h8MReR+/ppal5aW0tXVRW1tLWVlZVitVl588cVxx/mbsuf1ejl58iQmk4n8/PxpjdtfJZLX62XNmjUcO3aMXbt2ERISgslkIiwsbFpjERGR2cPg9XonvHNRUZH3zJkz0xiOiIjI0+veUeibN2/GYrHg9Xo5cuQIV65cISoqCrvdPq7vx7lz56ivr2doaIj4+HhKSko0Ml0kyMYasA8MDLB9+3aio6Pxer0cPXqUixcvsmnTJrKysqY1huPHj/PVV1+N22Y2m3n99dcxGAzcuXOH6upqUlJS7ktSzTazsaG0iMhMMhgMNV6vd0rKutWhUUREZIaMjUIvKiri+vXrXLx40fdgeOXKFYqLi8nPz+db3/oW8+fPp66ujpaWFk6dOkVcXBzbt2/HZrNRU1MT7FsReabdPWXvpZde8k3Zq6qqorGxkTVr1pCYmDjt48QLCgooLS2ltLTUV1G0bds2Ll68yN69ezly5AjJycmsWrVqWuOYCmMNpcccPHgQg8HA66+/Tn9/P5WVlUGMTkTkyaKlWyIiIjMg0Cj0qqoqLly4wLp168jMzMRkMhEeHu7r++H1erFarYSGhhITE4PRaJy1I55FnhWBpuxduHABgKNHjwIwZ84ctm3bNm1xWK1WX5XLli1bfNuTkpJYs2bNtF13qs2WhtIiIk8LJXpERERmQKBR6GO/pb77wfAb3/jGuL4ffX19VFdX09LSQnh4uJo1iwRZoCl7+rs5ebOpobSIyNNCiR4REZEZEGgU+r0Pi2NLQpxOJ9u3b8dut1NdXU12djZLly7ls88+49ixY+N+ey8i8qSa6obS6vUjIqJEj4iIyKxxd9+PzZs3YzQaGR4eBsBkMmE2mzEYDAwMDAQ5UnkcQ0NDlJWVYbfbMZlMZGZmUlhYyIcffjhuv6ioKHbu3BmkKEVmhr/R9rt37+b111+ntLR0XENpg8Hw0PON9fpxu93AaK+f0NBQXn/9dT799FMqKyvZunXrtN2PiMhsoESPiIjIDOrp6eHQoUP09vaOm6zldDr5/PPP/fb9KCoqoqGhgZaWFmJiYli9enUwb0Eek9FoZMWKFSQkJNDY2EhdXR1ZWVm+ii+Hw0FZWRlz584NcqQyWwT6uVFeXk5XVxder5c5c+awceNGQkNDgx3upPgbbT/WULqxsRGLxUJWVtaEGkqr14+IyCglekRERGaQ2+0mNzeXjIwMzp8/T11dHRkZGdTV1dHb28trr72GyWRicHBw3GjmwsLC4AUtU8pisZCTkwOMNtM1mUzExsb6lpOMNfRdvHhx0GKU2SXQz424uDhWr15NV1cXR44cob6+flJ9gobsQ5T9u0+5c3sAk8VEZlEa639nNeYQ0zTezXhT1VBavX5ERL6mRI+IiMgMSkxMJDExEcA3WcvhcNDW1kZRURFpaWlBjlBmQkdHB/v378ftdpOenk5UVBQw+rB64cIFUlNTSUhICHKUMlv4+7kxNDTEunXrAHyJEqfTOanzGk1GVnz/BRKy42j45BJ1ZY1kr8okZ03m1N7ADJjqXj8iIk8yJXpERESCwOVy+SZrRUZGAnD16lXOnz9PREQEK1euJD09PchRynRJSkpix44dNDc3c+bMGS5evMiSJUtoaWnB4XBoeZ74dffPjczM0WSM1+vl5MmTmEwm8vPzJ3U+S7jFl9SJSorEZDESkxY15XHPhKnu9SMi8iRTokdERGSG3TtZy+v1AqPLDEpKSjh06BCHDx/m+9//fpAjlelgs9lwOp1ER0djNo/+U2zsY0NDA+Hh4WRnZwczRJmF7v25YTab8Xq9HD16lCtXrrBp06ZHqgLrON9JxZ8fxO1yk/7CHKJTnsyJVFPZ60dE5EmnRI+IiMgM8jdZKyQkhNjYWAwGAyaTyfdRnk5Op5PKykocDgehoaEsWrSIhQsX0t3dTUdHB4WFhWoUK+P4+7nhcrk4efIkFy5cYN26dSQmJjI4ODjp3jNJC+LZ8bdbaTneSvXPz3HhQBNLtz03TXcyfaaq14+IyNPAMPZbxIkoKirynjlzZhrDERERebrduHHDN1FrTGFhIVlZWVRVVdHd3U1sbCxr164lNTU1SFGKyGwS6OdGbW3tuG1z5sxh27ZtEz6vrbkbZ/8Q0alWrp66zoldNZT865Xkv5I7JXGLiMjEGQyGGq/XO/GO+g86lxI9IiIiIiLPnrazN6j8+1M4egYJtYaQsyaT1T8swmRWRZmIyEybykSPlm6JiIiIiDyDMpal8b13S2fkWiNfdjDy64vjtpleSMWy/clbJiYiMtsp0SMiIjLFhoaGKCsrw263YzKZyMzMpKSkhPPnz1NfX8/Q0BDx8fGUlJRohLbIM8zj8VBeXo7NZsPtdrNz506ioqK4efMmVVVVDAwMkJ6eTklJCSEhIY91rSH7EGX/7lPu3B7AZDGRWZTG+t9ZjTlkZvqBmRYnY8qJA8B9wcbIJ1cw/svXIiIytZToERERmWJGo5EVK1aQkJBAY2MjdXV1ZGVlcerUKebOnUtxcTFlZWXU1NTwyiuvBDtcEQmizMxMIiMjaW5uBmBkZITPP/+c1NRU1q9fz/79+6murmbt2rWPdR2jyciK779AQnYcDZ9coq6skexVmb7x6tPNYDGBZTSp5G68DZEWjPlJM3LtewVKxh86dIj29na8Xi+JiYls2LCB6OjooMQoIvI4lOgRERGZYhaLhZycHGB0EozJZCI2Nhar1UpoaCgxMTEYjUYsFkuQIxWRYDIajSxbtozq6mrfts7OTgYHB5k/fz7JycmkpKRw7dq1x070WMItvqROVFIkJouRmLSoSZ3D/t572P/+f+IdHib0u9/h86x52AcGxiVLPv74Yzo6OnzHrF69mqVLl/q+9nQO4L3Wh+nFeRhMwekFFCgZn5ubS3FxMd3d3Rw4cID6+vrH/r6LiASDEj0iIiLToKOjg/379+N2u0lPTycqKor8/Hyqq6tpaWkhPDycoqIp6bcnIrNYT08Phw4dore3F7PZTF5eHqtWreL69escP36cgYEBIiMjffs7HA4AXyLYYrEwODg4JbF0nO+k4s8P4na5SX9hDtEp1gkf66qro+9Hf0r0j/4DppQUen7v91n5lz8m7jd/c1yyBCAnJ4dVq1YBEBoaOu487jPtYDRgXp42Jff0KAIl4+Pj44HRJXWA72sRkSeNEj0iIiLTICkpiR07dtDc3MyZM2e4ePEi1dXVZGdns3TpUj777DOOHTvGli1bgh2qBFGgJEB5eTldXV14vV7mzJnDxo0b73tglieD2+0mNzeXjIwMzp8/T11dHampqRw+fNj3Z/vP//zPvv0jIiIAGB4e9n0MDw+fkliSFsSz42+30nK8leqfn+PCgSaWbptYM2TnZ5+Pxvfd72CMi6P3j/490V+eI+YHvz0uWQLQ2trKjRs3SEpK4sUXX/QlrbxDI7jrbmHMS8AQHdz3s79kPMCuXbsYGRkhKiqKOXPmBDVGEZFHpUSPiIjIFLPZbDidTqKjozGbR/9XazKZfB/NZjMGg4GBgYFghimzgL8kQEZGBnFxcaxevZquri6OHDlCfX29KsCeUImJiSQmJgKQlpZGQ0MDfX19uFwuMjIysFgshIaGMjg4SH9/P7GxsYSFhdHU1ITVauXWrVssWLDgseOwNXfj7B8iOtWKOXT055I5dOKNmN23bQAYIyMxGAwYrFYG26/z05/+9L7KxZUrV+JwODh48CCHDx+mo6MDr9fLb7/wTbo8A5wauUnfz6pJTk5m/fr1WK0TryyaKv6S8UuWLGHHjh309PRw4MABTp8+rT5qIvJEUqJHRERkijmdTiorK3E4HISGhrJo0SLy8vIYHBykoaGBlpYWYmJiWL16dbBDlSDzlwQYGhpi3bp1AL4HYKfTGbQYZWq4XC5qa2uJjo5m/vz5nDp1iq6uLo4ePerbp6KigoULF7Jp0yaOHTtGRUUF6enprFix4rGvP9jnpPLvT+HoGSTUGsLirQtZ+NL8CR9vShp9n3rsdowhIXjtdsLS0vwmS8bEx8dz69YtjEYjbrcbc9FcqpqPERoayuvfeJ1PP/2UyspKtm7d+tj3Nxn+kvFut5umpiYSExOxWCwYDAbfayIiTxr99BIREZli6enpvPnmm/dtLywspLCwMAgRyWx3dxIgM3O0Ya7X6+XkyZOYTCby8/ODHKE8DpfLRUVFBU6nk+3bt2O1Wlm+fDm1tbUYDAYsFgvh4eF85zvf8R3zxhtvTGkMGcvS+N67pY98fNiml7nzt3+H44MPMaWk4B0cZHD5cqKNRl9CxGw2U1VVRX5+Pk6nk66uLoxGI/PmzaOpqQmXy0V/fz/PP/888fHxJCcn09zcjMfjwWicmsbMNpuNvXv34vV6efvtt7ly5Qo1NTU4HA6io6NZu3YtHo/nvmR8bm4uFRUV9Pf3YzKZfBMSRUSeREr0iIiIiATRvUkAs9mM1+vl6NGjXLlyhU2bNpGQkBDsMOURjf359vf3s3nzZoxGIy6Xi8zMTLKysujv7+f48ePk5eUFO9QHCnnhBWJ+/BfY/8fo1C3vb/8fVJpNOD780JcsWbhwIU1NTZSXl+P1ejEajaxatYrOzs7Rc4SEYDab6e7uxuFw0NLSgtfrZffu3cybN29KRpyfOHHCV0EEcPz4cSIjIyktLWXfvn2cOHGCHTt2+E3GT3VyTUQkWJToEREREQmSQEmAkydPcuHCBdatW0diYiKDg4NT1pBXJs7j8VBeXo7NZsPtdrNz506ioqIm1SzbZrNx+/ZtAPbt2weMVvd1d3fT1tZGWFgYubm5FBQUzNh9PSrrWz/A+tYPfF/fnyqBV199FYDz589z+fJlnnvuOW7dugWMVqmtWbOGY8eO8fOf/9zXr+y5556jvr7+sUect7S0YLfbycrKoqmpCYC4uDi8Xi8xMTGYzWZfY+gn3b2VS+++++59+7zzzjtBiExEZgMlekRERESCJFAS4MKFCwC+/i1z5sxh27ZtwQnyGZeZmUlkZCTNzc2+bZNplp2WlvZMPnD39fXR2dk5LgGxe/duXn/9dUpLS7lz5w7V1dWkpKQQFRV134jzrq4uYDRhNPa9rqqqoq+vz28TZ4/Hw+nTpykuLqatrc23PT8/nyNHjrBr1y7MZjObN2+eoe/A9Lq3cmmsQsntdrNnzx5SUlKCGZ6IBJkSPSIiIjPg3t++Hj58mLa2NkZGRkhOTmbDhg2+8b7y7AiUBNCErdnBaDSybNkyqqurx21/WLPsqagEetIVFBSQm5sLQE1NDa2trWzbto2LFy/S2NiIxWIhOTmZixcv4vF4/I44v9vBgwdHmzi/7r+Jc2NjI6GhoWRnZ9Pa2grAyMgIlZWVpKSksGzZMj799FP27t1LSEgImZmZU7JULBj8VS6NvRcvXrzI8PAwixYtCmaIIhJkSvSIiIjMgHt/+xoZGcmWLVtwuVx88sknnDlzho0bNwY5ShGZqIc1yw5UCbS8oIgDPz7Gta7b/MPPPiJ7ZSbrf2c15pCJjzp/ElitVl/yYcuWLb7tSUlJrFmzBhhNxNjt9vumdq1cuZKamhpfAm0iTZz9VRC99957GAwGjEYjFouFiIgIhoaGeO6556irq3vspWLBEKhyaUxDQwNRUVHMmzcvCNGJyGyhRI+IiMg08/fb15UrVwKjD4sWi4WhoaFghijyzOrp6eHQoUP09vZiNpvJy8tj1apVXL9+nePHjzMwMEBkZOS4Yx7WLPtBlUDDg8Os/K0XOFz7BbEDKVw5cpXsVZnkrMmc9nudTQKNOL98+TLnzp0jPz+fL7/8Eq/XO66Js9vtpre3F6/Xy9DQkK93lb8KotLSUrq6uqitrWXfvn1YrVZKSkoYHBy8b6mYx+MB8H09W/mrXPJ6vQB0dnZy+/ZtiouLMRgMwQxTRIJMiR4REZFp9LDfvtbU1OByuVi8eHEQohMRt9tNbm4uGRkZnD9/nrq6OlJTUzl8+DBz5syhqKiIQ4cOAfhGb585c+aRm2Wbw8x0eK9jjjaSkTaX3i/txKQ9e8s2nU6n3xHnH330EYODg3z11VdEREQwMDAwronzrl27CAkJwWQyERYW5jvfgyqInnvuOQA6OjrYv38/brfb71KxqKgo5syZM4PfhckL1PvorbfeoqGhAZPJ5LtfEXl2KdEjIiIyjR7029ezZ89SW1vLunXryMjICGaYIs+sxMREEhMTgdGeSQ0NDfT19eFyucjIyODAgQO+fSsqKli4cCGXLl0CJt8se6wS6PKpJqgO58vhRtJfmEN0ivWhxz5t0tPT/Y44nz9/Pl999RXDw8MMDw8DgZs4T7ZqJSkpiR07dty3VGzHjh309PRw4MABTp8+zSuvvDIl9zgdAvU+cjqdNDU1kZOTMy4BJiLPJiV6REREplGg376OLetYtmwZmZmZOBwOIiIighipyLPN5XJRW1tLdHQ08+fP59SpU3R1dfHbv/3bfPDBB7hcLn74wx8CsGHDhoeer7e319dj5t5KoNVb1hBfmsD16g6+/LCBCweaWLgxm7J/9yl3bg9gspjILEp7Knv3PMxEmjhnZWWxatWqSZ030FKxpqYmEhMTsVgsGAwG32uzVaDKJcD3/hQRMYz9VnEiioqKvGfOnJnGcERERJ4udrudwcFBYHzfiM8//xy73e7bz2q1+v3ttohMP5fLRUVFBQMDA2zfvp3o6Ghqamqora0FwGKxEB4ezne+850Jn/MnP/nJuK/HKoG8fQYYNkCEl2hHHP2nnJT865UseDGLtrMdJGTH0fDJJerKGtn8RyXPXO+e6XL9+vVxS8Wys7MpLCykoqLCl4hLTU1l3bp148a2w/1TEz0eDydOnODq1auMjIyQn58/6cSTiMi9DAZDjdfrnZKxm7M7ZS0iIvKEC/TbVyV1RGaHsSRPf38/mzdvxmg04nK5yMzMJCsri/7+fo4fP05eXt6kzvvOO+/ct23Dhg20nb1B5d+fwtEzyLAVFm9dyMKX5mMyG31JnaikSEwW4zPZu2cqjXzZwcivLwKQCHyLNEwvpGLZ/nUPmzfeeOOh57l3auKJEye4fPkyL730ElarlZ6enmmJX0TkUSnRIyIiIhJEQ0NDlJWVYbfbMZlMZGZmUlJSwsmTJ7ly5Qoul4v169dPOtEgE2Oz2bh9+zYA+/btA6CwsJDu7m7a2toICwsjNzeXgoKCKblexrI0vvduqd/XOs53UvHnB3G73M9s756pZFqcjCknDgD3BRsjn1zB+C9fT5S/qYmXLl1iwYIFZGVlAfh6PImIzBZK9IiIiIgEkdFoZMWKFSQkJNDY2EhdXR1ZWVkkJCQQFhbmWz4k0yMtLc1v9U0wJC2IZ8ffbqXleCvVPz/HhQNNLN329E9QundpVFdXF1VVVfT19ZGcnMz69evvW041EQaLCSyjPY7cjbch0oIxP2nCx/ubmuh0OnG73XR2dvL+++9jsVhYvnw5CxYsmHR8IiLTRYkeERERkSCyWCzk5OQAo0v9TCYTsbGxxMfHc/369SBHJzPF1tyNs3+I6FQr5tDRf6KbQ5+NRsz3Lo06ePAgoaGhvP7663z66adUVlaydevWRz6/p3MA77U+TC/Ow2AyTvg4f1MTQ0NDfa9/4xvf4OTJkxw+fJh58+ZhsVgeeL57E1p3N+kfM1uSjiLyZFOiR0RERCTIOjo62L9/P263m/T0dKKi1JvladDT08OhQ4fo7e3FbDaTl5fHqlWrKC8vp6urC6/Xy5w5c9i4cSODfU5f755Qa4ivd8/T7t6lUS6Xi/7+fp5//nni4+NJTk6mubkZj8eD0TjxJM3d3GfawWjAvDxtUscFmpo4d+5cBgcHMZlMGI1GDAbDhGK7N6E11qvN7XazZ88eUlJSJhWfiEggSvSIiIiIBFlSUhI7duygubmZM2fOcPHiRZYsWRLssOQxud1ucnNzycjI4Pz589TV1ZGRkUFcXByrV6+mq6uLI0eOUF9fT1FRUcDePfb33sP+9/8T7/Awkd97k6h/+4cYDIYpjXW6lk89iL+lUSEhIZjNZrq7u3G73fT29uL1ehkaGiI8PHzS1/AOjeCuu4UxLwFDdOjDD7hLoFHvYWFhHDlyhLKyMiIjI3n55ZcxmR5cfeWv18/Y9/PixYsMDw+zaNGiSd/fdFH1kciTTYkeERERkSCy2Wx0dnZy9OhR3zaDwcChQ4e4du0aMPogOGfOHKKjo4MVpjyCxMREX6PetLQ0GhoaGBoaYt26dfT09FBXVwfAuXPnGBkZ8Vvt82JiEn0/+lOif/QfMKWk0PN7v49l8WLCt34z4HUfJWkzncunRtwe/q+fVXOxox/XiIc9/6aEtLhwv0ujvF4va9as4dixY+zatYuQkBBMJhNhYWEPvMdAI8/ddbfA5cZUNHfScQeamgjw2muvTfg8/hJad2toaCAqKop58+ZNOsbpouojkSfbo9U/ioiIiMiUcDqdnDhxwvf1okWL6Orq4sqVKwwPDwNw8+ZN30QoefK4XC5qa2uJjo4mM3N0hPrIyAhmsxmj0ci8efOoq6ujvb2duLg4XnvtNdasWUNrayttv/gFABHf/Q7h3yrFEB7O4KefPfB6Yw/pYw4ePIjBYOD111+nv7+fysrKcfvfXW0yFm9/fz9paWm+5VPt7e14PJ5H/h6sXZjEi3njGyHfvTTq8uXLwOjSqKSkJEpLS3nllVeIjIxk4cKF91Uw3XuPYyPPX3zxRbZt20ZCQgIA5hVzCfuzDZiyJzdtayrdndDyer0Avo+dnZ3cvn2b/Pz8Ka/SutvIlx04/+LwuP+Gf33B7773vh9++tOf8otf/IKIiAg++OADhoeHuX79Oj/5yU/4yU9+Mm0xi8ijU0WPiIiISBANDw8TERFBcnIyTU1NrFmzhp/97Gfk5eWxfv36YIf3TJtMj527m/TezeVyUVFRgdPpZPv27ZjNZrxeLxcuXKCrq4tNmzbh8Xhobm72VfvA18t6vF1dABgjIzEYDBisVjy22wFjnmzPm5lYPmU2Gfntkhz+18HL47YHWhp18eJFGhsbsVgsZGVlsWrVqgfeI8zukeeBev289dZbNDQ0YDKZeO656Z2uNtFR8/7eD3dX9sTHxzM0NMSrr77K3r17VdkjMksp0SMiIiISJBrfPLtNtsfOvcaSPP39/WzevBmj0YjL5eLkyZNcuHCBdevWER0dzcGDB8dV+3i9Xk6ePInJZCJu/nzcBw7isdsxhoTgtdsxJvofEf4oSZvHWT71uAItjUpKSmLNmjUTvsfZ/ncmUELL6XTS1NRETk7OlH9v7zXRUfN3vx8aGhoAmDdvHs3NzXR2dtLd3U1xcTG3bt2adX2FRORrSvSIiIiIBMlUj2+WqfWgHjvwddWN0+n0e7zNZuP27dHqm7Gld4WFhVy4MLpkZqwvk9Fo5Nvf/rav2ufo0aNcuXKFTZs2Ed/by+3/309wfPAhppQUvIODhG/e5Pd6j5K0CVRt8vrrr1NaWsqdO3eorq4mJSVlWpcWTdST+HfmQb1+fvjDH85oLA8bNe/v/dDS0gKMfu/Hqo8+/vjjWddXSES+pkSPiIiISJBM9fhmmR7+euzcXXWTn5/v97i0tDS/k4mKioomVO2TmJiIOyWFmB//Bfb/MTp1y/q7v0PYq/6bIj9K0uZRl09N1tXbdvocoz2n2nschJiNJEZNbgrWg+5Rf2cm5mGj5sfeD83NzTQ2NuJyuUhPT6etrY2mpibmz59Pf38/t2/fpri4eFYk/0TkfoaxRmATUVRU5D1z5sw0hiMiIiLy7LDb7QwODgJfP2SXlpb6xjd3dnYSGRnJypUrfb1HZGaNJWQGBgbYvn070dHRvqqbC/UXCT8bh7PXhcliIrMojfW/sxpzyINHbQPcuHHjvgbbhYWF1NbWjts2Z84ctm3bNqFYA72fLl++PC5ps3r16hmvdFn1Hz8d9/XWF9L4s9Klkz6P/s48Ou/QCEN/ewLj/DhC3ljywH2PHz/OV199NW6b2Wzmrbfe4vDhwzQ1NfG9731v2peciTxLDAZDjdfrvX8d8KOcS4keEREREZH73Vt1Ex0dTUhIiK/qZvWK1Zh6QkjIjqf5cCt1ZY1s/qMSctZkBjv0cQKNN5dH4/F4KC8vx2az4Xa72blzJ1FRUb4ldy6Xi/Xr15OXlxfsUMcZqW5n5OPLWH7r+YdOIQuUUIuKiuLnP/85OTk5bNy4cSbCFnlmTGWiR0u3RERERET8eFiPnRPVJwCYMzCH7KRcTBYjMWlRwQn2IdYuTCI5OpSD528FO5SnQmZmJpGRkTQ3N/u2JSQkEBYWdl9V1kS09rfyX8/8Z9oHbhBqCmVT5iZ+sGRq+/eYV8zFvGLuhPadTX2FRGTylOgREREREfHjQT12xnSc76Tizw9yw3WG9BfmEJ1inckQJyTQePOgulEL/3sFeD3wp8Ngmr2PJTabjb179+L1enn77bfxeDzY7XZfM+izZ89SUlJCfn4+169ff6RrDHtcbMx8mcLk5VS0lLP3yh4KU4p4Pun5R4rx7h5GY/y9l0Xk6TR7f6KKiIiIiMxySQvi2fG3W2k53kr1z89x4UATS7c9F+ywZr9P/28wWsA9FOxIHurEiRMYjUbcbrfv68uXL5ORkUFLSwuxsbGPfY35sQuYHzs6Dv75xBf4uGU/dtedR47xzTffBMDtdrNnzx5SUlImfC4ljUSefEr0iIiIiIg8AltzN87+IaJTrZhDR/9ZbQ69vxHzkH2Isn/3KXduD0y6afNTqXEv9F2D/FL46ld+d/FXRXPixAmuXr3KyMgI+fn5jz0JbCJaWlqw2+1kZWXR1NQEwKVLl1iwYAEREREAZGdnT9n1BoYH+NXFXzInMo2ilIm16vAX49iyq4sXLzI8PMyiRYsmHMNUJo1EJDiU6BEREREReQSDfU4q//4Ujp5BQq0hLN66kIUvzb9vP6PJyIrvv0BCdhwNn1yirqyR7FWZM9q0earGmz829zB8/kew6a/h8v6AuwWqonnppZewWq309PRMe6gej4fTp09TXFxMW1sbAE6nE7fbzc2bNxkYGACgsbGRJUuW4PF4fNscDgf9/f1ER0dP+HoDwwP8x+N/Qr+rn7968a8JNT98opW/GO/W0NBAVFQU8+bNm1AMU500EpHgUKJHREREROQRZCxL43vvlj50P0u4xZfUiUqKDErT5u/+92O+z39395lHHm/+2Gr+N0QkQP634HLF6Davm7sfSx5URTM2Mj0xMfGBlwk0Geuzzz6jvb0dr9dLYmIiGzZsCJiMaWxsJDQ0lOzsbF8/ntDQ0eRYX1+fb78vv/wSu93OwMAAHR0dAFRXV9PY2OirhnkYx7CDPzv2J3QM3OCPi3+ExWjBMewgwhLxwOP8xTg2Vbmzs5Pbt29TXFyMwWB4aAxTnTQSkeBRokdEREREZJqNNW12u9xBadp88j99Y8L7Dg0NUVZWht1ux2QykZmZSUlJCYcOHZpwkiSgrktw/ST82PL1tr9OgB/ZgQdX0XR2dvL+++9jsVhYvnw5CxYseOCl/E3Gys3Npbi4mO7ubg4cOEB9fT1r1671e3xfXx+dnZ3jetTs3r2buXPnMjg4yEsvvcSJEye4efMm69evx2R69KV4Tb1XuNx7CYAfHftjAL6b9yZv5n/vgccFivGtt96ioaEBk8nEc89NrGfUVCaNRCS4lOgREREREZlmj9O02f7ee9j//n/iHR4m8ntvEvVv/3BaH7aNRiMrVqwgISGBxsZG6urqyMrKmlSSJKA1fwgF3x/9/Mh/gkv74LcP+15+UBUNwDe+8Q1OnjzJ4cOHmTdvHhaLBX+MRiPLli2jurp63PaxfjoejweA+Pj4gKEWFBSQm5sLQE1NDa2trWzbto2wsDCOHDlCWVkZkZGRvPzyy4+V5AFYmlTAr3+j4r7t586do76+Hq/XS15eHitWrBj3Zx8oRqfTSVNTEzk5OYSFPXwJGExt0khEgkuJHhERERGRaTTRps3+uOrq6PvRnxL9o/+AKSWFnt/7fSyLFxO+9ZvTFq/FYiEnJwcY7c9iMpmIjY31JUUmkiQJKCZj9D+AN8vve/lhVTQmkwmj0YjBYMBoNE7++sCuXbsYGRkhKiqKOXPmBNzParX6+tNs2bJl3GuvvfbaI117Mm7evMmpU6dYsWIFVquVL774gvj4+HGVTA+K8Yc//OGkrjeVSSMRCS4lekREREREptFEmzb74/zscwAivvsdjHFx9P7Rv2fw08+mNdED0NHRwf79+3G73aSnpxMVNdpTaKJJkkc13VU0Ho+HmJgYenp6uHPnDsePH2fr1q3cvHmTqqoqBgYGSE9Pp6SkhJCQkKm+vQfGdW9PoVu3bgGjPYCGh0cbabe2tj50ydqDTHSa2eMmjUQkuJToERERERF5gJ6rjZzZ9Zf0XG0kJCKKtf/mb0h6bvnXr/f0cOjQIXp7ezGbzeTl5bFq1SquX7/O8ePHR5MH30mnpGSrbxnSkH2If5rAyHX3bRsAxshIDAYDBqsVj+32tN9zUlISO3bsoLm5mTNnznDx4kWWLFnCjh076Onp4cCBA5w+fZpXXnllSq/7wCqawjT436+D1wO/OfzQc/X29uJ0OgHo7+/HZDLR3t5OWloaISEhdHR0YDKZGBkZ4fPPPyc1NZX169ezf/9+qqurJ7wsbSoaP8P9PYUiIyMBmDt3LlevXgVG+yc9SGt/K//1zH+mfeAGoaZQNmVu4gdLvk7SzIZpZiIy/ZToEREREREJYHhwgC/+n39FVOo8Nv/4lwzcbscUMn75itvtJjc3l4yMDM6fP09dXR2pqakcPnyYOXPmsHHjRvbt28fp06d58cUXgYmPXDcljU6X8tjtGENC8NrtGBOTpvWebTYbTqeT6OhozGaz7x6bmppITEzEYrFgMBh8r82YT/9vMFrA/eBkx5gPP/zQ93lFRQUZGRnY7Xb6+/t921944QU6OzsZHBzk1q1blJeX4/F4uHjxIitXrpxwA+rHbfzsr6dQTk4Oly9f9iV5jEajLwkWyLDHxcbMlylMXk5FSzl7r+yhMKWI55Oen5JpZiLyZFCiR0REREQkgBu1hxnq62LdH/wdcfPyiJuXd98+iYmJvgfktLQ0Ghoa6Ovrw+VykZGRQVJSEvHx8b7mwjDxkethm17mzt/+HY4PPsSUkoJ3cJDwzZum6W5HOZ1OKisrcTgchIaGsmjRInJzc6moqPBVxsydO5fi4uJpjWOcxr3Qdw3yS+GrX03okHfeeSfga9XV1Zw9e5aIiAjfEqm8vDwWLlzIJ598Ql9fH62trRNK1kxF4+dACgsLycjI4Pjx43g8HhYuXPjAqp35sQuYHzu6tOv5xBf4uGU/dtedKZ1mJiKznxI9IiIiIiIBDNhuAHDuF3/DnZvXiJ2XR/E7P8aaPPe+fV0uF7W1tURHRzN//nxOnTpFV1cXLpeL/v5+XC7XuP0nMnI95IUXiPnxX2D/H6NTt6y/+zuEvbp1em72X6Snp/Pmm2/et/2NN96Y3Ilu1ML/XjG61OpPh8H0iI8e7mH4/I9g01/D5f1+d5lo7xl/IiIiAEhISCAmJsY31WoqGlA/Tk8jt9vNoUOHGBgYAOC5554jJSWFpt4rAat2xgwMD/Cri79kTmQaRSlFUzbNTESeDEr0iIiIiIgEEGqNBSAh93me3/kHHPrxD6j71d+y5vf+y7j9XC4XFRUVOJ1Otm/fjtVqZfny5dTW1nLhwgUsFst9y24mOnLd+tYPsL71gym7p5EvOxj59cVx20wvpGLZPsWjsye51Cqgmv8NEQmQ/y24/C8jyL1u7n6UmUzvmXt798TGxhIWFsb58+c5dOgQHo+HyMjIgA2oAyWVLl++DMDZs2cpKSkBmFRPI389hbZt20Z7eztHjhwhKiqK/v7+gFU7YwaGB/iPx/+Eflc/f/XiXxNqDpuRaWYiMnso0SMiIiIiEkBqwRqMJgtGswVTSCgYDBgtoeP2GUvy9Pf3s3nzZoxGIy6Xi8zMTLKysujv7+f48ePk5X297OtxRq4/LtPiZEw5cQC4L9gY+eQKxn/5eso8wlKrgLouwfWT8OO7qkz+OgF+ZAeYdO+Ze3v3LFy4kE2bNnH06FGMRiPR0dH09vYGbEA9NDR0X1Lp0qVLzJkzh+vXrxMaGorD4aCjo2NSPY38xXXnzh06OjqA0eVmjY2Nvmqre6t2ABzDDv7s2J/QMXCDPy7+ERajBcewY9qnmYnI7KJEj4iIiIhIAJFJc1n5r/+K+g/+G5c/+TmpS1fz/Hd/f9w+NpuN27dHJ2Ht27cPGO2t0t3dTVtbG2FhYeTm5lJQUOA75lFHrgea8FVeXk5XVxder9fXAPrupTl3M1hMYBl9mHc33oZIC8b8KWzwPIGlVpOy5g+h4Pujnx/5T3BpH/z2YYBH6j3jr3ePzWZjzZo1REdHc/XqVU6ePOm3AbXT6WRgYOC+pJLb7eb69esAnDt3ju7ubl/j54n2NHpQT6F7+avaAWjqvcLl3ksA/OjYHwPw3bw3eTP/e4Gnmb322oSvKyJPBiV6REREREQeIGvtq2StfTXg62lpaZN6SAfIWJbG994tnXQs/iZ8ZWRkEBcXx+rVq+nq6uLIkSPU19dTVFT0wHN5OgfwXuvD9OI8DKYpXK4zgaVWkxKTMfofwJvl416aqt4zE2lAnZaWRm9vr9+kUlxcHENDQzPS0DhQ1U6EJYKlSQX8+jcqpu3aIvJkUKJHRERERGQaeDweysvLsdlsuN1udu7cCcAvf/nLcftFRUX5XnsYfxO+hoaGWLduHYCvamOs18uDuM+0g9GAeXnahO9pQh6y1GoqTVXvmYk0oD5//jxDQ0NBb2j8oKodERFQokdEREREZNpkZmYSGRlJc3MzAJGRkb6EgsPhoKysjLlz75/g9TB3T/jKzBwd0+71ejl58iRGo5HW1lZ++tOfYjKZyMzMpKSkhI8//tjX78XsMfCd7oVY8hIwRPtf4vXIHrDUaqrNZO+Z2dLQWFU7IvIwSvSIiIiIiEwDo9HIsmXLqK6uHrdtrOrmwoULACxevHhS5713wpfZbMbr9XL06FGuXLnChg0bMBqNJCQk0NjYSF1dna8pcU5ODqtWrcLwZSemQ62YiiafZHqoByy1mmpWq3XGes+oobGIPCmU6BERERERmWEej4cLFy6QmppKQkLChI8LNOHr5MmTXLhwgXXr1pGamorJZCI8PByr1YrJZCI2NhaA1tZWbty4QVJSEi/+3y8Sds/IdwlsJpNKIiKPQ4keEREREZEZ1tLSgsPhYPXq1ZM6LtCEr7HqoKNHjwKQkJBAb28vbreb9PR0oqKiyM/PZ+XKlTgcDg4ePMiJEyfYvHkzNpuNvXv34vV6efvtt/F4PJw4cYKrV68yMjJCfn4+q1atmsK7989fT6OoqCicTidHjx6lvb0dr9dLYWEhBQUFDA0NUVZWht1uH7dE7dChQ759ExMT2bBhA9HR0dMev4jIbKFEj4iIiIjINOnt7fU1Rh6b3hQREUFDQwPh4eFkZ2dP6nyBJnzdO2FrZGQEu91Oc3MzZ86c4eLFiyxZssT3enx8PN3d3QCcOHECo9GI2+32fX358mVeeuklrFYrPT09k4rxcdzb0wjgiy++wGazsXnzZkwmE4ODg8DoMrgVK1bct0QtNzeX4uJiuru7OXDgAPX19eTl5c2KZNbdWvtb+a9n/jPtAzcINYWyKXMTP1jyQ7/7BkqCjS3Xc7lcrF+/nry8vBm9BxGZnZToERERERGZJh9++KHv84qKChYuXEhBQQEdHR0UFhZOS9Nem82G0+kkOjoas3n0n/tms5mqqiry8/NxOp10d3eTmZlJS0sLdrudrKwsmpqaALh06RILFizw9fUZm/I13fz1NHI4HLS1tbF8+XKqq6vvS3Tk5OQwPDxMY2MjMDptbNGiRcBocgRGk1oTTWbZ33sP+9//T7zDw0R+702i/u0fYjAYpuV+hz0uNma+TGHycipaytl7ZQ+FKUU8n/S83/39JcESEhIICwujtrZ2WmIUkSeTEj0iIiIiItPEX/XNg7ZPBafTSWVlJQ6Hg9DQUBYtWsTChQtpamqivHy0OXJaWhqrVq1i//79FBcX09bW5jvW7XbT2dnJ+++/j8ViYfny5SxYsGDa4n0Qu310JPvVq1fp6+vDZDL5kjUAHR0d7Nu3D6/XC0BERAQAu3btYmRkhKioKNxu94SSWa66Ovp+9KdE/+g/YEpJoef3fh/L4sWEb/3mtNzb/NgFzI8d/b4+n/gCH7fsx+6643dff0kwgPz8fK5fvz4t8YnIk0uJHhERERGRp0h6erpvhPvdXn311XFfnz9/ntDQULKzs2ltbQUgNPTrUevf+MY3OHnyJIcPHyajLwxvxZVxx5teSMWy/blpuIOvjcUTGhrK9u3bqagYP1Y8IiICk8lEQkICt27d4vr162RlZbFjxw56eno4cOAAp0+fZv369Q9NZiV/9vnoOb/7HYxxcfT+0b9n8NPPpiTRc28fpLtHtA8bhjkee5Q5sWkUpRQ94CwiIhOjRI+IiIiIyDOor6+Pzs7OcUmH3bt3M3fuXAYHBzGZTBiNRgwGA6bFyZhyR5dwuS/YGPnkCsacuCmP6d6eRrGxscTGxo7GYDKNW0Zls9k4deoUWVlZeL1ebt26hcfjoampicTERCwWCwAmk2lCyazXbV0AGCMjMRgMGKxWPLbbU3Jf9y4dG0vE3Rm6w7//4v+D2+zmz9f8BaHmsCm5nog825ToERERERF5BPdWaVy5coWamhocDgfR0dGsXbuWtLS0YIcZUEFBAbm5uQDU1NTQ2trKtm3bCAsL48iRI5SVlREZGcnLL7+MOSwE/iUH4W68DZEWjPlJUx6Tv55GL730ElVVVezZs4eQkBDf652dnbS3t2M0GjGZTADMnTuX2tpaX+PryMhI7ty5M6FkljkxARfgsdsxhoTgtdsxJj7+Pfrrg2S1WnEMO/jLo/8Ju9HO/2v+/xuL0YJj2EGEJcLvefw19vZ4PAwMDACj/Yz6+/s1YUxElOgREREREXkU91ZpHD9+nMjISEpLS9m3bx8nTpxgx44dQY4yMKvVitVqBWDLli3jXnvttdf8HuPpHMB7rQ/Ti/MwmKa+kXSg3kWlpaX09vZSX19PY2Mj/f39vqSPx+PxNV4+ePAgpaWlJCWNJmjsdrtvStfDklmRvb04/vbvcHzwIaaUFLyDg4Rv3vRY9+PxeDh9+vS4Pkhjmnqv0OZsAyP8f5v+GzTBd/Pe5M387/k9l78k2J07d+jo6ACgurqaxsZGv8v2ROTZokSPiIiIiMgk+avSiIuLw+v1EhMTg9ls9i0dmg0mM8r7Qdxn2sFowLx85iuV7k10zJs3j9LSUgCuXbtGbW0ta9euJS7u6yVlk01mxfz4L7D/j9GpW9bf/R3CXt36WDE3Njbe1wdprHF0ijeV1zpfp7i4mBdeeOGh55rOBt4i8nRRokdEREREZBICVWnk5+dz5MgRdu3ahdlsZvPmzUGMcrzJjvL2xzs0grvuFsa8BAzRoQ8/YIo9KNGRlJREUdHjNzK2vvUDrG/94LHPMyZQH6S33nqLhoYGTCYTzz03vQ2tReTZo0SPiIiIiMgk+KvSGBkZobKykpSUFFauXEllZSWHDx/mjTfeCHK0oyYzyjsQd90tcLkxFc2djhCfaIEqpgL1QXI6nTQ1NZGTk0NYmBowi8jUUqJHRERERGQS/FVpvPfeexgMBl9jYIPBgN1uD2KU/g0MD/Cri79kTuTkR3mbV8zFvOLxkjwej4fy8nJsNhtut5udO3cSFRWF0+nk6NGjtLe34/V6KSwspKCg4LGuNZMeVDEVaOnYD384+aVzIiIToUSPiIiIiMgk+KvSKC0tpauri9raWsrKyrBarbz44otBjnS8geEB/uPxP6Hf1c9fvfjXQRvlnZmZSWRkJM3Nzb5tX3zxBTabjc2bN2MymXwNlJ8UU1ExJSIyVZToERERERGZhEANfpOSkmZtvxXHsIM/O/YndAzc4I+Lf/TQUd7TxWg0smzZMqqrq7+OzeGgra2NoqKiKRlHH8yx949TMSUiMlWmfiaiiIiIiIjMKk29V7jcewn7sJ0fHftj3vr0/6Dsyt5ghwXgW+J29epV/uEf/oGPPvqI69evP/L5xsbejzl+/Dhms5nS0lIGBwc5ceLEY8fsz90VU3++5i+CVjElIqKKHhERERGRp9zSpAJ+/RsVwQ7Dr9DQ0QleISEhlJSUcOjQIQ4fPsz3v//9SZ8rWGPvZ0vFlIgIKNEjIiIiIiIzqLe3F6fTCUB/fz+xsbHExsZiMBh8jaxNJtOkGzcHc+z9WMUUwI+O/TEA3817kzfzvzfl1xIReRglekREREREZMZ8+OGHvs8rKipYuHAhL730ElVVVezZs4fY2Fg2btwITK5x81SPvR8aGqKsrAy73Y7JZCIzM9NXcTSWZEpMTGTDhg2zumJKRJ49SvSIiIiIiMiMeeedd/xuLy0tvW/bZBo3T/XYe6PRyIoVK0hISKCxsZG6ujqysrLIzc2luLiY7u5uDhw4QH19PWvXrp3QOUVEZoISPSIiIiIi8kS4u3Hz+fPniYiIYOXKlaSnp0/52HuLxUJOTg4wOmnNZDIRGxtLfHw8AB6PB8D3tYjIbKFEj4iIiIiIPBHGGjdbhry81JbE0dibfFH+KW/cnk/oC6lYt4+Ot5+qsfcdHR3s378ft9tNeno6UVFRAOzatYuRkRGioqKYM2fOY96ViMjU0nh1ERERERGZle5t3Gw2m4mNjcUYFUr4b72AMSECU3gIAMacuCm/flJSEjt27KCoqIjr169z8eJFAHbs2MErr7zCwMAAp0+fnvLriog8DiV6RERERERkVvrwww9pbGwERhs3V1dX89JLL+EaHmbvZxUYzSZe9M6DSAvG/KQpvbbNZuPmzZsYjUbM5tGFEG632ze23WKxYDAYfK+JiMwW+qkkIiIiIiKz0sMaN3s6B3D9r2pML2ZgME3t77CdTieVlZU4HA5CQ0NZtGgRubm5VFRU0N/fj8lkYu7cuRQXF0/pdUVEHpcSPSIiIiIi8kRyn2kHowHz8rSH7zxJ6enpvPnmm/dtn8hodhGRYNLSLREREREReeJ4h0Zw193CmJeAITo02OGIiMwaSvSIiIiIiMgTx113C1xuTEVzgx2KiMisoqVbIiIiIiLyxDGvmIt5xexI8rT2t/Jfz/xn2gduEGoKZVPmJn6w5IfBDktEnlFK9IiIiIiIiDyGYY+LjZkvU5i8nIqWcvZe2UNhShHPJz0f7NBE5BmkRI+IiIiIiMhjmB+7gPmxCwB4PvEFPm7Zj911J8hRicizSj16REREREREpsDA8AC/uvhL5kSmUZRSFOxwROQZpYoeERERERGRxzQwPMB/PP4n9Lv6+asX/5pQc1iwQxKRZ5QqekRERERERB6DY9jBnx37E27Yb/CHy/8tFqMFx7Aj2GGJyDNKFT0iIiIiIiKPoan3Cpd7LwHwo2N/DMB3897kzfzvBTMsEXlGKdEjIiIiIiLyGJYmFfDr36gIdhgiIoCWbomIiIiIiIiIPDWU6BEREREREREReUoo0SMiIiIiIiIi8pRQokdERERERERE5CmhRI+IiIiIiIiIyFNCiR4RERERERERkaeEEj0iIiIiIiIiIk8JJXpERERERERERJ4S5mAHICIiIiIiEgwej4fy8nJsNhtut5udO3cSFRVFeXk5HR0dvv1Wr17N0qVLgxipiMjEKdEjIiIiIiLPrMzMTCIjI2lubh63PScnh1WrVgEQGhoajNBERB6Jlm6JiIiIiMgzyWg0smzZMmJiYu57rbW1lT179lBVVcXQ0FAQohMReTRK9IiIiIiIiNwlPz+f1157jZKSEm7cuMGJEyeCHZKIyIRp6ZaIiIiIiMhdFixY4Ps8Pj6e7u7uIEYjIjI5SvSIiIiIiMgzq7e3F6fTCUB/fz8mk4mamhry8/NxOp10d3eTmZkZ5ChFRCZOiR4REREREXlmffjhh77PKyoqWLhwIQMDA5SXlwOQlpbGmjVrghWeiMikGbxe78R3NhhuA9emLxwRERERERERkWfOPK/XmzQVJ5pUokdERERERERERGYvTd0SEREREREREXlKKNEjIiIiIiIiIvKUUKJHREREREREROQpoUSPiIiIiIiIiMhTQokeEREREREREZGnhBI9IiIiIiIiIiJPCSV6RERERERERESeEkr0iIiIiIiIiIg8JZToERERERERERF5Svz/AYbppSXQBZYGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_latent = model_down(val_x)\n",
    "from sklearn import manifold, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "train_val_split = np.random.rand(len(val_latent)) < 0.4\n",
    "X_tsne = manifold.TSNE(n_components=2, init='pca', n_iter=5000, method='exact').fit_transform(val_latent[train_val_split])\n",
    "y = val_y[train_val_split].argmax(axis=1).reshape([-1,1])\n",
    "x_min, x_max = X_tsne.min(0), X_tsne.max(0)\n",
    "X_norm = (X_tsne - x_min) / (x_max - x_min)\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(X_norm.shape[0]):\n",
    "    plt.text(X_norm[i, 0], X_norm[i, 1], str(y[i,0]), color=plt.cm.Set1(y[i,0]), \n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig(\"result/wifi_pca_latent16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = val_y[train_val_split].argmax(axis=1).reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1195, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=928898, shape=(32,), dtype=float32, numpy=\n",
       "array([-0.44223818,  0.34099025,  0.22630885,  0.3531104 ,  0.01432228,\n",
       "        0.28596947,  0.20210937, -0.18629518,  0.34547195, -0.10634677,\n",
       "       -0.45710066,  0.11655647,  0.05475373,  0.08871596,  0.08869676,\n",
       "       -0.03721688,  0.13843378, -0.02852063,  0.02335411,  0.22630875,\n",
       "        0.3144795 , -0.30959225,  0.29096606,  0.4847697 ,  0.32044274,\n",
       "       -0.08135206, -0.40763628, -0.03506229, -0.0032622 ,  0.06158475,\n",
       "        0.29149553,  0.15948303], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_encoder_decoder_ann.weights[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=928903, shape=(32,), dtype=float32, numpy=\n",
       "array([-0.44223818,  0.34099025,  0.22630885,  0.3531104 ,  0.01432228,\n",
       "        0.28596947,  0.20210937, -0.18629518,  0.34547195, -0.10634677,\n",
       "       -0.45710066,  0.11655647,  0.05475373,  0.08871596,  0.08869676,\n",
       "       -0.03721688,  0.13843378, -0.02852063,  0.02335411,  0.22630875,\n",
       "        0.3144795 , -0.30959225,  0.29096606,  0.4847697 ,  0.32044274,\n",
       "       -0.08135206, -0.40763628, -0.03506229, -0.0032622 ,  0.06158475,\n",
       "        0.29149553,  0.15948303], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_down.weights[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gps_wifi",
   "language": "python",
   "name": "gps_wifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
