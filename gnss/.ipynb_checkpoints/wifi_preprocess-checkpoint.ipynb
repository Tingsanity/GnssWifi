{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#File = 1\n",
    "#prFileName = 'dataRssi_at_%d.txt'%File\n",
    "#dirName = '/home/lyt/gnss_wifi/gnss/20201022/indoor/wifi/'\n",
    "#outputname = 'outplot/1out_in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = glob.glob('20201022/indoor/wifi/dataRssi_at_*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[0].split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "origin_data = []\n",
    "train_label = []\n",
    "for data in datas:\n",
    "    load_data = np.loadtxt(data)#f.read()\n",
    "    load_data = load_data[:,:15].astype(int)\n",
    "    load_data[load_data==0] = -100\n",
    "    load_data = load_data/100 + 1\n",
    "    #print(load_data.shape)\n",
    "    for i in range(len(load_data)):\n",
    "        origin_data.append(load_data[i])\n",
    "        mask = np.random.rand((15))\n",
    "        mask = (mask>0.1).astype(int)\n",
    "        noise = np.random.normal(scale = 0.05,size=(15))\n",
    "        load_data[i]*mask\n",
    "        load_data[i]+noise\n",
    "        train_data.append(load_data[i])\n",
    "        train_label.append(data.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0])\n",
    "    #print(load_data.shape)    \n",
    "    \n",
    "    #print(np.array(train_data).shape)\n",
    "origin_data = np.array(origin_data).astype('float32') \n",
    "train_data = np.array(train_data).astype('float32')\n",
    "train_label = np.array(pd.get_dummies(train_label)).astype('float32')\n",
    "#train_label = train_label.reshape(len(train_label),1)\n",
    "train_val_split = np.random.rand(len(train_data)) < 0.70\n",
    "train_x = train_data[train_val_split]\n",
    "train_o = origin_data[train_val_split]\n",
    "train_y = train_label[train_val_split]\n",
    "val_x = train_data[~train_val_split]\n",
    "val_o = origin_data[~train_val_split]\n",
    "val_y = train_label[~train_val_split]\n",
    "BUFFER_SIZE = train_x.shape[0]\n",
    "BATCH_SIZE = 5\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x,train_o,train_y))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_x,val_o)).batch(len(val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1352, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_label).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.array(pd.get_dummies(train_label)).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for a,b,c in train_dataset:\n",
    "    print(a.dtype)\n",
    "    print(b.dtype)\n",
    "    print(c.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        self.dense = dense\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name='bias',shape=[self.dense.input_shape[-1]],initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b = True)\n",
    "        return self.activation(z + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\"])\n",
    "#with mirrored_strategy.scope():\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(15), name='input_layer1')\n",
    "#flatten = tf.keras.layers.Flatten()(input)\n",
    "dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "dense2 = tf.keras.layers.Dense(24, activation='relu')\n",
    "dense3 = tf.keras.layers.Dense(16, activation='relu')\n",
    "model_encoder = dense1(input)\n",
    "model_encoder = dense2(model_encoder)\n",
    "model_encoder = dense3(model_encoder)\n",
    "model_down = tf.keras.Model(inputs=[input], outputs=model_encoder,name = \"encoder\")#input1, input2,input3,input4,input5,input6,input7,input8,input9,input10\n",
    "#model_down.summary()\n",
    "#input_encoder = tf.keras.layers.Input(shape=(15), name='input_layer2')\n",
    "input2 = tf.keras.layers.Input(shape=(16), name='input_layer2')\n",
    "model_decoder = DenseTranspose(dense3, activation = 'relu')(input2)\n",
    "model_decoder = DenseTranspose(dense2, activation = 'relu')(model_decoder)\n",
    "model_decoder = DenseTranspose(dense1, activation = 'relu')(model_decoder)\n",
    "model_up = tf.keras.Model(inputs=[input2], outputs=model_decoder,name = \"decoder\")\n",
    "\n",
    "#model_encoder_decoder.summary()\n",
    "input3 = tf.keras.layers.Input(shape=(16), name='input_layer3')\n",
    "model_ann = tf.keras.layers.Dense(16, activation='relu')(input3)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(16, activation='relu')(model_ann)\n",
    "model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(9, activation='softmax')(model_ann)\n",
    "model_ANN = tf.keras.Model(inputs=[input3], outputs=model_ann,name = \"ann\")\n",
    "\n",
    "input_full = tf.keras.layers.Input(shape=(15), name='input_layer4')\n",
    "encoder_out = model_down(input_full)\n",
    "decoder_out = model_up(encoder_out)\n",
    "ann_out = model_ANN(encoder_out)\n",
    "model_encoder_decoder_ann = tf.keras.Model(inputs=[input_full],outputs=[decoder_out,ann_out],name = 'encoder_decoder_ann')\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=1e-5)\n",
    "#model_encoder_decoder_ann.compile(optimizer = 'sgd', \n",
    "#                                  loss={'AE_out1': 'mean_squared_error','place_out2': 'categorical_crossentropy'}),\n",
    "#                                  loss_weights={'AE_out1': 0.5,'place_out2': 0.5},\n",
    "#                                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\"])\n",
    "#with mirrored_strategy.scope():\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(15), name='input_layer1')\n",
    "#flatten = tf.keras.layers.Flatten()(input)\n",
    "dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "dense2 = tf.keras.layers.Dense(24, activation='relu')\n",
    "dense3 = tf.keras.layers.Dense(16, activation='relu')\n",
    "model_encoder = dense1(input)\n",
    "model_encoder = dense2(model_encoder)\n",
    "model_encoder = dense3(model_encoder)\n",
    "model_down = tf.keras.Model(inputs=[input], outputs=model_encoder,name = \"encoder\")#input1, input2,input3,input4,input5,input6,input7,input8,input9,input10\n",
    "#model_down.summary()\n",
    "input_encoder = tf.keras.layers.Input(shape=(15), name='input_layer2')\n",
    "input_decoder = model_down(input_encoder)\n",
    "model_decoder = DenseTranspose(dense3, activation = 'relu')(input_decoder)\n",
    "model_decoder = DenseTranspose(dense2, activation = 'relu')(model_decoder)\n",
    "model_decoder = DenseTranspose(dense1, activation = 'relu')(model_decoder)\n",
    "\n",
    "#model_encoder_decoder.summary()\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(input_decoder)\n",
    "model_ann = tf.keras.layers.Dense(9, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(9, activation='relu')(model_ann)\n",
    "#model_ann = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)(model_ann)\n",
    "model_ann = tf.keras.layers.Dense(9, activation='softmax')(model_ann)\n",
    "model_encoder_decoder_ann = tf.keras.Model(inputs=[input_encoder],outputs=[model_decoder,model_ann],name = 'encoder_decoder')\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=1e-5)\n",
    "#model_encoder_decoder_ann.compile(optimizer = 'sgd', \n",
    "#                                  loss={'AE_out1': 'mean_squared_error','place_out2': 'categorical_crossentropy'}),\n",
    "#                                  loss_weights={'AE_out1': 0.5,'place_out2': 0.5},\n",
    "#                                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_decoder_ann\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer4 (InputLayer)       [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 (None, 16)           1704        input_layer4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 15)           1775        encoder[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ann (Model)                     (None, 9)            697         encoder[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,472\n",
      "Trainable params: 2,472\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_encoder_decoder_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_encoder_decoder_ann.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 32)\n",
      "(32,)\n",
      "(32, 24)\n",
      "(24,)\n",
      "(24, 16)\n",
      "(16,)\n",
      "(24,)\n",
      "(32,)\n",
      "(15,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 9)\n",
      "(9,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(model_encoder_decoder_ann.trainable_variables[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vars = model_encoder_decoder_ann.trainable_variables[:6]\n",
    "decode_vars = model_encoder_decoder_ann.trainable_variables[6:9]\n",
    "ann_vars = model_encoder_decoder_ann.trainable_variables[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(output,t_o,t_y):\n",
    "    #print(\"in loss\")\n",
    "    output_AE , output_label = output\n",
    "    mse = losses.MeanSquaredError()\n",
    "    AE_loss = mse(t_o,output_AE)\n",
    "    ann_loss =losses.categorical_crossentropy(t_y,output_label)\n",
    "    total_loss = AE_loss*100 + ann_loss\n",
    "    \n",
    "    return AE_loss,ann_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_A = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "initial_learning_rate=1e-4, decay_steps=5000, decay_rate=0.9)\n",
    "optimizer_A = tf.optimizers.SGD(learning_rate=learning_rate_A , momentum=1e-5)\n",
    "learning_rate_B = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.8)\n",
    "optimizer_B = tf.optimizers.Adam(learning_rate=1e-3)#learning_rate_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "\n",
    "def train_step(t_x,t_o,t_y):\n",
    "  \n",
    "    with tf.GradientTape() as AE_tape,tf.GradientTape() as ANN_tape:\n",
    "        output = model_encoder_decoder_ann(t_x, training=True)\n",
    "        AE_loss,ANN_loss = model_loss(output,t_o,t_y)\n",
    "        #gradients = tape.gradient(total_loss, shared_vars+decode_vars)\n",
    "        #optimizer_A.apply_gradients(zip(gradients, shared_vars+decode_vars))\n",
    "\n",
    "    if np.random.rand() < 0.5:\n",
    "        #print(\"AE\")\n",
    "        gradients_AE = AE_tape.gradient(AE_loss, shared_vars+decode_vars)\n",
    "        #gradients_AE = [tf.clip_by_value(g, -1,1) for g in gradients_AE]\n",
    "\n",
    "        #print(\"AE gradient : \",gradients_AE)\n",
    "        optimizer_A.apply_gradients(zip(gradients_AE, shared_vars+decode_vars))\n",
    "    else:\n",
    "        #print(\"ANN\")\n",
    "        gradients_ANN = ANN_tape.gradient(ANN_loss, shared_vars+ann_vars)\n",
    "        #print(\"ANN gradient : \",gradients_ANN)\n",
    "        #gradients_ANN = [tf.clip_by_value(g, -1,1) for g in gradients_ANN] \n",
    "        optimizer_B.apply_gradients(zip(gradients_ANN, shared_vars+ann_vars))\n",
    "    return np.array(AE_loss).mean(),np.array(ANN_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './wifi_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_dropoute3e4_model_{epoch}\")\n",
    "checkpoint = tf.train.Checkpoint(optimizerA=optimizer_A,\n",
    "                                 optimizerB=optimizer_B,\n",
    "                                 model_encoder_decoder_ann=model_encoder_decoder_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model_encoder_decoder.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(v_x,v_o,v_y):\n",
    "    output = model_encoder_decoder_ann(v_x)\n",
    "    output_AE , output_label = output\n",
    "    mse = losses.MeanSquaredError()\n",
    "    AE_loss = mse(v_o,output_AE)\n",
    "    ann_loss = losses.categorical_crossentropy(v_y,output_label)\n",
    "    total_loss = AE_loss*100 + ann_loss\n",
    "    #print(output_label[200])\n",
    "    #print(v_y[200])\n",
    "    #print(ann_loss)\n",
    "    print(\"AE loss : {}, ANN loss : {}, Total loss : {}\".format(np.array(AE_loss).mean(),np.array(ann_loss).mean(),np.array(total_loss).mean()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "validation(val_x,val_o,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 15)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    all_AE = []\n",
    "    all_ANN =[]\n",
    "    for x,o,y in train_dataset:\n",
    "        AE_loss,ANN_loss = train_step(x,o,y)\n",
    "        all_AE.append(AE_loss)\n",
    "        all_ANN.append(ANN_loss)\n",
    "    print(\"train AE loss : {}, train ANN loss : {}\".format(np.array(all_AE).mean(),np.array(all_ANN).mean()))\n",
    "    validation(val_x,val_o,val_y)\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(\"learning rate A : \",optimizer_A._decayed_lr(tf.float32))\n",
    "    print(\"learning rate B : \",optimizer_B._decayed_lr(tf.float32))\n",
    "    print(f'Time for epoch {epoch + 1} is {time.time() - start:.4f} sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=39955, shape=(), dtype=float32, numpy=2.908749e-05>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0., 0., 1., 0., 0., 0., 0., 0., 0.]#[0.11685812, 0.11185785, 0.11026918, 0.10764945, 0.10728354, 0.11020868,\n",
    " #0.10912149, 0.11363789, 0.11311384]#[0, 1, 0,0,0,0,0,0,0]\n",
    "b = [2.4371104e-08, 4.2344610e-11, 9.9997091e-01, 4.9084689e-13, 6.4827432e-06,\n",
    " 2.1712562e-08, 3.4163491e-11, 3.0665456e-11, 2.2567538e-05]\n",
    "tf.keras.losses.categorical_crossentropy(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train AE loss : 0.024533042684197426, train ANN loss : 2.1933341026306152\n",
      "AE loss : 0.020671728998422623, ANN loss : 2.195570230484009, Total loss : 4.2627434730529785\n",
      "learning rate A :  tf.Tensor(9.961722e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 1 is 2.9627 sec\n",
      "train AE loss : 0.02150137908756733, train ANN loss : 2.192610025405884\n",
      "AE loss : 0.021717365831136703, ANN loss : 2.1920313835144043, Total loss : 4.363767623901367\n",
      "learning rate A :  tf.Tensor(9.940333e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 2 is 2.9304 sec\n",
      "train AE loss : 0.020831555128097534, train ANN loss : 2.191659688949585\n",
      "AE loss : 0.02241639792919159, ANN loss : 2.1897335052490234, Total loss : 4.431373596191406\n",
      "learning rate A :  tf.Tensor(9.920245e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 3 is 2.9298 sec\n",
      "train AE loss : 0.022003620862960815, train ANN loss : 2.186269521713257\n",
      "AE loss : 0.021574700251221657, ANN loss : 2.1847615242004395, Total loss : 4.342231273651123\n",
      "learning rate A :  tf.Tensor(9.9020755e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 4 is 2.9441 sec\n",
      "train AE loss : 0.020321695134043694, train ANN loss : 2.1801068782806396\n",
      "AE loss : 0.020723076537251472, ANN loss : 2.1735072135925293, Total loss : 4.245814800262451\n",
      "learning rate A :  tf.Tensor(9.8820645e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 5 is 2.9406 sec\n",
      "train AE loss : 0.01999099738895893, train ANN loss : 2.1760056018829346\n",
      "AE loss : 0.019403457641601562, ANN loss : 2.1692421436309814, Total loss : 4.109588146209717\n",
      "learning rate A :  tf.Tensor(9.8625096e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 6 is 2.9418 sec\n",
      "train AE loss : 0.020187195390462875, train ANN loss : 2.161517858505249\n",
      "AE loss : 0.01765660010278225, ANN loss : 2.1637020111083984, Total loss : 3.9293618202209473\n",
      "learning rate A :  tf.Tensor(9.842786e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 7 is 2.9385 sec\n",
      "train AE loss : 0.01947437785565853, train ANN loss : 2.163630723953247\n",
      "AE loss : 0.02041221782565117, ANN loss : 2.1625730991363525, Total loss : 4.203794479370117\n",
      "learning rate A :  tf.Tensor(9.8226876e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 8 is 2.9417 sec\n",
      "train AE loss : 0.027213748544454575, train ANN loss : 2.146541118621826\n",
      "AE loss : 0.03851144388318062, ANN loss : 2.13004469871521, Total loss : 5.981189250946045\n",
      "learning rate A :  tf.Tensor(9.8061435e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 9 is 2.9614 sec\n",
      "train AE loss : 0.032774992287158966, train ANN loss : 2.150737762451172\n",
      "AE loss : 0.026710862293839455, ANN loss : 2.13450026512146, Total loss : 4.805586338043213\n",
      "learning rate A :  tf.Tensor(9.78344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 10 is 2.9213 sec\n",
      "train AE loss : 0.028945384547114372, train ANN loss : 2.1369729042053223\n",
      "AE loss : 0.028843967244029045, ANN loss : 2.1214890480041504, Total loss : 5.005886077880859\n",
      "learning rate A :  tf.Tensor(9.764903e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 11 is 5.8736 sec\n",
      "train AE loss : 0.03849087283015251, train ANN loss : 2.1428797245025635\n",
      "AE loss : 0.026588287204504013, ANN loss : 2.1292879581451416, Total loss : 4.788116455078125\n",
      "learning rate A :  tf.Tensor(9.7470176e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 12 is 2.9422 sec\n",
      "train AE loss : 0.02933885157108307, train ANN loss : 2.1337130069732666\n",
      "AE loss : 0.02920646034181118, ANN loss : 2.1190528869628906, Total loss : 5.039699077606201\n",
      "learning rate A :  tf.Tensor(9.725475e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 13 is 2.9457 sec\n",
      "train AE loss : 0.0334441140294075, train ANN loss : 2.1237454414367676\n",
      "AE loss : 0.051667992025613785, ANN loss : 2.117621898651123, Total loss : 7.284421443939209\n",
      "learning rate A :  tf.Tensor(9.704595e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 14 is 2.9305 sec\n",
      "train AE loss : 0.053949132561683655, train ANN loss : 2.1178319454193115\n",
      "AE loss : 0.04834092780947685, ANN loss : 2.09114670753479, Total loss : 6.925239086151123\n",
      "learning rate A :  tf.Tensor(9.685799e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 15 is 2.9357 sec\n",
      "train AE loss : 0.06628412753343582, train ANN loss : 2.100583553314209\n",
      "AE loss : 0.06815987825393677, ANN loss : 2.0751538276672363, Total loss : 8.891141891479492\n",
      "learning rate A :  tf.Tensor(9.669485e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 16 is 2.8552 sec\n",
      "train AE loss : 0.05906492471694946, train ANN loss : 2.099682569503784\n",
      "AE loss : 0.06900957971811295, ANN loss : 2.085550546646118, Total loss : 8.9865083694458\n",
      "learning rate A :  tf.Tensor(9.6503514e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 17 is 2.8230 sec\n",
      "train AE loss : 0.06263814121484756, train ANN loss : 2.0824012756347656\n",
      "AE loss : 0.0756315365433693, ANN loss : 2.0752344131469727, Total loss : 9.638387680053711\n",
      "learning rate A :  tf.Tensor(9.631864e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 18 is 2.9023 sec\n",
      "train AE loss : 0.10390523076057434, train ANN loss : 2.0820207595825195\n",
      "AE loss : 0.08138582855463028, ANN loss : 2.0581424236297607, Total loss : 10.196724891662598\n",
      "learning rate A :  tf.Tensor(9.611791e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 19 is 2.9240 sec\n",
      "train AE loss : 0.10174936801195145, train ANN loss : 2.0921318531036377\n",
      "AE loss : 0.08797679096460342, ANN loss : 2.0434656143188477, Total loss : 10.841145515441895\n",
      "learning rate A :  tf.Tensor(9.593175e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 20 is 2.9339 sec\n",
      "train AE loss : 0.10933569818735123, train ANN loss : 2.0825273990631104\n",
      "AE loss : 0.17007958889007568, ANN loss : 2.0999417304992676, Total loss : 19.107900619506836\n",
      "learning rate A :  tf.Tensor(9.5723764e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 21 is 2.9147 sec\n",
      "train AE loss : 0.12583263218402863, train ANN loss : 2.0884194374084473\n",
      "AE loss : 0.0729282796382904, ANN loss : 2.0614540576934814, Total loss : 9.354281425476074\n",
      "learning rate A :  tf.Tensor(9.55565e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 22 is 2.9241 sec\n",
      "train AE loss : 0.11930418759584427, train ANN loss : 2.0701260566711426\n",
      "AE loss : 0.09326112270355225, ANN loss : 2.043242931365967, Total loss : 11.369355201721191\n",
      "learning rate A :  tf.Tensor(9.5373434e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 23 is 2.9525 sec\n",
      "train AE loss : 0.12096966058015823, train ANN loss : 2.076327323913574\n",
      "AE loss : 0.16374769806861877, ANN loss : 2.035836935043335, Total loss : 18.410606384277344\n",
      "learning rate A :  tf.Tensor(9.518671e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 24 is 2.9235 sec\n",
      "train AE loss : 0.14188189804553986, train ANN loss : 2.0603339672088623\n",
      "AE loss : 0.12215623259544373, ANN loss : 2.0458507537841797, Total loss : 14.261474609375\n",
      "learning rate A :  tf.Tensor(9.500235e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 25 is 2.9370 sec\n",
      "train AE loss : 0.1582852452993393, train ANN loss : 2.054676055908203\n",
      "AE loss : 0.17072860896587372, ANN loss : 2.0230276584625244, Total loss : 19.095888137817383\n",
      "learning rate A :  tf.Tensor(9.484034e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 26 is 2.9321 sec\n",
      "train AE loss : 0.17095442116260529, train ANN loss : 2.0448098182678223\n",
      "AE loss : 0.14112165570259094, ANN loss : 2.0271193981170654, Total loss : 16.139286041259766\n",
      "learning rate A :  tf.Tensor(9.4636714e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 27 is 2.9196 sec\n",
      "train AE loss : 0.1643780916929245, train ANN loss : 2.0519559383392334\n",
      "AE loss : 0.19089668989181519, ANN loss : 2.0415968894958496, Total loss : 21.131261825561523\n",
      "learning rate A :  tf.Tensor(9.446338e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 28 is 2.9340 sec\n",
      "train AE loss : 0.19871161878108978, train ANN loss : 2.0495667457580566\n",
      "AE loss : 0.20132490992546082, ANN loss : 1.998695731163025, Total loss : 22.13118553161621\n",
      "learning rate A :  tf.Tensor(9.4304276e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 29 is 2.9228 sec\n",
      "train AE loss : 0.19895245134830475, train ANN loss : 2.01583194732666\n",
      "AE loss : 0.17266161739826202, ANN loss : 2.000866651535034, Total loss : 19.26702880859375\n",
      "learning rate A :  tf.Tensor(9.414344e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 30 is 2.9435 sec\n",
      "train AE loss : 0.1597868800163269, train ANN loss : 2.0673935413360596\n",
      "AE loss : 0.15495675802230835, ANN loss : 2.010268449783325, Total loss : 17.505945205688477\n",
      "learning rate A :  tf.Tensor(9.395715e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 31 is 2.9124 sec\n",
      "train AE loss : 0.1416708379983902, train ANN loss : 2.034393310546875\n",
      "AE loss : 0.2293749451637268, ANN loss : 2.008469581604004, Total loss : 24.94596290588379\n",
      "learning rate A :  tf.Tensor(9.3783085e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 32 is 2.9201 sec\n",
      "train AE loss : 0.17728467285633087, train ANN loss : 2.0250632762908936\n",
      "AE loss : 0.09361560642719269, ANN loss : 1.9857548475265503, Total loss : 11.347315788269043\n",
      "learning rate A :  tf.Tensor(9.35837e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 33 is 2.9374 sec\n",
      "train AE loss : 0.12563805282115936, train ANN loss : 2.021778106689453\n",
      "AE loss : 0.16143257915973663, ANN loss : 1.9728957414627075, Total loss : 18.116153717041016\n",
      "learning rate A :  tf.Tensor(9.340836e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 34 is 2.9289 sec\n",
      "train AE loss : 0.24749095737934113, train ANN loss : 1.9950008392333984\n",
      "AE loss : 0.28797248005867004, ANN loss : 1.9557079076766968, Total loss : 30.75295639038086\n",
      "learning rate A :  tf.Tensor(9.322548e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 35 is 2.9432 sec\n",
      "train AE loss : 0.31936877965927124, train ANN loss : 1.9994758367538452\n",
      "AE loss : 0.3401910662651062, ANN loss : 1.9342402219772339, Total loss : 35.953346252441406\n",
      "learning rate A :  tf.Tensor(9.303512e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 36 is 2.9208 sec\n",
      "train AE loss : 0.25592362880706787, train ANN loss : 1.9832494258880615\n",
      "AE loss : 0.23114454746246338, ANN loss : 1.9466478824615479, Total loss : 25.06110191345215\n",
      "learning rate A :  tf.Tensor(9.287059e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 37 is 2.9296 sec\n",
      "train AE loss : 0.2611956000328064, train ANN loss : 1.9987246990203857\n",
      "AE loss : 0.15058180689811707, ANN loss : 1.9483213424682617, Total loss : 17.006502151489258\n",
      "learning rate A :  tf.Tensor(9.26751e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 38 is 2.9267 sec\n",
      "train AE loss : 0.1504242718219757, train ANN loss : 1.9997221231460571\n",
      "AE loss : 0.26719027757644653, ANN loss : 2.0127031803131104, Total loss : 28.73172950744629\n",
      "learning rate A :  tf.Tensor(9.249171e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 39 is 2.9334 sec\n",
      "train AE loss : 0.20928293466567993, train ANN loss : 1.9932925701141357\n",
      "AE loss : 0.17091909050941467, ANN loss : 1.9174126386642456, Total loss : 19.009323120117188\n",
      "learning rate A :  tf.Tensor(9.232036e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 40 is 2.9291 sec\n",
      "train AE loss : 0.19384731352329254, train ANN loss : 1.9856184720993042\n",
      "AE loss : 0.11203330755233765, ANN loss : 1.9374607801437378, Total loss : 13.140791893005371\n",
      "learning rate A :  tf.Tensor(9.214156e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 41 is 2.9323 sec\n",
      "train AE loss : 0.1627855896949768, train ANN loss : 1.9420770406723022\n",
      "AE loss : 0.12627728283405304, ANN loss : 1.8936095237731934, Total loss : 14.521337509155273\n",
      "learning rate A :  tf.Tensor(9.1974725e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 42 is 2.9269 sec\n",
      "train AE loss : 0.23443233966827393, train ANN loss : 1.9730539321899414\n",
      "AE loss : 0.119196318089962, ANN loss : 1.900058627128601, Total loss : 13.819690704345703\n",
      "learning rate A :  tf.Tensor(9.179467e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 43 is 2.9277 sec\n",
      "train AE loss : 0.2640572786331177, train ANN loss : 1.9257439374923706\n",
      "AE loss : 0.3595432639122009, ANN loss : 1.865659236907959, Total loss : 37.819984436035156\n",
      "learning rate A :  tf.Tensor(9.161109e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 44 is 2.9332 sec\n",
      "train AE loss : 0.28528860211372375, train ANN loss : 1.9322340488433838\n",
      "AE loss : 0.474470317363739, ANN loss : 1.904312252998352, Total loss : 49.351348876953125\n",
      "learning rate A :  tf.Tensor(9.1427886e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 45 is 2.9693 sec\n",
      "train AE loss : 0.29387664794921875, train ANN loss : 1.9347375631332397\n",
      "AE loss : 0.2548046410083771, ANN loss : 1.862572193145752, Total loss : 27.34303855895996\n",
      "learning rate A :  tf.Tensor(9.1260416e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 46 is 2.9392 sec\n",
      "train AE loss : 0.2815667390823364, train ANN loss : 1.9046958684921265\n",
      "AE loss : 0.39729368686676025, ANN loss : 1.8560553789138794, Total loss : 41.585426330566406\n",
      "learning rate A :  tf.Tensor(9.107216e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 47 is 2.9661 sec\n",
      "train AE loss : 0.3766019344329834, train ANN loss : 1.8693617582321167\n",
      "AE loss : 0.38485264778137207, ANN loss : 1.8428919315338135, Total loss : 40.32815170288086\n",
      "learning rate A :  tf.Tensor(9.088046e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 48 is 2.9559 sec\n",
      "train AE loss : 0.28887617588043213, train ANN loss : 1.9014581441879272\n",
      "AE loss : 0.2814834713935852, ANN loss : 1.866335391998291, Total loss : 30.014680862426758\n",
      "learning rate A :  tf.Tensor(9.070444e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 49 is 2.9433 sec\n",
      "train AE loss : 0.37278029322624207, train ANN loss : 1.9453635215759277\n",
      "AE loss : 0.4202463924884796, ANN loss : 1.8381800651550293, Total loss : 43.86281967163086\n",
      "learning rate A :  tf.Tensor(9.052686e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 50 is 2.9548 sec\n",
      "train AE loss : 0.403096467256546, train ANN loss : 1.895881175994873\n",
      "AE loss : 0.3211536705493927, ANN loss : 1.8249017000198364, Total loss : 33.940269470214844\n",
      "learning rate A :  tf.Tensor(9.034581e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 51 is 2.9544 sec\n",
      "train AE loss : 0.3619362711906433, train ANN loss : 1.8892123699188232\n",
      "AE loss : 0.1559922695159912, ANN loss : 1.8546589612960815, Total loss : 17.453886032104492\n",
      "learning rate A :  tf.Tensor(9.016514e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 52 is 2.9701 sec\n",
      "train AE loss : 0.32352909445762634, train ANN loss : 1.9064126014709473\n",
      "AE loss : 0.2263120859861374, ANN loss : 1.8232605457305908, Total loss : 24.454469680786133\n",
      "learning rate A :  tf.Tensor(8.999431e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 53 is 2.9539 sec\n",
      "train AE loss : 0.318632036447525, train ANN loss : 1.8743783235549927\n",
      "AE loss : 0.21392495930194855, ANN loss : 1.8161633014678955, Total loss : 23.208660125732422\n",
      "learning rate A :  tf.Tensor(8.983894e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 54 is 2.9652 sec\n",
      "train AE loss : 0.30650654435157776, train ANN loss : 1.8632404804229736\n",
      "AE loss : 0.15852415561676025, ANN loss : 1.8309556245803833, Total loss : 17.68337059020996\n",
      "learning rate A :  tf.Tensor(8.9642264e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 55 is 2.9624 sec\n",
      "train AE loss : 0.23211237788200378, train ANN loss : 1.8621031045913696\n",
      "AE loss : 0.19547349214553833, ANN loss : 1.816525936126709, Total loss : 21.363874435424805\n",
      "learning rate A :  tf.Tensor(8.9466776e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 56 is 2.9613 sec\n",
      "train AE loss : 0.3063088357448578, train ANN loss : 1.8667194843292236\n",
      "AE loss : 0.3791281282901764, ANN loss : 1.8112651109695435, Total loss : 39.72407913208008\n",
      "learning rate A :  tf.Tensor(8.931044e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 57 is 3.0011 sec\n",
      "train AE loss : 0.3966390788555145, train ANN loss : 1.8411524295806885\n",
      "AE loss : 0.22099268436431885, ANN loss : 1.8229410648345947, Total loss : 23.922208786010742\n",
      "learning rate A :  tf.Tensor(8.9144974e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 58 is 2.9348 sec\n",
      "train AE loss : 0.3702203035354614, train ANN loss : 1.8713595867156982\n",
      "AE loss : 0.254475861787796, ANN loss : 1.803105354309082, Total loss : 27.25069236755371\n",
      "learning rate A :  tf.Tensor(8.896857e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 59 is 2.9432 sec\n",
      "train AE loss : 0.2930639684200287, train ANN loss : 1.8945645093917847\n",
      "AE loss : 0.31768977642059326, ANN loss : 1.7906047105789185, Total loss : 33.55958557128906\n",
      "learning rate A :  tf.Tensor(8.879253e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 60 is 2.9037 sec\n",
      "train AE loss : 0.3796159029006958, train ANN loss : 1.8056617975234985\n",
      "AE loss : 0.25487425923347473, ANN loss : 1.7942280769348145, Total loss : 27.281652450561523\n",
      "learning rate A :  tf.Tensor(8.861681e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 61 is 2.9007 sec\n",
      "train AE loss : 0.3700055778026581, train ANN loss : 1.8258806467056274\n",
      "AE loss : 0.19712205231189728, ANN loss : 1.813044548034668, Total loss : 21.525251388549805\n",
      "learning rate A :  tf.Tensor(8.8432134e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 62 is 2.9452 sec\n",
      "train AE loss : 0.5388181805610657, train ANN loss : 1.851256012916565\n",
      "AE loss : 0.5886120200157166, ANN loss : 1.7994670867919922, Total loss : 60.660667419433594\n",
      "learning rate A :  tf.Tensor(8.8259e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 63 is 2.9289 sec\n",
      "train AE loss : 0.4661521911621094, train ANN loss : 1.818363904953003\n",
      "AE loss : 0.5305653810501099, ANN loss : 1.7639628648757935, Total loss : 54.82049560546875\n",
      "learning rate A :  tf.Tensor(8.807322e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 64 is 2.9423 sec\n",
      "train AE loss : 0.5597179532051086, train ANN loss : 1.8197078704833984\n",
      "AE loss : 0.5689135193824768, ANN loss : 1.782815933227539, Total loss : 58.67416763305664\n",
      "learning rate A :  tf.Tensor(8.790265e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 65 is 3.1765 sec\n",
      "train AE loss : 0.3553556501865387, train ANN loss : 1.8345165252685547\n",
      "AE loss : 0.36051666736602783, ANN loss : 1.7865413427352905, Total loss : 37.83820343017578\n",
      "learning rate A :  tf.Tensor(8.772131e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 66 is 3.2208 sec\n",
      "train AE loss : 0.38296619057655334, train ANN loss : 1.8323627710342407\n",
      "AE loss : 0.3440689444541931, ANN loss : 1.7926017045974731, Total loss : 36.199493408203125\n",
      "learning rate A :  tf.Tensor(8.756618e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 67 is 3.2153 sec\n",
      "train AE loss : 0.33464792370796204, train ANN loss : 1.8542907238006592\n",
      "AE loss : 0.3284702003002167, ANN loss : 1.7649152278900146, Total loss : 34.611934661865234\n",
      "learning rate A :  tf.Tensor(8.740026e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 68 is 3.0216 sec\n",
      "train AE loss : 0.3481634557247162, train ANN loss : 1.8028334379196167\n",
      "AE loss : 0.21462677419185638, ANN loss : 1.7704793214797974, Total loss : 23.233156204223633\n",
      "learning rate A :  tf.Tensor(8.722364e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 69 is 2.9341 sec\n",
      "train AE loss : 0.4460996389389038, train ANN loss : 1.8128437995910645\n",
      "AE loss : 0.4813030958175659, ANN loss : 1.778126835823059, Total loss : 49.90843963623047\n",
      "learning rate A :  tf.Tensor(8.7052875e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 70 is 2.9478 sec\n",
      "train AE loss : 0.37793147563934326, train ANN loss : 1.8165045976638794\n",
      "AE loss : 0.2554124593734741, ANN loss : 1.7652720212936401, Total loss : 27.306516647338867\n",
      "learning rate A :  tf.Tensor(8.68861e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 71 is 2.9458 sec\n",
      "train AE loss : 0.3976407051086426, train ANN loss : 1.8318599462509155\n",
      "AE loss : 0.34541016817092896, ANN loss : 1.7712311744689941, Total loss : 36.31224822998047\n",
      "learning rate A :  tf.Tensor(8.6725144e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 72 is 2.9798 sec\n",
      "train AE loss : 0.37489959597587585, train ANN loss : 1.8137346506118774\n",
      "AE loss : 0.2976784110069275, ANN loss : 1.764119029045105, Total loss : 31.53196144104004\n",
      "learning rate A :  tf.Tensor(8.654258e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 73 is 8.8613 sec\n",
      "train AE loss : 0.6286779642105103, train ANN loss : 1.7825895547866821\n",
      "AE loss : 0.24616514146327972, ANN loss : 1.801185131072998, Total loss : 26.41769790649414\n",
      "learning rate A :  tf.Tensor(8.6400454e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 74 is 3.1239 sec\n",
      "train AE loss : 0.43164825439453125, train ANN loss : 1.7782992124557495\n",
      "AE loss : 0.38378649950027466, ANN loss : 1.7607903480529785, Total loss : 40.13943862915039\n",
      "learning rate A :  tf.Tensor(8.6233114e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 75 is 3.1076 sec\n",
      "train AE loss : 0.48539406061172485, train ANN loss : 1.779283881187439\n",
      "AE loss : 0.4321020245552063, ANN loss : 1.8085318803787231, Total loss : 45.01873016357422\n",
      "learning rate A :  tf.Tensor(8.604434e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 76 is 3.1850 sec\n",
      "train AE loss : 0.43355637788772583, train ANN loss : 1.7800136804580688\n",
      "AE loss : 0.4048340320587158, ANN loss : 1.751874566078186, Total loss : 42.23527526855469\n",
      "learning rate A :  tf.Tensor(8.587046e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 77 is 2.9942 sec\n",
      "train AE loss : 0.5465146899223328, train ANN loss : 1.7966995239257812\n",
      "AE loss : 0.41008809208869934, ANN loss : 1.7368903160095215, Total loss : 42.745697021484375\n",
      "learning rate A :  tf.Tensor(8.570957e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 78 is 2.9416 sec\n",
      "train AE loss : 0.5789774656295776, train ANN loss : 1.7608987092971802\n",
      "AE loss : 0.31156468391418457, ANN loss : 1.7328526973724365, Total loss : 32.889320373535156\n",
      "learning rate A :  tf.Tensor(8.553816e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 79 is 2.9280 sec\n",
      "train AE loss : 0.4464779198169708, train ANN loss : 1.7686647176742554\n",
      "AE loss : 0.40596675872802734, ANN loss : 1.7337538003921509, Total loss : 42.33042907714844\n",
      "learning rate A :  tf.Tensor(8.537789e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 80 is 2.9591 sec\n",
      "train AE loss : 0.649766743183136, train ANN loss : 1.7667927742004395\n",
      "AE loss : 0.646555483341217, ANN loss : 1.727392554283142, Total loss : 66.38294219970703\n",
      "learning rate A :  tf.Tensor(8.520894e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 81 is 2.9423 sec\n",
      "train AE loss : 0.6285425424575806, train ANN loss : 1.7623976469039917\n",
      "AE loss : 0.377481073141098, ANN loss : 1.7380058765411377, Total loss : 39.486114501953125\n",
      "learning rate A :  tf.Tensor(8.504571e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 82 is 2.8875 sec\n",
      "train AE loss : 0.5889191031455994, train ANN loss : 1.786087989807129\n",
      "AE loss : 0.5474215745925903, ANN loss : 1.7166920900344849, Total loss : 56.4588508605957\n",
      "learning rate A :  tf.Tensor(8.490246e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 83 is 2.9674 sec\n",
      "train AE loss : 0.6006406545639038, train ANN loss : 1.779091715812683\n",
      "AE loss : 0.5079730749130249, ANN loss : 1.7517259120941162, Total loss : 52.54903030395508\n",
      "learning rate A :  tf.Tensor(8.4732674e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 84 is 2.9298 sec\n",
      "train AE loss : 0.6531018018722534, train ANN loss : 1.7512060403823853\n",
      "AE loss : 0.31299757957458496, ANN loss : 1.7891803979873657, Total loss : 33.08893966674805\n",
      "learning rate A :  tf.Tensor(8.456322e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 85 is 2.9444 sec\n",
      "train AE loss : 0.5322281718254089, train ANN loss : 1.8104363679885864\n",
      "AE loss : 0.642685055732727, ANN loss : 1.7146869897842407, Total loss : 65.98320007324219\n",
      "learning rate A :  tf.Tensor(8.4411884e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 86 is 2.9280 sec\n",
      "train AE loss : 0.6256049871444702, train ANN loss : 1.7736718654632568\n",
      "AE loss : 0.44210779666900635, ANN loss : 1.731780767440796, Total loss : 45.942562103271484\n",
      "learning rate A :  tf.Tensor(8.4257284e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 87 is 2.9542 sec\n",
      "train AE loss : 0.7136322259902954, train ANN loss : 1.739015817642212\n",
      "AE loss : 0.4974636435508728, ANN loss : 1.6977510452270508, Total loss : 51.44411849975586\n",
      "learning rate A :  tf.Tensor(8.409587e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 88 is 2.9324 sec\n",
      "train AE loss : 0.9229720234870911, train ANN loss : 1.7477384805679321\n",
      "AE loss : 0.5936859846115112, ANN loss : 1.7028107643127441, Total loss : 61.071407318115234\n",
      "learning rate A :  tf.Tensor(8.392769e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 89 is 2.9268 sec\n",
      "train AE loss : 0.7013276815414429, train ANN loss : 1.752314805984497\n",
      "AE loss : 0.9876813888549805, ANN loss : 1.7456012964248657, Total loss : 100.51374816894531\n",
      "learning rate A :  tf.Tensor(8.3743966e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 90 is 2.9435 sec\n",
      "train AE loss : 0.8587110638618469, train ANN loss : 1.742569923400879\n",
      "AE loss : 0.5086709260940552, ANN loss : 1.698023796081543, Total loss : 52.56511688232422\n",
      "learning rate A :  tf.Tensor(8.356064e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 91 is 2.9250 sec\n",
      "train AE loss : 0.6982679963111877, train ANN loss : 1.7474786043167114\n",
      "AE loss : 0.600532591342926, ANN loss : 1.770056128501892, Total loss : 61.82331848144531\n",
      "learning rate A :  tf.Tensor(8.339353e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 92 is 2.9490 sec\n",
      "train AE loss : 0.6413617134094238, train ANN loss : 1.7628676891326904\n",
      "AE loss : 0.560418963432312, ANN loss : 1.7172937393188477, Total loss : 57.759193420410156\n",
      "learning rate A :  tf.Tensor(8.3210965e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 93 is 2.9193 sec\n",
      "train AE loss : 0.7878314256668091, train ANN loss : 1.738511085510254\n",
      "AE loss : 0.5595356225967407, ANN loss : 1.7391103506088257, Total loss : 57.69267654418945\n",
      "learning rate A :  tf.Tensor(8.304981e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 94 is 2.9296 sec\n",
      "train AE loss : 0.5169017314910889, train ANN loss : 1.7892051935195923\n",
      "AE loss : 0.6181570887565613, ANN loss : 1.7284139394760132, Total loss : 63.54412078857422\n",
      "learning rate A :  tf.Tensor(8.289421e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 95 is 2.9458 sec\n",
      "train AE loss : 0.6852090954780579, train ANN loss : 1.7468985319137573\n",
      "AE loss : 0.45297983288764954, ANN loss : 1.73121178150177, Total loss : 47.029197692871094\n",
      "learning rate A :  tf.Tensor(8.27232e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 96 is 2.9183 sec\n",
      "train AE loss : 0.7423847317695618, train ANN loss : 1.7314107418060303\n",
      "AE loss : 0.7859199047088623, ANN loss : 1.8196882009506226, Total loss : 80.41167449951172\n",
      "learning rate A :  tf.Tensor(8.255081e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 97 is 2.9376 sec\n",
      "train AE loss : 0.666205108165741, train ANN loss : 1.7508149147033691\n",
      "AE loss : 0.47127366065979004, ANN loss : 1.7145429849624634, Total loss : 48.84191131591797\n",
      "learning rate A :  tf.Tensor(8.241177e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 98 is 2.9263 sec\n",
      "train AE loss : 0.7263649702072144, train ANN loss : 1.7498968839645386\n",
      "AE loss : 0.502683699131012, ANN loss : 1.7179948091506958, Total loss : 51.986366271972656\n",
      "learning rate A :  tf.Tensor(8.2271225e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 99 is 2.9358 sec\n",
      "train AE loss : 0.7552627921104431, train ANN loss : 1.7166495323181152\n",
      "AE loss : 0.4112480580806732, ANN loss : 1.729777216911316, Total loss : 42.854576110839844\n",
      "learning rate A :  tf.Tensor(8.211707e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 100 is 2.9441 sec\n",
      "train AE loss : 0.7577546834945679, train ANN loss : 1.7661594152450562\n",
      "AE loss : 0.3811339735984802, ANN loss : 1.7698456048965454, Total loss : 39.88323974609375\n",
      "learning rate A :  tf.Tensor(8.193903e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 101 is 2.9312 sec\n",
      "train AE loss : 0.7119349837303162, train ANN loss : 1.7267937660217285\n",
      "AE loss : 0.6350165009498596, ANN loss : 1.7090331315994263, Total loss : 65.21067810058594\n",
      "learning rate A :  tf.Tensor(8.1776896e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 102 is 3.1138 sec\n",
      "train AE loss : 0.781286358833313, train ANN loss : 1.7556157112121582\n",
      "AE loss : 0.553950846195221, ANN loss : 1.6964925527572632, Total loss : 57.091575622558594\n",
      "learning rate A :  tf.Tensor(8.1604754e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 103 is 2.9210 sec\n",
      "train AE loss : 0.8003594279289246, train ANN loss : 1.7349395751953125\n",
      "AE loss : 0.618636429309845, ANN loss : 1.7291184663772583, Total loss : 63.59276580810547\n",
      "learning rate A :  tf.Tensor(8.1457e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 104 is 2.9471 sec\n",
      "train AE loss : 0.7178203463554382, train ANN loss : 1.7603908777236938\n",
      "AE loss : 0.3300550878047943, ANN loss : 1.814748764038086, Total loss : 34.82025909423828\n",
      "learning rate A :  tf.Tensor(8.1283826e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 105 is 2.9158 sec\n",
      "train AE loss : 0.8827829360961914, train ANN loss : 1.7793574333190918\n",
      "AE loss : 0.5729117393493652, ANN loss : 1.747108817100525, Total loss : 59.03828430175781\n",
      "learning rate A :  tf.Tensor(8.1111015e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 106 is 2.9170 sec\n",
      "train AE loss : 0.6276443004608154, train ANN loss : 1.758039116859436\n",
      "AE loss : 0.38562726974487305, ANN loss : 1.7247813940048218, Total loss : 40.287513732910156\n",
      "learning rate A :  tf.Tensor(8.0924925e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 107 is 2.9407 sec\n",
      "train AE loss : 0.7765583992004395, train ANN loss : 1.7610976696014404\n",
      "AE loss : 0.45146849751472473, ANN loss : 1.7277026176452637, Total loss : 46.87455368041992\n",
      "learning rate A :  tf.Tensor(8.076138e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 108 is 2.9229 sec\n",
      "train AE loss : 0.6931601762771606, train ANN loss : 1.760066270828247\n",
      "AE loss : 0.3891742527484894, ANN loss : 1.728980541229248, Total loss : 40.64640808105469\n",
      "learning rate A :  tf.Tensor(8.059478e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 109 is 2.9617 sec\n",
      "train AE loss : 0.5908219814300537, train ANN loss : 1.7640659809112549\n",
      "AE loss : 0.6552183032035828, ANN loss : 1.7402970790863037, Total loss : 67.26212310791016\n",
      "learning rate A :  tf.Tensor(8.045225e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 110 is 2.9418 sec\n",
      "train AE loss : 0.6536102294921875, train ANN loss : 1.8007760047912598\n",
      "AE loss : 0.42810285091400146, ANN loss : 1.7492713928222656, Total loss : 44.55956268310547\n",
      "learning rate A :  tf.Tensor(8.03049e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 111 is 2.9380 sec\n",
      "train AE loss : 0.744307279586792, train ANN loss : 1.7335760593414307\n",
      "AE loss : 0.4839969873428345, ANN loss : 1.727190613746643, Total loss : 50.126888275146484\n",
      "learning rate A :  tf.Tensor(8.0152735e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 112 is 2.9551 sec\n",
      "train AE loss : 0.805916428565979, train ANN loss : 1.7594949007034302\n",
      "AE loss : 0.34443822503089905, ANN loss : 1.7364490032196045, Total loss : 36.18027114868164\n",
      "learning rate A :  tf.Tensor(7.998234e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 113 is 2.9362 sec\n",
      "train AE loss : 0.687920331954956, train ANN loss : 1.7330058813095093\n",
      "AE loss : 0.46459057927131653, ANN loss : 1.7499988079071045, Total loss : 48.20905685424805\n",
      "learning rate A :  tf.Tensor(7.9820704e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 114 is 2.9481 sec\n",
      "train AE loss : 0.6285614371299744, train ANN loss : 1.7641539573669434\n",
      "AE loss : 0.311903715133667, ANN loss : 1.71489679813385, Total loss : 32.905269622802734\n",
      "learning rate A :  tf.Tensor(7.964764e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 115 is 2.9351 sec\n",
      "train AE loss : 0.5365884900093079, train ANN loss : 1.7706892490386963\n",
      "AE loss : 0.448186993598938, ANN loss : 1.7322735786437988, Total loss : 46.55097198486328\n",
      "learning rate A :  tf.Tensor(7.950511e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 116 is 2.9650 sec\n",
      "train AE loss : 0.614237904548645, train ANN loss : 1.7343425750732422\n",
      "AE loss : 0.39048558473587036, ANN loss : 1.771117925643921, Total loss : 40.81967544555664\n",
      "learning rate A :  tf.Tensor(7.935782e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 117 is 2.9412 sec\n",
      "train AE loss : 0.648466944694519, train ANN loss : 1.7706671953201294\n",
      "AE loss : 0.22876879572868347, ANN loss : 1.7673848867416382, Total loss : 24.644264221191406\n",
      "learning rate A :  tf.Tensor(7.92158e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 118 is 2.9797 sec\n",
      "train AE loss : 0.5161799788475037, train ANN loss : 1.7371408939361572\n",
      "AE loss : 0.36454468965530396, ANN loss : 1.7184265851974487, Total loss : 38.17289352416992\n",
      "learning rate A :  tf.Tensor(7.904905e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 119 is 2.9600 sec\n",
      "train AE loss : 0.6014490127563477, train ANN loss : 1.744016408920288\n",
      "AE loss : 0.40167850255966187, ANN loss : 1.721773624420166, Total loss : 41.88962936401367\n",
      "learning rate A :  tf.Tensor(7.888099e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 120 is 2.9671 sec\n",
      "train AE loss : 0.5265063047409058, train ANN loss : 1.7439790964126587\n",
      "AE loss : 0.33874979615211487, ANN loss : 1.7425833940505981, Total loss : 35.6175651550293\n",
      "learning rate A :  tf.Tensor(7.871993e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 121 is 2.9331 sec\n",
      "train AE loss : 0.5270293354988098, train ANN loss : 1.7406386137008667\n",
      "AE loss : 0.557211697101593, ANN loss : 1.732072353363037, Total loss : 57.45323944091797\n",
      "learning rate A :  tf.Tensor(7.856582e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 122 is 2.9287 sec\n",
      "train AE loss : 0.5841136574745178, train ANN loss : 1.7313505411148071\n",
      "AE loss : 0.3044833242893219, ANN loss : 1.8167606592178345, Total loss : 32.26509094238281\n",
      "learning rate A :  tf.Tensor(7.840373e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 123 is 2.9187 sec\n",
      "train AE loss : 0.42359310388565063, train ANN loss : 1.7377867698669434\n",
      "AE loss : 0.2242286503314972, ANN loss : 1.7625384330749512, Total loss : 24.18540382385254\n",
      "learning rate A :  tf.Tensor(7.824859e-05, shape=(), dtype=float32)\n",
      "learning rate B :  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "Time for epoch 124 is 2.9377 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ab3df7decd8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-d7e928b5a27f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mall_ANN\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mAE_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mANN_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mall_AE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mall_ANN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mANN_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-11cd0544643b>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(t_x, t_o, t_y)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAE_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mANN_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_encoder_decoder_ann\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mAE_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mANN_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_o\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m#gradients = tape.gradient(total_loss, shared_vars+decode_vars)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#optimizer_A.apply_gradients(zip(gradients, shared_vars+decode_vars))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-72ba20920097>\u001b[0m in \u001b[0;36mmodel_loss\u001b[0;34m(output, t_o, t_y)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mAE_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_o\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_AE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mann_loss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAE_loss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mann_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gps_wifi/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing)\u001b[0m\n\u001b[1;32m    969\u001b[0m   y_true = smart_cond.smart_cond(label_smoothing,\n\u001b[1;32m    970\u001b[0m                                  _smooth_labels, lambda: y_true)\n\u001b[0;32m--> 971\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gps_wifi/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   4462\u001b[0m         output.op.type != 'Softmax'):\n\u001b[1;32m   4463\u001b[0m       \u001b[0;31m# scale preds so that the class probas of each sample sum to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4464\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4465\u001b[0m       \u001b[0;31m# Compute cross entropy from probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4466\u001b[0m       \u001b[0mepsilon_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gps_wifi/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gps_wifi/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   1573\u001b[0m       gen_math_ops._sum(\n\u001b[1;32m   1574\u001b[0m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m           name=name))\n\u001b[0m\u001b[1;32m   1576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gps_wifi/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  11146\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sum\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11147\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keep_dims\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11148\u001b[0;31m         keep_dims)\n\u001b[0m\u001b[1;32m  11149\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11150\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gps_wifi",
   "language": "python",
   "name": "gps_wifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
